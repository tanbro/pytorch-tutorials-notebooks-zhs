



<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
      
      
        <meta name="lang:clipboard.copy" content="复制">
      
        <meta name="lang:clipboard.copied" content="已复制">
      
        <meta name="lang:search.language" content="ja">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="没有找到符合条件的结果">
      
        <meta name="lang:search.result.one" content="找到 1 个符合条件的结果">
      
        <meta name="lang:search.result.other" content="# 个符合条件的结果">
      
        <meta name="lang:search.tokenizer" content="[\uff0c\u3002]+">
      
      <link rel="shortcut icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.1.2">
    
    
      
        <title>聊天机器人教程 - PyTorch tutorials 中文翻译笔记</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/application.3020aac5.css">
      
        <link rel="stylesheet" href="../../assets/stylesheets/application-palette.224b79ff.css">
      
      
        
        
        <meta name="theme-color" content="">
      
    
    
      <script src="../../assets/javascripts/modernizr.01ccdecf.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../../assets/fonts/material-icons.css">
    
      <link rel="manifest" href="../../manifest.webmanifest">
    
    
    
      
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="white" data-md-color-accent="red">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448"
    viewBox="0 0 416 448" id="__github">
  <path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19-18.125
        8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5
        18.125 8.5 10.75 19 3.125 20.5zM320 304q0 10-3.125 20.5t-10.75
        19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19
        18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM360
        304q0-30-17.25-51t-46.75-21q-10.25 0-48.75 5.25-17.75 2.75-39.25
        2.75t-39.25-2.75q-38-5.25-48.75-5.25-29.5 0-46.75 21t-17.25 51q0 22 8
        38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0
        37.25-1.75t35-7.375 30.5-15 20.25-25.75 8-38.375zM416 260q0 51.75-15.25
        82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5-41.75
        1.125q-19.5 0-35.5-0.75t-36.875-3.125-38.125-7.5-34.25-12.875-30.25-20.25-21.5-28.75q-15.5-30.75-15.5-82.75
        0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25
        30.875q36.75-8.75 77.25-8.75 37 0 70 8 26.25-20.5
        46.75-30.25t47.25-9.75q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34
        99.5z" />
</svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#聊天机器人教程" tabindex="1" class="md-skip">
        跳转至
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="../.." title="PyTorch tutorials 中文翻译笔记" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              PyTorch tutorials 中文翻译笔记
            </span>
            <span class="md-header-nav__topic">
              聊天机器人教程
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            键入以开始搜索
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  

<a href="https://github.com/tanbro/pytorch-tutorials-notebooks-zhs/" title="前往 Github 仓库" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    tanbro/pytorch-tutorials-notebooks-zhs
  </div>
</a>
          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="../.." title="PyTorch tutorials 中文翻译笔记" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    PyTorch tutorials 中文翻译笔记
  </label>
  
    <div class="md-nav__source">
      


  

<a href="https://github.com/tanbro/pytorch-tutorials-notebooks-zhs/" title="前往 Github 仓库" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    tanbro/pytorch-tutorials-notebooks-zhs
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href="../.." title="README" class="md-nav__link">
      README
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2">
    
    <label class="md-nav__link" for="nav-2">
      入门
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        入门
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2-1" type="checkbox" id="nav-2-1">
    
    <label class="md-nav__link" for="nav-2-1">
      60分钟闪电战
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-2-1">
        60分钟闪电战
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../blitz/tensor_tutorial/" title="什么是PyTorch？" class="md-nav__link">
      什么是PyTorch？
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../blitz/autograd_tutorial/" title="Autograd：自动微分" class="md-nav__link">
      Autograd：自动微分
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../blitz/neural_networks_tutorial/" title="神经网络" class="md-nav__link">
      神经网络
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../blitz/cifar10_tutorial/" title="训练分类器" class="md-nav__link">
      训练分类器
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../blitz/data_parallel_tutorial/" title="数据并行" class="md-nav__link">
      数据并行
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../data_loading_tutorial/" title="数据加载和处理教程" class="md-nav__link">
      数据加载和处理教程
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../transfer_learning_tutorial/" title="迁移学习教程" class="md-nav__link">
      迁移学习教程
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3" checked>
    
    <label class="md-nav__link" for="nav-3">
      文本
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        文本
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        聊天机器人教程
      </label>
    
    <a href="./" title="聊天机器人教程" class="md-nav__link md-nav__link--active">
      聊天机器人教程
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="__toc">目录</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#聊天机器人教程" title="聊天机器人教程" class="md-nav__link">
    聊天机器人教程
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#准备工作" title="准备工作" class="md-nav__link">
    准备工作
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#数据加载与预处理" title="数据加载与预处理" class="md-nav__link">
    数据加载与预处理
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#创建格式化数据文件" title="创建格式化数据文件" class="md-nav__link">
    创建格式化数据文件
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#数据加载和修剪" title="数据加载和修剪" class="md-nav__link">
    数据加载和修剪
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#为模型准备数据" title="为模型准备数据" class="md-nav__link">
    为模型准备数据
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#定义模型" title="定义模型" class="md-nav__link">
    定义模型
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#seq2seq-模型" title="Seq2Seq 模型" class="md-nav__link">
    Seq2Seq 模型
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#编码器" title="编码器" class="md-nav__link">
    编码器
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#解码器" title="解码器" class="md-nav__link">
    解码器
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#定义训练过程" title="定义训练过程" class="md-nav__link">
    定义训练过程
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#损失掩码" title="损失掩码" class="md-nav__link">
    损失掩码
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#单次训练的迭代" title="单次训练的迭代" class="md-nav__link">
    单次训练的迭代
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#多次训练的迭代器" title="多次训练的迭代器" class="md-nav__link">
    多次训练的迭代器
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#定义评估过程" title="定义评估过程" class="md-nav__link">
    定义评估过程
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#greedy-decoding" title="Greedy decoding" class="md-nav__link">
    Greedy decoding
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#评估用户输入文本" title="评估用户输入文本" class="md-nav__link">
    评估用户输入文本
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#运行模型" title="运行模型" class="md-nav__link">
    运行模型
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#运行训练" title="运行训练" class="md-nav__link">
    运行训练
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#运行评估" title="运行评估" class="md-nav__link">
    运行评估
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#结论" title="结论" class="md-nav__link">
    结论
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../intermediate/char_rnn_classification_tutorial/" title="使用字符级RNN对名字进行分类" class="md-nav__link">
      使用字符级RNN对名字进行分类
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../intermediate/char_rnn_generation_tutorial/" title="使用字符级RNN生成名字" class="md-nav__link">
      使用字符级RNN生成名字
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="__toc">目录</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#聊天机器人教程" title="聊天机器人教程" class="md-nav__link">
    聊天机器人教程
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#准备工作" title="准备工作" class="md-nav__link">
    准备工作
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#数据加载与预处理" title="数据加载与预处理" class="md-nav__link">
    数据加载与预处理
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#创建格式化数据文件" title="创建格式化数据文件" class="md-nav__link">
    创建格式化数据文件
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#数据加载和修剪" title="数据加载和修剪" class="md-nav__link">
    数据加载和修剪
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#为模型准备数据" title="为模型准备数据" class="md-nav__link">
    为模型准备数据
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#定义模型" title="定义模型" class="md-nav__link">
    定义模型
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#seq2seq-模型" title="Seq2Seq 模型" class="md-nav__link">
    Seq2Seq 模型
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#编码器" title="编码器" class="md-nav__link">
    编码器
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#解码器" title="解码器" class="md-nav__link">
    解码器
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#定义训练过程" title="定义训练过程" class="md-nav__link">
    定义训练过程
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#损失掩码" title="损失掩码" class="md-nav__link">
    损失掩码
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#单次训练的迭代" title="单次训练的迭代" class="md-nav__link">
    单次训练的迭代
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#多次训练的迭代器" title="多次训练的迭代器" class="md-nav__link">
    多次训练的迭代器
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#定义评估过程" title="定义评估过程" class="md-nav__link">
    定义评估过程
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#greedy-decoding" title="Greedy decoding" class="md-nav__link">
    Greedy decoding
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#评估用户输入文本" title="评估用户输入文本" class="md-nav__link">
    评估用户输入文本
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#运行模型" title="运行模型" class="md-nav__link">
    运行模型
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#运行训练" title="运行训练" class="md-nav__link">
    运行训练
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#运行评估" title="运行评估" class="md-nav__link">
    运行评估
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#结论" title="结论" class="md-nav__link">
    结论
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/tanbro/pytorch-tutorials-notebooks-zhs/edit/master/docs/beginner/chatbot_tutorial.md" title="编辑此页" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                  <h1>聊天机器人教程</h1>
                
                <h2 id="聊天机器人教程">聊天机器人教程<a class="headerlink" href="#聊天机器人教程" title="Permanent link">&para;</a></h2>
<p><strong>作者：</strong> <a href="https://github.com/MatthewInkawhich">Matthew Inkawhich</a></p>
<p>在本教程中，我们将探索一个有趣的序列到序列(Seq2Seq)模型的用例。我们将使用 <a href="https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html">Cornell Movie-Dialogs Corpus</a> 的电影脚本训练一个简单的聊天机器人。</p>
<p>对话模型是人工智能研究的热门话题。
聊天机器人在包括客户服务应用和在线帮助台在内的各种场景都用应用。
这些机器人通常使由基于索引的模型驱动的，它们响应特定的问题，输出预定义的响应。在像公司IT帮助台这样高度受限的领域中，这些模型可能是住够了，不过在通常的状况下，他们是不够健壮的。
教一台机器与人类在多个领域内进行有意义的对话是一个远未解决的研究性问题。
在最近的深度学习模型热潮中，像 <a href="https://arxiv.org/abs/1506.05869">Google Neural Conversational Model</a> 这样的强大的生成模型的出现，标志着多领域生成对话模型的一大进步。
在这个教程中，我们将用<code>PyTorch</code>实现这种模型。</p>
<p><img alt="bot" src="https://pytorch.org/tutorials/_images/bot.png" /></p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="gp">&gt;</span> hello?
<span class="go">Bot: hello .</span>
<span class="gp">&gt;</span> where am I?
<span class="go">Bot: you re in a hospital .</span>
<span class="gp">&gt;</span> who are you?
<span class="go">Bot: i m a lawyer .</span>
<span class="gp">&gt;</span> how are you doing?
<span class="go">Bot: i m fine .</span>
<span class="gp">&gt;</span> are you my friend?
<span class="go">Bot: no .</span>
<span class="gp">&gt;</span> you<span class="err">&#39;</span>re under arrest
<span class="go">Bot: i m trying to help you !</span>
<span class="gp">&gt;</span> i<span class="err">&#39;</span>m just kidding
<span class="go">Bot: i m sorry .</span>
<span class="gp">&gt;</span> where are you from?
<span class="go">Bot: san francisco .</span>
<span class="gp">&gt;</span> it<span class="err">&#39;</span>s <span class="nb">time</span> <span class="k">for</span> me to leave
<span class="go">Bot: i know .</span>
<span class="gp">&gt;</span> goodbye
<span class="go">Bot: goodbye .</span>
</pre></div>
</td></tr></table>

<p><strong>教程要点：</strong></p>
<ul>
<li>加载和预处理 <a href="https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html">Cornell Movie-Dialogs Corpus</a> 数据集</li>
<li>使用 <a href="https://arxiv.org/abs/1508.04025">Luong attention mechanism(s)</a> 实现一个序列到序列模型</li>
<li>使用小批次联合训练编码器和解码器模型</li>
<li>实现贪婪搜索解码模块</li>
<li>与训练后的聊天机器人互动</li>
</ul>
<p><strong>致谢：</strong></p>
<p>本教程借用以下来源的代码：</p>
<ol>
<li>Yuan-Kuei Wu 的 pytorch-chatbot 实现: <a href="https://github.com/ywk991112/pytorch-chatbot">https://github.com/ywk991112/pytorch-chatbot</a></li>
<li>Sean Robertson 的 practical-pytorch seq2seq-translation 例子: <a href="https://github.com/spro/practical-pytorch/tree/master/seq2seq-translation">https://github.com/spro/practical-pytorch/tree/master/seq2seq-translation</a></li>
<li>FloydHub 的 Cornell Movie Corpus 预处理代码: <a href="https://github.com/floydhub/textutil-preprocess-cornell-movie-corpus">https://github.com/floydhub/textutil-preprocess-cornell-movie-corpus</a></li>
</ol>
<h3 id="准备工作">准备工作<a class="headerlink" href="#准备工作" title="Permanent link">&para;</a></h3>
<p>首先，从<a href="https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html">https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html</a>下载ZIP文件解压缩到当前目录的<code>data</code>子目录中。</p>
<p>然后，我们进行一些必要的引用。</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span><span class="p">,</span> <span class="n">division</span><span class="p">,</span> <span class="n">print_function</span><span class="p">,</span> <span class="n">unicode_literals</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.jit</span> <span class="kn">import</span> <span class="n">script</span><span class="p">,</span> <span class="n">trace</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="kn">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">optim</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="kn">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">csv</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">unicodedata</span>
<span class="kn">import</span> <span class="nn">codecs</span>
<span class="kn">from</span> <span class="nn">io</span> <span class="kn">import</span> <span class="nb">open</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">math</span>


<span class="n">USE_CUDA</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">USE_CUDA</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>cuda
</pre></div>
</td></tr></table>

<h3 id="数据加载与预处理">数据加载与预处理<a class="headerlink" href="#数据加载与预处理" title="Permanent link">&para;</a></h3>
<p>下一步，将我们之前下载的数据重新格式化，并把数据加载到我们所要使用的结构中。</p>
<p><a href="https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html">Cornell Movie-Dialogs Corpus</a>
是一个内容丰富的电影角色对话数据集：</p>
<ul>
<li>来自10,292对电影角色的220,579个对话</li>
<li>来自617部电影的9,035个角色</li>
<li>总共304,713句话</li>
</ul>
<p>这个数据集庞大且多样，其语言形式、时间段、情感都有很大变化。我们希望这种多样性使我们的模型能够适应多种形式的输入和询问。</p>
<p>首先，我们看几行数据文件，了解其原始格式。</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">corpus_name</span> <span class="o">=</span> <span class="s2">&quot;cornell movie-dialogs corpus&quot;</span>
<span class="n">corpus</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="n">corpus_name</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">printLines</span><span class="p">(</span><span class="nb">file</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="nb">file</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;iso-8859-1&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">datafile</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">datafile</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>

<span class="n">printLines</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="s2">&quot;movie_lines.txt&quot;</span><span class="p">))</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!

L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!

L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.

L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?

L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let&#39;s go.

L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow

L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you&#39;re gonna need to learn how to lie.

L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No

L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I&#39;m kidding.  You know how sometimes you just become this &quot;persona&quot;?  And you don&#39;t know how to quit?

L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?
</pre></div>
</td></tr></table>

<h4 id="创建格式化数据文件">创建格式化数据文件<a class="headerlink" href="#创建格式化数据文件" title="Permanent link">&para;</a></h4>
<p>为方便起见，我们将创建一个格式良好的数据文件，其中每一行包含一个以制表符分隔的<em>询问语句</em>和一个<em>响应语句</em>对。</p>
<p>以下函数用于分析 <em>movie_lines.txt</em> 数据文件的原始行数据</p>
<ul>
<li><code>loadLines</code> 将每一行分割成具有多个字段(lineID, characterID, movieID, character, text)的字典</li>
<li><code>loadConversations</code> 将来自 <code>loadLines</code> 的，包含多个字段的行，根据 <em>movie_conversations.txt</em> 组成对话。</li>
<li><code>extractSentencePairs</code> 从对话中提取“询问-回答”句子对</li>
</ul>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Splits each line of the file into a dictionary of fields</span>
<span class="k">def</span> <span class="nf">loadLines</span><span class="p">(</span><span class="n">fileName</span><span class="p">,</span> <span class="n">fields</span><span class="p">):</span>
    <span class="n">lines</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">fileName</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;iso-8859-1&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">values</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; +++$+++ &quot;</span><span class="p">)</span>
            <span class="c1"># Extract fields</span>
            <span class="n">lineObj</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">field</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">fields</span><span class="p">):</span>
                <span class="n">lineObj</span><span class="p">[</span><span class="n">field</span><span class="p">]</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">lines</span><span class="p">[</span><span class="n">lineObj</span><span class="p">[</span><span class="s1">&#39;lineID&#39;</span><span class="p">]]</span> <span class="o">=</span> <span class="n">lineObj</span>
    <span class="k">return</span> <span class="n">lines</span>


<span class="c1"># Groups fields of lines from `loadLines` into conversations based on *movie_conversations.txt*</span>
<span class="k">def</span> <span class="nf">loadConversations</span><span class="p">(</span><span class="n">fileName</span><span class="p">,</span> <span class="n">lines</span><span class="p">,</span> <span class="n">fields</span><span class="p">):</span>
    <span class="n">conversations</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">fileName</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;iso-8859-1&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">values</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; +++$+++ &quot;</span><span class="p">)</span>
            <span class="c1"># Extract fields</span>
            <span class="n">convObj</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">field</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">fields</span><span class="p">):</span>
                <span class="n">convObj</span><span class="p">[</span><span class="n">field</span><span class="p">]</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="c1"># Convert string to list (convObj[&quot;utteranceIDs&quot;] == &quot;[&#39;L598485&#39;, &#39;L598486&#39;, ...]&quot;)</span>
            <span class="n">lineIds</span> <span class="o">=</span> <span class="nb">eval</span><span class="p">(</span><span class="n">convObj</span><span class="p">[</span><span class="s2">&quot;utteranceIDs&quot;</span><span class="p">])</span>
            <span class="c1"># Reassemble lines</span>
            <span class="n">convObj</span><span class="p">[</span><span class="s2">&quot;lines&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">lineId</span> <span class="ow">in</span> <span class="n">lineIds</span><span class="p">:</span>
                <span class="n">convObj</span><span class="p">[</span><span class="s2">&quot;lines&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lines</span><span class="p">[</span><span class="n">lineId</span><span class="p">])</span>
            <span class="n">conversations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">convObj</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">conversations</span>


<span class="c1"># Extracts pairs of sentences from conversations</span>
<span class="k">def</span> <span class="nf">extractSentencePairs</span><span class="p">(</span><span class="n">conversations</span><span class="p">):</span>
    <span class="n">qa_pairs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">conversation</span> <span class="ow">in</span> <span class="n">conversations</span><span class="p">:</span>
        <span class="c1"># Iterate over all the lines of the conversation</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">conversation</span><span class="p">[</span><span class="s2">&quot;lines&quot;</span><span class="p">])</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>  <span class="c1"># We ignore the last line (no answer for it)</span>
            <span class="n">inputLine</span> <span class="o">=</span> <span class="n">conversation</span><span class="p">[</span><span class="s2">&quot;lines&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;text&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
            <span class="n">targetLine</span> <span class="o">=</span> <span class="n">conversation</span><span class="p">[</span><span class="s2">&quot;lines&quot;</span><span class="p">][</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;text&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
            <span class="c1"># Filter wrong samples (if one of the lists is empty)</span>
            <span class="k">if</span> <span class="n">inputLine</span> <span class="ow">and</span> <span class="n">targetLine</span><span class="p">:</span>
                <span class="n">qa_pairs</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">inputLine</span><span class="p">,</span> <span class="n">targetLine</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">qa_pairs</span>
</pre></div>
</td></tr></table>

<p>现在，调用这些函数，并创建文件，文件名是 <em>formatted_movie_lines.txt</em> 。</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Define path to new file</span>
<span class="n">datafile</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="s2">&quot;formatted_movie_lines.txt&quot;</span><span class="p">)</span>

<span class="n">delimiter</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span>
<span class="c1"># Unescape the delimiter</span>
<span class="n">delimiter</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">codecs</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">delimiter</span><span class="p">,</span> <span class="s2">&quot;unicode_escape&quot;</span><span class="p">))</span>

<span class="c1"># Initialize lines dict, conversations list, and field ids</span>
<span class="n">lines</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">conversations</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">MOVIE_LINES_FIELDS</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;lineID&quot;</span><span class="p">,</span> <span class="s2">&quot;characterID&quot;</span><span class="p">,</span> <span class="s2">&quot;movieID&quot;</span><span class="p">,</span> <span class="s2">&quot;character&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">]</span>
<span class="n">MOVIE_CONVERSATIONS_FIELDS</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;character1ID&quot;</span><span class="p">,</span> <span class="s2">&quot;character2ID&quot;</span><span class="p">,</span> <span class="s2">&quot;movieID&quot;</span><span class="p">,</span> <span class="s2">&quot;utteranceIDs&quot;</span><span class="p">]</span>

<span class="c1"># Load lines and process conversations</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Processing corpus...&quot;</span><span class="p">)</span>
<span class="n">lines</span> <span class="o">=</span> <span class="n">loadLines</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="s2">&quot;movie_lines.txt&quot;</span><span class="p">),</span> <span class="n">MOVIE_LINES_FIELDS</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Loading conversations...&quot;</span><span class="p">)</span>
<span class="n">conversations</span> <span class="o">=</span> <span class="n">loadConversations</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="s2">&quot;movie_conversations.txt&quot;</span><span class="p">),</span>
                                  <span class="n">lines</span><span class="p">,</span> <span class="n">MOVIE_CONVERSATIONS_FIELDS</span><span class="p">)</span>

<span class="c1"># Write new csv file</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Writing newly formatted file...&quot;</span><span class="p">)</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">datafile</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">outputfile</span><span class="p">:</span>
    <span class="n">writer</span> <span class="o">=</span> <span class="n">csv</span><span class="o">.</span><span class="n">writer</span><span class="p">(</span><span class="n">outputfile</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="n">delimiter</span><span class="p">,</span> <span class="n">lineterminator</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">extractSentencePairs</span><span class="p">(</span><span class="n">conversations</span><span class="p">):</span>
        <span class="n">writer</span><span class="o">.</span><span class="n">writerow</span><span class="p">(</span><span class="n">pair</span><span class="p">)</span>

<span class="c1"># Print a sample of lines</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Sample lines from file:&quot;</span><span class="p">)</span>
<span class="n">printLines</span><span class="p">(</span><span class="n">datafile</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>Processing corpus...

Loading conversations...

Writing newly formatted file...

Sample lines from file:
Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.   Well, I thought we&#39;d start with pronunciation, if that&#39;s okay with you.

Well, I thought we&#39;d start with pronunciation, if that&#39;s okay with you. Not the hacking and gagging and spitting part.  Please.

Not the hacking and gagging and spitting part.  Please. Okay... then how &#39;bout we try out some French cuisine.  Saturday?  Night?

You&#39;re asking me out.  That&#39;s so cute. What&#39;s your name again?  Forget it.

No, no, it&#39;s my fault -- we didn&#39;t have a proper introduction ---   Cameron.

Cameron.    The thing is, Cameron -- I&#39;m at the mercy of a particularly hideous breed of loser.  My sister.  I can&#39;t date until she does.

The thing is, Cameron -- I&#39;m at the mercy of a particularly hideous breed of loser.  My sister.  I can&#39;t date until she does.   Seems like she could get a date easy enough...

Why?    Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.

Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.   That&#39;s a shame.

Gosh, if only we could find Kat a boyfriend...  Let me see what I can do.
</pre></div>
</td></tr></table>

<h4 id="数据加载和修剪">数据加载和修剪<a class="headerlink" href="#数据加载和修剪" title="Permanent link">&para;</a></h4>
<p>我们的下一个任务是创建词汇表并将询问/响应句子对加载到内存中。</p>
<p>注意我们处理的是<strong>词</strong>序列，它们没有隐式映射到离散数值空间。因此，我们必须创建一个将我们在数据集中遇到的每个唯一词映射到索引值的映射(Mapping)。</p>
<p>为此，我们定义一个<code>Voc</code>类，它保存从词到索引值的映射，以及从索引值到词的反向映射，唯一词的数量，总词数。
这个类提供了像词表添加词的方法(<code>addWord</code>)，添加句子中所有词的方法(<code>addSentence</code>)，以及削减不常见词的方法(<code>trim</code>)。
More on trimming later.</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Default word tokens</span>
<span class="n">PAD_token</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># Used for padding short sentences</span>
<span class="n">SOS_token</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># Start-of-sentence token</span>
<span class="n">EOS_token</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># End-of-sentence token</span>

<span class="k">class</span> <span class="nc">Voc</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trimmed</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word2index</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word2count</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">index2word</span> <span class="o">=</span> <span class="p">{</span><span class="n">PAD_token</span><span class="p">:</span> <span class="s2">&quot;PAD&quot;</span><span class="p">,</span> <span class="n">SOS_token</span><span class="p">:</span> <span class="s2">&quot;SOS&quot;</span><span class="p">,</span> <span class="n">EOS_token</span><span class="p">:</span> <span class="s2">&quot;EOS&quot;</span><span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_words</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c1"># Count SOS, EOS, PAD</span>

    <span class="k">def</span> <span class="nf">addSentence</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentence</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">addWord</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">addWord</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">word2index</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">word2index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_words</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">word2count</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">index2word</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_words</span><span class="p">]</span> <span class="o">=</span> <span class="n">word</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_words</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">word2count</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="c1"># Remove words below a certain count threshold</span>
    <span class="k">def</span> <span class="nf">trim</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">min_count</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trimmed</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trimmed</span> <span class="o">=</span> <span class="bp">True</span>

        <span class="n">keep_words</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">word2count</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">v</span> <span class="o">&gt;=</span> <span class="n">min_count</span><span class="p">:</span>
                <span class="n">keep_words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>

        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;keep_words {} / {} = {:.4f}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">keep_words</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">word2index</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">keep_words</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">word2index</span><span class="p">)</span>
        <span class="p">))</span>

        <span class="c1"># Reinitialize dictionaries</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word2index</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word2count</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">index2word</span> <span class="o">=</span> <span class="p">{</span><span class="n">PAD_token</span><span class="p">:</span> <span class="s2">&quot;PAD&quot;</span><span class="p">,</span> <span class="n">SOS_token</span><span class="p">:</span> <span class="s2">&quot;SOS&quot;</span><span class="p">,</span> <span class="n">EOS_token</span><span class="p">:</span> <span class="s2">&quot;EOS&quot;</span><span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_words</span> <span class="o">=</span> <span class="mi">3</span> <span class="c1"># Count default tokens</span>

        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">keep_words</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">addWord</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<p>现在，可以开始装配词表和询问/回复句子对。在准备好使用这些数据之前，我们还得执行一些预处理。</p>
<ul>
<li>首先，我们必须使用 <code>unicodeToAscii</code> 将字符串从 Unicode 转为 ASCII 。</li>
<li>其次，我们应该把所有字母转为小写，并且裁剪掉所有的除基本标点符号之外的所有非字母字符(<code>normalizeString</code>)。</li>
<li>最后，为了训练更快的收敛，我们将过滤掉(<code>filterPairs</code>)长度大于 <code>MAX_LENGTH</code> 阈值的句子。</li>
</ul>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">MAX_LENGTH</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># Maximum sentence length to consider</span>

<span class="c1"># Turn a Unicode string to plain ASCII, thanks to</span>
<span class="c1"># https://stackoverflow.com/a/518232/2809427</span>
<span class="k">def</span> <span class="nf">unicodeToAscii</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="k">return</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
        <span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">unicodedata</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="s1">&#39;NFD&#39;</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">unicodedata</span><span class="o">.</span><span class="n">category</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="o">!=</span> <span class="s1">&#39;Mn&#39;</span>
    <span class="p">)</span>

<span class="c1"># Lowercase, trim, and remove non-letter characters</span>
<span class="k">def</span> <span class="nf">normalizeString</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">unicodeToAscii</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;([.!?])&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot; \1&quot;</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;[^a-zA-Z.!?]+&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;\s+&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">s</span>

<span class="c1"># Read query/response pairs and return a voc object</span>
<span class="k">def</span> <span class="nf">readVocs</span><span class="p">(</span><span class="n">datafile</span><span class="p">,</span> <span class="n">corpus_name</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Reading lines...&quot;</span><span class="p">)</span>
    <span class="c1"># Read the file and split into lines</span>
    <span class="n">lines</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">datafile</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span><span class="o">.</span>\
        <span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="c1"># Split every line into pairs and normalize</span>
    <span class="n">pairs</span> <span class="o">=</span> <span class="p">[[</span><span class="n">normalizeString</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">l</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">)]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">]</span>
    <span class="n">voc</span> <span class="o">=</span> <span class="n">Voc</span><span class="p">(</span><span class="n">corpus_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">voc</span><span class="p">,</span> <span class="n">pairs</span>

<span class="c1"># Returns True if both sentences in a pair &#39;p&#39; are under the MAX_LENGTH threshold</span>
<span class="k">def</span> <span class="nf">filterPair</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
    <span class="c1"># Input sequences need to preserve the last word for EOS token</span>
    <span class="k">return</span> <span class="nb">all</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">))</span> <span class="o">&lt;</span> <span class="n">MAX_LENGTH</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">p</span><span class="p">)</span>

<span class="c1"># Filter pairs using filterPair condition</span>
<span class="k">def</span> <span class="nf">filterPairs</span><span class="p">(</span><span class="n">pairs</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">pair</span> <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span> <span class="k">if</span> <span class="n">filterPair</span><span class="p">(</span><span class="n">pair</span><span class="p">)]</span>

<span class="c1"># Using the functions defined above, return a populated voc object and pairs list</span>
<span class="k">def</span> <span class="nf">loadPrepareData</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">corpus_name</span><span class="p">,</span> <span class="n">datafile</span><span class="p">,</span> <span class="n">save_dir</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Start preparing training data ...&quot;</span><span class="p">)</span>
    <span class="n">voc</span><span class="p">,</span> <span class="n">pairs</span> <span class="o">=</span> <span class="n">readVocs</span><span class="p">(</span><span class="n">datafile</span><span class="p">,</span> <span class="n">corpus_name</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Read {!s} sentence pairs&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pairs</span><span class="p">)))</span>
    <span class="n">pairs</span> <span class="o">=</span> <span class="n">filterPairs</span><span class="p">(</span><span class="n">pairs</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Trimmed to {!s} sentence pairs&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pairs</span><span class="p">)))</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Counting words...&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">:</span>
        <span class="n">voc</span><span class="o">.</span><span class="n">addSentence</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">voc</span><span class="o">.</span><span class="n">addSentence</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Counted words:&quot;</span><span class="p">,</span> <span class="n">voc</span><span class="o">.</span><span class="n">num_words</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">voc</span><span class="p">,</span> <span class="n">pairs</span>


<span class="c1"># Load/Assemble voc and pairs</span>
<span class="n">save_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="s2">&quot;save&quot;</span><span class="p">)</span>
<span class="n">voc</span><span class="p">,</span> <span class="n">pairs</span> <span class="o">=</span> <span class="n">loadPrepareData</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">corpus_name</span><span class="p">,</span> <span class="n">datafile</span><span class="p">,</span> <span class="n">save_dir</span><span class="p">)</span>
<span class="c1"># Print some pairs to validate</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">pairs:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">[:</span><span class="mi">10</span><span class="p">]:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">pair</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>Start preparing training data ...
Reading lines...
Read 221282 sentence pairs
Trimmed to 64217 sentence pairs
Counting words...
Counted words: 17996

pairs:
[&#39;there .&#39;, &#39;where ?&#39;]
[&#39;you have my word . as a gentleman&#39;, &#39;you re sweet .&#39;]
[&#39;hi .&#39;, &#39;looks like things worked out tonight huh ?&#39;]
[&#39;you know chastity ?&#39;, &#39;i believe we share an art instructor&#39;]
[&#39;have fun tonight ?&#39;, &#39;tons&#39;]
[&#39;well no . . .&#39;, &#39;then that s all you had to say .&#39;]
[&#39;then that s all you had to say .&#39;, &#39;but&#39;]
[&#39;but&#39;, &#39;you always been this selfish ?&#39;]
[&#39;do you listen to this crap ?&#39;, &#39;what crap ?&#39;]
[&#39;what good stuff ?&#39;, &#39;the real you .&#39;]
</pre></div>
</td></tr></table>

<p>另一种有利于在训练期间实现更快收敛的策略是修剪掉我们词汇表中很少使用的单词。减小特征空间也会缓和模型逼近的难度。我们将通过两个步骤完成此操作：</p>
<ol>
<li>
<p>用<code>voc.trim</code>修剪掉数量小于<code>MIN_COUNT</code>阈值的词。</p>
</li>
<li>
<p>过滤掉含有被修剪词的句子对。</p>
</li>
</ol>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="n">MIN_COUNT</span> <span class="o">=</span> <span class="mi">3</span>    <span class="c1"># Minimum word count threshold for trimming</span>

<span class="k">def</span> <span class="nf">trimRareWords</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="n">pairs</span><span class="p">,</span> <span class="n">MIN_COUNT</span><span class="p">):</span>
    <span class="c1"># Trim words used under the MIN_COUNT from the voc</span>
    <span class="n">voc</span><span class="o">.</span><span class="n">trim</span><span class="p">(</span><span class="n">MIN_COUNT</span><span class="p">)</span>
    <span class="c1"># Filter out pairs with trimmed words</span>
    <span class="n">keep_pairs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">:</span>
        <span class="n">input_sentence</span> <span class="o">=</span> <span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">output_sentence</span> <span class="o">=</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">keep_input</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="n">keep_output</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="c1"># Check input sentence</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">input_sentence</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">voc</span><span class="o">.</span><span class="n">word2index</span><span class="p">:</span>
                <span class="n">keep_input</span> <span class="o">=</span> <span class="bp">False</span>
                <span class="k">break</span>
        <span class="c1"># Check output sentence</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">output_sentence</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">voc</span><span class="o">.</span><span class="n">word2index</span><span class="p">:</span>
                <span class="n">keep_output</span> <span class="o">=</span> <span class="bp">False</span>
                <span class="k">break</span>

        <span class="c1"># Only keep pairs that do not contain trimmed word(s) in their input or output sentence</span>
        <span class="k">if</span> <span class="n">keep_input</span> <span class="ow">and</span> <span class="n">keep_output</span><span class="p">:</span>
            <span class="n">keep_pairs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pair</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Trimmed from {} pairs to {}, {:.4f} of total&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pairs</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">keep_pairs</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">keep_pairs</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">pairs</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">keep_pairs</span>


<span class="c1"># Trim voc and pairs</span>
<span class="n">pairs</span> <span class="o">=</span> <span class="n">trimRareWords</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="n">pairs</span><span class="p">,</span> <span class="n">MIN_COUNT</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>keep_words 7820 / 17993 = 0.4346
Trimmed from 64217 pairs to 53120, 0.8272 of total
</pre></div>
</td></tr></table>

<h3 id="为模型准备数据">为模型准备数据<a class="headerlink" href="#为模型准备数据" title="Permanent link">&para;</a></h3>
<p>尽管我们已经投入很大精力来准备和整理数据，把它们放到了漂亮的词表对象和句子对列表，但我们的模型最终需要的的输入却是Tensor的张量数字。
可以在<a href="../intermediate/seq2seq_translation_tutorial">Seq2Seq翻译教程</a>中找到为模型准备预处理数据的方法。
在那个教程中，我们使用的批次值为1，这意味着我们所要做的就是将句子对中的单词转换为词汇表中对应的索引值，并将其提供给模型。</p>
<p>另外，如果你对加速培训和/或想要利用GPU并行化功能感兴趣，则需要使用小批量培训。</p>
<p>使用小批量也意味着我们必须注意批量中句子长度的变化。
为了容纳同一批次中不同大小的句子，我们要让批量输入张量的形状 <em>(max_length，batch_size)</em> 中的短于 <em>max_length</em> 的句子在 <em>EOS_token</em> 之后用零填充。</p>
<p>如果我们只是简单地通过将单词转换为其索引值(<code>indexesFromSentence</code>)和零填充的方法将英语句子转换为张量，张量的形状将是 <em>(batch_size，max_length)</em> ，并且在第一维上的索引将在所有时间步骤中返回完整序列。
但是，我们需要能够沿着时间、跨批次、在所有序列上进行索引。
因此，我们将输入批处理的形状转置为 <em>(max_length，batch_size)</em> ，以便跨第一维的索引返回批中所有句子的时间步长。
我们在<code>zeroPadding</code>函数中隐式处理这个转置。</p>
<p><img alt="batches" src="https://pytorch.org/tutorials/_images/seq2seq_batches.png" /></p>
<p>函数<code>inputVar</code>处理句子到张量的转换过程，最终创建一个形状正确的零填充张量。
它还返回批次中每个序列的长度(<code>lengths</code>)的张量，它稍后会被传给编码器。</p>
<p>函数<code>outputVar</code>的执行过程与<code>inputVar</code>类似，但是不返回长度（<code>lenghts</code>）张量，而是返回二进制的掩码张量和目标句子的最大长度。
二进制掩码与输出目标张量具有同样的形状，但是其中的每个 <em>PAD_token</em> 元素都为0，其它所有元素都为1。</p>
<p><code>batch2TrainData</code>简单的使用一堆句子对并使用上述函数返回输入和目标张量。</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">indexesFromSentence</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="n">sentence</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">voc</span><span class="o">.</span><span class="n">word2index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="n">EOS_token</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">zeroPadding</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">fillvalue</span><span class="o">=</span><span class="n">PAD_token</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">itertools</span><span class="o">.</span><span class="n">zip_longest</span><span class="p">(</span><span class="o">*</span><span class="n">l</span><span class="p">,</span> <span class="n">fillvalue</span><span class="o">=</span><span class="n">fillvalue</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">binaryMatrix</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">PAD_token</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">seq</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">l</span><span class="p">):</span>
        <span class="n">m</span><span class="o">.</span><span class="n">append</span><span class="p">([])</span>
        <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">seq</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">token</span> <span class="o">==</span> <span class="n">PAD_token</span><span class="p">:</span>
                <span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">m</span>

<span class="c1"># Returns padded input sequence tensor and lengths</span>
<span class="k">def</span> <span class="nf">inputVar</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">voc</span><span class="p">):</span>
    <span class="n">indexes_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">indexesFromSentence</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">l</span><span class="p">]</span>
    <span class="n">lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">indexes</span><span class="p">)</span> <span class="k">for</span> <span class="n">indexes</span> <span class="ow">in</span> <span class="n">indexes_batch</span><span class="p">])</span>
    <span class="n">padList</span> <span class="o">=</span> <span class="n">zeroPadding</span><span class="p">(</span><span class="n">indexes_batch</span><span class="p">)</span>
    <span class="n">padVar</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">padList</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">padVar</span><span class="p">,</span> <span class="n">lengths</span>

<span class="c1"># Returns padded target sequence tensor, padding mask, and max target length</span>
<span class="k">def</span> <span class="nf">outputVar</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">voc</span><span class="p">):</span>
    <span class="n">indexes_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">indexesFromSentence</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">l</span><span class="p">]</span>
    <span class="n">max_target_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">indexes</span><span class="p">)</span> <span class="k">for</span> <span class="n">indexes</span> <span class="ow">in</span> <span class="n">indexes_batch</span><span class="p">])</span>
    <span class="n">padList</span> <span class="o">=</span> <span class="n">zeroPadding</span><span class="p">(</span><span class="n">indexes_batch</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">binaryMatrix</span><span class="p">(</span><span class="n">padList</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ByteTensor</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>
    <span class="n">padVar</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">padList</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">padVar</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">max_target_len</span>

<span class="c1"># Returns all items for a given batch of pairs</span>
<span class="k">def</span> <span class="nf">batch2TrainData</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="n">pair_batch</span><span class="p">):</span>
    <span class="n">pair_batch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">)),</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">input_batch</span><span class="p">,</span> <span class="n">output_batch</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pair_batch</span><span class="p">:</span>
        <span class="n">input_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">output_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">inp</span><span class="p">,</span> <span class="n">lengths</span> <span class="o">=</span> <span class="n">inputVar</span><span class="p">(</span><span class="n">input_batch</span><span class="p">,</span> <span class="n">voc</span><span class="p">)</span>
    <span class="n">output</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">max_target_len</span> <span class="o">=</span> <span class="n">outputVar</span><span class="p">(</span><span class="n">output_batch</span><span class="p">,</span> <span class="n">voc</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">inp</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">max_target_len</span>


<span class="c1"># Example for validation</span>
<span class="n">small_batch_size</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">batch2TrainData</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="p">[</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">pairs</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">small_batch_size</span><span class="p">)])</span>
<span class="n">input_variable</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">target_variable</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">max_target_len</span> <span class="o">=</span> <span class="n">batches</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;input_variable:&quot;</span><span class="p">,</span> <span class="n">input_variable</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;lengths:&quot;</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;target_variable:&quot;</span><span class="p">,</span> <span class="n">target_variable</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;mask:&quot;</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;max_target_len:&quot;</span><span class="p">,</span> <span class="n">max_target_len</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>input_variable: tensor([[122, 443,  70,  16, 111],
        [  7,  45, 519, 997,  66],
        [ 26, 571,   3,   4,   2],
        [ 36,   4,   2,   2,   0],
        [  4,   2,   0,   0,   0],
        [  2,   0,   0,   0,   0]])
lengths: tensor([6, 5, 4, 4, 3])
target_variable: tensor([[ 192,  785, 7319,   88,   95],
        [   4,   12,    4, 2556,    4],
        [   2,  786,    2,    4,    4],
        [   0,    2,    0,    2,    4],
        [   0,    0,    0,    0,    2]])
mask: tensor([[1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1],
        [0, 1, 0, 1, 1],
        [0, 0, 0, 0, 1]], dtype=torch.uint8)
max_target_len: 5
</pre></div>
</td></tr></table>

<h3 id="定义模型">定义模型<a class="headerlink" href="#定义模型" title="Permanent link">&para;</a></h3>
<h4 id="seq2seq-模型">Seq2Seq 模型<a class="headerlink" href="#seq2seq-模型" title="Permanent link">&para;</a></h4>
<p>我们这个聊天机器人的大脑是序列到序列（seq2seq）模型。seq2seq 模型的目标是将可变长度序列作为输入，并使用固定大小的模型将可变长度序列作为输出返回。</p>
<p><a href="https://arxiv.org/abs/1409.3215">Sutskever 等人</a> 发现通过使用两个独立的递归神经网络(RNN)，我们可以完成这项任务。</p>
<p>第一个 RNN 充当<strong>编码器</strong>，它将可变长度输入序列编码成固定长度的上下文向量。
理论上，这个上下文向量（RNN的最终隐藏层）将包含关于输入给机器人的询问句的语义信息。</p>
<p>第二个 RNN 是<strong>解码器</strong>，它接收输入词和上下文向量，并返回序列中后续词的猜测值，以及用于下次迭代的隐藏层。</p>
<p><img alt="model" src="https://jeddy92.github.io/images/ts_intro/seq2seq_ts.png" /></p>
<p>图片来源：<a href="https://jeddy92.github.io/JEddy92.github.io/ts_seq2seq_intro/">https://jeddy92.github.io/JEddy92.github.io/ts_seq2seq_intro/</a></p>
<h5 id="编码器">编码器<a class="headerlink" href="#编码器" title="Permanent link">&para;</a></h5>
<p>编码器RNN一次迭代输入句子的一个标记(token)，每个时间步骤输出一个“输出”向量和一个“隐藏状态”向量。
然后将隐藏状态向量传递给下一个时间步骤，同时记录输出向量。
编码器将其在序列中的每个点看到的上下文转换为高维空间中的一组点，解码器将使用这组点来为给定任务生成有意义的输出。</p>
<p>我们这个编码器的的核心是多层门控单元(GRU)，由<a href="https://arxiv.org/pdf/1406.1078v3.pdf">Cho 等人</a>于2014年发明。
我们将使用GRU的一种变种——双向GRU，它使用两种独立的RNN：一个以正常的顺序接收输入序列，另一个以反方向接收输入序列。
在同一时间步骤中对每个网络的输出求和。
使用双向GRU讲给我们带来对过去和未来上下文进行编码的优势。</p>
<p>双向RNN:</p>
<p><img alt="rnn_bidir" src="https://colah.github.io/posts/2015-09-NN-Types-FP/img/RNN-bidirectional.png" /></p>
<p>图片来源：<a href="https://colah.github.io/posts/2015-09-NN-Types-FP/">https://colah.github.io/posts/2015-09-NN-Types-FP/</a></p>
<p>注意，一个<code>embedding</code>层用于在任意大小的特征空间中编码我们的单词索引。
对于我们的模型，这个层会将每个词映射到大小为<em>hidden_size</em>的特征空间。
训练后，这些值应该编码了近义词之间的语义相似度。</p>
<p>最有，如果要填充后的一批序列传入RNN模块，我们必须围绕RNN进行打包和解包，这些方法分别是：
- <code>nn.utils.rnn.pack_padded_sequence</code>
- <code>nn.utils.rnn.pad_packed_sequence</code></p>
<p><strong>计算图：</strong></p>
<ol>
<li>将词的索引值转为嵌入</li>
<li>为RNN模块打包填充后的序列批次</li>
<li>通过GRU前向传递</li>
<li>解包填充</li>
<li>双向GRU输出求和</li>
<li>返回输出和最终隐藏状态</li>
</ol>
<p><strong>输入：</strong></p>
<ul>
<li><code>input_seq</code>: 输入句子批次；形状=<em>(max_length,batch_size)</em></li>
<li><code>input_lengths</code>: 由批次中每个句子的长度的所构成的列表；形状=<em>(batch_size)</em></li>
<li><code>hidden</code>: 隐藏状态；形状=<em>(n_layers x num_directions, batch_size, hidden_size)</em></li>
</ul>
<p><strong>输出：</strong></p>
<ul>
<li><code>outputs</code>: 从GRN最终隐藏层的输出特征；形状=<em>(max_length, batch_size, hidden_size)</em></li>
<li><code>hidden</code>: 从GRU更新的隐藏状态；形状=<em>(n_layers x num_directions, batch_size, hidden_size)</em></li>
</ul>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">EncoderRNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">embedding</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EncoderRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span> <span class="o">=</span> <span class="n">n_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">embedding</span>

        <span class="c1"># Initialize GRU; the input_size and hidden_size params are both set to &#39;hidden_size&#39;</span>
        <span class="c1">#   because our input size is a word embedding with number of features == hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gru</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span>
                          <span class="n">dropout</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span> <span class="k">if</span> <span class="n">n_layers</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">dropout</span><span class="p">),</span> <span class="n">bidirectional</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_seq</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">hidden</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c1"># Convert word indexes to embeddings</span>
        <span class="n">embedded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">input_seq</span><span class="p">)</span>
        <span class="c1"># Pack padded batch of sequences for RNN module</span>
        <span class="n">packed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">pack_padded_sequence</span><span class="p">(</span><span class="n">embedded</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">)</span>
        <span class="c1"># Forward pass through GRU</span>
        <span class="n">outputs</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gru</span><span class="p">(</span><span class="n">packed</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>
        <span class="c1"># Unpack padding</span>
        <span class="n">outputs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">pad_packed_sequence</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
        <span class="c1"># Sum bidirectional GRU outputs</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">]</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[:,</span> <span class="p">:</span> <span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">:]</span>
        <span class="c1"># Return output and final hidden state</span>
        <span class="k">return</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">hidden</span>
</pre></div>
</td></tr></table>

<h5 id="解码器">解码器<a class="headerlink" href="#解码器" title="Permanent link">&para;</a></h5>
<p>解码器RNN以“一个标记接一个标记(token-by-token)”的形式生成回复句子。
它使用编码器的上下文向量和内置隐藏状态生成序列中的后续词。
它持续的生成词，直到输出<em>EOS_token</em>——表示句子的结束。
寻常的Seq2Seq解码器常常遇到的一个问题就是，如果我们依赖于上下文向量来编码整个输入的语义，那么我们很可能丢失信息。</p>
<p>在处理长输入序列的时候尤其如此，这极大的限制了我们这个解码器的能力。
为了解决这个问题，<a href="https://arxiv.org/abs/1409.0473">Bahdanau 等人</a>创建了“注意力机制”，允许解码器只关注输入序列的某些部分，而不是在每一步都使用整个固定的上下文。</p>
<p>在上层，用解码器的当前隐藏状态和编码器的输出计算注意力。
输出注意力权重和输入序列具有相同的形状，这让我们可以将它和编码器输出相乘，得到编码器输出中的要被加以注意力的部分的加权和。</p>
<p><a href="https://github.com/spro">Sean Robertson</a> 的图示很好的描述了这点：</p>
<p><img alt="attn2" src="https://pytorch.org/tutorials/_images/attn2.png" /></p>
<p><a href="https://arxiv.org/abs/1508.04025">Luong 等人</a>创建了“全局注意力”来改进<a href="https://arxiv.org/abs/1409.0473">Bahdanau 等人</a>的基础工作。
“全局注意力”最关键的不同之处在于：它会考虑所有的编码器隐藏状态，而不是<a href="https://arxiv.org/abs/1409.0473">Bahdanau 等人</a>的只考虑当前时间步骤中的编码器隐藏状态的“局部注意力”方式。
另一个不同之处在于，使用“全局注意力”，我们仅仅使用当前时间步骤的编码器的隐藏状态来计算注意力的权重或能量值。
<a href="https://arxiv.org/abs/1409.0473">Bahdanau 等人</a>的注意力计算需要了解上一个时间步骤中编码器的状态。
此外，<a href="https://arxiv.org/abs/1508.04025">Luong 等人</a>提供了用于计算编码器输出和解码器输出之间的注意力的的多种方法，他们被成为“得分函数”（score functions）：</p>
<p><img alt="scores" src="https://pytorch.org/tutorials/_images/scores.png" /></p>
<p>其中，
- <span><span class="MathJax_Preview">h_t</span><script type="math/tex">h_t</script></span> 为当前目标解码器状的态
- <span><span class="MathJax_Preview">\bar{h}_s</span><script type="math/tex">\bar{h}_s</script></span> 为所有编码器的状态</p>
<p>总体而言，全局注意力机制可以通过下图来总结。
注意我们将在被称作<code>Attn</code>的分离的<code>nn.Module</code>中实现“注意力层”。
这个模块的输出是一个 softmax 标准化权重张量，其形状是 <em>(batch_size, 1, max_length)</em> 。</p>
<p><img alt="global_attn" src="https://pytorch.org/tutorials/_images/global_attn.png" /></p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Luong attention layer</span>
<span class="k">class</span> <span class="nc">Attn</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Attn</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">=</span> <span class="n">method</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;dot&#39;</span><span class="p">,</span> <span class="s1">&#39;general&#39;</span><span class="p">,</span> <span class="s1">&#39;concat&#39;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">method</span><span class="p">,</span> <span class="s2">&quot;is not an appropriate attention method.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;general&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;concat&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">dot_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">hidden</span> <span class="o">*</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">general_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">):</span>
        <span class="n">energy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span><span class="n">encoder_output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">hidden</span> <span class="o">*</span> <span class="n">energy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">concat_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">):</span>
        <span class="n">energy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">hidden</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">encoder_output</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">encoder_output</span><span class="p">),</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">tanh</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">*</span> <span class="n">energy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">):</span>
        <span class="c1"># Calculate the attention weights (energies) based on the given method</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;general&#39;</span><span class="p">:</span>
            <span class="n">attn_energies</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">general_score</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;concat&#39;</span><span class="p">:</span>
            <span class="n">attn_energies</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">concat_score</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;dot&#39;</span><span class="p">:</span>
            <span class="n">attn_energies</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dot_score</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">)</span>

        <span class="c1"># Transpose max_length and batch_size dimensions</span>
        <span class="n">attn_energies</span> <span class="o">=</span> <span class="n">attn_energies</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>

        <span class="c1"># Return the softmax normalized probability scores (with added dimension)</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_energies</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<p>现在，我们已经定义了注意力子模块，可以实现实际的解码器模块了。
对于解码器，我们将手动的在每个时间步骤中提供批数据。
这意味着我们的嵌入词张量和GRU输出的形状都是 <em>(1, batch_size, hidden_size)</em> 。</p>
<p><strong>计算图：</strong></p>
<ol>
<li>获得当前输入词的强如嵌入。</li>
<li>单向GRU前向。</li>
<li>有第二步的GRU输出计算注意力权重。</li>
<li>注意力权重与编码器输出相乘，得到“加权和”(weighted sum)上下文向量。</li>
<li>使用 Luong 等人的方法将加权上下文向量和GRU输出相加。</li>
<li>使用 Luong 等人的方法(不用 softmax)预测后续词。</li>
<li>返回输出和最终隐藏层。</li>
</ol>
<p><strong>输入：</strong></p>
<ul>
<li><code>input_step</code>: 输入序列批的一个时间步骤 (一个词)；形状=<em>(1, batch_size)</em></li>
<li><code>last_hidden</code>:  GRU的最终隐藏层；形状=(n_layers x num_directions, batch_size, hidden_size)*</li>
<li><code>encoder_outputs</code>: 编码器的模型输出; 性转=<em>(max_length, batch_size, hidden_size)</em></li>
</ul>
<p><strong>输出：</strong></p>
<ul>
<li><code>输出</code>: softmax 正规化张量，给出了被解码序列中每个词是正确的后续词的概率; 形状=<em>(batch_size, voc.num_words)</em></li>
<li><code>hidden</code>: GRU的最终隐藏状态; 形状=<em>(n_layers x num_directions, batch_size, hidden_size)</em></li>
</ul>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">LuongAttnDecoderRNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attn_model</span><span class="p">,</span> <span class="n">embedding</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LuongAttnDecoderRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># Keep for reference</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn_model</span> <span class="o">=</span> <span class="n">attn_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span> <span class="o">=</span> <span class="n">n_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>

        <span class="c1"># Define layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">embedding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gru</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span> <span class="k">if</span> <span class="n">n_layers</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">dropout</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">concat</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">Attn</span><span class="p">(</span><span class="n">attn_model</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_step</span><span class="p">,</span> <span class="n">last_hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">):</span>
        <span class="c1"># Note: we run this one step (word) at a time</span>
        <span class="c1"># Get embedding of current input word</span>
        <span class="n">embedded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">input_step</span><span class="p">)</span>
        <span class="n">embedded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_dropout</span><span class="p">(</span><span class="n">embedded</span><span class="p">)</span>
        <span class="c1"># Forward through unidirectional GRU</span>
        <span class="n">rnn_output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gru</span><span class="p">(</span><span class="n">embedded</span><span class="p">,</span> <span class="n">last_hidden</span><span class="p">)</span>
        <span class="c1"># Calculate attention weights from the current GRU output</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span><span class="n">rnn_output</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">)</span>
        <span class="c1"># Multiply attention weights to encoder outputs to get new &quot;weighted sum&quot; context vector</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">attn_weights</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="c1"># Concatenate weighted context vector and GRU output using Luong eq. 5</span>
        <span class="n">rnn_output</span> <span class="o">=</span> <span class="n">rnn_output</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">concat_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">rnn_output</span><span class="p">,</span> <span class="n">context</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">concat_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">concat_input</span><span class="p">))</span>
        <span class="c1"># Predict next word using Luong eq. 6</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="p">(</span><span class="n">concat_output</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Return output and final hidden state</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span>
</pre></div>
</td></tr></table>

<h3 id="定义训练过程">定义训练过程<a class="headerlink" href="#定义训练过程" title="Permanent link">&para;</a></h3>
<h4 id="损失掩码">损失掩码<a class="headerlink" href="#损失掩码" title="Permanent link">&para;</a></h4>
<p>由于我们处理的是批量填充序列，因此在计算损失时我们不能简单地仅考虑张量的全部元素。
而是通过定义<code>maskNLLLoss</code>损失函数，基于解码器输出张量，目标张量和描述目标张量填充的二进制掩码张量，来计算损失。
该损失函数计算了对应于掩码向量中<em>1</em>的元素的负对数相似度。</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3
4
5
6</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">maskNLLLoss</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
    <span class="n">nTotal</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">crossEntropy</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">crossEntropy</span><span class="o">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">nTotal</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>
</td></tr></table>

<h4 id="单次训练的迭代">单次训练的迭代<a class="headerlink" href="#单次训练的迭代" title="Permanent link">&para;</a></h4>
<p>函数 <code>train</code> 包含一个单次训练迭代（单个输入批次）算法。</p>
<p>我们用一些巧妙的技巧来促进收敛：</p>
<ul>
<li>
<p>第一个技巧是使用 <strong>tacher forcing</strong>。
  这意味着某些情况下，通过设置<code>teacher_forcing_ratio</code>，我们使用当前目标词，而不是使用解码器的当前猜测结果，作为解码器的后续输入。
  这项技术是训练解码器的轮子，有助于更有效的训练。
  &ldquo;tacher forcing&rdquo; 会导致模型在推理期间不稳定，这是因为解码器在训练中没有足够的机会真正的制作它自己的输出序列。
  所有，我们必须注意地如何设置<code>teacher_forcing_ratio</code>，不要被快速收敛欺骗。</p>
</li>
<li>
<p>我们要实现的第二个技巧是<strong>梯度裁剪</strong>。
  这是一种用于对抗“梯度爆炸”的常用技术。
  本质上，通过裁剪或者设置最大阈值，我们可防止梯度爆炸性增长和溢出(NaN)，或者在成本函数中从悬崖跌落。</p>
</li>
</ul>
<p><img alt="grad_clip" src="https://pytorch.org/tutorials/_images/grad_clip.png" /></p>
<p>图像来源: <a href="https://www.deeplearningbook.org/">Goodfellow 等人 <em>Deep Learning</em>. 2016.</a></p>
<p><strong>运算顺序：</strong></p>
<ol>
<li>通过编码器向前传递整个输入批次。</li>
<li>将解码器输入初始化为<code>SOS_token</code>和编码器最终隐藏层的隐藏状态。</li>
<li>在每个时间步骤中，通过解码器向前传递输入批的序列。</li>
<li>如果用到了 &ldquo;teacher forcing&rdquo;：将当前目标作为下一个解码器的输入；其它：将当前解码器输出作为下一个解码器的输入。</li>
<li>计算和累积损失。</li>
<li>进行反向传播</li>
<li>剪切梯度。</li>
<li>更新编码器和解码器模型的参数。</li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>只需将 PyTorch RNN 模块(<code>RNN</code>, <code>LSTM</code>, <code>GRU</code>)的整个输入序列（或批次的序列）传入，它们就可以被用于任何其它类似的非递归层。
我们在<code>encoder</code>中这样使用<code>GUR</code>层。
实际情况是，在底层，每一个时间步骤上都有一个交互过程迭代计算隐藏状态。</p>
<p>另外，也可以每个时间步骤运行这些模型。
在这种情况下，我们在训练过程里手动循环遍历序列，就像对<code>decoder</code>模型做的那样。
只要维护好这些模块正确的概念模型，实现序列模型就可以非常简单。</p>
</div>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">input_variable</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">target_variable</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">max_target_len</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">embedding</span><span class="p">,</span>
          <span class="n">encoder_optimizer</span><span class="p">,</span> <span class="n">decoder_optimizer</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">clip</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">MAX_LENGTH</span><span class="p">):</span>

    <span class="c1"># Zero gradients</span>
    <span class="n">encoder_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">decoder_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># Set device options</span>
    <span class="n">input_variable</span> <span class="o">=</span> <span class="n">input_variable</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">lengths</span> <span class="o">=</span> <span class="n">lengths</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">target_variable</span> <span class="o">=</span> <span class="n">target_variable</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Initialize variables</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">print_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">n_totals</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># Forward pass through encoder</span>
    <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">encoder_hidden</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">input_variable</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span>

    <span class="c1"># Create initial decoder input (start with SOS tokens for each sentence)</span>
    <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="n">SOS_token</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)]])</span>
    <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">decoder_input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Set initial decoder hidden state to the encoder&#39;s final hidden state</span>
    <span class="n">decoder_hidden</span> <span class="o">=</span> <span class="n">encoder_hidden</span><span class="p">[:</span><span class="n">decoder</span><span class="o">.</span><span class="n">n_layers</span><span class="p">]</span>

    <span class="c1"># Determine if we are using teacher forcing this iteration</span>
    <span class="n">use_teacher_forcing</span> <span class="o">=</span> <span class="bp">True</span> <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">teacher_forcing_ratio</span> <span class="k">else</span> <span class="bp">False</span>

    <span class="c1"># Forward batch of sequences through decoder one time step at a time</span>
    <span class="k">if</span> <span class="n">use_teacher_forcing</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_target_len</span><span class="p">):</span>
            <span class="n">decoder_output</span><span class="p">,</span> <span class="n">decoder_hidden</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span>
                <span class="n">decoder_input</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span>
            <span class="p">)</span>
            <span class="c1"># Teacher forcing: next input is current target</span>
            <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">target_variable</span><span class="p">[</span><span class="n">t</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># Calculate and accumulate loss</span>
            <span class="n">mask_loss</span><span class="p">,</span> <span class="n">nTotal</span> <span class="o">=</span> <span class="n">maskNLLLoss</span><span class="p">(</span><span class="n">decoder_output</span><span class="p">,</span> <span class="n">target_variable</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="n">mask</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>
            <span class="n">loss</span> <span class="o">+=</span> <span class="n">mask_loss</span>
            <span class="n">print_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mask_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">nTotal</span><span class="p">)</span>
            <span class="n">n_totals</span> <span class="o">+=</span> <span class="n">nTotal</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_target_len</span><span class="p">):</span>
            <span class="n">decoder_output</span><span class="p">,</span> <span class="n">decoder_hidden</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span>
                <span class="n">decoder_input</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span>
            <span class="p">)</span>
            <span class="c1"># No teacher forcing: next input is decoder&#39;s own current output</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">topi</span> <span class="o">=</span> <span class="n">decoder_output</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="n">topi</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)]])</span>
            <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">decoder_input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="c1"># Calculate and accumulate loss</span>
            <span class="n">mask_loss</span><span class="p">,</span> <span class="n">nTotal</span> <span class="o">=</span> <span class="n">maskNLLLoss</span><span class="p">(</span><span class="n">decoder_output</span><span class="p">,</span> <span class="n">target_variable</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="n">mask</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>
            <span class="n">loss</span> <span class="o">+=</span> <span class="n">mask_loss</span>
            <span class="n">print_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mask_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">nTotal</span><span class="p">)</span>
            <span class="n">n_totals</span> <span class="o">+=</span> <span class="n">nTotal</span>

    <span class="c1"># Perform backpropatation</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># Clip gradients: gradients are modified in place</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">encoder</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">clip</span><span class="p">)</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">decoder</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">clip</span><span class="p">)</span>

    <span class="c1"># Adjust model weights</span>
    <span class="n">encoder_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">decoder_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">print_losses</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_totals</span>
</pre></div>
</td></tr></table>

<h4 id="多次训练的迭代器">多次训练的迭代器<a class="headerlink" href="#多次训练的迭代器" title="Permanent link">&para;</a></h4>
<p>现在是时候把完整和的训练过程和数据结合在一起了。
函数<code>trainIters</code>负责运行把模型、优化器、数据等传递给<code>n_iterations</code>并运行。
这个函数很容易解释的，因为我们已经对<code>train</code>函数做了繁多的工作。</p>
<p>有一个要注意的事情：当我们保存模型时，所有的编码器和解码器状态字典(参数)、优化器的状态字典、损失、迭代器、等等，都被保存在一个tarball中。
被这样保存的模型可以为我们提供了具有最大灵活性的检查点。
从检查点加载模型之后，我们可以使用模型参数进行预测，或者从上次对出的地方继续训练。</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">trainIters</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">voc</span><span class="p">,</span> <span class="n">pairs</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">encoder_optimizer</span><span class="p">,</span> <span class="n">decoder_optimizer</span><span class="p">,</span> <span class="n">embedding</span><span class="p">,</span> <span class="n">encoder_n_layers</span><span class="p">,</span> <span class="n">decoder_n_layers</span><span class="p">,</span> <span class="n">save_dir</span><span class="p">,</span> <span class="n">n_iteration</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">print_every</span><span class="p">,</span> <span class="n">save_every</span><span class="p">,</span> <span class="n">clip</span><span class="p">,</span> <span class="n">corpus_name</span><span class="p">,</span> <span class="n">loadFilename</span><span class="p">):</span>

    <span class="c1"># Load batches for each iteration</span>
    <span class="n">training_batches</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch2TrainData</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="p">[</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">pairs</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)])</span>
                      <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iteration</span><span class="p">)]</span>

    <span class="c1"># Initializations</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Initializing ...&#39;</span><span class="p">)</span>
    <span class="n">start_iteration</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">print_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="n">loadFilename</span><span class="p">:</span>
        <span class="n">start_iteration</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;iteration&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>

    <span class="c1"># Training loop</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Training...&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">start_iteration</span><span class="p">,</span> <span class="n">n_iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">training_batch</span> <span class="o">=</span> <span class="n">training_batches</span><span class="p">[</span><span class="n">iteration</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
        <span class="c1"># Extract fields from batch</span>
        <span class="n">input_variable</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">target_variable</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">max_target_len</span> <span class="o">=</span> <span class="n">training_batch</span>

        <span class="c1"># Run a training iteration with batch</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">input_variable</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">target_variable</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">max_target_len</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span>
                     <span class="n">decoder</span><span class="p">,</span> <span class="n">embedding</span><span class="p">,</span> <span class="n">encoder_optimizer</span><span class="p">,</span> <span class="n">decoder_optimizer</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">clip</span><span class="p">)</span>
        <span class="n">print_loss</span> <span class="o">+=</span> <span class="n">loss</span>

        <span class="c1"># Print progress</span>
        <span class="k">if</span> <span class="n">iteration</span> <span class="o">%</span> <span class="n">print_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">print_loss_avg</span> <span class="o">=</span> <span class="n">print_loss</span> <span class="o">/</span> <span class="n">print_every</span>
            <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">iteration</span><span class="p">,</span> <span class="n">iteration</span> <span class="o">/</span> <span class="n">n_iteration</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span> <span class="n">print_loss_avg</span><span class="p">))</span>
            <span class="n">print_loss</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># Save checkpoint</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">iteration</span> <span class="o">%</span> <span class="n">save_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
            <span class="n">directory</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">corpus_name</span><span class="p">,</span> <span class="s1">&#39;{}-{}_{}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">encoder_n_layers</span><span class="p">,</span> <span class="n">decoder_n_layers</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">))</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">directory</span><span class="p">):</span>
                <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">directory</span><span class="p">)</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">({</span>
                <span class="s1">&#39;iteration&#39;</span><span class="p">:</span> <span class="n">iteration</span><span class="p">,</span>
                <span class="s1">&#39;en&#39;</span><span class="p">:</span> <span class="n">encoder</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
                <span class="s1">&#39;de&#39;</span><span class="p">:</span> <span class="n">decoder</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
                <span class="s1">&#39;en_opt&#39;</span><span class="p">:</span> <span class="n">encoder_optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
                <span class="s1">&#39;de_opt&#39;</span><span class="p">:</span> <span class="n">decoder_optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
                <span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span>
                <span class="s1">&#39;voc_dict&#39;</span><span class="p">:</span> <span class="n">voc</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">,</span>
                <span class="s1">&#39;embedding&#39;</span><span class="p">:</span> <span class="n">embedding</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
            <span class="p">},</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="s1">&#39;{}_{}.tar&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">iteration</span><span class="p">,</span> <span class="s1">&#39;checkpoint&#39;</span><span class="p">)))</span>
</pre></div>
</td></tr></table>

<h3 id="定义评估过程">定义评估过程<a class="headerlink" href="#定义评估过程" title="Permanent link">&para;</a></h3>
<p>在训练模型后，我们要与机器人交谈。
首先，我们必须定义模型如何对输入进行编解码。</p>
<h4 id="greedy-decoding">Greedy decoding<a class="headerlink" href="#greedy-decoding" title="Permanent link">&para;</a></h4>
<p>&ldquo;Greedy decoding&rdquo; 是在训练过程中<strong>不</strong>使用 &ldquo;tacher forcing&rdquo; 的情况下使用的一种解码方式。
换而言之，在每个时间步骤，我们只需从<code>decoder_output</code>选定softmax值最高的词。
这个解码方式在单时间步骤级别上是最佳的。</p>
<p>为了方便地使用 &ldquo;Greedy decoding&rdquo;，我们定义一个<code>GreedySearchDecoder</code>类。
在运行时，这个类的对象采用的参数是：</p>
<ul>
<li>形状为 <em>(input_seq length, 1)</em> 的输入序列(<code>input_seq</code>)</li>
<li>标量输入长度(<code>input_length</code>) tensor</li>
<li>答复句子长度限制值 <code>max_length</code></li>
</ul>
<p>使用以下计算图对输入句子进行评估输入：</p>
<p><strong>计算图：</strong></p>
<ol>
<li>通过编码器模型转发输入。</li>
<li>将编码器的最终隐藏层用作解码器的第一个隐藏输入。</li>
<li>将解码器的第一个输入初始化为<code>SOS_token</code></li>
<li>初始化张量，在其末尾附加附加解码后的词</li>
<li>一次迭代解码一个词的标记(token)：<ol>
<li>通过解码器前向传播。</li>
<li>获得可能性最大词的token和softmax分值(score)。</li>
<li>记录token和score</li>
<li>将当前token用作下一个解码器的输入</li>
</ol>
</li>
<li>返回收集到的词的标记(token)和softmax分值(score)列表</li>
</ol>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">GreedySearchDecoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GreedySearchDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">encoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">decoder</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_seq</span><span class="p">,</span> <span class="n">input_length</span><span class="p">,</span> <span class="n">max_length</span><span class="p">):</span>
        <span class="c1"># Forward input through encoder model</span>
        <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">encoder_hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">input_seq</span><span class="p">,</span> <span class="n">input_length</span><span class="p">)</span>
        <span class="c1"># Prepare encoder&#39;s final hidden layer to be first hidden input to the decoder</span>
        <span class="n">decoder_hidden</span> <span class="o">=</span> <span class="n">encoder_hidden</span><span class="p">[:</span><span class="n">decoder</span><span class="o">.</span><span class="n">n_layers</span><span class="p">]</span>
        <span class="c1"># Initialize decoder input with SOS_token</span>
        <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span> <span class="o">*</span> <span class="n">SOS_token</span>
        <span class="c1"># Initialize tensors to append decoded words to</span>
        <span class="n">all_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
        <span class="n">all_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="c1"># Iteratively decode one word token at a time</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_length</span><span class="p">):</span>
            <span class="c1"># Forward pass through decoder</span>
            <span class="n">decoder_output</span><span class="p">,</span> <span class="n">decoder_hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">decoder_input</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">)</span>
            <span class="c1"># Obtain most likely word token and its softmax score</span>
            <span class="n">decoder_scores</span><span class="p">,</span> <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">decoder_output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># Record token and score</span>
            <span class="n">all_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">all_tokens</span><span class="p">,</span> <span class="n">decoder_input</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">all_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">all_scores</span><span class="p">,</span> <span class="n">decoder_scores</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="c1"># Prepare current token to be next decoder input (add a dimension)</span>
            <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">decoder_input</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="c1"># Return collections of word tokens and scores</span>
        <span class="k">return</span> <span class="n">all_tokens</span><span class="p">,</span> <span class="n">all_scores</span>
</pre></div>
</td></tr></table>

<h4 id="评估用户输入文本">评估用户输入文本<a class="headerlink" href="#评估用户输入文本" title="Permanent link">&para;</a></h4>
<p>现在我们已经定义了解码方法，可以编写对输入句子文本进行评估的函数。
函数<code>evaluate</code>用于管理处理输入句子的底层过程。</p>
<p>首先将句子格式化为 <em>batch_size==1</em> 的词索引值输入批。
要实现这一点，需要将句子中的词转为他们对应的索引值，并调换坐标轴以生成我们模型所需的张量。
还要创建一个<code>lengths</code>张量，用于记录输入句子的长度。
此时，<code>lengths</code>是一个标量，因为我们一次只评估一个句子(<em>batch_size==1</em>)。</p>
<p>然后，通过<code>GreedySearchDecoder</code>对象(<code>searcher</code>)，我们可以获得解码后的回复句的张量。</p>
<p>最后，将回复的索引值转为词，并返回解码出来的词列表。</p>
<p><code>evaluateInput</code> acts as the user interface for our chatbot. 
<code>evaluateInput</code>充当聊天机器人用户界面。
调用它的时候，提供一个文本输入界面，我们在这里输入自己的询问句。
按<em>回车</em>完成后，用和处理训练数据一致的方法对输入文本进行正规化处理，传递给<code>evaluate</code>函数，获得解码后的输出句子。
循环执行这个过程，这样就可以和机器人不停地聊天。输入“q”或者&rdquo;quit&rdquo;退出。</p>
<p>最后，如果句子中含有词表中不存在的词，应该进行优雅的错误处理：输出错误提示信息，提醒用户输入其它词。</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">searcher</span><span class="p">,</span> <span class="n">voc</span><span class="p">,</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">MAX_LENGTH</span><span class="p">):</span>
    <span class="c1">### Format input sentence as a batch</span>
    <span class="c1"># words -&gt; indexes</span>
    <span class="n">indexes_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">indexesFromSentence</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)]</span>
    <span class="c1"># Create lengths tensor</span>
    <span class="n">lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">indexes</span><span class="p">)</span> <span class="k">for</span> <span class="n">indexes</span> <span class="ow">in</span> <span class="n">indexes_batch</span><span class="p">])</span>
    <span class="c1"># Transpose dimensions of batch to match models&#39; expectations</span>
    <span class="n">input_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">indexes_batch</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="c1"># Use appropriate device</span>
    <span class="n">input_batch</span> <span class="o">=</span> <span class="n">input_batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">lengths</span> <span class="o">=</span> <span class="n">lengths</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="c1"># Decode sentence with searcher</span>
    <span class="n">tokens</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="n">searcher</span><span class="p">(</span><span class="n">input_batch</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">max_length</span><span class="p">)</span>
    <span class="c1"># indexes -&gt; words</span>
    <span class="n">decoded_words</span> <span class="o">=</span> <span class="p">[</span><span class="n">voc</span><span class="o">.</span><span class="n">index2word</span><span class="p">[</span><span class="n">token</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">decoded_words</span>


<span class="k">def</span> <span class="nf">evaluateInput</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">searcher</span><span class="p">,</span> <span class="n">voc</span><span class="p">):</span>
    <span class="n">input_sentence</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
    <span class="k">while</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Get input sentence</span>
            <span class="n">input_sentence</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s1">&#39;&gt; &#39;</span><span class="p">)</span>
            <span class="c1"># Check if it is quit case</span>
            <span class="k">if</span> <span class="n">input_sentence</span> <span class="o">==</span> <span class="s1">&#39;q&#39;</span> <span class="ow">or</span> <span class="n">input_sentence</span> <span class="o">==</span> <span class="s1">&#39;quit&#39;</span><span class="p">:</span> <span class="k">break</span>
            <span class="c1"># Normalize sentence</span>
            <span class="n">input_sentence</span> <span class="o">=</span> <span class="n">normalizeString</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">)</span>
            <span class="c1"># Evaluate sentence</span>
            <span class="n">output_words</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">searcher</span><span class="p">,</span> <span class="n">voc</span><span class="p">,</span> <span class="n">input_sentence</span><span class="p">)</span>
            <span class="c1"># Format and print response sentence</span>
            <span class="n">output_words</span><span class="p">[:]</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">output_words</span> <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="s1">&#39;EOS&#39;</span> <span class="ow">or</span> <span class="n">x</span> <span class="o">==</span> <span class="s1">&#39;PAD&#39;</span><span class="p">)]</span>
            <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Bot:&#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_words</span><span class="p">))</span>

        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Error: Encountered unknown word.&quot;</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<h3 id="运行模型">运行模型<a class="headerlink" href="#运行模型" title="Permanent link">&para;</a></h3>
<p>最后，是时候运行我们的模型了！</p>
<p>不管训练还是测试，我们都得初始化独立的编码器和解码器模型。
在下面的代码块中，我们设置了所需配置，选择从头开始或者从检查点加载并构建和初始化模型。
可以自由地使用不同的模型设置来优化性能。</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Configure models</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;cb_model&#39;</span>
<span class="n">attn_model</span> <span class="o">=</span> <span class="s1">&#39;dot&#39;</span>
<span class="c1">#attn_model = &#39;general&#39;</span>
<span class="c1">#attn_model = &#39;concat&#39;</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">encoder_n_layers</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">decoder_n_layers</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>

<span class="c1"># Set checkpoint to load from; set to None if starting from scratch</span>
<span class="n">loadFilename</span> <span class="o">=</span> <span class="bp">None</span>
<span class="n">checkpoint_iter</span> <span class="o">=</span> <span class="mi">4000</span>
<span class="c1">#loadFilename = os.path.join(save_dir, model_name, corpus_name,</span>
<span class="c1">#                            &#39;{}-{}_{}&#39;.format(encoder_n_layers, decoder_n_layers, hidden_size),</span>
<span class="c1">#                            &#39;{}_checkpoint.tar&#39;.format(checkpoint_iter))</span>


<span class="c1"># Load model if a loadFilename is provided</span>
<span class="k">if</span> <span class="n">loadFilename</span><span class="p">:</span>
    <span class="c1"># If loading on same machine the model was trained on</span>
    <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">loadFilename</span><span class="p">)</span>
    <span class="c1"># If loading a model trained on GPU to CPU</span>
    <span class="c1">#checkpoint = torch.load(loadFilename, map_location=torch.device(&#39;cpu&#39;))</span>
    <span class="n">encoder_sd</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;en&#39;</span><span class="p">]</span>
    <span class="n">decoder_sd</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;de&#39;</span><span class="p">]</span>
    <span class="n">encoder_optimizer_sd</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;en_opt&#39;</span><span class="p">]</span>
    <span class="n">decoder_optimizer_sd</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;de_opt&#39;</span><span class="p">]</span>
    <span class="n">embedding_sd</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;embedding&#39;</span><span class="p">]</span>
    <span class="n">voc</span><span class="o">.</span><span class="vm">__dict__</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;voc_dict&#39;</span><span class="p">]</span>


<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Building encoder and decoder ...&#39;</span><span class="p">)</span>
<span class="c1"># Initialize word embeddings</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">voc</span><span class="o">.</span><span class="n">num_words</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
<span class="k">if</span> <span class="n">loadFilename</span><span class="p">:</span>
    <span class="n">embedding</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">embedding_sd</span><span class="p">)</span>
<span class="c1"># Initialize encoder &amp; decoder models</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="n">EncoderRNN</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">embedding</span><span class="p">,</span> <span class="n">encoder_n_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">LuongAttnDecoderRNN</span><span class="p">(</span><span class="n">attn_model</span><span class="p">,</span> <span class="n">embedding</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">voc</span><span class="o">.</span><span class="n">num_words</span><span class="p">,</span> <span class="n">decoder_n_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
<span class="k">if</span> <span class="n">loadFilename</span><span class="p">:</span>
    <span class="n">encoder</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">encoder_sd</span><span class="p">)</span>
    <span class="n">decoder</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">decoder_sd</span><span class="p">)</span>
<span class="c1"># Use appropriate device</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">decoder</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Models built and ready to go!&#39;</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>Building encoder and decoder ...
Models built and ready to go!
</pre></div>
</td></tr></table>

<h4 id="运行训练">运行训练<a class="headerlink" href="#运行训练" title="Permanent link">&para;</a></h4>
<p>运行以下代码块训练模型。</p>
<ul>
<li>首先是设置训练参数</li>
<li>然后初始化优化器(optimizer)</li>
<li>最后调用<code>trainIters</code>函数运行训练迭代器。</li>
</ul>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Configure training/optimization</span>
<span class="n">clip</span> <span class="o">=</span> <span class="mf">50.0</span>
<span class="n">teacher_forcing_ratio</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.0001</span>
<span class="n">decoder_learning_ratio</span> <span class="o">=</span> <span class="mf">5.0</span>
<span class="n">n_iteration</span> <span class="o">=</span> <span class="mi">4000</span>
<span class="n">print_every</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">save_every</span> <span class="o">=</span> <span class="mi">500</span>

<span class="c1"># Ensure dropout layers are in train mode</span>
<span class="n">encoder</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="n">decoder</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

<span class="c1"># Initialize optimizers</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Building optimizers ...&#39;</span><span class="p">)</span>
<span class="n">encoder_optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">encoder</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
<span class="n">decoder_optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">decoder</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">decoder_learning_ratio</span><span class="p">)</span>
<span class="k">if</span> <span class="n">loadFilename</span><span class="p">:</span>
    <span class="n">encoder_optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">encoder_optimizer_sd</span><span class="p">)</span>
    <span class="n">decoder_optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">decoder_optimizer_sd</span><span class="p">)</span>

<span class="c1"># Run training iterations</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Starting Training!&quot;</span><span class="p">)</span>
<span class="n">trainIters</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">voc</span><span class="p">,</span> <span class="n">pairs</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">encoder_optimizer</span><span class="p">,</span> <span class="n">decoder_optimizer</span><span class="p">,</span>
           <span class="n">embedding</span><span class="p">,</span> <span class="n">encoder_n_layers</span><span class="p">,</span> <span class="n">decoder_n_layers</span><span class="p">,</span> <span class="n">save_dir</span><span class="p">,</span> <span class="n">n_iteration</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span>
           <span class="n">print_every</span><span class="p">,</span> <span class="n">save_every</span><span class="p">,</span> <span class="n">clip</span><span class="p">,</span> <span class="n">corpus_name</span><span class="p">,</span> <span class="n">loadFilename</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>   1
   2
   3
   4
   5
   6
   7
   8
   9
  10
  11
  12
  13
  14
  15
  16
  17
  18
  19
  20
  21
  22
  23
  24
  25
  26
  27
  28
  29
  30
  31
  32
  33
  34
  35
  36
  37
  38
  39
  40
  41
  42
  43
  44
  45
  46
  47
  48
  49
  50
  51
  52
  53
  54
  55
  56
  57
  58
  59
  60
  61
  62
  63
  64
  65
  66
  67
  68
  69
  70
  71
  72
  73
  74
  75
  76
  77
  78
  79
  80
  81
  82
  83
  84
  85
  86
  87
  88
  89
  90
  91
  92
  93
  94
  95
  96
  97
  98
  99
 100
 101
 102
 103
 104
 105
 106
 107
 108
 109
 110
 111
 112
 113
 114
 115
 116
 117
 118
 119
 120
 121
 122
 123
 124
 125
 126
 127
 128
 129
 130
 131
 132
 133
 134
 135
 136
 137
 138
 139
 140
 141
 142
 143
 144
 145
 146
 147
 148
 149
 150
 151
 152
 153
 154
 155
 156
 157
 158
 159
 160
 161
 162
 163
 164
 165
 166
 167
 168
 169
 170
 171
 172
 173
 174
 175
 176
 177
 178
 179
 180
 181
 182
 183
 184
 185
 186
 187
 188
 189
 190
 191
 192
 193
 194
 195
 196
 197
 198
 199
 200
 201
 202
 203
 204
 205
 206
 207
 208
 209
 210
 211
 212
 213
 214
 215
 216
 217
 218
 219
 220
 221
 222
 223
 224
 225
 226
 227
 228
 229
 230
 231
 232
 233
 234
 235
 236
 237
 238
 239
 240
 241
 242
 243
 244
 245
 246
 247
 248
 249
 250
 251
 252
 253
 254
 255
 256
 257
 258
 259
 260
 261
 262
 263
 264
 265
 266
 267
 268
 269
 270
 271
 272
 273
 274
 275
 276
 277
 278
 279
 280
 281
 282
 283
 284
 285
 286
 287
 288
 289
 290
 291
 292
 293
 294
 295
 296
 297
 298
 299
 300
 301
 302
 303
 304
 305
 306
 307
 308
 309
 310
 311
 312
 313
 314
 315
 316
 317
 318
 319
 320
 321
 322
 323
 324
 325
 326
 327
 328
 329
 330
 331
 332
 333
 334
 335
 336
 337
 338
 339
 340
 341
 342
 343
 344
 345
 346
 347
 348
 349
 350
 351
 352
 353
 354
 355
 356
 357
 358
 359
 360
 361
 362
 363
 364
 365
 366
 367
 368
 369
 370
 371
 372
 373
 374
 375
 376
 377
 378
 379
 380
 381
 382
 383
 384
 385
 386
 387
 388
 389
 390
 391
 392
 393
 394
 395
 396
 397
 398
 399
 400
 401
 402
 403
 404
 405
 406
 407
 408
 409
 410
 411
 412
 413
 414
 415
 416
 417
 418
 419
 420
 421
 422
 423
 424
 425
 426
 427
 428
 429
 430
 431
 432
 433
 434
 435
 436
 437
 438
 439
 440
 441
 442
 443
 444
 445
 446
 447
 448
 449
 450
 451
 452
 453
 454
 455
 456
 457
 458
 459
 460
 461
 462
 463
 464
 465
 466
 467
 468
 469
 470
 471
 472
 473
 474
 475
 476
 477
 478
 479
 480
 481
 482
 483
 484
 485
 486
 487
 488
 489
 490
 491
 492
 493
 494
 495
 496
 497
 498
 499
 500
 501
 502
 503
 504
 505
 506
 507
 508
 509
 510
 511
 512
 513
 514
 515
 516
 517
 518
 519
 520
 521
 522
 523
 524
 525
 526
 527
 528
 529
 530
 531
 532
 533
 534
 535
 536
 537
 538
 539
 540
 541
 542
 543
 544
 545
 546
 547
 548
 549
 550
 551
 552
 553
 554
 555
 556
 557
 558
 559
 560
 561
 562
 563
 564
 565
 566
 567
 568
 569
 570
 571
 572
 573
 574
 575
 576
 577
 578
 579
 580
 581
 582
 583
 584
 585
 586
 587
 588
 589
 590
 591
 592
 593
 594
 595
 596
 597
 598
 599
 600
 601
 602
 603
 604
 605
 606
 607
 608
 609
 610
 611
 612
 613
 614
 615
 616
 617
 618
 619
 620
 621
 622
 623
 624
 625
 626
 627
 628
 629
 630
 631
 632
 633
 634
 635
 636
 637
 638
 639
 640
 641
 642
 643
 644
 645
 646
 647
 648
 649
 650
 651
 652
 653
 654
 655
 656
 657
 658
 659
 660
 661
 662
 663
 664
 665
 666
 667
 668
 669
 670
 671
 672
 673
 674
 675
 676
 677
 678
 679
 680
 681
 682
 683
 684
 685
 686
 687
 688
 689
 690
 691
 692
 693
 694
 695
 696
 697
 698
 699
 700
 701
 702
 703
 704
 705
 706
 707
 708
 709
 710
 711
 712
 713
 714
 715
 716
 717
 718
 719
 720
 721
 722
 723
 724
 725
 726
 727
 728
 729
 730
 731
 732
 733
 734
 735
 736
 737
 738
 739
 740
 741
 742
 743
 744
 745
 746
 747
 748
 749
 750
 751
 752
 753
 754
 755
 756
 757
 758
 759
 760
 761
 762
 763
 764
 765
 766
 767
 768
 769
 770
 771
 772
 773
 774
 775
 776
 777
 778
 779
 780
 781
 782
 783
 784
 785
 786
 787
 788
 789
 790
 791
 792
 793
 794
 795
 796
 797
 798
 799
 800
 801
 802
 803
 804
 805
 806
 807
 808
 809
 810
 811
 812
 813
 814
 815
 816
 817
 818
 819
 820
 821
 822
 823
 824
 825
 826
 827
 828
 829
 830
 831
 832
 833
 834
 835
 836
 837
 838
 839
 840
 841
 842
 843
 844
 845
 846
 847
 848
 849
 850
 851
 852
 853
 854
 855
 856
 857
 858
 859
 860
 861
 862
 863
 864
 865
 866
 867
 868
 869
 870
 871
 872
 873
 874
 875
 876
 877
 878
 879
 880
 881
 882
 883
 884
 885
 886
 887
 888
 889
 890
 891
 892
 893
 894
 895
 896
 897
 898
 899
 900
 901
 902
 903
 904
 905
 906
 907
 908
 909
 910
 911
 912
 913
 914
 915
 916
 917
 918
 919
 920
 921
 922
 923
 924
 925
 926
 927
 928
 929
 930
 931
 932
 933
 934
 935
 936
 937
 938
 939
 940
 941
 942
 943
 944
 945
 946
 947
 948
 949
 950
 951
 952
 953
 954
 955
 956
 957
 958
 959
 960
 961
 962
 963
 964
 965
 966
 967
 968
 969
 970
 971
 972
 973
 974
 975
 976
 977
 978
 979
 980
 981
 982
 983
 984
 985
 986
 987
 988
 989
 990
 991
 992
 993
 994
 995
 996
 997
 998
 999
1000
1001
1002
1003
1004
1005
1006
1007
1008
1009
1010
1011
1012
1013
1014
1015
1016
1017
1018
1019
1020
1021
1022
1023
1024
1025
1026
1027
1028
1029
1030
1031
1032
1033
1034
1035
1036
1037
1038
1039
1040
1041
1042
1043
1044
1045
1046
1047
1048
1049
1050
1051
1052
1053
1054
1055
1056
1057
1058
1059
1060
1061
1062
1063
1064
1065
1066
1067
1068
1069
1070
1071
1072
1073
1074
1075
1076
1077
1078
1079
1080
1081
1082
1083
1084
1085
1086
1087
1088
1089
1090
1091
1092
1093
1094
1095
1096
1097
1098
1099
1100
1101
1102
1103
1104
1105
1106
1107
1108
1109
1110
1111
1112
1113
1114
1115
1116
1117
1118
1119
1120
1121
1122
1123
1124
1125
1126
1127
1128
1129
1130
1131
1132
1133
1134
1135
1136
1137
1138
1139
1140
1141
1142
1143
1144
1145
1146
1147
1148
1149
1150
1151
1152
1153
1154
1155
1156
1157
1158
1159
1160
1161
1162
1163
1164
1165
1166
1167
1168
1169
1170
1171
1172
1173
1174
1175
1176
1177
1178
1179
1180
1181
1182
1183
1184
1185
1186
1187
1188
1189
1190
1191
1192
1193
1194
1195
1196
1197
1198
1199
1200
1201
1202
1203
1204
1205
1206
1207
1208
1209
1210
1211
1212
1213
1214
1215
1216
1217
1218
1219
1220
1221
1222
1223
1224
1225
1226
1227
1228
1229
1230
1231
1232
1233
1234
1235
1236
1237
1238
1239
1240
1241
1242
1243
1244
1245
1246
1247
1248
1249
1250
1251
1252
1253
1254
1255
1256
1257
1258
1259
1260
1261
1262
1263
1264
1265
1266
1267
1268
1269
1270
1271
1272
1273
1274
1275
1276
1277
1278
1279
1280
1281
1282
1283
1284
1285
1286
1287
1288
1289
1290
1291
1292
1293
1294
1295
1296
1297
1298
1299
1300
1301
1302
1303
1304
1305
1306
1307
1308
1309
1310
1311
1312
1313
1314
1315
1316
1317
1318
1319
1320
1321
1322
1323
1324
1325
1326
1327
1328
1329
1330
1331
1332
1333
1334
1335
1336
1337
1338
1339
1340
1341
1342
1343
1344
1345
1346
1347
1348
1349
1350
1351
1352
1353
1354
1355
1356
1357
1358
1359
1360
1361
1362
1363
1364
1365
1366
1367
1368
1369
1370
1371
1372
1373
1374
1375
1376
1377
1378
1379
1380
1381
1382
1383
1384
1385
1386
1387
1388
1389
1390
1391
1392
1393
1394
1395
1396
1397
1398
1399
1400
1401
1402
1403
1404
1405
1406
1407
1408
1409
1410
1411
1412
1413
1414
1415
1416
1417
1418
1419
1420
1421
1422
1423
1424
1425
1426
1427
1428
1429
1430
1431
1432
1433
1434
1435
1436
1437
1438
1439
1440
1441
1442
1443
1444
1445
1446
1447
1448
1449
1450
1451
1452
1453
1454
1455
1456
1457
1458
1459
1460
1461
1462
1463
1464
1465
1466
1467
1468
1469
1470
1471
1472
1473
1474
1475
1476
1477
1478
1479
1480
1481
1482
1483
1484
1485
1486
1487
1488
1489
1490
1491
1492
1493
1494
1495
1496
1497
1498
1499
1500
1501
1502
1503
1504
1505
1506
1507
1508
1509
1510
1511
1512
1513
1514
1515
1516
1517
1518
1519
1520
1521
1522
1523
1524
1525
1526
1527
1528
1529
1530
1531
1532
1533
1534
1535
1536
1537
1538
1539
1540
1541
1542
1543
1544
1545
1546
1547
1548
1549
1550
1551
1552
1553
1554
1555
1556
1557
1558
1559
1560
1561
1562
1563
1564
1565
1566
1567
1568
1569
1570
1571
1572
1573
1574
1575
1576
1577
1578
1579
1580
1581
1582
1583
1584
1585
1586
1587
1588
1589
1590
1591
1592
1593
1594
1595
1596
1597
1598
1599
1600
1601
1602
1603
1604
1605
1606
1607
1608
1609
1610
1611
1612
1613
1614
1615
1616
1617
1618
1619
1620
1621
1622
1623
1624
1625
1626
1627
1628
1629
1630
1631
1632
1633
1634
1635
1636
1637
1638
1639
1640
1641
1642
1643
1644
1645
1646
1647
1648
1649
1650
1651
1652
1653
1654
1655
1656
1657
1658
1659
1660
1661
1662
1663
1664
1665
1666
1667
1668
1669
1670
1671
1672
1673
1674
1675
1676
1677
1678
1679
1680
1681
1682
1683
1684
1685
1686
1687
1688
1689
1690
1691
1692
1693
1694
1695
1696
1697
1698
1699
1700
1701
1702
1703
1704
1705
1706
1707
1708
1709
1710
1711
1712
1713
1714
1715
1716
1717
1718
1719
1720
1721
1722
1723
1724
1725
1726
1727
1728
1729
1730
1731
1732
1733
1734
1735
1736
1737
1738
1739
1740
1741
1742
1743
1744
1745
1746
1747
1748
1749
1750
1751
1752
1753
1754
1755
1756
1757
1758
1759
1760
1761
1762
1763
1764
1765
1766
1767
1768
1769
1770
1771
1772
1773
1774
1775
1776
1777
1778
1779
1780
1781
1782
1783
1784
1785
1786
1787
1788
1789
1790
1791
1792
1793
1794
1795
1796
1797
1798
1799
1800
1801
1802
1803
1804
1805
1806
1807
1808
1809
1810
1811
1812
1813
1814
1815
1816
1817
1818
1819
1820
1821
1822
1823
1824
1825
1826
1827
1828
1829
1830
1831
1832
1833
1834
1835
1836
1837
1838
1839
1840
1841
1842
1843
1844
1845
1846
1847
1848
1849
1850
1851
1852
1853
1854
1855
1856
1857
1858
1859
1860
1861
1862
1863
1864
1865
1866
1867
1868
1869
1870
1871
1872
1873
1874
1875
1876
1877
1878
1879
1880
1881
1882
1883
1884
1885
1886
1887
1888
1889
1890
1891
1892
1893
1894
1895
1896
1897
1898
1899
1900
1901
1902
1903
1904
1905
1906
1907
1908
1909
1910
1911
1912
1913
1914
1915
1916
1917
1918
1919
1920
1921
1922
1923
1924
1925
1926
1927
1928
1929
1930
1931
1932
1933
1934
1935
1936
1937
1938
1939
1940
1941
1942
1943
1944
1945
1946
1947
1948
1949
1950
1951
1952
1953
1954
1955
1956
1957
1958
1959
1960
1961
1962
1963
1964
1965
1966
1967
1968
1969
1970
1971
1972
1973
1974
1975
1976
1977
1978
1979
1980
1981
1982
1983
1984
1985
1986
1987
1988
1989
1990
1991
1992
1993
1994
1995
1996
1997
1998
1999
2000
2001
2002
2003
2004
2005
2006
2007
2008
2009
2010
2011
2012
2013
2014
2015
2016
2017
2018
2019
2020
2021
2022
2023
2024
2025
2026
2027
2028
2029
2030
2031
2032
2033
2034
2035
2036
2037
2038
2039
2040
2041
2042
2043
2044
2045
2046
2047
2048
2049
2050
2051
2052
2053
2054
2055
2056
2057
2058
2059
2060
2061
2062
2063
2064
2065
2066
2067
2068
2069
2070
2071
2072
2073
2074
2075
2076
2077
2078
2079
2080
2081
2082
2083
2084
2085
2086
2087
2088
2089
2090
2091
2092
2093
2094
2095
2096
2097
2098
2099
2100
2101
2102
2103
2104
2105
2106
2107
2108
2109
2110
2111
2112
2113
2114
2115
2116
2117
2118
2119
2120
2121
2122
2123
2124
2125
2126
2127
2128
2129
2130
2131
2132
2133
2134
2135
2136
2137
2138
2139
2140
2141
2142
2143
2144
2145
2146
2147
2148
2149
2150
2151
2152
2153
2154
2155
2156
2157
2158
2159
2160
2161
2162
2163
2164
2165
2166
2167
2168
2169
2170
2171
2172
2173
2174
2175
2176
2177
2178
2179
2180
2181
2182
2183
2184
2185
2186
2187
2188
2189
2190
2191
2192
2193
2194
2195
2196
2197
2198
2199
2200
2201
2202
2203
2204
2205
2206
2207
2208
2209
2210
2211
2212
2213
2214
2215
2216
2217
2218
2219
2220
2221
2222
2223
2224
2225
2226
2227
2228
2229
2230
2231
2232
2233
2234
2235
2236
2237
2238
2239
2240
2241
2242
2243
2244
2245
2246
2247
2248
2249
2250
2251
2252
2253
2254
2255
2256
2257
2258
2259
2260
2261
2262
2263
2264
2265
2266
2267
2268
2269
2270
2271
2272
2273
2274
2275
2276
2277
2278
2279
2280
2281
2282
2283
2284
2285
2286
2287
2288
2289
2290
2291
2292
2293
2294
2295
2296
2297
2298
2299
2300
2301
2302
2303
2304
2305
2306
2307
2308
2309
2310
2311
2312
2313
2314
2315
2316
2317
2318
2319
2320
2321
2322
2323
2324
2325
2326
2327
2328
2329
2330
2331
2332
2333
2334
2335
2336
2337
2338
2339
2340
2341
2342
2343
2344
2345
2346
2347
2348
2349
2350
2351
2352
2353
2354
2355
2356
2357
2358
2359
2360
2361
2362
2363
2364
2365
2366
2367
2368
2369
2370
2371
2372
2373
2374
2375
2376
2377
2378
2379
2380
2381
2382
2383
2384
2385
2386
2387
2388
2389
2390
2391
2392
2393
2394
2395
2396
2397
2398
2399
2400
2401
2402
2403
2404
2405
2406
2407
2408
2409
2410
2411
2412
2413
2414
2415
2416
2417
2418
2419
2420
2421
2422
2423
2424
2425
2426
2427
2428
2429
2430
2431
2432
2433
2434
2435
2436
2437
2438
2439
2440
2441
2442
2443
2444
2445
2446
2447
2448
2449
2450
2451
2452
2453
2454
2455
2456
2457
2458
2459
2460
2461
2462
2463
2464
2465
2466
2467
2468
2469
2470
2471
2472
2473
2474
2475
2476
2477
2478
2479
2480
2481
2482
2483
2484
2485
2486
2487
2488
2489
2490
2491
2492
2493
2494
2495
2496
2497
2498
2499
2500
2501
2502
2503
2504
2505
2506
2507
2508
2509
2510
2511
2512
2513
2514
2515
2516
2517
2518
2519
2520
2521
2522
2523
2524
2525
2526
2527
2528
2529
2530
2531
2532
2533
2534
2535
2536
2537
2538
2539
2540
2541
2542
2543
2544
2545
2546
2547
2548
2549
2550
2551
2552
2553
2554
2555
2556
2557
2558
2559
2560
2561
2562
2563
2564
2565
2566
2567
2568
2569
2570
2571
2572
2573
2574
2575
2576
2577
2578
2579
2580
2581
2582
2583
2584
2585
2586
2587
2588
2589
2590
2591
2592
2593
2594
2595
2596
2597
2598
2599
2600
2601
2602
2603
2604
2605
2606
2607
2608
2609
2610
2611
2612
2613
2614
2615
2616
2617
2618
2619
2620
2621
2622
2623
2624
2625
2626
2627
2628
2629
2630
2631
2632
2633
2634
2635
2636
2637
2638
2639
2640
2641
2642
2643
2644
2645
2646
2647
2648
2649
2650
2651
2652
2653
2654
2655
2656
2657
2658
2659
2660
2661
2662
2663
2664
2665
2666
2667
2668
2669
2670
2671
2672
2673
2674
2675
2676
2677
2678
2679
2680
2681
2682
2683
2684
2685
2686
2687
2688
2689
2690
2691
2692
2693
2694
2695
2696
2697
2698
2699
2700
2701
2702
2703
2704
2705
2706
2707
2708
2709
2710
2711
2712
2713
2714
2715
2716
2717
2718
2719
2720
2721
2722
2723
2724
2725
2726
2727
2728
2729
2730
2731
2732
2733
2734
2735
2736
2737
2738
2739
2740
2741
2742
2743
2744
2745
2746
2747
2748
2749
2750
2751
2752
2753
2754
2755
2756
2757
2758
2759
2760
2761
2762
2763
2764
2765
2766
2767
2768
2769
2770
2771
2772
2773
2774
2775
2776
2777
2778
2779
2780
2781
2782
2783
2784
2785
2786
2787
2788
2789
2790
2791
2792
2793
2794
2795
2796
2797
2798
2799
2800
2801
2802
2803
2804
2805
2806
2807
2808
2809
2810
2811
2812
2813
2814
2815
2816
2817
2818
2819
2820
2821
2822
2823
2824
2825
2826
2827
2828
2829
2830
2831
2832
2833
2834
2835
2836
2837
2838
2839
2840
2841
2842
2843
2844
2845
2846
2847
2848
2849
2850
2851
2852
2853
2854
2855
2856
2857
2858
2859
2860
2861
2862
2863
2864
2865
2866
2867
2868
2869
2870
2871
2872
2873
2874
2875
2876
2877
2878
2879
2880
2881
2882
2883
2884
2885
2886
2887
2888
2889
2890
2891
2892
2893
2894
2895
2896
2897
2898
2899
2900
2901
2902
2903
2904
2905
2906
2907
2908
2909
2910
2911
2912
2913
2914
2915
2916
2917
2918
2919
2920
2921
2922
2923
2924
2925
2926
2927
2928
2929
2930
2931
2932
2933
2934
2935
2936
2937
2938
2939
2940
2941
2942
2943
2944
2945
2946
2947
2948
2949
2950
2951
2952
2953
2954
2955
2956
2957
2958
2959
2960
2961
2962
2963
2964
2965
2966
2967
2968
2969
2970
2971
2972
2973
2974
2975
2976
2977
2978
2979
2980
2981
2982
2983
2984
2985
2986
2987
2988
2989
2990
2991
2992
2993
2994
2995
2996
2997
2998
2999
3000
3001
3002
3003
3004
3005
3006
3007
3008
3009
3010
3011
3012
3013
3014
3015
3016
3017
3018
3019
3020
3021
3022
3023
3024
3025
3026
3027
3028
3029
3030
3031
3032
3033
3034
3035
3036
3037
3038
3039
3040
3041
3042
3043
3044
3045
3046
3047
3048
3049
3050
3051
3052
3053
3054
3055
3056
3057
3058
3059
3060
3061
3062
3063
3064
3065
3066
3067
3068
3069
3070
3071
3072
3073
3074
3075
3076
3077
3078
3079
3080
3081
3082
3083
3084
3085
3086
3087
3088
3089
3090
3091
3092
3093
3094
3095
3096
3097
3098
3099
3100
3101
3102
3103
3104
3105
3106
3107
3108
3109
3110
3111
3112
3113
3114
3115
3116
3117
3118
3119
3120
3121
3122
3123
3124
3125
3126
3127
3128
3129
3130
3131
3132
3133
3134
3135
3136
3137
3138
3139
3140
3141
3142
3143
3144
3145
3146
3147
3148
3149
3150
3151
3152
3153
3154
3155
3156
3157
3158
3159
3160
3161
3162
3163
3164
3165
3166
3167
3168
3169
3170
3171
3172
3173
3174
3175
3176
3177
3178
3179
3180
3181
3182
3183
3184
3185
3186
3187
3188
3189
3190
3191
3192
3193
3194
3195
3196
3197
3198
3199
3200
3201
3202
3203
3204
3205
3206
3207
3208
3209
3210
3211
3212
3213
3214
3215
3216
3217
3218
3219
3220
3221
3222
3223
3224
3225
3226
3227
3228
3229
3230
3231
3232
3233
3234
3235
3236
3237
3238
3239
3240
3241
3242
3243
3244
3245
3246
3247
3248
3249
3250
3251
3252
3253
3254
3255
3256
3257
3258
3259
3260
3261
3262
3263
3264
3265
3266
3267
3268
3269
3270
3271
3272
3273
3274
3275
3276
3277
3278
3279
3280
3281
3282
3283
3284
3285
3286
3287
3288
3289
3290
3291
3292
3293
3294
3295
3296
3297
3298
3299
3300
3301
3302
3303
3304
3305
3306
3307
3308
3309
3310
3311
3312
3313
3314
3315
3316
3317
3318
3319
3320
3321
3322
3323
3324
3325
3326
3327
3328
3329
3330
3331
3332
3333
3334
3335
3336
3337
3338
3339
3340
3341
3342
3343
3344
3345
3346
3347
3348
3349
3350
3351
3352
3353
3354
3355
3356
3357
3358
3359
3360
3361
3362
3363
3364
3365
3366
3367
3368
3369
3370
3371
3372
3373
3374
3375
3376
3377
3378
3379
3380
3381
3382
3383
3384
3385
3386
3387
3388
3389
3390
3391
3392
3393
3394
3395
3396
3397
3398
3399
3400
3401
3402
3403
3404
3405
3406
3407
3408
3409
3410
3411
3412
3413
3414
3415
3416
3417
3418
3419
3420
3421
3422
3423
3424
3425
3426
3427
3428
3429
3430
3431
3432
3433
3434
3435
3436
3437
3438
3439
3440
3441
3442
3443
3444
3445
3446
3447
3448
3449
3450
3451
3452
3453
3454
3455
3456
3457
3458
3459
3460
3461
3462
3463
3464
3465
3466
3467
3468
3469
3470
3471
3472
3473
3474
3475
3476
3477
3478
3479
3480
3481
3482
3483
3484
3485
3486
3487
3488
3489
3490
3491
3492
3493
3494
3495
3496
3497
3498
3499
3500
3501
3502
3503
3504
3505
3506
3507
3508
3509
3510
3511
3512
3513
3514
3515
3516
3517
3518
3519
3520
3521
3522
3523
3524
3525
3526
3527
3528
3529
3530
3531
3532
3533
3534
3535
3536
3537
3538
3539
3540
3541
3542
3543
3544
3545
3546
3547
3548
3549
3550
3551
3552
3553
3554
3555
3556
3557
3558
3559
3560
3561
3562
3563
3564
3565
3566
3567
3568
3569
3570
3571
3572
3573
3574
3575
3576
3577
3578
3579
3580
3581
3582
3583
3584
3585
3586
3587
3588
3589
3590
3591
3592
3593
3594
3595
3596
3597
3598
3599
3600
3601
3602
3603
3604
3605
3606
3607
3608
3609
3610
3611
3612
3613
3614
3615
3616
3617
3618
3619
3620
3621
3622
3623
3624
3625
3626
3627
3628
3629
3630
3631
3632
3633
3634
3635
3636
3637
3638
3639
3640
3641
3642
3643
3644
3645
3646
3647
3648
3649
3650
3651
3652
3653
3654
3655
3656
3657
3658
3659
3660
3661
3662
3663
3664
3665
3666
3667
3668
3669
3670
3671
3672
3673
3674
3675
3676
3677
3678
3679
3680
3681
3682
3683
3684
3685
3686
3687
3688
3689
3690
3691
3692
3693
3694
3695
3696
3697
3698
3699
3700
3701
3702
3703
3704
3705
3706
3707
3708
3709
3710
3711
3712
3713
3714
3715
3716
3717
3718
3719
3720
3721
3722
3723
3724
3725
3726
3727
3728
3729
3730
3731
3732
3733
3734
3735
3736
3737
3738
3739
3740
3741
3742
3743
3744
3745
3746
3747
3748
3749
3750
3751
3752
3753
3754
3755
3756
3757
3758
3759
3760
3761
3762
3763
3764
3765
3766
3767
3768
3769
3770
3771
3772
3773
3774
3775
3776
3777
3778
3779
3780
3781
3782
3783
3784
3785
3786
3787
3788
3789
3790
3791
3792
3793
3794
3795
3796
3797
3798
3799
3800
3801
3802
3803
3804
3805
3806
3807
3808
3809
3810
3811
3812
3813
3814
3815
3816
3817
3818
3819
3820
3821
3822
3823
3824
3825
3826
3827
3828
3829
3830
3831
3832
3833
3834
3835
3836
3837
3838
3839
3840
3841
3842
3843
3844
3845
3846
3847
3848
3849
3850
3851
3852
3853
3854
3855
3856
3857
3858
3859
3860
3861
3862
3863
3864
3865
3866
3867
3868
3869
3870
3871
3872
3873
3874
3875
3876
3877
3878
3879
3880
3881
3882
3883
3884
3885
3886
3887
3888
3889
3890
3891
3892
3893
3894
3895
3896
3897
3898
3899
3900
3901
3902
3903
3904
3905
3906
3907
3908
3909
3910
3911
3912
3913
3914
3915
3916
3917
3918
3919
3920
3921
3922
3923
3924
3925
3926
3927
3928
3929
3930
3931
3932
3933
3934
3935
3936
3937
3938
3939
3940
3941
3942
3943
3944
3945
3946
3947
3948
3949
3950
3951
3952
3953
3954
3955
3956
3957
3958
3959
3960
3961
3962
3963
3964
3965
3966
3967
3968
3969
3970
3971
3972
3973
3974
3975
3976
3977
3978
3979
3980
3981
3982
3983
3984
3985
3986
3987
3988
3989
3990
3991
3992
3993
3994
3995
3996
3997
3998
3999
4000
4001
4002
4003
4004</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>Building optimizers ...
Starting Training!
Initializing ...
Training...
Iteration: 1; Percent complete: 0.0%; Average loss: 8.9628
Iteration: 2; Percent complete: 0.1%; Average loss: 8.8343
Iteration: 3; Percent complete: 0.1%; Average loss: 8.6592
Iteration: 4; Percent complete: 0.1%; Average loss: 8.3297
Iteration: 5; Percent complete: 0.1%; Average loss: 7.9770
Iteration: 6; Percent complete: 0.1%; Average loss: 7.3291
Iteration: 7; Percent complete: 0.2%; Average loss: 6.7586
Iteration: 8; Percent complete: 0.2%; Average loss: 6.9260
Iteration: 9; Percent complete: 0.2%; Average loss: 6.8659
Iteration: 10; Percent complete: 0.2%; Average loss: 6.6029
Iteration: 11; Percent complete: 0.3%; Average loss: 6.2245
Iteration: 12; Percent complete: 0.3%; Average loss: 6.1180
Iteration: 13; Percent complete: 0.3%; Average loss: 5.6463
Iteration: 14; Percent complete: 0.4%; Average loss: 5.6409
Iteration: 15; Percent complete: 0.4%; Average loss: 5.4941
Iteration: 16; Percent complete: 0.4%; Average loss: 5.6748
Iteration: 17; Percent complete: 0.4%; Average loss: 5.3386
Iteration: 18; Percent complete: 0.4%; Average loss: 5.3936
Iteration: 19; Percent complete: 0.5%; Average loss: 4.8958
Iteration: 20; Percent complete: 0.5%; Average loss: 4.9486
Iteration: 21; Percent complete: 0.5%; Average loss: 4.8804
Iteration: 22; Percent complete: 0.5%; Average loss: 4.8959
Iteration: 23; Percent complete: 0.6%; Average loss: 4.8363
Iteration: 24; Percent complete: 0.6%; Average loss: 4.9209
Iteration: 25; Percent complete: 0.6%; Average loss: 4.9964
Iteration: 26; Percent complete: 0.7%; Average loss: 4.8157
Iteration: 27; Percent complete: 0.7%; Average loss: 4.8140
Iteration: 28; Percent complete: 0.7%; Average loss: 4.9719
Iteration: 29; Percent complete: 0.7%; Average loss: 4.6794
Iteration: 30; Percent complete: 0.8%; Average loss: 4.7190
Iteration: 31; Percent complete: 0.8%; Average loss: 4.6346
Iteration: 32; Percent complete: 0.8%; Average loss: 4.9546
Iteration: 33; Percent complete: 0.8%; Average loss: 4.7239
Iteration: 34; Percent complete: 0.9%; Average loss: 4.8934
Iteration: 35; Percent complete: 0.9%; Average loss: 4.9053
Iteration: 36; Percent complete: 0.9%; Average loss: 4.8707
Iteration: 37; Percent complete: 0.9%; Average loss: 4.7875
Iteration: 38; Percent complete: 0.9%; Average loss: 4.6638
Iteration: 39; Percent complete: 1.0%; Average loss: 4.8682
Iteration: 40; Percent complete: 1.0%; Average loss: 4.5905
Iteration: 41; Percent complete: 1.0%; Average loss: 4.5374
Iteration: 42; Percent complete: 1.1%; Average loss: 4.6670
Iteration: 43; Percent complete: 1.1%; Average loss: 4.8148
Iteration: 44; Percent complete: 1.1%; Average loss: 4.6093
Iteration: 45; Percent complete: 1.1%; Average loss: 4.6322
Iteration: 46; Percent complete: 1.1%; Average loss: 4.5139
Iteration: 47; Percent complete: 1.2%; Average loss: 4.6834
Iteration: 48; Percent complete: 1.2%; Average loss: 4.5035
Iteration: 49; Percent complete: 1.2%; Average loss: 4.8119
Iteration: 50; Percent complete: 1.2%; Average loss: 4.5661
Iteration: 51; Percent complete: 1.3%; Average loss: 4.8288
Iteration: 52; Percent complete: 1.3%; Average loss: 4.5483
Iteration: 53; Percent complete: 1.3%; Average loss: 4.6328
Iteration: 54; Percent complete: 1.4%; Average loss: 4.5040
Iteration: 55; Percent complete: 1.4%; Average loss: 4.6982
Iteration: 56; Percent complete: 1.4%; Average loss: 4.5518
Iteration: 57; Percent complete: 1.4%; Average loss: 4.5344
Iteration: 58; Percent complete: 1.5%; Average loss: 4.5677
Iteration: 59; Percent complete: 1.5%; Average loss: 4.7938
Iteration: 60; Percent complete: 1.5%; Average loss: 4.5126
Iteration: 61; Percent complete: 1.5%; Average loss: 4.4967
Iteration: 62; Percent complete: 1.6%; Average loss: 4.6333
Iteration: 63; Percent complete: 1.6%; Average loss: 4.5888
Iteration: 64; Percent complete: 1.6%; Average loss: 4.3075
Iteration: 65; Percent complete: 1.6%; Average loss: 4.5133
Iteration: 66; Percent complete: 1.7%; Average loss: 4.5084
Iteration: 67; Percent complete: 1.7%; Average loss: 4.8161
Iteration: 68; Percent complete: 1.7%; Average loss: 4.5948
Iteration: 69; Percent complete: 1.7%; Average loss: 4.6278
Iteration: 70; Percent complete: 1.8%; Average loss: 4.4582
Iteration: 71; Percent complete: 1.8%; Average loss: 4.4484
Iteration: 72; Percent complete: 1.8%; Average loss: 4.4757
Iteration: 73; Percent complete: 1.8%; Average loss: 4.5937
Iteration: 74; Percent complete: 1.8%; Average loss: 4.3474
Iteration: 75; Percent complete: 1.9%; Average loss: 4.4921
Iteration: 76; Percent complete: 1.9%; Average loss: 4.6886
Iteration: 77; Percent complete: 1.9%; Average loss: 4.3638
Iteration: 78; Percent complete: 1.9%; Average loss: 4.6839
Iteration: 79; Percent complete: 2.0%; Average loss: 4.4069
Iteration: 80; Percent complete: 2.0%; Average loss: 4.4496
Iteration: 81; Percent complete: 2.0%; Average loss: 4.5592
Iteration: 82; Percent complete: 2.1%; Average loss: 4.7000
Iteration: 83; Percent complete: 2.1%; Average loss: 4.3022
Iteration: 84; Percent complete: 2.1%; Average loss: 4.5013
Iteration: 85; Percent complete: 2.1%; Average loss: 4.3784
Iteration: 86; Percent complete: 2.1%; Average loss: 4.5025
Iteration: 87; Percent complete: 2.2%; Average loss: 4.5840
Iteration: 88; Percent complete: 2.2%; Average loss: 4.4873
Iteration: 89; Percent complete: 2.2%; Average loss: 4.2982
Iteration: 90; Percent complete: 2.2%; Average loss: 4.3686
Iteration: 91; Percent complete: 2.3%; Average loss: 4.3155
Iteration: 92; Percent complete: 2.3%; Average loss: 4.2115
Iteration: 93; Percent complete: 2.3%; Average loss: 4.2979
Iteration: 94; Percent complete: 2.4%; Average loss: 4.3565
Iteration: 95; Percent complete: 2.4%; Average loss: 4.3283
Iteration: 96; Percent complete: 2.4%; Average loss: 4.4177
Iteration: 97; Percent complete: 2.4%; Average loss: 4.3689
Iteration: 98; Percent complete: 2.5%; Average loss: 4.4567
Iteration: 99; Percent complete: 2.5%; Average loss: 4.3242
Iteration: 100; Percent complete: 2.5%; Average loss: 4.4249
Iteration: 101; Percent complete: 2.5%; Average loss: 4.1899
Iteration: 102; Percent complete: 2.5%; Average loss: 4.5148
Iteration: 103; Percent complete: 2.6%; Average loss: 4.2772
Iteration: 104; Percent complete: 2.6%; Average loss: 4.2775
Iteration: 105; Percent complete: 2.6%; Average loss: 4.5088
Iteration: 106; Percent complete: 2.6%; Average loss: 4.5121
Iteration: 107; Percent complete: 2.7%; Average loss: 4.3327
Iteration: 108; Percent complete: 2.7%; Average loss: 4.5252
Iteration: 109; Percent complete: 2.7%; Average loss: 4.1831
Iteration: 110; Percent complete: 2.8%; Average loss: 4.1833
Iteration: 111; Percent complete: 2.8%; Average loss: 4.3274
Iteration: 112; Percent complete: 2.8%; Average loss: 4.5191
Iteration: 113; Percent complete: 2.8%; Average loss: 4.3452
Iteration: 114; Percent complete: 2.9%; Average loss: 4.4353
Iteration: 115; Percent complete: 2.9%; Average loss: 4.3186
Iteration: 116; Percent complete: 2.9%; Average loss: 4.3743
Iteration: 117; Percent complete: 2.9%; Average loss: 4.3036
Iteration: 118; Percent complete: 2.9%; Average loss: 4.4618
Iteration: 119; Percent complete: 3.0%; Average loss: 4.4117
Iteration: 120; Percent complete: 3.0%; Average loss: 4.1575
Iteration: 121; Percent complete: 3.0%; Average loss: 4.3188
Iteration: 122; Percent complete: 3.0%; Average loss: 4.4365
Iteration: 123; Percent complete: 3.1%; Average loss: 4.1238
Iteration: 124; Percent complete: 3.1%; Average loss: 4.3645
Iteration: 125; Percent complete: 3.1%; Average loss: 4.2155
Iteration: 126; Percent complete: 3.1%; Average loss: 4.7236
Iteration: 127; Percent complete: 3.2%; Average loss: 4.1829
Iteration: 128; Percent complete: 3.2%; Average loss: 4.2557
Iteration: 129; Percent complete: 3.2%; Average loss: 4.4819
Iteration: 130; Percent complete: 3.2%; Average loss: 4.4005
Iteration: 131; Percent complete: 3.3%; Average loss: 4.2772
Iteration: 132; Percent complete: 3.3%; Average loss: 4.2948
Iteration: 133; Percent complete: 3.3%; Average loss: 4.2153
Iteration: 134; Percent complete: 3.4%; Average loss: 4.3869
Iteration: 135; Percent complete: 3.4%; Average loss: 4.3561
Iteration: 136; Percent complete: 3.4%; Average loss: 4.2666
Iteration: 137; Percent complete: 3.4%; Average loss: 4.2705
Iteration: 138; Percent complete: 3.5%; Average loss: 4.3425
Iteration: 139; Percent complete: 3.5%; Average loss: 4.3308
Iteration: 140; Percent complete: 3.5%; Average loss: 4.2482
Iteration: 141; Percent complete: 3.5%; Average loss: 4.2002
Iteration: 142; Percent complete: 3.5%; Average loss: 4.0395
Iteration: 143; Percent complete: 3.6%; Average loss: 4.4818
Iteration: 144; Percent complete: 3.6%; Average loss: 4.0831
Iteration: 145; Percent complete: 3.6%; Average loss: 4.1008
Iteration: 146; Percent complete: 3.6%; Average loss: 4.2408
Iteration: 147; Percent complete: 3.7%; Average loss: 4.1107
Iteration: 148; Percent complete: 3.7%; Average loss: 4.3374
Iteration: 149; Percent complete: 3.7%; Average loss: 4.0637
Iteration: 150; Percent complete: 3.8%; Average loss: 4.2662
Iteration: 151; Percent complete: 3.8%; Average loss: 4.2423
Iteration: 152; Percent complete: 3.8%; Average loss: 4.1396
Iteration: 153; Percent complete: 3.8%; Average loss: 4.2338
Iteration: 154; Percent complete: 3.9%; Average loss: 4.1130
Iteration: 155; Percent complete: 3.9%; Average loss: 4.2585
Iteration: 156; Percent complete: 3.9%; Average loss: 4.2221
Iteration: 157; Percent complete: 3.9%; Average loss: 4.1251
Iteration: 158; Percent complete: 4.0%; Average loss: 4.2255
Iteration: 159; Percent complete: 4.0%; Average loss: 4.3224
Iteration: 160; Percent complete: 4.0%; Average loss: 4.0658
Iteration: 161; Percent complete: 4.0%; Average loss: 4.2790
Iteration: 162; Percent complete: 4.0%; Average loss: 4.1332
Iteration: 163; Percent complete: 4.1%; Average loss: 3.9183
Iteration: 164; Percent complete: 4.1%; Average loss: 4.0841
Iteration: 165; Percent complete: 4.1%; Average loss: 3.7811
Iteration: 166; Percent complete: 4.2%; Average loss: 4.2144
Iteration: 167; Percent complete: 4.2%; Average loss: 3.9774
Iteration: 168; Percent complete: 4.2%; Average loss: 4.1725
Iteration: 169; Percent complete: 4.2%; Average loss: 4.2217
Iteration: 170; Percent complete: 4.2%; Average loss: 4.2590
Iteration: 171; Percent complete: 4.3%; Average loss: 4.2249
Iteration: 172; Percent complete: 4.3%; Average loss: 4.1298
Iteration: 173; Percent complete: 4.3%; Average loss: 4.2693
Iteration: 174; Percent complete: 4.3%; Average loss: 4.0924
Iteration: 175; Percent complete: 4.4%; Average loss: 3.8366
Iteration: 176; Percent complete: 4.4%; Average loss: 4.0131
Iteration: 177; Percent complete: 4.4%; Average loss: 4.0707
Iteration: 178; Percent complete: 4.5%; Average loss: 4.3873
Iteration: 179; Percent complete: 4.5%; Average loss: 3.9353
Iteration: 180; Percent complete: 4.5%; Average loss: 4.1841
Iteration: 181; Percent complete: 4.5%; Average loss: 3.9691
Iteration: 182; Percent complete: 4.5%; Average loss: 4.2445
Iteration: 183; Percent complete: 4.6%; Average loss: 3.9909
Iteration: 184; Percent complete: 4.6%; Average loss: 4.2100
Iteration: 185; Percent complete: 4.6%; Average loss: 4.0717
Iteration: 186; Percent complete: 4.7%; Average loss: 4.1351
Iteration: 187; Percent complete: 4.7%; Average loss: 3.9354
Iteration: 188; Percent complete: 4.7%; Average loss: 3.8134
Iteration: 189; Percent complete: 4.7%; Average loss: 4.3966
Iteration: 190; Percent complete: 4.8%; Average loss: 4.0509
Iteration: 191; Percent complete: 4.8%; Average loss: 3.9912
Iteration: 192; Percent complete: 4.8%; Average loss: 3.9772
Iteration: 193; Percent complete: 4.8%; Average loss: 3.9212
Iteration: 194; Percent complete: 4.9%; Average loss: 4.0303
Iteration: 195; Percent complete: 4.9%; Average loss: 4.1305
Iteration: 196; Percent complete: 4.9%; Average loss: 4.3996
Iteration: 197; Percent complete: 4.9%; Average loss: 4.3042
Iteration: 198; Percent complete: 5.0%; Average loss: 4.3489
Iteration: 199; Percent complete: 5.0%; Average loss: 4.2896
Iteration: 200; Percent complete: 5.0%; Average loss: 3.8091
Iteration: 201; Percent complete: 5.0%; Average loss: 4.0971
Iteration: 202; Percent complete: 5.1%; Average loss: 3.9714
Iteration: 203; Percent complete: 5.1%; Average loss: 4.0337
Iteration: 204; Percent complete: 5.1%; Average loss: 4.1092
Iteration: 205; Percent complete: 5.1%; Average loss: 4.0293
Iteration: 206; Percent complete: 5.1%; Average loss: 4.2051
Iteration: 207; Percent complete: 5.2%; Average loss: 4.1471
Iteration: 208; Percent complete: 5.2%; Average loss: 4.2239
Iteration: 209; Percent complete: 5.2%; Average loss: 3.8455
Iteration: 210; Percent complete: 5.2%; Average loss: 3.9024
Iteration: 211; Percent complete: 5.3%; Average loss: 4.2336
Iteration: 212; Percent complete: 5.3%; Average loss: 4.0724
Iteration: 213; Percent complete: 5.3%; Average loss: 4.0936
Iteration: 214; Percent complete: 5.3%; Average loss: 3.9975
Iteration: 215; Percent complete: 5.4%; Average loss: 3.9113
Iteration: 216; Percent complete: 5.4%; Average loss: 4.0042
Iteration: 217; Percent complete: 5.4%; Average loss: 4.0473
Iteration: 218; Percent complete: 5.5%; Average loss: 3.9720
Iteration: 219; Percent complete: 5.5%; Average loss: 3.8636
Iteration: 220; Percent complete: 5.5%; Average loss: 3.7938
Iteration: 221; Percent complete: 5.5%; Average loss: 3.9970
Iteration: 222; Percent complete: 5.5%; Average loss: 3.9214
Iteration: 223; Percent complete: 5.6%; Average loss: 3.9747
Iteration: 224; Percent complete: 5.6%; Average loss: 4.4310
Iteration: 225; Percent complete: 5.6%; Average loss: 4.2264
Iteration: 226; Percent complete: 5.7%; Average loss: 4.1461
Iteration: 227; Percent complete: 5.7%; Average loss: 3.8592
Iteration: 228; Percent complete: 5.7%; Average loss: 3.8012
Iteration: 229; Percent complete: 5.7%; Average loss: 4.0243
Iteration: 230; Percent complete: 5.8%; Average loss: 4.1253
Iteration: 231; Percent complete: 5.8%; Average loss: 4.0399
Iteration: 232; Percent complete: 5.8%; Average loss: 3.8528
Iteration: 233; Percent complete: 5.8%; Average loss: 4.2015
Iteration: 234; Percent complete: 5.9%; Average loss: 3.8354
Iteration: 235; Percent complete: 5.9%; Average loss: 4.0100
Iteration: 236; Percent complete: 5.9%; Average loss: 4.1361
Iteration: 237; Percent complete: 5.9%; Average loss: 4.0682
Iteration: 238; Percent complete: 5.9%; Average loss: 3.8058
Iteration: 239; Percent complete: 6.0%; Average loss: 4.0169
Iteration: 240; Percent complete: 6.0%; Average loss: 3.8472
Iteration: 241; Percent complete: 6.0%; Average loss: 4.0268
Iteration: 242; Percent complete: 6.0%; Average loss: 3.8856
Iteration: 243; Percent complete: 6.1%; Average loss: 3.9712
Iteration: 244; Percent complete: 6.1%; Average loss: 4.0298
Iteration: 245; Percent complete: 6.1%; Average loss: 3.9250
Iteration: 246; Percent complete: 6.2%; Average loss: 4.0529
Iteration: 247; Percent complete: 6.2%; Average loss: 4.0036
Iteration: 248; Percent complete: 6.2%; Average loss: 4.2044
Iteration: 249; Percent complete: 6.2%; Average loss: 4.1692
Iteration: 250; Percent complete: 6.2%; Average loss: 3.9902
Iteration: 251; Percent complete: 6.3%; Average loss: 3.9847
Iteration: 252; Percent complete: 6.3%; Average loss: 3.9312
Iteration: 253; Percent complete: 6.3%; Average loss: 4.0968
Iteration: 254; Percent complete: 6.3%; Average loss: 3.7440
Iteration: 255; Percent complete: 6.4%; Average loss: 3.7814
Iteration: 256; Percent complete: 6.4%; Average loss: 3.8778
Iteration: 257; Percent complete: 6.4%; Average loss: 3.9462
Iteration: 258; Percent complete: 6.5%; Average loss: 3.8670
Iteration: 259; Percent complete: 6.5%; Average loss: 3.9583
Iteration: 260; Percent complete: 6.5%; Average loss: 3.8852
Iteration: 261; Percent complete: 6.5%; Average loss: 3.9413
Iteration: 262; Percent complete: 6.6%; Average loss: 4.1769
Iteration: 263; Percent complete: 6.6%; Average loss: 3.9672
Iteration: 264; Percent complete: 6.6%; Average loss: 4.0755
Iteration: 265; Percent complete: 6.6%; Average loss: 3.6813
Iteration: 266; Percent complete: 6.7%; Average loss: 3.6722
Iteration: 267; Percent complete: 6.7%; Average loss: 4.1432
Iteration: 268; Percent complete: 6.7%; Average loss: 4.3056
Iteration: 269; Percent complete: 6.7%; Average loss: 4.2333
Iteration: 270; Percent complete: 6.8%; Average loss: 3.9264
Iteration: 271; Percent complete: 6.8%; Average loss: 4.1835
Iteration: 272; Percent complete: 6.8%; Average loss: 4.2209
Iteration: 273; Percent complete: 6.8%; Average loss: 3.8886
Iteration: 274; Percent complete: 6.9%; Average loss: 3.9627
Iteration: 275; Percent complete: 6.9%; Average loss: 4.0668
Iteration: 276; Percent complete: 6.9%; Average loss: 4.1140
Iteration: 277; Percent complete: 6.9%; Average loss: 3.9334
Iteration: 278; Percent complete: 7.0%; Average loss: 3.9210
Iteration: 279; Percent complete: 7.0%; Average loss: 3.9074
Iteration: 280; Percent complete: 7.0%; Average loss: 3.6459
Iteration: 281; Percent complete: 7.0%; Average loss: 3.4367
Iteration: 282; Percent complete: 7.0%; Average loss: 4.0782
Iteration: 283; Percent complete: 7.1%; Average loss: 3.8470
Iteration: 284; Percent complete: 7.1%; Average loss: 3.8382
Iteration: 285; Percent complete: 7.1%; Average loss: 4.2122
Iteration: 286; Percent complete: 7.1%; Average loss: 4.0994
Iteration: 287; Percent complete: 7.2%; Average loss: 3.8104
Iteration: 288; Percent complete: 7.2%; Average loss: 4.0726
Iteration: 289; Percent complete: 7.2%; Average loss: 3.9776
Iteration: 290; Percent complete: 7.2%; Average loss: 3.9142
Iteration: 291; Percent complete: 7.3%; Average loss: 3.9161
Iteration: 292; Percent complete: 7.3%; Average loss: 4.1642
Iteration: 293; Percent complete: 7.3%; Average loss: 4.1852
Iteration: 294; Percent complete: 7.3%; Average loss: 3.9292
Iteration: 295; Percent complete: 7.4%; Average loss: 4.2290
Iteration: 296; Percent complete: 7.4%; Average loss: 3.6722
Iteration: 297; Percent complete: 7.4%; Average loss: 3.9260
Iteration: 298; Percent complete: 7.4%; Average loss: 3.8653
Iteration: 299; Percent complete: 7.5%; Average loss: 3.9611
Iteration: 300; Percent complete: 7.5%; Average loss: 4.0588
Iteration: 301; Percent complete: 7.5%; Average loss: 3.8498
Iteration: 302; Percent complete: 7.5%; Average loss: 3.8037
Iteration: 303; Percent complete: 7.6%; Average loss: 4.0109
Iteration: 304; Percent complete: 7.6%; Average loss: 3.7357
Iteration: 305; Percent complete: 7.6%; Average loss: 4.0324
Iteration: 306; Percent complete: 7.6%; Average loss: 3.7437
Iteration: 307; Percent complete: 7.7%; Average loss: 3.9824
Iteration: 308; Percent complete: 7.7%; Average loss: 4.0195
Iteration: 309; Percent complete: 7.7%; Average loss: 3.6357
Iteration: 310; Percent complete: 7.8%; Average loss: 4.2062
Iteration: 311; Percent complete: 7.8%; Average loss: 3.9330
Iteration: 312; Percent complete: 7.8%; Average loss: 4.0195
Iteration: 313; Percent complete: 7.8%; Average loss: 3.8258
Iteration: 314; Percent complete: 7.8%; Average loss: 3.6191
Iteration: 315; Percent complete: 7.9%; Average loss: 3.7524
Iteration: 316; Percent complete: 7.9%; Average loss: 3.7817
Iteration: 317; Percent complete: 7.9%; Average loss: 3.7227
Iteration: 318; Percent complete: 8.0%; Average loss: 3.6559
Iteration: 319; Percent complete: 8.0%; Average loss: 3.9009
Iteration: 320; Percent complete: 8.0%; Average loss: 4.0995
Iteration: 321; Percent complete: 8.0%; Average loss: 3.9927
Iteration: 322; Percent complete: 8.1%; Average loss: 4.1229
Iteration: 323; Percent complete: 8.1%; Average loss: 3.8154
Iteration: 324; Percent complete: 8.1%; Average loss: 4.0058
Iteration: 325; Percent complete: 8.1%; Average loss: 3.5169
Iteration: 326; Percent complete: 8.2%; Average loss: 3.8350
Iteration: 327; Percent complete: 8.2%; Average loss: 3.8259
Iteration: 328; Percent complete: 8.2%; Average loss: 3.9748
Iteration: 329; Percent complete: 8.2%; Average loss: 3.7931
Iteration: 330; Percent complete: 8.2%; Average loss: 3.8386
Iteration: 331; Percent complete: 8.3%; Average loss: 3.8571
Iteration: 332; Percent complete: 8.3%; Average loss: 3.8165
Iteration: 333; Percent complete: 8.3%; Average loss: 3.8858
Iteration: 334; Percent complete: 8.3%; Average loss: 3.6886
Iteration: 335; Percent complete: 8.4%; Average loss: 3.6719
Iteration: 336; Percent complete: 8.4%; Average loss: 3.7104
Iteration: 337; Percent complete: 8.4%; Average loss: 3.5886
Iteration: 338; Percent complete: 8.5%; Average loss: 3.7231
Iteration: 339; Percent complete: 8.5%; Average loss: 3.8111
Iteration: 340; Percent complete: 8.5%; Average loss: 4.0626
Iteration: 341; Percent complete: 8.5%; Average loss: 3.8089
Iteration: 342; Percent complete: 8.6%; Average loss: 3.9092
Iteration: 343; Percent complete: 8.6%; Average loss: 3.8564
Iteration: 344; Percent complete: 8.6%; Average loss: 3.7870
Iteration: 345; Percent complete: 8.6%; Average loss: 4.0120
Iteration: 346; Percent complete: 8.6%; Average loss: 3.7148
Iteration: 347; Percent complete: 8.7%; Average loss: 3.8811
Iteration: 348; Percent complete: 8.7%; Average loss: 3.8372
Iteration: 349; Percent complete: 8.7%; Average loss: 3.7754
Iteration: 350; Percent complete: 8.8%; Average loss: 4.1021
Iteration: 351; Percent complete: 8.8%; Average loss: 3.9062
Iteration: 352; Percent complete: 8.8%; Average loss: 3.7697
Iteration: 353; Percent complete: 8.8%; Average loss: 3.6772
Iteration: 354; Percent complete: 8.8%; Average loss: 4.0363
Iteration: 355; Percent complete: 8.9%; Average loss: 3.9751
Iteration: 356; Percent complete: 8.9%; Average loss: 3.7483
Iteration: 357; Percent complete: 8.9%; Average loss: 3.9562
Iteration: 358; Percent complete: 8.9%; Average loss: 3.8029
Iteration: 359; Percent complete: 9.0%; Average loss: 3.9083
Iteration: 360; Percent complete: 9.0%; Average loss: 3.8005
Iteration: 361; Percent complete: 9.0%; Average loss: 3.8683
Iteration: 362; Percent complete: 9.0%; Average loss: 3.8801
Iteration: 363; Percent complete: 9.1%; Average loss: 3.8870
Iteration: 364; Percent complete: 9.1%; Average loss: 3.8691
Iteration: 365; Percent complete: 9.1%; Average loss: 3.6718
Iteration: 366; Percent complete: 9.2%; Average loss: 4.0133
Iteration: 367; Percent complete: 9.2%; Average loss: 3.6297
Iteration: 368; Percent complete: 9.2%; Average loss: 3.8958
Iteration: 369; Percent complete: 9.2%; Average loss: 3.7986
Iteration: 370; Percent complete: 9.2%; Average loss: 3.7870
Iteration: 371; Percent complete: 9.3%; Average loss: 3.9622
Iteration: 372; Percent complete: 9.3%; Average loss: 3.8877
Iteration: 373; Percent complete: 9.3%; Average loss: 3.7196
Iteration: 374; Percent complete: 9.3%; Average loss: 3.7833
Iteration: 375; Percent complete: 9.4%; Average loss: 3.9449
Iteration: 376; Percent complete: 9.4%; Average loss: 3.9530
Iteration: 377; Percent complete: 9.4%; Average loss: 3.4805
Iteration: 378; Percent complete: 9.4%; Average loss: 3.6414
Iteration: 379; Percent complete: 9.5%; Average loss: 3.6594
Iteration: 380; Percent complete: 9.5%; Average loss: 3.7621
Iteration: 381; Percent complete: 9.5%; Average loss: 3.9670
Iteration: 382; Percent complete: 9.6%; Average loss: 3.5196
Iteration: 383; Percent complete: 9.6%; Average loss: 3.5022
Iteration: 384; Percent complete: 9.6%; Average loss: 3.9580
Iteration: 385; Percent complete: 9.6%; Average loss: 3.7315
Iteration: 386; Percent complete: 9.7%; Average loss: 4.0758
Iteration: 387; Percent complete: 9.7%; Average loss: 3.8051
Iteration: 388; Percent complete: 9.7%; Average loss: 3.8562
Iteration: 389; Percent complete: 9.7%; Average loss: 3.7532
Iteration: 390; Percent complete: 9.8%; Average loss: 4.1445
Iteration: 391; Percent complete: 9.8%; Average loss: 3.9031
Iteration: 392; Percent complete: 9.8%; Average loss: 3.6054
Iteration: 393; Percent complete: 9.8%; Average loss: 4.0023
Iteration: 394; Percent complete: 9.8%; Average loss: 3.8720
Iteration: 395; Percent complete: 9.9%; Average loss: 3.8136
Iteration: 396; Percent complete: 9.9%; Average loss: 3.7782
Iteration: 397; Percent complete: 9.9%; Average loss: 3.8818
Iteration: 398; Percent complete: 10.0%; Average loss: 3.6830
Iteration: 399; Percent complete: 10.0%; Average loss: 3.6042
Iteration: 400; Percent complete: 10.0%; Average loss: 3.6033
Iteration: 401; Percent complete: 10.0%; Average loss: 4.0543
Iteration: 402; Percent complete: 10.1%; Average loss: 3.8603
Iteration: 403; Percent complete: 10.1%; Average loss: 3.7808
Iteration: 404; Percent complete: 10.1%; Average loss: 4.1854
Iteration: 405; Percent complete: 10.1%; Average loss: 3.7163
Iteration: 406; Percent complete: 10.2%; Average loss: 3.7211
Iteration: 407; Percent complete: 10.2%; Average loss: 4.0517
Iteration: 408; Percent complete: 10.2%; Average loss: 3.9546
Iteration: 409; Percent complete: 10.2%; Average loss: 3.7168
Iteration: 410; Percent complete: 10.2%; Average loss: 3.8187
Iteration: 411; Percent complete: 10.3%; Average loss: 3.7692
Iteration: 412; Percent complete: 10.3%; Average loss: 3.7195
Iteration: 413; Percent complete: 10.3%; Average loss: 3.8198
Iteration: 414; Percent complete: 10.3%; Average loss: 3.9124
Iteration: 415; Percent complete: 10.4%; Average loss: 4.0625
Iteration: 416; Percent complete: 10.4%; Average loss: 3.7785
Iteration: 417; Percent complete: 10.4%; Average loss: 3.5422
Iteration: 418; Percent complete: 10.4%; Average loss: 3.5326
Iteration: 419; Percent complete: 10.5%; Average loss: 3.7042
Iteration: 420; Percent complete: 10.5%; Average loss: 4.1719
Iteration: 421; Percent complete: 10.5%; Average loss: 3.8295
Iteration: 422; Percent complete: 10.5%; Average loss: 3.8512
Iteration: 423; Percent complete: 10.6%; Average loss: 3.9442
Iteration: 424; Percent complete: 10.6%; Average loss: 3.8014
Iteration: 425; Percent complete: 10.6%; Average loss: 3.6830
Iteration: 426; Percent complete: 10.7%; Average loss: 3.8851
Iteration: 427; Percent complete: 10.7%; Average loss: 3.4497
Iteration: 428; Percent complete: 10.7%; Average loss: 3.8967
Iteration: 429; Percent complete: 10.7%; Average loss: 3.8292
Iteration: 430; Percent complete: 10.8%; Average loss: 4.0449
Iteration: 431; Percent complete: 10.8%; Average loss: 3.7376
Iteration: 432; Percent complete: 10.8%; Average loss: 3.8036
Iteration: 433; Percent complete: 10.8%; Average loss: 4.0134
Iteration: 434; Percent complete: 10.8%; Average loss: 3.7928
Iteration: 435; Percent complete: 10.9%; Average loss: 3.7534
Iteration: 436; Percent complete: 10.9%; Average loss: 3.6854
Iteration: 437; Percent complete: 10.9%; Average loss: 3.6860
Iteration: 438; Percent complete: 10.9%; Average loss: 3.6491
Iteration: 439; Percent complete: 11.0%; Average loss: 3.6968
Iteration: 440; Percent complete: 11.0%; Average loss: 3.7408
Iteration: 441; Percent complete: 11.0%; Average loss: 4.0733
Iteration: 442; Percent complete: 11.1%; Average loss: 4.0408
Iteration: 443; Percent complete: 11.1%; Average loss: 3.7522
Iteration: 444; Percent complete: 11.1%; Average loss: 3.7933
Iteration: 445; Percent complete: 11.1%; Average loss: 3.8746
Iteration: 446; Percent complete: 11.2%; Average loss: 3.7394
Iteration: 447; Percent complete: 11.2%; Average loss: 3.6027
Iteration: 448; Percent complete: 11.2%; Average loss: 3.6644
Iteration: 449; Percent complete: 11.2%; Average loss: 3.7040
Iteration: 450; Percent complete: 11.2%; Average loss: 3.6673
Iteration: 451; Percent complete: 11.3%; Average loss: 3.8281
Iteration: 452; Percent complete: 11.3%; Average loss: 3.7541
Iteration: 453; Percent complete: 11.3%; Average loss: 3.6738
Iteration: 454; Percent complete: 11.3%; Average loss: 3.6599
Iteration: 455; Percent complete: 11.4%; Average loss: 3.7282
Iteration: 456; Percent complete: 11.4%; Average loss: 3.5735
Iteration: 457; Percent complete: 11.4%; Average loss: 3.7592
Iteration: 458; Percent complete: 11.5%; Average loss: 3.7065
Iteration: 459; Percent complete: 11.5%; Average loss: 3.8106
Iteration: 460; Percent complete: 11.5%; Average loss: 3.5145
Iteration: 461; Percent complete: 11.5%; Average loss: 3.5765
Iteration: 462; Percent complete: 11.6%; Average loss: 3.8841
Iteration: 463; Percent complete: 11.6%; Average loss: 3.8804
Iteration: 464; Percent complete: 11.6%; Average loss: 3.7238
Iteration: 465; Percent complete: 11.6%; Average loss: 4.0917
Iteration: 466; Percent complete: 11.7%; Average loss: 3.6467
Iteration: 467; Percent complete: 11.7%; Average loss: 3.7112
Iteration: 468; Percent complete: 11.7%; Average loss: 3.9512
Iteration: 469; Percent complete: 11.7%; Average loss: 3.6835
Iteration: 470; Percent complete: 11.8%; Average loss: 3.4151
Iteration: 471; Percent complete: 11.8%; Average loss: 3.7216
Iteration: 472; Percent complete: 11.8%; Average loss: 3.7019
Iteration: 473; Percent complete: 11.8%; Average loss: 3.6703
Iteration: 474; Percent complete: 11.8%; Average loss: 3.9414
Iteration: 475; Percent complete: 11.9%; Average loss: 3.7452
Iteration: 476; Percent complete: 11.9%; Average loss: 3.9532
Iteration: 477; Percent complete: 11.9%; Average loss: 3.8105
Iteration: 478; Percent complete: 11.9%; Average loss: 3.8009
Iteration: 479; Percent complete: 12.0%; Average loss: 3.7122
Iteration: 480; Percent complete: 12.0%; Average loss: 3.7430
Iteration: 481; Percent complete: 12.0%; Average loss: 3.8451
Iteration: 482; Percent complete: 12.0%; Average loss: 3.7796
Iteration: 483; Percent complete: 12.1%; Average loss: 3.6644
Iteration: 484; Percent complete: 12.1%; Average loss: 3.7532
Iteration: 485; Percent complete: 12.1%; Average loss: 3.8663
Iteration: 486; Percent complete: 12.2%; Average loss: 3.5834
Iteration: 487; Percent complete: 12.2%; Average loss: 3.9960
Iteration: 488; Percent complete: 12.2%; Average loss: 3.6028
Iteration: 489; Percent complete: 12.2%; Average loss: 3.9242
Iteration: 490; Percent complete: 12.2%; Average loss: 3.5886
Iteration: 491; Percent complete: 12.3%; Average loss: 3.7678
Iteration: 492; Percent complete: 12.3%; Average loss: 3.7378
Iteration: 493; Percent complete: 12.3%; Average loss: 3.6492
Iteration: 494; Percent complete: 12.3%; Average loss: 3.7200
Iteration: 495; Percent complete: 12.4%; Average loss: 3.7285
Iteration: 496; Percent complete: 12.4%; Average loss: 3.6411
Iteration: 497; Percent complete: 12.4%; Average loss: 3.7766
Iteration: 498; Percent complete: 12.4%; Average loss: 3.5712
Iteration: 499; Percent complete: 12.5%; Average loss: 3.6934
Iteration: 500; Percent complete: 12.5%; Average loss: 3.4050
Iteration: 501; Percent complete: 12.5%; Average loss: 3.7637
Iteration: 502; Percent complete: 12.6%; Average loss: 3.6943
Iteration: 503; Percent complete: 12.6%; Average loss: 4.0403
Iteration: 504; Percent complete: 12.6%; Average loss: 3.8772
Iteration: 505; Percent complete: 12.6%; Average loss: 3.6806
Iteration: 506; Percent complete: 12.7%; Average loss: 3.9077
Iteration: 507; Percent complete: 12.7%; Average loss: 3.8560
Iteration: 508; Percent complete: 12.7%; Average loss: 3.7142
Iteration: 509; Percent complete: 12.7%; Average loss: 3.7543
Iteration: 510; Percent complete: 12.8%; Average loss: 3.8389
Iteration: 511; Percent complete: 12.8%; Average loss: 3.7334
Iteration: 512; Percent complete: 12.8%; Average loss: 3.7529
Iteration: 513; Percent complete: 12.8%; Average loss: 3.7681
Iteration: 514; Percent complete: 12.8%; Average loss: 3.9264
Iteration: 515; Percent complete: 12.9%; Average loss: 3.9280
Iteration: 516; Percent complete: 12.9%; Average loss: 3.5613
Iteration: 517; Percent complete: 12.9%; Average loss: 3.7961
Iteration: 518; Percent complete: 13.0%; Average loss: 4.2622
Iteration: 519; Percent complete: 13.0%; Average loss: 3.7063
Iteration: 520; Percent complete: 13.0%; Average loss: 3.7242
Iteration: 521; Percent complete: 13.0%; Average loss: 3.6466
Iteration: 522; Percent complete: 13.1%; Average loss: 3.6605
Iteration: 523; Percent complete: 13.1%; Average loss: 3.6951
Iteration: 524; Percent complete: 13.1%; Average loss: 3.8780
Iteration: 525; Percent complete: 13.1%; Average loss: 3.6828
Iteration: 526; Percent complete: 13.2%; Average loss: 3.6776
Iteration: 527; Percent complete: 13.2%; Average loss: 3.6464
Iteration: 528; Percent complete: 13.2%; Average loss: 3.8845
Iteration: 529; Percent complete: 13.2%; Average loss: 3.5866
Iteration: 530; Percent complete: 13.2%; Average loss: 3.6426
Iteration: 531; Percent complete: 13.3%; Average loss: 3.5789
Iteration: 532; Percent complete: 13.3%; Average loss: 3.9215
Iteration: 533; Percent complete: 13.3%; Average loss: 3.7895
Iteration: 534; Percent complete: 13.4%; Average loss: 3.7361
Iteration: 535; Percent complete: 13.4%; Average loss: 3.5429
Iteration: 536; Percent complete: 13.4%; Average loss: 3.6919
Iteration: 537; Percent complete: 13.4%; Average loss: 3.6720
Iteration: 538; Percent complete: 13.5%; Average loss: 3.4389
Iteration: 539; Percent complete: 13.5%; Average loss: 3.6855
Iteration: 540; Percent complete: 13.5%; Average loss: 3.5914
Iteration: 541; Percent complete: 13.5%; Average loss: 3.6102
Iteration: 542; Percent complete: 13.6%; Average loss: 3.5880
Iteration: 543; Percent complete: 13.6%; Average loss: 3.6183
Iteration: 544; Percent complete: 13.6%; Average loss: 3.6859
Iteration: 545; Percent complete: 13.6%; Average loss: 3.7343
Iteration: 546; Percent complete: 13.7%; Average loss: 3.8276
Iteration: 547; Percent complete: 13.7%; Average loss: 3.8533
Iteration: 548; Percent complete: 13.7%; Average loss: 3.6370
Iteration: 549; Percent complete: 13.7%; Average loss: 3.7400
Iteration: 550; Percent complete: 13.8%; Average loss: 3.4673
Iteration: 551; Percent complete: 13.8%; Average loss: 3.6513
Iteration: 552; Percent complete: 13.8%; Average loss: 3.8294
Iteration: 553; Percent complete: 13.8%; Average loss: 3.8554
Iteration: 554; Percent complete: 13.9%; Average loss: 3.7860
Iteration: 555; Percent complete: 13.9%; Average loss: 3.7085
Iteration: 556; Percent complete: 13.9%; Average loss: 3.6445
Iteration: 557; Percent complete: 13.9%; Average loss: 3.5229
Iteration: 558; Percent complete: 14.0%; Average loss: 3.9394
Iteration: 559; Percent complete: 14.0%; Average loss: 3.8552
Iteration: 560; Percent complete: 14.0%; Average loss: 3.8065
Iteration: 561; Percent complete: 14.0%; Average loss: 3.7205
Iteration: 562; Percent complete: 14.1%; Average loss: 3.7204
Iteration: 563; Percent complete: 14.1%; Average loss: 3.6498
Iteration: 564; Percent complete: 14.1%; Average loss: 3.6531
Iteration: 565; Percent complete: 14.1%; Average loss: 3.6938
Iteration: 566; Percent complete: 14.1%; Average loss: 3.4877
Iteration: 567; Percent complete: 14.2%; Average loss: 3.7694
Iteration: 568; Percent complete: 14.2%; Average loss: 3.6035
Iteration: 569; Percent complete: 14.2%; Average loss: 3.6045
Iteration: 570; Percent complete: 14.2%; Average loss: 3.7176
Iteration: 571; Percent complete: 14.3%; Average loss: 3.6273
Iteration: 572; Percent complete: 14.3%; Average loss: 3.6440
Iteration: 573; Percent complete: 14.3%; Average loss: 3.5800
Iteration: 574; Percent complete: 14.3%; Average loss: 3.7222
Iteration: 575; Percent complete: 14.4%; Average loss: 3.4439
Iteration: 576; Percent complete: 14.4%; Average loss: 3.4557
Iteration: 577; Percent complete: 14.4%; Average loss: 3.8653
Iteration: 578; Percent complete: 14.4%; Average loss: 3.4434
Iteration: 579; Percent complete: 14.5%; Average loss: 4.0301
Iteration: 580; Percent complete: 14.5%; Average loss: 3.7929
Iteration: 581; Percent complete: 14.5%; Average loss: 3.6134
Iteration: 582; Percent complete: 14.5%; Average loss: 3.7934
Iteration: 583; Percent complete: 14.6%; Average loss: 3.5687
Iteration: 584; Percent complete: 14.6%; Average loss: 3.3886
Iteration: 585; Percent complete: 14.6%; Average loss: 3.8155
Iteration: 586; Percent complete: 14.6%; Average loss: 3.8307
Iteration: 587; Percent complete: 14.7%; Average loss: 3.5291
Iteration: 588; Percent complete: 14.7%; Average loss: 3.3265
Iteration: 589; Percent complete: 14.7%; Average loss: 3.4991
Iteration: 590; Percent complete: 14.8%; Average loss: 3.7146
Iteration: 591; Percent complete: 14.8%; Average loss: 3.8169
Iteration: 592; Percent complete: 14.8%; Average loss: 3.4924
Iteration: 593; Percent complete: 14.8%; Average loss: 3.4134
Iteration: 594; Percent complete: 14.8%; Average loss: 3.5661
Iteration: 595; Percent complete: 14.9%; Average loss: 3.6817
Iteration: 596; Percent complete: 14.9%; Average loss: 3.6530
Iteration: 597; Percent complete: 14.9%; Average loss: 3.3046
Iteration: 598; Percent complete: 14.9%; Average loss: 3.8228
Iteration: 599; Percent complete: 15.0%; Average loss: 3.9314
Iteration: 600; Percent complete: 15.0%; Average loss: 3.7030
Iteration: 601; Percent complete: 15.0%; Average loss: 3.5892
Iteration: 602; Percent complete: 15.0%; Average loss: 3.5931
Iteration: 603; Percent complete: 15.1%; Average loss: 3.7933
Iteration: 604; Percent complete: 15.1%; Average loss: 3.5734
Iteration: 605; Percent complete: 15.1%; Average loss: 3.5999
Iteration: 606; Percent complete: 15.2%; Average loss: 3.4313
Iteration: 607; Percent complete: 15.2%; Average loss: 3.6277
Iteration: 608; Percent complete: 15.2%; Average loss: 3.7649
Iteration: 609; Percent complete: 15.2%; Average loss: 3.6757
Iteration: 610; Percent complete: 15.2%; Average loss: 3.9065
Iteration: 611; Percent complete: 15.3%; Average loss: 3.5413
Iteration: 612; Percent complete: 15.3%; Average loss: 3.5871
Iteration: 613; Percent complete: 15.3%; Average loss: 3.8822
Iteration: 614; Percent complete: 15.3%; Average loss: 3.6300
Iteration: 615; Percent complete: 15.4%; Average loss: 3.8569
Iteration: 616; Percent complete: 15.4%; Average loss: 3.7766
Iteration: 617; Percent complete: 15.4%; Average loss: 3.7841
Iteration: 618; Percent complete: 15.4%; Average loss: 3.8145
Iteration: 619; Percent complete: 15.5%; Average loss: 3.8911
Iteration: 620; Percent complete: 15.5%; Average loss: 3.4458
Iteration: 621; Percent complete: 15.5%; Average loss: 3.3701
Iteration: 622; Percent complete: 15.6%; Average loss: 3.7175
Iteration: 623; Percent complete: 15.6%; Average loss: 3.5337
Iteration: 624; Percent complete: 15.6%; Average loss: 4.0521
Iteration: 625; Percent complete: 15.6%; Average loss: 3.8515
Iteration: 626; Percent complete: 15.7%; Average loss: 3.7054
Iteration: 627; Percent complete: 15.7%; Average loss: 3.5507
Iteration: 628; Percent complete: 15.7%; Average loss: 3.6639
Iteration: 629; Percent complete: 15.7%; Average loss: 3.5412
Iteration: 630; Percent complete: 15.8%; Average loss: 3.6610
Iteration: 631; Percent complete: 15.8%; Average loss: 3.6869
Iteration: 632; Percent complete: 15.8%; Average loss: 3.9102
Iteration: 633; Percent complete: 15.8%; Average loss: 3.4345
Iteration: 634; Percent complete: 15.8%; Average loss: 3.7326
Iteration: 635; Percent complete: 15.9%; Average loss: 3.8203
Iteration: 636; Percent complete: 15.9%; Average loss: 3.3605
Iteration: 637; Percent complete: 15.9%; Average loss: 3.8944
Iteration: 638; Percent complete: 16.0%; Average loss: 3.5699
Iteration: 639; Percent complete: 16.0%; Average loss: 3.5902
Iteration: 640; Percent complete: 16.0%; Average loss: 3.8047
Iteration: 641; Percent complete: 16.0%; Average loss: 3.4144
Iteration: 642; Percent complete: 16.1%; Average loss: 3.6899
Iteration: 643; Percent complete: 16.1%; Average loss: 3.8405
Iteration: 644; Percent complete: 16.1%; Average loss: 3.5504
Iteration: 645; Percent complete: 16.1%; Average loss: 3.4787
Iteration: 646; Percent complete: 16.2%; Average loss: 3.5888
Iteration: 647; Percent complete: 16.2%; Average loss: 3.5492
Iteration: 648; Percent complete: 16.2%; Average loss: 3.5877
Iteration: 649; Percent complete: 16.2%; Average loss: 3.8612
Iteration: 650; Percent complete: 16.2%; Average loss: 3.4469
Iteration: 651; Percent complete: 16.3%; Average loss: 3.5281
Iteration: 652; Percent complete: 16.3%; Average loss: 3.4904
Iteration: 653; Percent complete: 16.3%; Average loss: 3.5642
Iteration: 654; Percent complete: 16.4%; Average loss: 3.3360
Iteration: 655; Percent complete: 16.4%; Average loss: 4.0082
Iteration: 656; Percent complete: 16.4%; Average loss: 3.6184
Iteration: 657; Percent complete: 16.4%; Average loss: 3.6745
Iteration: 658; Percent complete: 16.4%; Average loss: 3.6890
Iteration: 659; Percent complete: 16.5%; Average loss: 3.6936
Iteration: 660; Percent complete: 16.5%; Average loss: 3.4461
Iteration: 661; Percent complete: 16.5%; Average loss: 3.4395
Iteration: 662; Percent complete: 16.6%; Average loss: 3.3765
Iteration: 663; Percent complete: 16.6%; Average loss: 3.6121
Iteration: 664; Percent complete: 16.6%; Average loss: 3.5725
Iteration: 665; Percent complete: 16.6%; Average loss: 3.5572
Iteration: 666; Percent complete: 16.7%; Average loss: 3.8594
Iteration: 667; Percent complete: 16.7%; Average loss: 3.4797
Iteration: 668; Percent complete: 16.7%; Average loss: 3.6888
Iteration: 669; Percent complete: 16.7%; Average loss: 3.6844
Iteration: 670; Percent complete: 16.8%; Average loss: 3.4451
Iteration: 671; Percent complete: 16.8%; Average loss: 3.7818
Iteration: 672; Percent complete: 16.8%; Average loss: 3.4658
Iteration: 673; Percent complete: 16.8%; Average loss: 3.2167
Iteration: 674; Percent complete: 16.9%; Average loss: 3.3791
Iteration: 675; Percent complete: 16.9%; Average loss: 3.5105
Iteration: 676; Percent complete: 16.9%; Average loss: 3.5353
Iteration: 677; Percent complete: 16.9%; Average loss: 3.4139
Iteration: 678; Percent complete: 17.0%; Average loss: 3.7559
Iteration: 679; Percent complete: 17.0%; Average loss: 3.9090
Iteration: 680; Percent complete: 17.0%; Average loss: 3.7739
Iteration: 681; Percent complete: 17.0%; Average loss: 3.5693
Iteration: 682; Percent complete: 17.1%; Average loss: 3.8804
Iteration: 683; Percent complete: 17.1%; Average loss: 3.6567
Iteration: 684; Percent complete: 17.1%; Average loss: 3.5583
Iteration: 685; Percent complete: 17.1%; Average loss: 3.5636
Iteration: 686; Percent complete: 17.2%; Average loss: 3.3426
Iteration: 687; Percent complete: 17.2%; Average loss: 3.6325
Iteration: 688; Percent complete: 17.2%; Average loss: 3.5755
Iteration: 689; Percent complete: 17.2%; Average loss: 3.9421
Iteration: 690; Percent complete: 17.2%; Average loss: 3.7110
Iteration: 691; Percent complete: 17.3%; Average loss: 3.6895
Iteration: 692; Percent complete: 17.3%; Average loss: 3.6401
Iteration: 693; Percent complete: 17.3%; Average loss: 3.9377
Iteration: 694; Percent complete: 17.3%; Average loss: 3.5453
Iteration: 695; Percent complete: 17.4%; Average loss: 3.7622
Iteration: 696; Percent complete: 17.4%; Average loss: 3.5246
Iteration: 697; Percent complete: 17.4%; Average loss: 3.6749
Iteration: 698; Percent complete: 17.4%; Average loss: 3.5956
Iteration: 699; Percent complete: 17.5%; Average loss: 3.6051
Iteration: 700; Percent complete: 17.5%; Average loss: 3.9431
Iteration: 701; Percent complete: 17.5%; Average loss: 3.5296
Iteration: 702; Percent complete: 17.5%; Average loss: 3.8589
Iteration: 703; Percent complete: 17.6%; Average loss: 3.3820
Iteration: 704; Percent complete: 17.6%; Average loss: 3.4485
Iteration: 705; Percent complete: 17.6%; Average loss: 3.3530
Iteration: 706; Percent complete: 17.6%; Average loss: 3.6531
Iteration: 707; Percent complete: 17.7%; Average loss: 3.4037
Iteration: 708; Percent complete: 17.7%; Average loss: 3.5056
Iteration: 709; Percent complete: 17.7%; Average loss: 3.7576
Iteration: 710; Percent complete: 17.8%; Average loss: 3.3733
Iteration: 711; Percent complete: 17.8%; Average loss: 3.5497
Iteration: 712; Percent complete: 17.8%; Average loss: 3.4299
Iteration: 713; Percent complete: 17.8%; Average loss: 3.6077
Iteration: 714; Percent complete: 17.8%; Average loss: 3.6160
Iteration: 715; Percent complete: 17.9%; Average loss: 3.6875
Iteration: 716; Percent complete: 17.9%; Average loss: 3.8424
Iteration: 717; Percent complete: 17.9%; Average loss: 3.5597
Iteration: 718; Percent complete: 17.9%; Average loss: 3.3719
Iteration: 719; Percent complete: 18.0%; Average loss: 3.7144
Iteration: 720; Percent complete: 18.0%; Average loss: 3.7321
Iteration: 721; Percent complete: 18.0%; Average loss: 3.7388
Iteration: 722; Percent complete: 18.1%; Average loss: 3.4843
Iteration: 723; Percent complete: 18.1%; Average loss: 3.4233
Iteration: 724; Percent complete: 18.1%; Average loss: 3.5898
Iteration: 725; Percent complete: 18.1%; Average loss: 3.8557
Iteration: 726; Percent complete: 18.1%; Average loss: 3.8385
Iteration: 727; Percent complete: 18.2%; Average loss: 3.7131
Iteration: 728; Percent complete: 18.2%; Average loss: 3.7178
Iteration: 729; Percent complete: 18.2%; Average loss: 4.1074
Iteration: 730; Percent complete: 18.2%; Average loss: 3.5278
Iteration: 731; Percent complete: 18.3%; Average loss: 3.7509
Iteration: 732; Percent complete: 18.3%; Average loss: 3.6628
Iteration: 733; Percent complete: 18.3%; Average loss: 3.5518
Iteration: 734; Percent complete: 18.4%; Average loss: 3.6094
Iteration: 735; Percent complete: 18.4%; Average loss: 3.5243
Iteration: 736; Percent complete: 18.4%; Average loss: 3.7603
Iteration: 737; Percent complete: 18.4%; Average loss: 3.3238
Iteration: 738; Percent complete: 18.4%; Average loss: 3.5140
Iteration: 739; Percent complete: 18.5%; Average loss: 3.5051
Iteration: 740; Percent complete: 18.5%; Average loss: 3.6614
Iteration: 741; Percent complete: 18.5%; Average loss: 3.7263
Iteration: 742; Percent complete: 18.6%; Average loss: 3.7878
Iteration: 743; Percent complete: 18.6%; Average loss: 3.7736
Iteration: 744; Percent complete: 18.6%; Average loss: 3.3851
Iteration: 745; Percent complete: 18.6%; Average loss: 3.6624
Iteration: 746; Percent complete: 18.6%; Average loss: 3.5097
Iteration: 747; Percent complete: 18.7%; Average loss: 3.8095
Iteration: 748; Percent complete: 18.7%; Average loss: 3.3399
Iteration: 749; Percent complete: 18.7%; Average loss: 3.7161
Iteration: 750; Percent complete: 18.8%; Average loss: 3.6133
Iteration: 751; Percent complete: 18.8%; Average loss: 3.7416
Iteration: 752; Percent complete: 18.8%; Average loss: 3.7949
Iteration: 753; Percent complete: 18.8%; Average loss: 3.5560
Iteration: 754; Percent complete: 18.9%; Average loss: 3.7997
Iteration: 755; Percent complete: 18.9%; Average loss: 3.4895
Iteration: 756; Percent complete: 18.9%; Average loss: 3.7303
Iteration: 757; Percent complete: 18.9%; Average loss: 3.5593
Iteration: 758; Percent complete: 18.9%; Average loss: 3.5854
Iteration: 759; Percent complete: 19.0%; Average loss: 3.3611
Iteration: 760; Percent complete: 19.0%; Average loss: 3.5116
Iteration: 761; Percent complete: 19.0%; Average loss: 3.6401
Iteration: 762; Percent complete: 19.1%; Average loss: 3.6278
Iteration: 763; Percent complete: 19.1%; Average loss: 3.6157
Iteration: 764; Percent complete: 19.1%; Average loss: 3.4133
Iteration: 765; Percent complete: 19.1%; Average loss: 3.6686
Iteration: 766; Percent complete: 19.1%; Average loss: 3.5565
Iteration: 767; Percent complete: 19.2%; Average loss: 3.5312
Iteration: 768; Percent complete: 19.2%; Average loss: 3.6221
Iteration: 769; Percent complete: 19.2%; Average loss: 3.6621
Iteration: 770; Percent complete: 19.2%; Average loss: 3.6298
Iteration: 771; Percent complete: 19.3%; Average loss: 3.4978
Iteration: 772; Percent complete: 19.3%; Average loss: 3.5311
Iteration: 773; Percent complete: 19.3%; Average loss: 3.4143
Iteration: 774; Percent complete: 19.4%; Average loss: 3.1761
Iteration: 775; Percent complete: 19.4%; Average loss: 3.5972
Iteration: 776; Percent complete: 19.4%; Average loss: 3.5982
Iteration: 777; Percent complete: 19.4%; Average loss: 3.5905
Iteration: 778; Percent complete: 19.4%; Average loss: 3.4382
Iteration: 779; Percent complete: 19.5%; Average loss: 3.4797
Iteration: 780; Percent complete: 19.5%; Average loss: 3.4883
Iteration: 781; Percent complete: 19.5%; Average loss: 3.6410
Iteration: 782; Percent complete: 19.6%; Average loss: 3.7022
Iteration: 783; Percent complete: 19.6%; Average loss: 3.6563
Iteration: 784; Percent complete: 19.6%; Average loss: 3.5219
Iteration: 785; Percent complete: 19.6%; Average loss: 3.3917
Iteration: 786; Percent complete: 19.7%; Average loss: 3.5456
Iteration: 787; Percent complete: 19.7%; Average loss: 3.8979
Iteration: 788; Percent complete: 19.7%; Average loss: 3.5261
Iteration: 789; Percent complete: 19.7%; Average loss: 3.6247
Iteration: 790; Percent complete: 19.8%; Average loss: 3.6166
Iteration: 791; Percent complete: 19.8%; Average loss: 3.3911
Iteration: 792; Percent complete: 19.8%; Average loss: 3.5538
Iteration: 793; Percent complete: 19.8%; Average loss: 3.6301
Iteration: 794; Percent complete: 19.9%; Average loss: 3.5063
Iteration: 795; Percent complete: 19.9%; Average loss: 3.6609
Iteration: 796; Percent complete: 19.9%; Average loss: 3.5223
Iteration: 797; Percent complete: 19.9%; Average loss: 3.7565
Iteration: 798; Percent complete: 20.0%; Average loss: 3.5414
Iteration: 799; Percent complete: 20.0%; Average loss: 3.4539
Iteration: 800; Percent complete: 20.0%; Average loss: 3.5645
Iteration: 801; Percent complete: 20.0%; Average loss: 3.4538
Iteration: 802; Percent complete: 20.1%; Average loss: 3.6022
Iteration: 803; Percent complete: 20.1%; Average loss: 3.5889
Iteration: 804; Percent complete: 20.1%; Average loss: 3.4370
Iteration: 805; Percent complete: 20.1%; Average loss: 3.4511
Iteration: 806; Percent complete: 20.2%; Average loss: 3.5771
Iteration: 807; Percent complete: 20.2%; Average loss: 3.5379
Iteration: 808; Percent complete: 20.2%; Average loss: 3.3270
Iteration: 809; Percent complete: 20.2%; Average loss: 3.1621
Iteration: 810; Percent complete: 20.2%; Average loss: 3.6326
Iteration: 811; Percent complete: 20.3%; Average loss: 3.7955
Iteration: 812; Percent complete: 20.3%; Average loss: 3.5261
Iteration: 813; Percent complete: 20.3%; Average loss: 3.5820
Iteration: 814; Percent complete: 20.3%; Average loss: 3.3628
Iteration: 815; Percent complete: 20.4%; Average loss: 3.6605
Iteration: 816; Percent complete: 20.4%; Average loss: 3.5128
Iteration: 817; Percent complete: 20.4%; Average loss: 3.7156
Iteration: 818; Percent complete: 20.4%; Average loss: 3.9293
Iteration: 819; Percent complete: 20.5%; Average loss: 3.4814
Iteration: 820; Percent complete: 20.5%; Average loss: 3.4905
Iteration: 821; Percent complete: 20.5%; Average loss: 3.3458
Iteration: 822; Percent complete: 20.5%; Average loss: 3.4010
Iteration: 823; Percent complete: 20.6%; Average loss: 3.7499
Iteration: 824; Percent complete: 20.6%; Average loss: 3.4270
Iteration: 825; Percent complete: 20.6%; Average loss: 3.5985
Iteration: 826; Percent complete: 20.6%; Average loss: 3.3891
Iteration: 827; Percent complete: 20.7%; Average loss: 3.5869
Iteration: 828; Percent complete: 20.7%; Average loss: 3.4493
Iteration: 829; Percent complete: 20.7%; Average loss: 3.8053
Iteration: 830; Percent complete: 20.8%; Average loss: 3.5465
Iteration: 831; Percent complete: 20.8%; Average loss: 3.3203
Iteration: 832; Percent complete: 20.8%; Average loss: 3.7204
Iteration: 833; Percent complete: 20.8%; Average loss: 3.2725
Iteration: 834; Percent complete: 20.8%; Average loss: 3.5377
Iteration: 835; Percent complete: 20.9%; Average loss: 3.5848
Iteration: 836; Percent complete: 20.9%; Average loss: 3.5857
Iteration: 837; Percent complete: 20.9%; Average loss: 3.5773
Iteration: 838; Percent complete: 20.9%; Average loss: 3.3663
Iteration: 839; Percent complete: 21.0%; Average loss: 3.6197
Iteration: 840; Percent complete: 21.0%; Average loss: 3.6037
Iteration: 841; Percent complete: 21.0%; Average loss: 3.5231
Iteration: 842; Percent complete: 21.1%; Average loss: 3.5001
Iteration: 843; Percent complete: 21.1%; Average loss: 3.5841
Iteration: 844; Percent complete: 21.1%; Average loss: 3.6276
Iteration: 845; Percent complete: 21.1%; Average loss: 3.7463
Iteration: 846; Percent complete: 21.1%; Average loss: 3.4006
Iteration: 847; Percent complete: 21.2%; Average loss: 3.3359
Iteration: 848; Percent complete: 21.2%; Average loss: 3.3949
Iteration: 849; Percent complete: 21.2%; Average loss: 3.5315
Iteration: 850; Percent complete: 21.2%; Average loss: 3.6480
Iteration: 851; Percent complete: 21.3%; Average loss: 3.5539
Iteration: 852; Percent complete: 21.3%; Average loss: 3.6276
Iteration: 853; Percent complete: 21.3%; Average loss: 3.4616
Iteration: 854; Percent complete: 21.3%; Average loss: 3.3897
Iteration: 855; Percent complete: 21.4%; Average loss: 3.5216
Iteration: 856; Percent complete: 21.4%; Average loss: 3.3250
Iteration: 857; Percent complete: 21.4%; Average loss: 3.5170
Iteration: 858; Percent complete: 21.4%; Average loss: 3.6663
Iteration: 859; Percent complete: 21.5%; Average loss: 3.6312
Iteration: 860; Percent complete: 21.5%; Average loss: 3.4810
Iteration: 861; Percent complete: 21.5%; Average loss: 3.4135
Iteration: 862; Percent complete: 21.6%; Average loss: 3.5074
Iteration: 863; Percent complete: 21.6%; Average loss: 3.4013
Iteration: 864; Percent complete: 21.6%; Average loss: 3.4131
Iteration: 865; Percent complete: 21.6%; Average loss: 3.4332
Iteration: 866; Percent complete: 21.6%; Average loss: 3.6537
Iteration: 867; Percent complete: 21.7%; Average loss: 3.5194
Iteration: 868; Percent complete: 21.7%; Average loss: 3.5412
Iteration: 869; Percent complete: 21.7%; Average loss: 3.8455
Iteration: 870; Percent complete: 21.8%; Average loss: 3.5537
Iteration: 871; Percent complete: 21.8%; Average loss: 3.4311
Iteration: 872; Percent complete: 21.8%; Average loss: 3.5531
Iteration: 873; Percent complete: 21.8%; Average loss: 3.6795
Iteration: 874; Percent complete: 21.9%; Average loss: 3.9202
Iteration: 875; Percent complete: 21.9%; Average loss: 3.4337
Iteration: 876; Percent complete: 21.9%; Average loss: 3.6008
Iteration: 877; Percent complete: 21.9%; Average loss: 3.6333
Iteration: 878; Percent complete: 21.9%; Average loss: 3.5520
Iteration: 879; Percent complete: 22.0%; Average loss: 3.7014
Iteration: 880; Percent complete: 22.0%; Average loss: 3.4167
Iteration: 881; Percent complete: 22.0%; Average loss: 3.4480
Iteration: 882; Percent complete: 22.1%; Average loss: 3.3562
Iteration: 883; Percent complete: 22.1%; Average loss: 3.7513
Iteration: 884; Percent complete: 22.1%; Average loss: 3.6109
Iteration: 885; Percent complete: 22.1%; Average loss: 3.4533
Iteration: 886; Percent complete: 22.1%; Average loss: 3.5784
Iteration: 887; Percent complete: 22.2%; Average loss: 3.8337
Iteration: 888; Percent complete: 22.2%; Average loss: 3.6128
Iteration: 889; Percent complete: 22.2%; Average loss: 3.5425
Iteration: 890; Percent complete: 22.2%; Average loss: 3.5607
Iteration: 891; Percent complete: 22.3%; Average loss: 3.3575
Iteration: 892; Percent complete: 22.3%; Average loss: 3.4576
Iteration: 893; Percent complete: 22.3%; Average loss: 3.3373
Iteration: 894; Percent complete: 22.4%; Average loss: 3.5568
Iteration: 895; Percent complete: 22.4%; Average loss: 3.6575
Iteration: 896; Percent complete: 22.4%; Average loss: 3.7700
Iteration: 897; Percent complete: 22.4%; Average loss: 3.3262
Iteration: 898; Percent complete: 22.4%; Average loss: 3.6849
Iteration: 899; Percent complete: 22.5%; Average loss: 3.3986
Iteration: 900; Percent complete: 22.5%; Average loss: 3.7253
Iteration: 901; Percent complete: 22.5%; Average loss: 3.3032
Iteration: 902; Percent complete: 22.6%; Average loss: 3.6491
Iteration: 903; Percent complete: 22.6%; Average loss: 3.6549
Iteration: 904; Percent complete: 22.6%; Average loss: 3.4655
Iteration: 905; Percent complete: 22.6%; Average loss: 3.4510
Iteration: 906; Percent complete: 22.7%; Average loss: 3.4310
Iteration: 907; Percent complete: 22.7%; Average loss: 3.4675
Iteration: 908; Percent complete: 22.7%; Average loss: 3.4511
Iteration: 909; Percent complete: 22.7%; Average loss: 3.4042
Iteration: 910; Percent complete: 22.8%; Average loss: 3.3496
Iteration: 911; Percent complete: 22.8%; Average loss: 3.5439
Iteration: 912; Percent complete: 22.8%; Average loss: 3.4406
Iteration: 913; Percent complete: 22.8%; Average loss: 3.3593
Iteration: 914; Percent complete: 22.9%; Average loss: 3.5348
Iteration: 915; Percent complete: 22.9%; Average loss: 3.7245
Iteration: 916; Percent complete: 22.9%; Average loss: 3.6471
Iteration: 917; Percent complete: 22.9%; Average loss: 3.6803
Iteration: 918; Percent complete: 22.9%; Average loss: 3.4830
Iteration: 919; Percent complete: 23.0%; Average loss: 3.2568
Iteration: 920; Percent complete: 23.0%; Average loss: 3.7267
Iteration: 921; Percent complete: 23.0%; Average loss: 3.6653
Iteration: 922; Percent complete: 23.1%; Average loss: 3.4491
Iteration: 923; Percent complete: 23.1%; Average loss: 3.6209
Iteration: 924; Percent complete: 23.1%; Average loss: 3.6114
Iteration: 925; Percent complete: 23.1%; Average loss: 3.1260
Iteration: 926; Percent complete: 23.2%; Average loss: 3.7398
Iteration: 927; Percent complete: 23.2%; Average loss: 3.3786
Iteration: 928; Percent complete: 23.2%; Average loss: 3.5939
Iteration: 929; Percent complete: 23.2%; Average loss: 3.2974
Iteration: 930; Percent complete: 23.2%; Average loss: 3.5081
Iteration: 931; Percent complete: 23.3%; Average loss: 3.6164
Iteration: 932; Percent complete: 23.3%; Average loss: 3.4276
Iteration: 933; Percent complete: 23.3%; Average loss: 3.4698
Iteration: 934; Percent complete: 23.4%; Average loss: 3.4674
Iteration: 935; Percent complete: 23.4%; Average loss: 3.4487
Iteration: 936; Percent complete: 23.4%; Average loss: 3.8329
Iteration: 937; Percent complete: 23.4%; Average loss: 3.4054
Iteration: 938; Percent complete: 23.4%; Average loss: 3.6054
Iteration: 939; Percent complete: 23.5%; Average loss: 3.4963
Iteration: 940; Percent complete: 23.5%; Average loss: 3.6354
Iteration: 941; Percent complete: 23.5%; Average loss: 3.6042
Iteration: 942; Percent complete: 23.5%; Average loss: 3.3475
Iteration: 943; Percent complete: 23.6%; Average loss: 3.3167
Iteration: 944; Percent complete: 23.6%; Average loss: 3.3889
Iteration: 945; Percent complete: 23.6%; Average loss: 3.2977
Iteration: 946; Percent complete: 23.6%; Average loss: 3.4713
Iteration: 947; Percent complete: 23.7%; Average loss: 3.4775
Iteration: 948; Percent complete: 23.7%; Average loss: 3.5051
Iteration: 949; Percent complete: 23.7%; Average loss: 3.1252
Iteration: 950; Percent complete: 23.8%; Average loss: 3.6533
Iteration: 951; Percent complete: 23.8%; Average loss: 3.5571
Iteration: 952; Percent complete: 23.8%; Average loss: 3.5177
Iteration: 953; Percent complete: 23.8%; Average loss: 3.3169
Iteration: 954; Percent complete: 23.8%; Average loss: 3.7467
Iteration: 955; Percent complete: 23.9%; Average loss: 3.4274
Iteration: 956; Percent complete: 23.9%; Average loss: 3.5838
Iteration: 957; Percent complete: 23.9%; Average loss: 3.3769
Iteration: 958; Percent complete: 23.9%; Average loss: 3.4067
Iteration: 959; Percent complete: 24.0%; Average loss: 3.4789
Iteration: 960; Percent complete: 24.0%; Average loss: 3.4014
Iteration: 961; Percent complete: 24.0%; Average loss: 3.5607
Iteration: 962; Percent complete: 24.1%; Average loss: 3.3497
Iteration: 963; Percent complete: 24.1%; Average loss: 3.3740
Iteration: 964; Percent complete: 24.1%; Average loss: 3.3644
Iteration: 965; Percent complete: 24.1%; Average loss: 3.6642
Iteration: 966; Percent complete: 24.1%; Average loss: 3.5393
Iteration: 967; Percent complete: 24.2%; Average loss: 3.5352
Iteration: 968; Percent complete: 24.2%; Average loss: 3.4962
Iteration: 969; Percent complete: 24.2%; Average loss: 3.4899
Iteration: 970; Percent complete: 24.2%; Average loss: 3.5454
Iteration: 971; Percent complete: 24.3%; Average loss: 3.4335
Iteration: 972; Percent complete: 24.3%; Average loss: 3.4028
Iteration: 973; Percent complete: 24.3%; Average loss: 3.4868
Iteration: 974; Percent complete: 24.3%; Average loss: 3.3918
Iteration: 975; Percent complete: 24.4%; Average loss: 3.4541
Iteration: 976; Percent complete: 24.4%; Average loss: 3.2493
Iteration: 977; Percent complete: 24.4%; Average loss: 3.4464
Iteration: 978; Percent complete: 24.4%; Average loss: 3.4639
Iteration: 979; Percent complete: 24.5%; Average loss: 3.3465
Iteration: 980; Percent complete: 24.5%; Average loss: 3.2723
Iteration: 981; Percent complete: 24.5%; Average loss: 3.2729
Iteration: 982; Percent complete: 24.6%; Average loss: 3.6451
Iteration: 983; Percent complete: 24.6%; Average loss: 3.4533
Iteration: 984; Percent complete: 24.6%; Average loss: 3.5463
Iteration: 985; Percent complete: 24.6%; Average loss: 3.4242
Iteration: 986; Percent complete: 24.6%; Average loss: 3.5540
Iteration: 987; Percent complete: 24.7%; Average loss: 3.4739
Iteration: 988; Percent complete: 24.7%; Average loss: 3.4252
Iteration: 989; Percent complete: 24.7%; Average loss: 3.3033
Iteration: 990; Percent complete: 24.8%; Average loss: 3.4102
Iteration: 991; Percent complete: 24.8%; Average loss: 3.5793
Iteration: 992; Percent complete: 24.8%; Average loss: 3.4535
Iteration: 993; Percent complete: 24.8%; Average loss: 3.4009
Iteration: 994; Percent complete: 24.9%; Average loss: 3.5699
Iteration: 995; Percent complete: 24.9%; Average loss: 3.4716
Iteration: 996; Percent complete: 24.9%; Average loss: 3.5791
Iteration: 997; Percent complete: 24.9%; Average loss: 3.2969
Iteration: 998; Percent complete: 24.9%; Average loss: 3.1974
Iteration: 999; Percent complete: 25.0%; Average loss: 3.2873
Iteration: 1000; Percent complete: 25.0%; Average loss: 3.6855
Iteration: 1001; Percent complete: 25.0%; Average loss: 3.2325
Iteration: 1002; Percent complete: 25.1%; Average loss: 3.5274
Iteration: 1003; Percent complete: 25.1%; Average loss: 3.7042
Iteration: 1004; Percent complete: 25.1%; Average loss: 3.6182
Iteration: 1005; Percent complete: 25.1%; Average loss: 3.2686
Iteration: 1006; Percent complete: 25.1%; Average loss: 3.2909
Iteration: 1007; Percent complete: 25.2%; Average loss: 3.4840
Iteration: 1008; Percent complete: 25.2%; Average loss: 3.5833
Iteration: 1009; Percent complete: 25.2%; Average loss: 3.4722
Iteration: 1010; Percent complete: 25.2%; Average loss: 3.4814
Iteration: 1011; Percent complete: 25.3%; Average loss: 3.6635
Iteration: 1012; Percent complete: 25.3%; Average loss: 3.6570
Iteration: 1013; Percent complete: 25.3%; Average loss: 3.3822
Iteration: 1014; Percent complete: 25.4%; Average loss: 3.4184
Iteration: 1015; Percent complete: 25.4%; Average loss: 3.5297
Iteration: 1016; Percent complete: 25.4%; Average loss: 3.0870
Iteration: 1017; Percent complete: 25.4%; Average loss: 3.4151
Iteration: 1018; Percent complete: 25.4%; Average loss: 3.1984
Iteration: 1019; Percent complete: 25.5%; Average loss: 3.5597
Iteration: 1020; Percent complete: 25.5%; Average loss: 3.5073
Iteration: 1021; Percent complete: 25.5%; Average loss: 3.4539
Iteration: 1022; Percent complete: 25.6%; Average loss: 3.4809
Iteration: 1023; Percent complete: 25.6%; Average loss: 3.3777
Iteration: 1024; Percent complete: 25.6%; Average loss: 3.7249
Iteration: 1025; Percent complete: 25.6%; Average loss: 3.4981
Iteration: 1026; Percent complete: 25.7%; Average loss: 3.4003
Iteration: 1027; Percent complete: 25.7%; Average loss: 3.2648
Iteration: 1028; Percent complete: 25.7%; Average loss: 3.5892
Iteration: 1029; Percent complete: 25.7%; Average loss: 3.6561
Iteration: 1030; Percent complete: 25.8%; Average loss: 3.2470
Iteration: 1031; Percent complete: 25.8%; Average loss: 3.3200
Iteration: 1032; Percent complete: 25.8%; Average loss: 3.5689
Iteration: 1033; Percent complete: 25.8%; Average loss: 3.3992
Iteration: 1034; Percent complete: 25.9%; Average loss: 3.5077
Iteration: 1035; Percent complete: 25.9%; Average loss: 3.3738
Iteration: 1036; Percent complete: 25.9%; Average loss: 3.5188
Iteration: 1037; Percent complete: 25.9%; Average loss: 3.3653
Iteration: 1038; Percent complete: 25.9%; Average loss: 3.3464
Iteration: 1039; Percent complete: 26.0%; Average loss: 3.4738
Iteration: 1040; Percent complete: 26.0%; Average loss: 3.3141
Iteration: 1041; Percent complete: 26.0%; Average loss: 3.3878
Iteration: 1042; Percent complete: 26.1%; Average loss: 3.4279
Iteration: 1043; Percent complete: 26.1%; Average loss: 3.4310
Iteration: 1044; Percent complete: 26.1%; Average loss: 3.3725
Iteration: 1045; Percent complete: 26.1%; Average loss: 3.8419
Iteration: 1046; Percent complete: 26.2%; Average loss: 3.3358
Iteration: 1047; Percent complete: 26.2%; Average loss: 3.6724
Iteration: 1048; Percent complete: 26.2%; Average loss: 3.3505
Iteration: 1049; Percent complete: 26.2%; Average loss: 3.2956
Iteration: 1050; Percent complete: 26.2%; Average loss: 3.2860
Iteration: 1051; Percent complete: 26.3%; Average loss: 3.4088
Iteration: 1052; Percent complete: 26.3%; Average loss: 3.6164
Iteration: 1053; Percent complete: 26.3%; Average loss: 3.7105
Iteration: 1054; Percent complete: 26.4%; Average loss: 3.6051
Iteration: 1055; Percent complete: 26.4%; Average loss: 3.3170
Iteration: 1056; Percent complete: 26.4%; Average loss: 3.4801
Iteration: 1057; Percent complete: 26.4%; Average loss: 3.4772
Iteration: 1058; Percent complete: 26.5%; Average loss: 3.3950
Iteration: 1059; Percent complete: 26.5%; Average loss: 3.1410
Iteration: 1060; Percent complete: 26.5%; Average loss: 3.4419
Iteration: 1061; Percent complete: 26.5%; Average loss: 3.6071
Iteration: 1062; Percent complete: 26.6%; Average loss: 3.4191
Iteration: 1063; Percent complete: 26.6%; Average loss: 3.4889
Iteration: 1064; Percent complete: 26.6%; Average loss: 3.7739
Iteration: 1065; Percent complete: 26.6%; Average loss: 3.5487
Iteration: 1066; Percent complete: 26.7%; Average loss: 3.4202
Iteration: 1067; Percent complete: 26.7%; Average loss: 3.6634
Iteration: 1068; Percent complete: 26.7%; Average loss: 3.2873
Iteration: 1069; Percent complete: 26.7%; Average loss: 3.5069
Iteration: 1070; Percent complete: 26.8%; Average loss: 3.2480
Iteration: 1071; Percent complete: 26.8%; Average loss: 3.4556
Iteration: 1072; Percent complete: 26.8%; Average loss: 3.5446
Iteration: 1073; Percent complete: 26.8%; Average loss: 3.5157
Iteration: 1074; Percent complete: 26.9%; Average loss: 3.4550
Iteration: 1075; Percent complete: 26.9%; Average loss: 3.5131
Iteration: 1076; Percent complete: 26.9%; Average loss: 3.4119
Iteration: 1077; Percent complete: 26.9%; Average loss: 3.4230
Iteration: 1078; Percent complete: 27.0%; Average loss: 3.5849
Iteration: 1079; Percent complete: 27.0%; Average loss: 3.1230
Iteration: 1080; Percent complete: 27.0%; Average loss: 3.1496
Iteration: 1081; Percent complete: 27.0%; Average loss: 3.3635
Iteration: 1082; Percent complete: 27.1%; Average loss: 3.7272
Iteration: 1083; Percent complete: 27.1%; Average loss: 3.2388
Iteration: 1084; Percent complete: 27.1%; Average loss: 3.5667
Iteration: 1085; Percent complete: 27.1%; Average loss: 3.3594
Iteration: 1086; Percent complete: 27.2%; Average loss: 3.3956
Iteration: 1087; Percent complete: 27.2%; Average loss: 3.1149
Iteration: 1088; Percent complete: 27.2%; Average loss: 3.3598
Iteration: 1089; Percent complete: 27.2%; Average loss: 3.8926
Iteration: 1090; Percent complete: 27.3%; Average loss: 3.7141
Iteration: 1091; Percent complete: 27.3%; Average loss: 3.6096
Iteration: 1092; Percent complete: 27.3%; Average loss: 3.4429
Iteration: 1093; Percent complete: 27.3%; Average loss: 3.6357
Iteration: 1094; Percent complete: 27.4%; Average loss: 3.3907
Iteration: 1095; Percent complete: 27.4%; Average loss: 3.3197
Iteration: 1096; Percent complete: 27.4%; Average loss: 3.1119
Iteration: 1097; Percent complete: 27.4%; Average loss: 3.0901
Iteration: 1098; Percent complete: 27.5%; Average loss: 3.5611
Iteration: 1099; Percent complete: 27.5%; Average loss: 3.7190
Iteration: 1100; Percent complete: 27.5%; Average loss: 3.6275
Iteration: 1101; Percent complete: 27.5%; Average loss: 3.5717
Iteration: 1102; Percent complete: 27.6%; Average loss: 3.1786
Iteration: 1103; Percent complete: 27.6%; Average loss: 3.7171
Iteration: 1104; Percent complete: 27.6%; Average loss: 3.3975
Iteration: 1105; Percent complete: 27.6%; Average loss: 3.4284
Iteration: 1106; Percent complete: 27.7%; Average loss: 3.2208
Iteration: 1107; Percent complete: 27.7%; Average loss: 3.5802
Iteration: 1108; Percent complete: 27.7%; Average loss: 3.2912
Iteration: 1109; Percent complete: 27.7%; Average loss: 3.5566
Iteration: 1110; Percent complete: 27.8%; Average loss: 3.1842
Iteration: 1111; Percent complete: 27.8%; Average loss: 3.5535
Iteration: 1112; Percent complete: 27.8%; Average loss: 3.2853
Iteration: 1113; Percent complete: 27.8%; Average loss: 3.5890
Iteration: 1114; Percent complete: 27.9%; Average loss: 3.2193
Iteration: 1115; Percent complete: 27.9%; Average loss: 3.2593
Iteration: 1116; Percent complete: 27.9%; Average loss: 3.5051
Iteration: 1117; Percent complete: 27.9%; Average loss: 3.3173
Iteration: 1118; Percent complete: 28.0%; Average loss: 3.3634
Iteration: 1119; Percent complete: 28.0%; Average loss: 3.4383
Iteration: 1120; Percent complete: 28.0%; Average loss: 3.2621
Iteration: 1121; Percent complete: 28.0%; Average loss: 3.2845
Iteration: 1122; Percent complete: 28.1%; Average loss: 3.3336
Iteration: 1123; Percent complete: 28.1%; Average loss: 3.2216
Iteration: 1124; Percent complete: 28.1%; Average loss: 3.2762
Iteration: 1125; Percent complete: 28.1%; Average loss: 3.5245
Iteration: 1126; Percent complete: 28.1%; Average loss: 3.1087
Iteration: 1127; Percent complete: 28.2%; Average loss: 3.6523
Iteration: 1128; Percent complete: 28.2%; Average loss: 3.3426
Iteration: 1129; Percent complete: 28.2%; Average loss: 3.3690
Iteration: 1130; Percent complete: 28.2%; Average loss: 3.4334
Iteration: 1131; Percent complete: 28.3%; Average loss: 3.2746
Iteration: 1132; Percent complete: 28.3%; Average loss: 3.5091
Iteration: 1133; Percent complete: 28.3%; Average loss: 3.5834
Iteration: 1134; Percent complete: 28.3%; Average loss: 3.2084
Iteration: 1135; Percent complete: 28.4%; Average loss: 3.5185
Iteration: 1136; Percent complete: 28.4%; Average loss: 3.4850
Iteration: 1137; Percent complete: 28.4%; Average loss: 3.1962
Iteration: 1138; Percent complete: 28.4%; Average loss: 3.4021
Iteration: 1139; Percent complete: 28.5%; Average loss: 3.6018
Iteration: 1140; Percent complete: 28.5%; Average loss: 3.5646
Iteration: 1141; Percent complete: 28.5%; Average loss: 3.2766
Iteration: 1142; Percent complete: 28.5%; Average loss: 3.3545
Iteration: 1143; Percent complete: 28.6%; Average loss: 3.2054
Iteration: 1144; Percent complete: 28.6%; Average loss: 3.5684
Iteration: 1145; Percent complete: 28.6%; Average loss: 3.3730
Iteration: 1146; Percent complete: 28.6%; Average loss: 3.3580
Iteration: 1147; Percent complete: 28.7%; Average loss: 3.5411
Iteration: 1148; Percent complete: 28.7%; Average loss: 3.5103
Iteration: 1149; Percent complete: 28.7%; Average loss: 3.5753
Iteration: 1150; Percent complete: 28.7%; Average loss: 3.2917
Iteration: 1151; Percent complete: 28.8%; Average loss: 3.3203
Iteration: 1152; Percent complete: 28.8%; Average loss: 3.2972
Iteration: 1153; Percent complete: 28.8%; Average loss: 3.4172
Iteration: 1154; Percent complete: 28.8%; Average loss: 3.3854
Iteration: 1155; Percent complete: 28.9%; Average loss: 3.6790
Iteration: 1156; Percent complete: 28.9%; Average loss: 3.6520
Iteration: 1157; Percent complete: 28.9%; Average loss: 3.5157
Iteration: 1158; Percent complete: 28.9%; Average loss: 3.1978
Iteration: 1159; Percent complete: 29.0%; Average loss: 3.3821
Iteration: 1160; Percent complete: 29.0%; Average loss: 3.7369
Iteration: 1161; Percent complete: 29.0%; Average loss: 3.2006
Iteration: 1162; Percent complete: 29.0%; Average loss: 3.1708
Iteration: 1163; Percent complete: 29.1%; Average loss: 3.4499
Iteration: 1164; Percent complete: 29.1%; Average loss: 3.3072
Iteration: 1165; Percent complete: 29.1%; Average loss: 3.4391
Iteration: 1166; Percent complete: 29.1%; Average loss: 3.4102
Iteration: 1167; Percent complete: 29.2%; Average loss: 3.5171
Iteration: 1168; Percent complete: 29.2%; Average loss: 3.2941
Iteration: 1169; Percent complete: 29.2%; Average loss: 3.4647
Iteration: 1170; Percent complete: 29.2%; Average loss: 3.4245
Iteration: 1171; Percent complete: 29.3%; Average loss: 3.4860
Iteration: 1172; Percent complete: 29.3%; Average loss: 3.2974
Iteration: 1173; Percent complete: 29.3%; Average loss: 3.3110
Iteration: 1174; Percent complete: 29.3%; Average loss: 3.2935
Iteration: 1175; Percent complete: 29.4%; Average loss: 3.6118
Iteration: 1176; Percent complete: 29.4%; Average loss: 3.3962
Iteration: 1177; Percent complete: 29.4%; Average loss: 3.3924
Iteration: 1178; Percent complete: 29.4%; Average loss: 3.3507
Iteration: 1179; Percent complete: 29.5%; Average loss: 3.5898
Iteration: 1180; Percent complete: 29.5%; Average loss: 3.5854
Iteration: 1181; Percent complete: 29.5%; Average loss: 3.2520
Iteration: 1182; Percent complete: 29.5%; Average loss: 3.3434
Iteration: 1183; Percent complete: 29.6%; Average loss: 3.3307
Iteration: 1184; Percent complete: 29.6%; Average loss: 3.2823
Iteration: 1185; Percent complete: 29.6%; Average loss: 3.4077
Iteration: 1186; Percent complete: 29.6%; Average loss: 3.2617
Iteration: 1187; Percent complete: 29.7%; Average loss: 3.3286
Iteration: 1188; Percent complete: 29.7%; Average loss: 3.6794
Iteration: 1189; Percent complete: 29.7%; Average loss: 3.3547
Iteration: 1190; Percent complete: 29.8%; Average loss: 3.4249
Iteration: 1191; Percent complete: 29.8%; Average loss: 3.3854
Iteration: 1192; Percent complete: 29.8%; Average loss: 3.2782
Iteration: 1193; Percent complete: 29.8%; Average loss: 3.2950
Iteration: 1194; Percent complete: 29.8%; Average loss: 3.3090
Iteration: 1195; Percent complete: 29.9%; Average loss: 3.3662
Iteration: 1196; Percent complete: 29.9%; Average loss: 3.5265
Iteration: 1197; Percent complete: 29.9%; Average loss: 3.3203
Iteration: 1198; Percent complete: 29.9%; Average loss: 3.2653
Iteration: 1199; Percent complete: 30.0%; Average loss: 3.3040
Iteration: 1200; Percent complete: 30.0%; Average loss: 3.4843
Iteration: 1201; Percent complete: 30.0%; Average loss: 3.5059
Iteration: 1202; Percent complete: 30.0%; Average loss: 3.2906
Iteration: 1203; Percent complete: 30.1%; Average loss: 3.5463
Iteration: 1204; Percent complete: 30.1%; Average loss: 3.6588
Iteration: 1205; Percent complete: 30.1%; Average loss: 3.2958
Iteration: 1206; Percent complete: 30.1%; Average loss: 3.4777
Iteration: 1207; Percent complete: 30.2%; Average loss: 3.2783
Iteration: 1208; Percent complete: 30.2%; Average loss: 3.4041
Iteration: 1209; Percent complete: 30.2%; Average loss: 3.3784
Iteration: 1210; Percent complete: 30.2%; Average loss: 3.4538
Iteration: 1211; Percent complete: 30.3%; Average loss: 3.2769
Iteration: 1212; Percent complete: 30.3%; Average loss: 3.4682
Iteration: 1213; Percent complete: 30.3%; Average loss: 3.1071
Iteration: 1214; Percent complete: 30.3%; Average loss: 3.4931
Iteration: 1215; Percent complete: 30.4%; Average loss: 3.5234
Iteration: 1216; Percent complete: 30.4%; Average loss: 3.3680
Iteration: 1217; Percent complete: 30.4%; Average loss: 3.3327
Iteration: 1218; Percent complete: 30.4%; Average loss: 3.4088
Iteration: 1219; Percent complete: 30.5%; Average loss: 3.6000
Iteration: 1220; Percent complete: 30.5%; Average loss: 3.5628
Iteration: 1221; Percent complete: 30.5%; Average loss: 3.2329
Iteration: 1222; Percent complete: 30.6%; Average loss: 3.4396
Iteration: 1223; Percent complete: 30.6%; Average loss: 3.3859
Iteration: 1224; Percent complete: 30.6%; Average loss: 3.2597
Iteration: 1225; Percent complete: 30.6%; Average loss: 3.2235
Iteration: 1226; Percent complete: 30.6%; Average loss: 3.5944
Iteration: 1227; Percent complete: 30.7%; Average loss: 3.2327
Iteration: 1228; Percent complete: 30.7%; Average loss: 3.2696
Iteration: 1229; Percent complete: 30.7%; Average loss: 3.1657
Iteration: 1230; Percent complete: 30.8%; Average loss: 3.4409
Iteration: 1231; Percent complete: 30.8%; Average loss: 3.2590
Iteration: 1232; Percent complete: 30.8%; Average loss: 3.5405
Iteration: 1233; Percent complete: 30.8%; Average loss: 3.4164
Iteration: 1234; Percent complete: 30.9%; Average loss: 3.2695
Iteration: 1235; Percent complete: 30.9%; Average loss: 3.2286
Iteration: 1236; Percent complete: 30.9%; Average loss: 3.3534
Iteration: 1237; Percent complete: 30.9%; Average loss: 3.5711
Iteration: 1238; Percent complete: 30.9%; Average loss: 3.4164
Iteration: 1239; Percent complete: 31.0%; Average loss: 3.3308
Iteration: 1240; Percent complete: 31.0%; Average loss: 3.4876
Iteration: 1241; Percent complete: 31.0%; Average loss: 3.5128
Iteration: 1242; Percent complete: 31.1%; Average loss: 3.4155
Iteration: 1243; Percent complete: 31.1%; Average loss: 3.3458
Iteration: 1244; Percent complete: 31.1%; Average loss: 3.5423
Iteration: 1245; Percent complete: 31.1%; Average loss: 3.4959
Iteration: 1246; Percent complete: 31.1%; Average loss: 3.2350
Iteration: 1247; Percent complete: 31.2%; Average loss: 3.1694
Iteration: 1248; Percent complete: 31.2%; Average loss: 3.4860
Iteration: 1249; Percent complete: 31.2%; Average loss: 3.3868
Iteration: 1250; Percent complete: 31.2%; Average loss: 3.3078
Iteration: 1251; Percent complete: 31.3%; Average loss: 3.6386
Iteration: 1252; Percent complete: 31.3%; Average loss: 3.5785
Iteration: 1253; Percent complete: 31.3%; Average loss: 3.7612
Iteration: 1254; Percent complete: 31.4%; Average loss: 3.2868
Iteration: 1255; Percent complete: 31.4%; Average loss: 3.4709
Iteration: 1256; Percent complete: 31.4%; Average loss: 3.3390
Iteration: 1257; Percent complete: 31.4%; Average loss: 3.4660
Iteration: 1258; Percent complete: 31.4%; Average loss: 3.2997
Iteration: 1259; Percent complete: 31.5%; Average loss: 3.2849
Iteration: 1260; Percent complete: 31.5%; Average loss: 3.2408
Iteration: 1261; Percent complete: 31.5%; Average loss: 3.3716
Iteration: 1262; Percent complete: 31.6%; Average loss: 3.4106
Iteration: 1263; Percent complete: 31.6%; Average loss: 3.3841
Iteration: 1264; Percent complete: 31.6%; Average loss: 3.4994
Iteration: 1265; Percent complete: 31.6%; Average loss: 3.5058
Iteration: 1266; Percent complete: 31.6%; Average loss: 3.2695
Iteration: 1267; Percent complete: 31.7%; Average loss: 3.2332
Iteration: 1268; Percent complete: 31.7%; Average loss: 3.5514
Iteration: 1269; Percent complete: 31.7%; Average loss: 3.3619
Iteration: 1270; Percent complete: 31.8%; Average loss: 3.3013
Iteration: 1271; Percent complete: 31.8%; Average loss: 3.5951
Iteration: 1272; Percent complete: 31.8%; Average loss: 3.5497
Iteration: 1273; Percent complete: 31.8%; Average loss: 3.4859
Iteration: 1274; Percent complete: 31.9%; Average loss: 3.2828
Iteration: 1275; Percent complete: 31.9%; Average loss: 3.5645
Iteration: 1276; Percent complete: 31.9%; Average loss: 3.4103
Iteration: 1277; Percent complete: 31.9%; Average loss: 3.3707
Iteration: 1278; Percent complete: 31.9%; Average loss: 3.4423
Iteration: 1279; Percent complete: 32.0%; Average loss: 3.3864
Iteration: 1280; Percent complete: 32.0%; Average loss: 3.4208
Iteration: 1281; Percent complete: 32.0%; Average loss: 3.6646
Iteration: 1282; Percent complete: 32.0%; Average loss: 3.5360
Iteration: 1283; Percent complete: 32.1%; Average loss: 3.3565
Iteration: 1284; Percent complete: 32.1%; Average loss: 3.2624
Iteration: 1285; Percent complete: 32.1%; Average loss: 3.1663
Iteration: 1286; Percent complete: 32.1%; Average loss: 3.4615
Iteration: 1287; Percent complete: 32.2%; Average loss: 3.4985
Iteration: 1288; Percent complete: 32.2%; Average loss: 3.3606
Iteration: 1289; Percent complete: 32.2%; Average loss: 3.2504
Iteration: 1290; Percent complete: 32.2%; Average loss: 3.1189
Iteration: 1291; Percent complete: 32.3%; Average loss: 3.1394
Iteration: 1292; Percent complete: 32.3%; Average loss: 3.5708
Iteration: 1293; Percent complete: 32.3%; Average loss: 3.3905
Iteration: 1294; Percent complete: 32.4%; Average loss: 3.6477
Iteration: 1295; Percent complete: 32.4%; Average loss: 3.3037
Iteration: 1296; Percent complete: 32.4%; Average loss: 3.5348
Iteration: 1297; Percent complete: 32.4%; Average loss: 3.2369
Iteration: 1298; Percent complete: 32.5%; Average loss: 3.2639
Iteration: 1299; Percent complete: 32.5%; Average loss: 3.3452
Iteration: 1300; Percent complete: 32.5%; Average loss: 3.1631
Iteration: 1301; Percent complete: 32.5%; Average loss: 3.3955
Iteration: 1302; Percent complete: 32.6%; Average loss: 3.2996
Iteration: 1303; Percent complete: 32.6%; Average loss: 3.3941
Iteration: 1304; Percent complete: 32.6%; Average loss: 3.4682
Iteration: 1305; Percent complete: 32.6%; Average loss: 3.2827
Iteration: 1306; Percent complete: 32.6%; Average loss: 3.4269
Iteration: 1307; Percent complete: 32.7%; Average loss: 3.5330
Iteration: 1308; Percent complete: 32.7%; Average loss: 3.3512
Iteration: 1309; Percent complete: 32.7%; Average loss: 3.1152
Iteration: 1310; Percent complete: 32.8%; Average loss: 3.3459
Iteration: 1311; Percent complete: 32.8%; Average loss: 3.4356
Iteration: 1312; Percent complete: 32.8%; Average loss: 3.5147
Iteration: 1313; Percent complete: 32.8%; Average loss: 3.3596
Iteration: 1314; Percent complete: 32.9%; Average loss: 3.5494
Iteration: 1315; Percent complete: 32.9%; Average loss: 3.2821
Iteration: 1316; Percent complete: 32.9%; Average loss: 3.3712
Iteration: 1317; Percent complete: 32.9%; Average loss: 3.5842
Iteration: 1318; Percent complete: 33.0%; Average loss: 3.3862
Iteration: 1319; Percent complete: 33.0%; Average loss: 3.5419
Iteration: 1320; Percent complete: 33.0%; Average loss: 3.3121
Iteration: 1321; Percent complete: 33.0%; Average loss: 3.4419
Iteration: 1322; Percent complete: 33.1%; Average loss: 3.3700
Iteration: 1323; Percent complete: 33.1%; Average loss: 3.2580
Iteration: 1324; Percent complete: 33.1%; Average loss: 3.2560
Iteration: 1325; Percent complete: 33.1%; Average loss: 3.1759
Iteration: 1326; Percent complete: 33.1%; Average loss: 3.3677
Iteration: 1327; Percent complete: 33.2%; Average loss: 3.4352
Iteration: 1328; Percent complete: 33.2%; Average loss: 3.1544
Iteration: 1329; Percent complete: 33.2%; Average loss: 3.2133
Iteration: 1330; Percent complete: 33.2%; Average loss: 3.1968
Iteration: 1331; Percent complete: 33.3%; Average loss: 3.1073
Iteration: 1332; Percent complete: 33.3%; Average loss: 3.3342
Iteration: 1333; Percent complete: 33.3%; Average loss: 3.3237
Iteration: 1334; Percent complete: 33.4%; Average loss: 3.5931
Iteration: 1335; Percent complete: 33.4%; Average loss: 3.5538
Iteration: 1336; Percent complete: 33.4%; Average loss: 3.4758
Iteration: 1337; Percent complete: 33.4%; Average loss: 3.2387
Iteration: 1338; Percent complete: 33.5%; Average loss: 3.1353
Iteration: 1339; Percent complete: 33.5%; Average loss: 3.3341
Iteration: 1340; Percent complete: 33.5%; Average loss: 3.4654
Iteration: 1341; Percent complete: 33.5%; Average loss: 3.1349
Iteration: 1342; Percent complete: 33.6%; Average loss: 3.2993
Iteration: 1343; Percent complete: 33.6%; Average loss: 3.1209
Iteration: 1344; Percent complete: 33.6%; Average loss: 3.3624
Iteration: 1345; Percent complete: 33.6%; Average loss: 3.3508
Iteration: 1346; Percent complete: 33.7%; Average loss: 3.0245
Iteration: 1347; Percent complete: 33.7%; Average loss: 3.3446
Iteration: 1348; Percent complete: 33.7%; Average loss: 3.4064
Iteration: 1349; Percent complete: 33.7%; Average loss: 3.3198
Iteration: 1350; Percent complete: 33.8%; Average loss: 3.4660
Iteration: 1351; Percent complete: 33.8%; Average loss: 3.1197
Iteration: 1352; Percent complete: 33.8%; Average loss: 3.4816
Iteration: 1353; Percent complete: 33.8%; Average loss: 3.4092
Iteration: 1354; Percent complete: 33.9%; Average loss: 3.1850
Iteration: 1355; Percent complete: 33.9%; Average loss: 3.3011
Iteration: 1356; Percent complete: 33.9%; Average loss: 3.3445
Iteration: 1357; Percent complete: 33.9%; Average loss: 3.5676
Iteration: 1358; Percent complete: 34.0%; Average loss: 3.3175
Iteration: 1359; Percent complete: 34.0%; Average loss: 3.5758
Iteration: 1360; Percent complete: 34.0%; Average loss: 3.0686
Iteration: 1361; Percent complete: 34.0%; Average loss: 3.1018
Iteration: 1362; Percent complete: 34.1%; Average loss: 3.4027
Iteration: 1363; Percent complete: 34.1%; Average loss: 3.4235
Iteration: 1364; Percent complete: 34.1%; Average loss: 3.2070
Iteration: 1365; Percent complete: 34.1%; Average loss: 3.4107
Iteration: 1366; Percent complete: 34.2%; Average loss: 3.2742
Iteration: 1367; Percent complete: 34.2%; Average loss: 3.3266
Iteration: 1368; Percent complete: 34.2%; Average loss: 3.4072
Iteration: 1369; Percent complete: 34.2%; Average loss: 3.4958
Iteration: 1370; Percent complete: 34.2%; Average loss: 3.2853
Iteration: 1371; Percent complete: 34.3%; Average loss: 3.5436
Iteration: 1372; Percent complete: 34.3%; Average loss: 3.5354
Iteration: 1373; Percent complete: 34.3%; Average loss: 3.2952
Iteration: 1374; Percent complete: 34.4%; Average loss: 3.4385
Iteration: 1375; Percent complete: 34.4%; Average loss: 3.3569
Iteration: 1376; Percent complete: 34.4%; Average loss: 3.0008
Iteration: 1377; Percent complete: 34.4%; Average loss: 3.8009
Iteration: 1378; Percent complete: 34.4%; Average loss: 3.3266
Iteration: 1379; Percent complete: 34.5%; Average loss: 3.1768
Iteration: 1380; Percent complete: 34.5%; Average loss: 3.5706
Iteration: 1381; Percent complete: 34.5%; Average loss: 3.4232
Iteration: 1382; Percent complete: 34.5%; Average loss: 3.5156
Iteration: 1383; Percent complete: 34.6%; Average loss: 3.4124
Iteration: 1384; Percent complete: 34.6%; Average loss: 3.2435
Iteration: 1385; Percent complete: 34.6%; Average loss: 3.3121
Iteration: 1386; Percent complete: 34.6%; Average loss: 3.4889
Iteration: 1387; Percent complete: 34.7%; Average loss: 3.3456
Iteration: 1388; Percent complete: 34.7%; Average loss: 3.4371
Iteration: 1389; Percent complete: 34.7%; Average loss: 3.3818
Iteration: 1390; Percent complete: 34.8%; Average loss: 3.4394
Iteration: 1391; Percent complete: 34.8%; Average loss: 3.1665
Iteration: 1392; Percent complete: 34.8%; Average loss: 3.4033
Iteration: 1393; Percent complete: 34.8%; Average loss: 3.5298
Iteration: 1394; Percent complete: 34.8%; Average loss: 3.2085
Iteration: 1395; Percent complete: 34.9%; Average loss: 3.0194
Iteration: 1396; Percent complete: 34.9%; Average loss: 3.3512
Iteration: 1397; Percent complete: 34.9%; Average loss: 3.3296
Iteration: 1398; Percent complete: 34.9%; Average loss: 3.0238
Iteration: 1399; Percent complete: 35.0%; Average loss: 3.1455
Iteration: 1400; Percent complete: 35.0%; Average loss: 2.9905
Iteration: 1401; Percent complete: 35.0%; Average loss: 3.1701
Iteration: 1402; Percent complete: 35.0%; Average loss: 3.4527
Iteration: 1403; Percent complete: 35.1%; Average loss: 3.2137
Iteration: 1404; Percent complete: 35.1%; Average loss: 3.1841
Iteration: 1405; Percent complete: 35.1%; Average loss: 3.2687
Iteration: 1406; Percent complete: 35.1%; Average loss: 3.4170
Iteration: 1407; Percent complete: 35.2%; Average loss: 3.2435
Iteration: 1408; Percent complete: 35.2%; Average loss: 3.5382
Iteration: 1409; Percent complete: 35.2%; Average loss: 3.2707
Iteration: 1410; Percent complete: 35.2%; Average loss: 3.4647
Iteration: 1411; Percent complete: 35.3%; Average loss: 3.3420
Iteration: 1412; Percent complete: 35.3%; Average loss: 3.1993
Iteration: 1413; Percent complete: 35.3%; Average loss: 3.5202
Iteration: 1414; Percent complete: 35.4%; Average loss: 3.2845
Iteration: 1415; Percent complete: 35.4%; Average loss: 3.2461
Iteration: 1416; Percent complete: 35.4%; Average loss: 3.3519
Iteration: 1417; Percent complete: 35.4%; Average loss: 3.3718
Iteration: 1418; Percent complete: 35.4%; Average loss: 3.4330
Iteration: 1419; Percent complete: 35.5%; Average loss: 3.4659
Iteration: 1420; Percent complete: 35.5%; Average loss: 3.4026
Iteration: 1421; Percent complete: 35.5%; Average loss: 3.2468
Iteration: 1422; Percent complete: 35.5%; Average loss: 3.3979
Iteration: 1423; Percent complete: 35.6%; Average loss: 3.1711
Iteration: 1424; Percent complete: 35.6%; Average loss: 3.4085
Iteration: 1425; Percent complete: 35.6%; Average loss: 3.2346
Iteration: 1426; Percent complete: 35.6%; Average loss: 3.3699
Iteration: 1427; Percent complete: 35.7%; Average loss: 3.3563
Iteration: 1428; Percent complete: 35.7%; Average loss: 3.4010
Iteration: 1429; Percent complete: 35.7%; Average loss: 3.3687
Iteration: 1430; Percent complete: 35.8%; Average loss: 3.1894
Iteration: 1431; Percent complete: 35.8%; Average loss: 3.4224
Iteration: 1432; Percent complete: 35.8%; Average loss: 3.1035
Iteration: 1433; Percent complete: 35.8%; Average loss: 3.2205
Iteration: 1434; Percent complete: 35.9%; Average loss: 3.0578
Iteration: 1435; Percent complete: 35.9%; Average loss: 3.3864
Iteration: 1436; Percent complete: 35.9%; Average loss: 3.4729
Iteration: 1437; Percent complete: 35.9%; Average loss: 3.4461
Iteration: 1438; Percent complete: 35.9%; Average loss: 3.3293
Iteration: 1439; Percent complete: 36.0%; Average loss: 3.2806
Iteration: 1440; Percent complete: 36.0%; Average loss: 3.2169
Iteration: 1441; Percent complete: 36.0%; Average loss: 3.4162
Iteration: 1442; Percent complete: 36.0%; Average loss: 3.2754
Iteration: 1443; Percent complete: 36.1%; Average loss: 3.5577
Iteration: 1444; Percent complete: 36.1%; Average loss: 3.1890
Iteration: 1445; Percent complete: 36.1%; Average loss: 3.2804
Iteration: 1446; Percent complete: 36.1%; Average loss: 3.4320
Iteration: 1447; Percent complete: 36.2%; Average loss: 3.2852
Iteration: 1448; Percent complete: 36.2%; Average loss: 3.5093
Iteration: 1449; Percent complete: 36.2%; Average loss: 3.1936
Iteration: 1450; Percent complete: 36.2%; Average loss: 3.1954
Iteration: 1451; Percent complete: 36.3%; Average loss: 3.3873
Iteration: 1452; Percent complete: 36.3%; Average loss: 3.2636
Iteration: 1453; Percent complete: 36.3%; Average loss: 3.2706
Iteration: 1454; Percent complete: 36.4%; Average loss: 3.1513
Iteration: 1455; Percent complete: 36.4%; Average loss: 3.4187
Iteration: 1456; Percent complete: 36.4%; Average loss: 3.4063
Iteration: 1457; Percent complete: 36.4%; Average loss: 3.2307
Iteration: 1458; Percent complete: 36.4%; Average loss: 3.3445
Iteration: 1459; Percent complete: 36.5%; Average loss: 3.2706
Iteration: 1460; Percent complete: 36.5%; Average loss: 3.0638
Iteration: 1461; Percent complete: 36.5%; Average loss: 2.9783
Iteration: 1462; Percent complete: 36.5%; Average loss: 3.4609
Iteration: 1463; Percent complete: 36.6%; Average loss: 3.1469
Iteration: 1464; Percent complete: 36.6%; Average loss: 3.3021
Iteration: 1465; Percent complete: 36.6%; Average loss: 3.2708
Iteration: 1466; Percent complete: 36.6%; Average loss: 3.1350
Iteration: 1467; Percent complete: 36.7%; Average loss: 3.1991
Iteration: 1468; Percent complete: 36.7%; Average loss: 3.3467
Iteration: 1469; Percent complete: 36.7%; Average loss: 3.2377
Iteration: 1470; Percent complete: 36.8%; Average loss: 3.1275
Iteration: 1471; Percent complete: 36.8%; Average loss: 3.3203
Iteration: 1472; Percent complete: 36.8%; Average loss: 3.6607
Iteration: 1473; Percent complete: 36.8%; Average loss: 3.4064
Iteration: 1474; Percent complete: 36.9%; Average loss: 3.1828
Iteration: 1475; Percent complete: 36.9%; Average loss: 3.3238
Iteration: 1476; Percent complete: 36.9%; Average loss: 3.4740
Iteration: 1477; Percent complete: 36.9%; Average loss: 3.0380
Iteration: 1478; Percent complete: 37.0%; Average loss: 3.6326
Iteration: 1479; Percent complete: 37.0%; Average loss: 3.6307
Iteration: 1480; Percent complete: 37.0%; Average loss: 3.3536
Iteration: 1481; Percent complete: 37.0%; Average loss: 3.3328
Iteration: 1482; Percent complete: 37.0%; Average loss: 3.5123
Iteration: 1483; Percent complete: 37.1%; Average loss: 3.2374
Iteration: 1484; Percent complete: 37.1%; Average loss: 3.2034
Iteration: 1485; Percent complete: 37.1%; Average loss: 3.4141
Iteration: 1486; Percent complete: 37.1%; Average loss: 3.3675
Iteration: 1487; Percent complete: 37.2%; Average loss: 3.1559
Iteration: 1488; Percent complete: 37.2%; Average loss: 3.1924
Iteration: 1489; Percent complete: 37.2%; Average loss: 3.3427
Iteration: 1490; Percent complete: 37.2%; Average loss: 3.5639
Iteration: 1491; Percent complete: 37.3%; Average loss: 3.3479
Iteration: 1492; Percent complete: 37.3%; Average loss: 3.2384
Iteration: 1493; Percent complete: 37.3%; Average loss: 3.1886
Iteration: 1494; Percent complete: 37.4%; Average loss: 3.6953
Iteration: 1495; Percent complete: 37.4%; Average loss: 3.5848
Iteration: 1496; Percent complete: 37.4%; Average loss: 3.2389
Iteration: 1497; Percent complete: 37.4%; Average loss: 3.3284
Iteration: 1498; Percent complete: 37.5%; Average loss: 3.1658
Iteration: 1499; Percent complete: 37.5%; Average loss: 3.3764
Iteration: 1500; Percent complete: 37.5%; Average loss: 3.1852
Iteration: 1501; Percent complete: 37.5%; Average loss: 3.2811
Iteration: 1502; Percent complete: 37.5%; Average loss: 3.3556
Iteration: 1503; Percent complete: 37.6%; Average loss: 3.0454
Iteration: 1504; Percent complete: 37.6%; Average loss: 3.2825
Iteration: 1505; Percent complete: 37.6%; Average loss: 3.1059
Iteration: 1506; Percent complete: 37.6%; Average loss: 3.2047
Iteration: 1507; Percent complete: 37.7%; Average loss: 3.1982
Iteration: 1508; Percent complete: 37.7%; Average loss: 2.9349
Iteration: 1509; Percent complete: 37.7%; Average loss: 3.3957
Iteration: 1510; Percent complete: 37.8%; Average loss: 2.9322
Iteration: 1511; Percent complete: 37.8%; Average loss: 3.1681
Iteration: 1512; Percent complete: 37.8%; Average loss: 3.5575
Iteration: 1513; Percent complete: 37.8%; Average loss: 2.9218
Iteration: 1514; Percent complete: 37.9%; Average loss: 3.2164
Iteration: 1515; Percent complete: 37.9%; Average loss: 3.3670
Iteration: 1516; Percent complete: 37.9%; Average loss: 3.2559
Iteration: 1517; Percent complete: 37.9%; Average loss: 3.2558
Iteration: 1518; Percent complete: 38.0%; Average loss: 3.3094
Iteration: 1519; Percent complete: 38.0%; Average loss: 3.5517
Iteration: 1520; Percent complete: 38.0%; Average loss: 3.2588
Iteration: 1521; Percent complete: 38.0%; Average loss: 3.1360
Iteration: 1522; Percent complete: 38.0%; Average loss: 3.2725
Iteration: 1523; Percent complete: 38.1%; Average loss: 3.3827
Iteration: 1524; Percent complete: 38.1%; Average loss: 3.2261
Iteration: 1525; Percent complete: 38.1%; Average loss: 3.2958
Iteration: 1526; Percent complete: 38.1%; Average loss: 3.1877
Iteration: 1527; Percent complete: 38.2%; Average loss: 3.1610
Iteration: 1528; Percent complete: 38.2%; Average loss: 3.1740
Iteration: 1529; Percent complete: 38.2%; Average loss: 3.4668
Iteration: 1530; Percent complete: 38.2%; Average loss: 3.3595
Iteration: 1531; Percent complete: 38.3%; Average loss: 3.3976
Iteration: 1532; Percent complete: 38.3%; Average loss: 3.3456
Iteration: 1533; Percent complete: 38.3%; Average loss: 3.2906
Iteration: 1534; Percent complete: 38.4%; Average loss: 3.3467
Iteration: 1535; Percent complete: 38.4%; Average loss: 3.2514
Iteration: 1536; Percent complete: 38.4%; Average loss: 3.0888
Iteration: 1537; Percent complete: 38.4%; Average loss: 3.0713
Iteration: 1538; Percent complete: 38.5%; Average loss: 3.2358
Iteration: 1539; Percent complete: 38.5%; Average loss: 3.2046
Iteration: 1540; Percent complete: 38.5%; Average loss: 3.6050
Iteration: 1541; Percent complete: 38.5%; Average loss: 3.4687
Iteration: 1542; Percent complete: 38.6%; Average loss: 3.2880
Iteration: 1543; Percent complete: 38.6%; Average loss: 3.3498
Iteration: 1544; Percent complete: 38.6%; Average loss: 3.2711
Iteration: 1545; Percent complete: 38.6%; Average loss: 3.0882
Iteration: 1546; Percent complete: 38.6%; Average loss: 3.4675
Iteration: 1547; Percent complete: 38.7%; Average loss: 3.3136
Iteration: 1548; Percent complete: 38.7%; Average loss: 3.4352
Iteration: 1549; Percent complete: 38.7%; Average loss: 3.2541
Iteration: 1550; Percent complete: 38.8%; Average loss: 3.2738
Iteration: 1551; Percent complete: 38.8%; Average loss: 3.2710
Iteration: 1552; Percent complete: 38.8%; Average loss: 3.2654
Iteration: 1553; Percent complete: 38.8%; Average loss: 3.4501
Iteration: 1554; Percent complete: 38.9%; Average loss: 3.2156
Iteration: 1555; Percent complete: 38.9%; Average loss: 3.3635
Iteration: 1556; Percent complete: 38.9%; Average loss: 3.4191
Iteration: 1557; Percent complete: 38.9%; Average loss: 3.1605
Iteration: 1558; Percent complete: 39.0%; Average loss: 3.2360
Iteration: 1559; Percent complete: 39.0%; Average loss: 3.4577
Iteration: 1560; Percent complete: 39.0%; Average loss: 3.1584
Iteration: 1561; Percent complete: 39.0%; Average loss: 3.4698
Iteration: 1562; Percent complete: 39.1%; Average loss: 3.4334
Iteration: 1563; Percent complete: 39.1%; Average loss: 3.2543
Iteration: 1564; Percent complete: 39.1%; Average loss: 3.3854
Iteration: 1565; Percent complete: 39.1%; Average loss: 3.2546
Iteration: 1566; Percent complete: 39.1%; Average loss: 3.3022
Iteration: 1567; Percent complete: 39.2%; Average loss: 3.3012
Iteration: 1568; Percent complete: 39.2%; Average loss: 3.5721
Iteration: 1569; Percent complete: 39.2%; Average loss: 3.0039
Iteration: 1570; Percent complete: 39.2%; Average loss: 3.2307
Iteration: 1571; Percent complete: 39.3%; Average loss: 3.2429
Iteration: 1572; Percent complete: 39.3%; Average loss: 3.2137
Iteration: 1573; Percent complete: 39.3%; Average loss: 3.4119
Iteration: 1574; Percent complete: 39.4%; Average loss: 3.3878
Iteration: 1575; Percent complete: 39.4%; Average loss: 3.1604
Iteration: 1576; Percent complete: 39.4%; Average loss: 3.1626
Iteration: 1577; Percent complete: 39.4%; Average loss: 3.3007
Iteration: 1578; Percent complete: 39.5%; Average loss: 3.2592
Iteration: 1579; Percent complete: 39.5%; Average loss: 3.2758
Iteration: 1580; Percent complete: 39.5%; Average loss: 3.6267
Iteration: 1581; Percent complete: 39.5%; Average loss: 3.4473
Iteration: 1582; Percent complete: 39.6%; Average loss: 3.4671
Iteration: 1583; Percent complete: 39.6%; Average loss: 3.4252
Iteration: 1584; Percent complete: 39.6%; Average loss: 3.3068
Iteration: 1585; Percent complete: 39.6%; Average loss: 3.5446
Iteration: 1586; Percent complete: 39.6%; Average loss: 3.0792
Iteration: 1587; Percent complete: 39.7%; Average loss: 3.2319
Iteration: 1588; Percent complete: 39.7%; Average loss: 3.1976
Iteration: 1589; Percent complete: 39.7%; Average loss: 3.4802
Iteration: 1590; Percent complete: 39.8%; Average loss: 3.2631
Iteration: 1591; Percent complete: 39.8%; Average loss: 3.0641
Iteration: 1592; Percent complete: 39.8%; Average loss: 3.4544
Iteration: 1593; Percent complete: 39.8%; Average loss: 3.3098
Iteration: 1594; Percent complete: 39.9%; Average loss: 3.2107
Iteration: 1595; Percent complete: 39.9%; Average loss: 3.4340
Iteration: 1596; Percent complete: 39.9%; Average loss: 3.1531
Iteration: 1597; Percent complete: 39.9%; Average loss: 3.3466
Iteration: 1598; Percent complete: 40.0%; Average loss: 3.1351
Iteration: 1599; Percent complete: 40.0%; Average loss: 3.4859
Iteration: 1600; Percent complete: 40.0%; Average loss: 3.3846
Iteration: 1601; Percent complete: 40.0%; Average loss: 3.2511
Iteration: 1602; Percent complete: 40.1%; Average loss: 3.2000
Iteration: 1603; Percent complete: 40.1%; Average loss: 3.3334
Iteration: 1604; Percent complete: 40.1%; Average loss: 3.1224
Iteration: 1605; Percent complete: 40.1%; Average loss: 3.2327
Iteration: 1606; Percent complete: 40.2%; Average loss: 3.3431
Iteration: 1607; Percent complete: 40.2%; Average loss: 3.2837
Iteration: 1608; Percent complete: 40.2%; Average loss: 3.3838
Iteration: 1609; Percent complete: 40.2%; Average loss: 3.3573
Iteration: 1610; Percent complete: 40.2%; Average loss: 3.1297
Iteration: 1611; Percent complete: 40.3%; Average loss: 3.4124
Iteration: 1612; Percent complete: 40.3%; Average loss: 3.1478
Iteration: 1613; Percent complete: 40.3%; Average loss: 3.0738
Iteration: 1614; Percent complete: 40.4%; Average loss: 3.2790
Iteration: 1615; Percent complete: 40.4%; Average loss: 3.2837
Iteration: 1616; Percent complete: 40.4%; Average loss: 3.1461
Iteration: 1617; Percent complete: 40.4%; Average loss: 3.3423
Iteration: 1618; Percent complete: 40.5%; Average loss: 3.3120
Iteration: 1619; Percent complete: 40.5%; Average loss: 3.3059
Iteration: 1620; Percent complete: 40.5%; Average loss: 3.5898
Iteration: 1621; Percent complete: 40.5%; Average loss: 3.0933
Iteration: 1622; Percent complete: 40.6%; Average loss: 2.9213
Iteration: 1623; Percent complete: 40.6%; Average loss: 3.4153
Iteration: 1624; Percent complete: 40.6%; Average loss: 3.3667
Iteration: 1625; Percent complete: 40.6%; Average loss: 3.3382
Iteration: 1626; Percent complete: 40.6%; Average loss: 3.3901
Iteration: 1627; Percent complete: 40.7%; Average loss: 3.3473
Iteration: 1628; Percent complete: 40.7%; Average loss: 3.3875
Iteration: 1629; Percent complete: 40.7%; Average loss: 3.1542
Iteration: 1630; Percent complete: 40.8%; Average loss: 3.3152
Iteration: 1631; Percent complete: 40.8%; Average loss: 3.1604
Iteration: 1632; Percent complete: 40.8%; Average loss: 3.0765
Iteration: 1633; Percent complete: 40.8%; Average loss: 3.2234
Iteration: 1634; Percent complete: 40.8%; Average loss: 3.1725
Iteration: 1635; Percent complete: 40.9%; Average loss: 3.2468
Iteration: 1636; Percent complete: 40.9%; Average loss: 3.1424
Iteration: 1637; Percent complete: 40.9%; Average loss: 3.0094
Iteration: 1638; Percent complete: 40.9%; Average loss: 3.1595
Iteration: 1639; Percent complete: 41.0%; Average loss: 3.4891
Iteration: 1640; Percent complete: 41.0%; Average loss: 3.1030
Iteration: 1641; Percent complete: 41.0%; Average loss: 3.1865
Iteration: 1642; Percent complete: 41.0%; Average loss: 3.2970
Iteration: 1643; Percent complete: 41.1%; Average loss: 3.2997
Iteration: 1644; Percent complete: 41.1%; Average loss: 3.4412
Iteration: 1645; Percent complete: 41.1%; Average loss: 3.1460
Iteration: 1646; Percent complete: 41.1%; Average loss: 3.3175
Iteration: 1647; Percent complete: 41.2%; Average loss: 3.2048
Iteration: 1648; Percent complete: 41.2%; Average loss: 3.3547
Iteration: 1649; Percent complete: 41.2%; Average loss: 3.2310
Iteration: 1650; Percent complete: 41.2%; Average loss: 3.0978
Iteration: 1651; Percent complete: 41.3%; Average loss: 3.3480
Iteration: 1652; Percent complete: 41.3%; Average loss: 3.2720
Iteration: 1653; Percent complete: 41.3%; Average loss: 3.0122
Iteration: 1654; Percent complete: 41.3%; Average loss: 3.1171
Iteration: 1655; Percent complete: 41.4%; Average loss: 3.1559
Iteration: 1656; Percent complete: 41.4%; Average loss: 3.3425
Iteration: 1657; Percent complete: 41.4%; Average loss: 3.1542
Iteration: 1658; Percent complete: 41.4%; Average loss: 3.2723
Iteration: 1659; Percent complete: 41.5%; Average loss: 3.0934
Iteration: 1660; Percent complete: 41.5%; Average loss: 3.2247
Iteration: 1661; Percent complete: 41.5%; Average loss: 2.9709
Iteration: 1662; Percent complete: 41.5%; Average loss: 3.2690
Iteration: 1663; Percent complete: 41.6%; Average loss: 3.1796
Iteration: 1664; Percent complete: 41.6%; Average loss: 3.3143
Iteration: 1665; Percent complete: 41.6%; Average loss: 3.2776
Iteration: 1666; Percent complete: 41.6%; Average loss: 3.2233
Iteration: 1667; Percent complete: 41.7%; Average loss: 3.2503
Iteration: 1668; Percent complete: 41.7%; Average loss: 3.1394
Iteration: 1669; Percent complete: 41.7%; Average loss: 3.2403
Iteration: 1670; Percent complete: 41.8%; Average loss: 3.3145
Iteration: 1671; Percent complete: 41.8%; Average loss: 3.3339
Iteration: 1672; Percent complete: 41.8%; Average loss: 3.1796
Iteration: 1673; Percent complete: 41.8%; Average loss: 3.1970
Iteration: 1674; Percent complete: 41.9%; Average loss: 3.0952
Iteration: 1675; Percent complete: 41.9%; Average loss: 3.4530
Iteration: 1676; Percent complete: 41.9%; Average loss: 3.1250
Iteration: 1677; Percent complete: 41.9%; Average loss: 3.3322
Iteration: 1678; Percent complete: 41.9%; Average loss: 3.3537
Iteration: 1679; Percent complete: 42.0%; Average loss: 3.3503
Iteration: 1680; Percent complete: 42.0%; Average loss: 3.3016
Iteration: 1681; Percent complete: 42.0%; Average loss: 3.0670
Iteration: 1682; Percent complete: 42.0%; Average loss: 3.2754
Iteration: 1683; Percent complete: 42.1%; Average loss: 3.4398
Iteration: 1684; Percent complete: 42.1%; Average loss: 3.1970
Iteration: 1685; Percent complete: 42.1%; Average loss: 3.2584
Iteration: 1686; Percent complete: 42.1%; Average loss: 3.2808
Iteration: 1687; Percent complete: 42.2%; Average loss: 3.3765
Iteration: 1688; Percent complete: 42.2%; Average loss: 3.2109
Iteration: 1689; Percent complete: 42.2%; Average loss: 3.1530
Iteration: 1690; Percent complete: 42.2%; Average loss: 3.3300
Iteration: 1691; Percent complete: 42.3%; Average loss: 3.3281
Iteration: 1692; Percent complete: 42.3%; Average loss: 3.2467
Iteration: 1693; Percent complete: 42.3%; Average loss: 3.3082
Iteration: 1694; Percent complete: 42.4%; Average loss: 3.3633
Iteration: 1695; Percent complete: 42.4%; Average loss: 3.2194
Iteration: 1696; Percent complete: 42.4%; Average loss: 3.2342
Iteration: 1697; Percent complete: 42.4%; Average loss: 3.1391
Iteration: 1698; Percent complete: 42.4%; Average loss: 3.3145
Iteration: 1699; Percent complete: 42.5%; Average loss: 3.2582
Iteration: 1700; Percent complete: 42.5%; Average loss: 3.1322
Iteration: 1701; Percent complete: 42.5%; Average loss: 2.9353
Iteration: 1702; Percent complete: 42.5%; Average loss: 2.9088
Iteration: 1703; Percent complete: 42.6%; Average loss: 3.4067
Iteration: 1704; Percent complete: 42.6%; Average loss: 3.0896
Iteration: 1705; Percent complete: 42.6%; Average loss: 3.1764
Iteration: 1706; Percent complete: 42.6%; Average loss: 3.2341
Iteration: 1707; Percent complete: 42.7%; Average loss: 3.4767
Iteration: 1708; Percent complete: 42.7%; Average loss: 3.1424
Iteration: 1709; Percent complete: 42.7%; Average loss: 3.1852
Iteration: 1710; Percent complete: 42.8%; Average loss: 3.2949
Iteration: 1711; Percent complete: 42.8%; Average loss: 3.6801
Iteration: 1712; Percent complete: 42.8%; Average loss: 3.4333
Iteration: 1713; Percent complete: 42.8%; Average loss: 3.2099
Iteration: 1714; Percent complete: 42.9%; Average loss: 3.2764
Iteration: 1715; Percent complete: 42.9%; Average loss: 3.3786
Iteration: 1716; Percent complete: 42.9%; Average loss: 3.3092
Iteration: 1717; Percent complete: 42.9%; Average loss: 3.1970
Iteration: 1718; Percent complete: 43.0%; Average loss: 3.2539
Iteration: 1719; Percent complete: 43.0%; Average loss: 3.2503
Iteration: 1720; Percent complete: 43.0%; Average loss: 3.4296
Iteration: 1721; Percent complete: 43.0%; Average loss: 3.3935
Iteration: 1722; Percent complete: 43.0%; Average loss: 3.3546
Iteration: 1723; Percent complete: 43.1%; Average loss: 3.1472
Iteration: 1724; Percent complete: 43.1%; Average loss: 3.3800
Iteration: 1725; Percent complete: 43.1%; Average loss: 3.2387
Iteration: 1726; Percent complete: 43.1%; Average loss: 3.2898
Iteration: 1727; Percent complete: 43.2%; Average loss: 3.3427
Iteration: 1728; Percent complete: 43.2%; Average loss: 3.4329
Iteration: 1729; Percent complete: 43.2%; Average loss: 3.0428
Iteration: 1730; Percent complete: 43.2%; Average loss: 3.3042
Iteration: 1731; Percent complete: 43.3%; Average loss: 3.2811
Iteration: 1732; Percent complete: 43.3%; Average loss: 3.2720
Iteration: 1733; Percent complete: 43.3%; Average loss: 2.9724
Iteration: 1734; Percent complete: 43.4%; Average loss: 3.0596
Iteration: 1735; Percent complete: 43.4%; Average loss: 3.2289
Iteration: 1736; Percent complete: 43.4%; Average loss: 3.2371
Iteration: 1737; Percent complete: 43.4%; Average loss: 3.1310
Iteration: 1738; Percent complete: 43.5%; Average loss: 3.3377
Iteration: 1739; Percent complete: 43.5%; Average loss: 3.0105
Iteration: 1740; Percent complete: 43.5%; Average loss: 3.2333
Iteration: 1741; Percent complete: 43.5%; Average loss: 3.0533
Iteration: 1742; Percent complete: 43.5%; Average loss: 3.3368
Iteration: 1743; Percent complete: 43.6%; Average loss: 3.2564
Iteration: 1744; Percent complete: 43.6%; Average loss: 3.1726
Iteration: 1745; Percent complete: 43.6%; Average loss: 3.3298
Iteration: 1746; Percent complete: 43.6%; Average loss: 3.3548
Iteration: 1747; Percent complete: 43.7%; Average loss: 3.2832
Iteration: 1748; Percent complete: 43.7%; Average loss: 3.2664
Iteration: 1749; Percent complete: 43.7%; Average loss: 3.3260
Iteration: 1750; Percent complete: 43.8%; Average loss: 3.2545
Iteration: 1751; Percent complete: 43.8%; Average loss: 3.3269
Iteration: 1752; Percent complete: 43.8%; Average loss: 3.1759
Iteration: 1753; Percent complete: 43.8%; Average loss: 3.2739
Iteration: 1754; Percent complete: 43.9%; Average loss: 3.3581
Iteration: 1755; Percent complete: 43.9%; Average loss: 3.1249
Iteration: 1756; Percent complete: 43.9%; Average loss: 3.0306
Iteration: 1757; Percent complete: 43.9%; Average loss: 3.2474
Iteration: 1758; Percent complete: 44.0%; Average loss: 3.2258
Iteration: 1759; Percent complete: 44.0%; Average loss: 3.2227
Iteration: 1760; Percent complete: 44.0%; Average loss: 3.0180
Iteration: 1761; Percent complete: 44.0%; Average loss: 3.1512
Iteration: 1762; Percent complete: 44.0%; Average loss: 3.3239
Iteration: 1763; Percent complete: 44.1%; Average loss: 3.1343
Iteration: 1764; Percent complete: 44.1%; Average loss: 3.2860
Iteration: 1765; Percent complete: 44.1%; Average loss: 3.0836
Iteration: 1766; Percent complete: 44.1%; Average loss: 3.1725
Iteration: 1767; Percent complete: 44.2%; Average loss: 3.1051
Iteration: 1768; Percent complete: 44.2%; Average loss: 3.2529
Iteration: 1769; Percent complete: 44.2%; Average loss: 3.1671
Iteration: 1770; Percent complete: 44.2%; Average loss: 3.2439
Iteration: 1771; Percent complete: 44.3%; Average loss: 3.2535
Iteration: 1772; Percent complete: 44.3%; Average loss: 3.3010
Iteration: 1773; Percent complete: 44.3%; Average loss: 3.2438
Iteration: 1774; Percent complete: 44.4%; Average loss: 3.3068
Iteration: 1775; Percent complete: 44.4%; Average loss: 3.1590
Iteration: 1776; Percent complete: 44.4%; Average loss: 3.2097
Iteration: 1777; Percent complete: 44.4%; Average loss: 2.9020
Iteration: 1778; Percent complete: 44.5%; Average loss: 3.2168
Iteration: 1779; Percent complete: 44.5%; Average loss: 3.1102
Iteration: 1780; Percent complete: 44.5%; Average loss: 3.4255
Iteration: 1781; Percent complete: 44.5%; Average loss: 3.3038
Iteration: 1782; Percent complete: 44.5%; Average loss: 3.1064
Iteration: 1783; Percent complete: 44.6%; Average loss: 3.1988
Iteration: 1784; Percent complete: 44.6%; Average loss: 3.3512
Iteration: 1785; Percent complete: 44.6%; Average loss: 3.0708
Iteration: 1786; Percent complete: 44.6%; Average loss: 3.2355
Iteration: 1787; Percent complete: 44.7%; Average loss: 3.0953
Iteration: 1788; Percent complete: 44.7%; Average loss: 2.9998
Iteration: 1789; Percent complete: 44.7%; Average loss: 3.4002
Iteration: 1790; Percent complete: 44.8%; Average loss: 3.0080
Iteration: 1791; Percent complete: 44.8%; Average loss: 3.2352
Iteration: 1792; Percent complete: 44.8%; Average loss: 3.2270
Iteration: 1793; Percent complete: 44.8%; Average loss: 3.2408
Iteration: 1794; Percent complete: 44.9%; Average loss: 3.3257
Iteration: 1795; Percent complete: 44.9%; Average loss: 3.1964
Iteration: 1796; Percent complete: 44.9%; Average loss: 3.2535
Iteration: 1797; Percent complete: 44.9%; Average loss: 3.1854
Iteration: 1798; Percent complete: 45.0%; Average loss: 3.1728
Iteration: 1799; Percent complete: 45.0%; Average loss: 3.1533
Iteration: 1800; Percent complete: 45.0%; Average loss: 3.4538
Iteration: 1801; Percent complete: 45.0%; Average loss: 3.1524
Iteration: 1802; Percent complete: 45.1%; Average loss: 3.1420
Iteration: 1803; Percent complete: 45.1%; Average loss: 3.2081
Iteration: 1804; Percent complete: 45.1%; Average loss: 3.3607
Iteration: 1805; Percent complete: 45.1%; Average loss: 3.3200
Iteration: 1806; Percent complete: 45.1%; Average loss: 3.2826
Iteration: 1807; Percent complete: 45.2%; Average loss: 3.4802
Iteration: 1808; Percent complete: 45.2%; Average loss: 3.4306
Iteration: 1809; Percent complete: 45.2%; Average loss: 3.2518
Iteration: 1810; Percent complete: 45.2%; Average loss: 3.3997
Iteration: 1811; Percent complete: 45.3%; Average loss: 3.3589
Iteration: 1812; Percent complete: 45.3%; Average loss: 3.3485
Iteration: 1813; Percent complete: 45.3%; Average loss: 3.0921
Iteration: 1814; Percent complete: 45.4%; Average loss: 3.3622
Iteration: 1815; Percent complete: 45.4%; Average loss: 3.3516
Iteration: 1816; Percent complete: 45.4%; Average loss: 2.9022
Iteration: 1817; Percent complete: 45.4%; Average loss: 3.2554
Iteration: 1818; Percent complete: 45.5%; Average loss: 3.3143
Iteration: 1819; Percent complete: 45.5%; Average loss: 3.1725
Iteration: 1820; Percent complete: 45.5%; Average loss: 2.9438
Iteration: 1821; Percent complete: 45.5%; Average loss: 3.0712
Iteration: 1822; Percent complete: 45.6%; Average loss: 3.0540
Iteration: 1823; Percent complete: 45.6%; Average loss: 3.5431
Iteration: 1824; Percent complete: 45.6%; Average loss: 3.1571
Iteration: 1825; Percent complete: 45.6%; Average loss: 3.4339
Iteration: 1826; Percent complete: 45.6%; Average loss: 2.8519
Iteration: 1827; Percent complete: 45.7%; Average loss: 3.0372
Iteration: 1828; Percent complete: 45.7%; Average loss: 3.1696
Iteration: 1829; Percent complete: 45.7%; Average loss: 3.4680
Iteration: 1830; Percent complete: 45.8%; Average loss: 3.2369
Iteration: 1831; Percent complete: 45.8%; Average loss: 2.9861
Iteration: 1832; Percent complete: 45.8%; Average loss: 3.1067
Iteration: 1833; Percent complete: 45.8%; Average loss: 3.0964
Iteration: 1834; Percent complete: 45.9%; Average loss: 3.1945
Iteration: 1835; Percent complete: 45.9%; Average loss: 3.5193
Iteration: 1836; Percent complete: 45.9%; Average loss: 3.0755
Iteration: 1837; Percent complete: 45.9%; Average loss: 3.1492
Iteration: 1838; Percent complete: 46.0%; Average loss: 3.0795
Iteration: 1839; Percent complete: 46.0%; Average loss: 3.2730
Iteration: 1840; Percent complete: 46.0%; Average loss: 3.1958
Iteration: 1841; Percent complete: 46.0%; Average loss: 3.2035
Iteration: 1842; Percent complete: 46.1%; Average loss: 3.3586
Iteration: 1843; Percent complete: 46.1%; Average loss: 3.0348
Iteration: 1844; Percent complete: 46.1%; Average loss: 3.2771
Iteration: 1845; Percent complete: 46.1%; Average loss: 3.0922
Iteration: 1846; Percent complete: 46.2%; Average loss: 2.9237
Iteration: 1847; Percent complete: 46.2%; Average loss: 3.4554
Iteration: 1848; Percent complete: 46.2%; Average loss: 2.8235
Iteration: 1849; Percent complete: 46.2%; Average loss: 3.3057
Iteration: 1850; Percent complete: 46.2%; Average loss: 3.2573
Iteration: 1851; Percent complete: 46.3%; Average loss: 3.1695
Iteration: 1852; Percent complete: 46.3%; Average loss: 2.8142
Iteration: 1853; Percent complete: 46.3%; Average loss: 3.2474
Iteration: 1854; Percent complete: 46.4%; Average loss: 3.1892
Iteration: 1855; Percent complete: 46.4%; Average loss: 3.1886
Iteration: 1856; Percent complete: 46.4%; Average loss: 3.1296
Iteration: 1857; Percent complete: 46.4%; Average loss: 3.0317
Iteration: 1858; Percent complete: 46.5%; Average loss: 3.4448
Iteration: 1859; Percent complete: 46.5%; Average loss: 3.3902
Iteration: 1860; Percent complete: 46.5%; Average loss: 3.3883
Iteration: 1861; Percent complete: 46.5%; Average loss: 3.4157
Iteration: 1862; Percent complete: 46.6%; Average loss: 3.4026
Iteration: 1863; Percent complete: 46.6%; Average loss: 3.2992
Iteration: 1864; Percent complete: 46.6%; Average loss: 3.6761
Iteration: 1865; Percent complete: 46.6%; Average loss: 3.0394
Iteration: 1866; Percent complete: 46.7%; Average loss: 3.1771
Iteration: 1867; Percent complete: 46.7%; Average loss: 3.0232
Iteration: 1868; Percent complete: 46.7%; Average loss: 3.3837
Iteration: 1869; Percent complete: 46.7%; Average loss: 2.9927
Iteration: 1870; Percent complete: 46.8%; Average loss: 3.0303
Iteration: 1871; Percent complete: 46.8%; Average loss: 3.1939
Iteration: 1872; Percent complete: 46.8%; Average loss: 3.2114
Iteration: 1873; Percent complete: 46.8%; Average loss: 3.2382
Iteration: 1874; Percent complete: 46.9%; Average loss: 3.0336
Iteration: 1875; Percent complete: 46.9%; Average loss: 3.2703
Iteration: 1876; Percent complete: 46.9%; Average loss: 3.0808
Iteration: 1877; Percent complete: 46.9%; Average loss: 3.0507
Iteration: 1878; Percent complete: 46.9%; Average loss: 3.0155
Iteration: 1879; Percent complete: 47.0%; Average loss: 2.9668
Iteration: 1880; Percent complete: 47.0%; Average loss: 3.0084
Iteration: 1881; Percent complete: 47.0%; Average loss: 3.3885
Iteration: 1882; Percent complete: 47.0%; Average loss: 3.2488
Iteration: 1883; Percent complete: 47.1%; Average loss: 3.2768
Iteration: 1884; Percent complete: 47.1%; Average loss: 3.0737
Iteration: 1885; Percent complete: 47.1%; Average loss: 3.1842
Iteration: 1886; Percent complete: 47.1%; Average loss: 3.3481
Iteration: 1887; Percent complete: 47.2%; Average loss: 3.1089
Iteration: 1888; Percent complete: 47.2%; Average loss: 3.4178
Iteration: 1889; Percent complete: 47.2%; Average loss: 3.0599
Iteration: 1890; Percent complete: 47.2%; Average loss: 3.3511
Iteration: 1891; Percent complete: 47.3%; Average loss: 3.1818
Iteration: 1892; Percent complete: 47.3%; Average loss: 3.0544
Iteration: 1893; Percent complete: 47.3%; Average loss: 3.0559
Iteration: 1894; Percent complete: 47.3%; Average loss: 3.4408
Iteration: 1895; Percent complete: 47.4%; Average loss: 3.3146
Iteration: 1896; Percent complete: 47.4%; Average loss: 3.3139
Iteration: 1897; Percent complete: 47.4%; Average loss: 3.0016
Iteration: 1898; Percent complete: 47.4%; Average loss: 3.0670
Iteration: 1899; Percent complete: 47.5%; Average loss: 3.0942
Iteration: 1900; Percent complete: 47.5%; Average loss: 3.2458
Iteration: 1901; Percent complete: 47.5%; Average loss: 3.1989
Iteration: 1902; Percent complete: 47.5%; Average loss: 3.1257
Iteration: 1903; Percent complete: 47.6%; Average loss: 3.5761
Iteration: 1904; Percent complete: 47.6%; Average loss: 3.2674
Iteration: 1905; Percent complete: 47.6%; Average loss: 3.1304
Iteration: 1906; Percent complete: 47.6%; Average loss: 3.0014
Iteration: 1907; Percent complete: 47.7%; Average loss: 3.2440
Iteration: 1908; Percent complete: 47.7%; Average loss: 3.1727
Iteration: 1909; Percent complete: 47.7%; Average loss: 3.0841
Iteration: 1910; Percent complete: 47.8%; Average loss: 3.2016
Iteration: 1911; Percent complete: 47.8%; Average loss: 3.2067
Iteration: 1912; Percent complete: 47.8%; Average loss: 3.0726
Iteration: 1913; Percent complete: 47.8%; Average loss: 3.2902
Iteration: 1914; Percent complete: 47.9%; Average loss: 3.1222
Iteration: 1915; Percent complete: 47.9%; Average loss: 3.5211
Iteration: 1916; Percent complete: 47.9%; Average loss: 3.2679
Iteration: 1917; Percent complete: 47.9%; Average loss: 2.8842
Iteration: 1918; Percent complete: 47.9%; Average loss: 3.1873
Iteration: 1919; Percent complete: 48.0%; Average loss: 3.3119
Iteration: 1920; Percent complete: 48.0%; Average loss: 3.3168
Iteration: 1921; Percent complete: 48.0%; Average loss: 2.9828
Iteration: 1922; Percent complete: 48.0%; Average loss: 3.2156
Iteration: 1923; Percent complete: 48.1%; Average loss: 3.1331
Iteration: 1924; Percent complete: 48.1%; Average loss: 3.0838
Iteration: 1925; Percent complete: 48.1%; Average loss: 3.2389
Iteration: 1926; Percent complete: 48.1%; Average loss: 3.3909
Iteration: 1927; Percent complete: 48.2%; Average loss: 3.4071
Iteration: 1928; Percent complete: 48.2%; Average loss: 3.0535
Iteration: 1929; Percent complete: 48.2%; Average loss: 3.1975
Iteration: 1930; Percent complete: 48.2%; Average loss: 3.2160
Iteration: 1931; Percent complete: 48.3%; Average loss: 3.1258
Iteration: 1932; Percent complete: 48.3%; Average loss: 3.2427
Iteration: 1933; Percent complete: 48.3%; Average loss: 3.0206
Iteration: 1934; Percent complete: 48.4%; Average loss: 3.0848
Iteration: 1935; Percent complete: 48.4%; Average loss: 3.2541
Iteration: 1936; Percent complete: 48.4%; Average loss: 3.0758
Iteration: 1937; Percent complete: 48.4%; Average loss: 3.3375
Iteration: 1938; Percent complete: 48.4%; Average loss: 3.1487
Iteration: 1939; Percent complete: 48.5%; Average loss: 3.1254
Iteration: 1940; Percent complete: 48.5%; Average loss: 3.1642
Iteration: 1941; Percent complete: 48.5%; Average loss: 3.2941
Iteration: 1942; Percent complete: 48.5%; Average loss: 3.2600
Iteration: 1943; Percent complete: 48.6%; Average loss: 3.2646
Iteration: 1944; Percent complete: 48.6%; Average loss: 3.0745
Iteration: 1945; Percent complete: 48.6%; Average loss: 3.0826
Iteration: 1946; Percent complete: 48.6%; Average loss: 3.2309
Iteration: 1947; Percent complete: 48.7%; Average loss: 3.0008
Iteration: 1948; Percent complete: 48.7%; Average loss: 3.1870
Iteration: 1949; Percent complete: 48.7%; Average loss: 3.0244
Iteration: 1950; Percent complete: 48.8%; Average loss: 3.0549
Iteration: 1951; Percent complete: 48.8%; Average loss: 3.2872
Iteration: 1952; Percent complete: 48.8%; Average loss: 3.3491
Iteration: 1953; Percent complete: 48.8%; Average loss: 3.1662
Iteration: 1954; Percent complete: 48.9%; Average loss: 3.3495
Iteration: 1955; Percent complete: 48.9%; Average loss: 3.1466
Iteration: 1956; Percent complete: 48.9%; Average loss: 3.2604
Iteration: 1957; Percent complete: 48.9%; Average loss: 3.0930
Iteration: 1958; Percent complete: 48.9%; Average loss: 3.0189
Iteration: 1959; Percent complete: 49.0%; Average loss: 3.4434
Iteration: 1960; Percent complete: 49.0%; Average loss: 3.2460
Iteration: 1961; Percent complete: 49.0%; Average loss: 3.2129
Iteration: 1962; Percent complete: 49.0%; Average loss: 3.0597
Iteration: 1963; Percent complete: 49.1%; Average loss: 3.2764
Iteration: 1964; Percent complete: 49.1%; Average loss: 3.3456
Iteration: 1965; Percent complete: 49.1%; Average loss: 3.1536
Iteration: 1966; Percent complete: 49.1%; Average loss: 3.0357
Iteration: 1967; Percent complete: 49.2%; Average loss: 3.2141
Iteration: 1968; Percent complete: 49.2%; Average loss: 3.4683
Iteration: 1969; Percent complete: 49.2%; Average loss: 3.1661
Iteration: 1970; Percent complete: 49.2%; Average loss: 3.4083
Iteration: 1971; Percent complete: 49.3%; Average loss: 3.3786
Iteration: 1972; Percent complete: 49.3%; Average loss: 2.9513
Iteration: 1973; Percent complete: 49.3%; Average loss: 3.2350
Iteration: 1974; Percent complete: 49.4%; Average loss: 3.2921
Iteration: 1975; Percent complete: 49.4%; Average loss: 3.4953
Iteration: 1976; Percent complete: 49.4%; Average loss: 3.3263
Iteration: 1977; Percent complete: 49.4%; Average loss: 3.2543
Iteration: 1978; Percent complete: 49.5%; Average loss: 3.1496
Iteration: 1979; Percent complete: 49.5%; Average loss: 3.1834
Iteration: 1980; Percent complete: 49.5%; Average loss: 3.1350
Iteration: 1981; Percent complete: 49.5%; Average loss: 3.2269
Iteration: 1982; Percent complete: 49.5%; Average loss: 2.9852
Iteration: 1983; Percent complete: 49.6%; Average loss: 3.1887
Iteration: 1984; Percent complete: 49.6%; Average loss: 3.0983
Iteration: 1985; Percent complete: 49.6%; Average loss: 3.0130
Iteration: 1986; Percent complete: 49.6%; Average loss: 3.1037
Iteration: 1987; Percent complete: 49.7%; Average loss: 3.1281
Iteration: 1988; Percent complete: 49.7%; Average loss: 3.2399
Iteration: 1989; Percent complete: 49.7%; Average loss: 3.1808
Iteration: 1990; Percent complete: 49.8%; Average loss: 3.1546
Iteration: 1991; Percent complete: 49.8%; Average loss: 3.0688
Iteration: 1992; Percent complete: 49.8%; Average loss: 3.0411
Iteration: 1993; Percent complete: 49.8%; Average loss: 3.2910
Iteration: 1994; Percent complete: 49.9%; Average loss: 3.0836
Iteration: 1995; Percent complete: 49.9%; Average loss: 3.3743
Iteration: 1996; Percent complete: 49.9%; Average loss: 3.2843
Iteration: 1997; Percent complete: 49.9%; Average loss: 3.1963
Iteration: 1998; Percent complete: 50.0%; Average loss: 3.2841
Iteration: 1999; Percent complete: 50.0%; Average loss: 3.1529
Iteration: 2000; Percent complete: 50.0%; Average loss: 3.2082
Iteration: 2001; Percent complete: 50.0%; Average loss: 3.1149
Iteration: 2002; Percent complete: 50.0%; Average loss: 3.4692
Iteration: 2003; Percent complete: 50.1%; Average loss: 3.1024
Iteration: 2004; Percent complete: 50.1%; Average loss: 3.1624
Iteration: 2005; Percent complete: 50.1%; Average loss: 3.1737
Iteration: 2006; Percent complete: 50.1%; Average loss: 3.1565
Iteration: 2007; Percent complete: 50.2%; Average loss: 3.3474
Iteration: 2008; Percent complete: 50.2%; Average loss: 3.0540
Iteration: 2009; Percent complete: 50.2%; Average loss: 2.9797
Iteration: 2010; Percent complete: 50.2%; Average loss: 3.2565
Iteration: 2011; Percent complete: 50.3%; Average loss: 3.3492
Iteration: 2012; Percent complete: 50.3%; Average loss: 3.1628
Iteration: 2013; Percent complete: 50.3%; Average loss: 3.1572
Iteration: 2014; Percent complete: 50.3%; Average loss: 3.2238
Iteration: 2015; Percent complete: 50.4%; Average loss: 3.2448
Iteration: 2016; Percent complete: 50.4%; Average loss: 3.2495
Iteration: 2017; Percent complete: 50.4%; Average loss: 2.9318
Iteration: 2018; Percent complete: 50.4%; Average loss: 3.2012
Iteration: 2019; Percent complete: 50.5%; Average loss: 3.2043
Iteration: 2020; Percent complete: 50.5%; Average loss: 3.4169
Iteration: 2021; Percent complete: 50.5%; Average loss: 3.2700
Iteration: 2022; Percent complete: 50.5%; Average loss: 3.0297
Iteration: 2023; Percent complete: 50.6%; Average loss: 2.9735
Iteration: 2024; Percent complete: 50.6%; Average loss: 2.7525
Iteration: 2025; Percent complete: 50.6%; Average loss: 3.3517
Iteration: 2026; Percent complete: 50.6%; Average loss: 3.2983
Iteration: 2027; Percent complete: 50.7%; Average loss: 2.9989
Iteration: 2028; Percent complete: 50.7%; Average loss: 2.8659
Iteration: 2029; Percent complete: 50.7%; Average loss: 3.2319
Iteration: 2030; Percent complete: 50.7%; Average loss: 3.1767
Iteration: 2031; Percent complete: 50.8%; Average loss: 3.0906
Iteration: 2032; Percent complete: 50.8%; Average loss: 2.9274
Iteration: 2033; Percent complete: 50.8%; Average loss: 3.3168
Iteration: 2034; Percent complete: 50.8%; Average loss: 3.0443
Iteration: 2035; Percent complete: 50.9%; Average loss: 3.3384
Iteration: 2036; Percent complete: 50.9%; Average loss: 3.1877
Iteration: 2037; Percent complete: 50.9%; Average loss: 3.2322
Iteration: 2038; Percent complete: 50.9%; Average loss: 3.0349
Iteration: 2039; Percent complete: 51.0%; Average loss: 3.1963
Iteration: 2040; Percent complete: 51.0%; Average loss: 2.9312
Iteration: 2041; Percent complete: 51.0%; Average loss: 3.0710
Iteration: 2042; Percent complete: 51.0%; Average loss: 3.2978
Iteration: 2043; Percent complete: 51.1%; Average loss: 3.0428
Iteration: 2044; Percent complete: 51.1%; Average loss: 3.1851
Iteration: 2045; Percent complete: 51.1%; Average loss: 3.4392
Iteration: 2046; Percent complete: 51.1%; Average loss: 3.0813
Iteration: 2047; Percent complete: 51.2%; Average loss: 3.0319
Iteration: 2048; Percent complete: 51.2%; Average loss: 3.1984
Iteration: 2049; Percent complete: 51.2%; Average loss: 3.1841
Iteration: 2050; Percent complete: 51.2%; Average loss: 3.0969
Iteration: 2051; Percent complete: 51.3%; Average loss: 3.2731
Iteration: 2052; Percent complete: 51.3%; Average loss: 2.9168
Iteration: 2053; Percent complete: 51.3%; Average loss: 3.1649
Iteration: 2054; Percent complete: 51.3%; Average loss: 3.1549
Iteration: 2055; Percent complete: 51.4%; Average loss: 3.2137
Iteration: 2056; Percent complete: 51.4%; Average loss: 3.0599
Iteration: 2057; Percent complete: 51.4%; Average loss: 3.0577
Iteration: 2058; Percent complete: 51.4%; Average loss: 3.0192
Iteration: 2059; Percent complete: 51.5%; Average loss: 3.0032
Iteration: 2060; Percent complete: 51.5%; Average loss: 3.3567
Iteration: 2061; Percent complete: 51.5%; Average loss: 3.0327
Iteration: 2062; Percent complete: 51.5%; Average loss: 3.0040
Iteration: 2063; Percent complete: 51.6%; Average loss: 3.1066
Iteration: 2064; Percent complete: 51.6%; Average loss: 3.3245
Iteration: 2065; Percent complete: 51.6%; Average loss: 3.1876
Iteration: 2066; Percent complete: 51.6%; Average loss: 3.1613
Iteration: 2067; Percent complete: 51.7%; Average loss: 2.9272
Iteration: 2068; Percent complete: 51.7%; Average loss: 3.3433
Iteration: 2069; Percent complete: 51.7%; Average loss: 3.1538
Iteration: 2070; Percent complete: 51.7%; Average loss: 2.9813
Iteration: 2071; Percent complete: 51.8%; Average loss: 3.0278
Iteration: 2072; Percent complete: 51.8%; Average loss: 3.4067
Iteration: 2073; Percent complete: 51.8%; Average loss: 3.1327
Iteration: 2074; Percent complete: 51.8%; Average loss: 3.1426
Iteration: 2075; Percent complete: 51.9%; Average loss: 3.0571
Iteration: 2076; Percent complete: 51.9%; Average loss: 3.0072
Iteration: 2077; Percent complete: 51.9%; Average loss: 3.0279
Iteration: 2078; Percent complete: 51.9%; Average loss: 3.1783
Iteration: 2079; Percent complete: 52.0%; Average loss: 3.2371
Iteration: 2080; Percent complete: 52.0%; Average loss: 3.1959
Iteration: 2081; Percent complete: 52.0%; Average loss: 3.2585
Iteration: 2082; Percent complete: 52.0%; Average loss: 3.0316
Iteration: 2083; Percent complete: 52.1%; Average loss: 3.2417
Iteration: 2084; Percent complete: 52.1%; Average loss: 3.1226
Iteration: 2085; Percent complete: 52.1%; Average loss: 3.0579
Iteration: 2086; Percent complete: 52.1%; Average loss: 3.0619
Iteration: 2087; Percent complete: 52.2%; Average loss: 3.0890
Iteration: 2088; Percent complete: 52.2%; Average loss: 3.1590
Iteration: 2089; Percent complete: 52.2%; Average loss: 3.2042
Iteration: 2090; Percent complete: 52.2%; Average loss: 3.0238
Iteration: 2091; Percent complete: 52.3%; Average loss: 3.2154
Iteration: 2092; Percent complete: 52.3%; Average loss: 3.0813
Iteration: 2093; Percent complete: 52.3%; Average loss: 3.0748
Iteration: 2094; Percent complete: 52.3%; Average loss: 3.0433
Iteration: 2095; Percent complete: 52.4%; Average loss: 3.1053
Iteration: 2096; Percent complete: 52.4%; Average loss: 3.1631
Iteration: 2097; Percent complete: 52.4%; Average loss: 3.2154
Iteration: 2098; Percent complete: 52.4%; Average loss: 3.1838
Iteration: 2099; Percent complete: 52.5%; Average loss: 3.3070
Iteration: 2100; Percent complete: 52.5%; Average loss: 3.1872
Iteration: 2101; Percent complete: 52.5%; Average loss: 3.2999
Iteration: 2102; Percent complete: 52.5%; Average loss: 3.4601
Iteration: 2103; Percent complete: 52.6%; Average loss: 3.0459
Iteration: 2104; Percent complete: 52.6%; Average loss: 3.0534
Iteration: 2105; Percent complete: 52.6%; Average loss: 2.9912
Iteration: 2106; Percent complete: 52.6%; Average loss: 3.0896
Iteration: 2107; Percent complete: 52.7%; Average loss: 3.1473
Iteration: 2108; Percent complete: 52.7%; Average loss: 3.0212
Iteration: 2109; Percent complete: 52.7%; Average loss: 3.4153
Iteration: 2110; Percent complete: 52.8%; Average loss: 3.0631
Iteration: 2111; Percent complete: 52.8%; Average loss: 3.3058
Iteration: 2112; Percent complete: 52.8%; Average loss: 3.0522
Iteration: 2113; Percent complete: 52.8%; Average loss: 3.1656
Iteration: 2114; Percent complete: 52.8%; Average loss: 3.1904
Iteration: 2115; Percent complete: 52.9%; Average loss: 3.2335
Iteration: 2116; Percent complete: 52.9%; Average loss: 3.4354
Iteration: 2117; Percent complete: 52.9%; Average loss: 3.0054
Iteration: 2118; Percent complete: 52.9%; Average loss: 3.0576
Iteration: 2119; Percent complete: 53.0%; Average loss: 3.1077
Iteration: 2120; Percent complete: 53.0%; Average loss: 3.0456
Iteration: 2121; Percent complete: 53.0%; Average loss: 3.2399
Iteration: 2122; Percent complete: 53.0%; Average loss: 3.1187
Iteration: 2123; Percent complete: 53.1%; Average loss: 3.2444
Iteration: 2124; Percent complete: 53.1%; Average loss: 3.1705
Iteration: 2125; Percent complete: 53.1%; Average loss: 2.8593
Iteration: 2126; Percent complete: 53.1%; Average loss: 3.0219
Iteration: 2127; Percent complete: 53.2%; Average loss: 3.2337
Iteration: 2128; Percent complete: 53.2%; Average loss: 3.2209
Iteration: 2129; Percent complete: 53.2%; Average loss: 3.1871
Iteration: 2130; Percent complete: 53.2%; Average loss: 2.9840
Iteration: 2131; Percent complete: 53.3%; Average loss: 3.0416
Iteration: 2132; Percent complete: 53.3%; Average loss: 3.1449
Iteration: 2133; Percent complete: 53.3%; Average loss: 3.0035
Iteration: 2134; Percent complete: 53.3%; Average loss: 3.1888
Iteration: 2135; Percent complete: 53.4%; Average loss: 3.3041
Iteration: 2136; Percent complete: 53.4%; Average loss: 3.1467
Iteration: 2137; Percent complete: 53.4%; Average loss: 3.2681
Iteration: 2138; Percent complete: 53.4%; Average loss: 3.0517
Iteration: 2139; Percent complete: 53.5%; Average loss: 2.9784
Iteration: 2140; Percent complete: 53.5%; Average loss: 3.2595
Iteration: 2141; Percent complete: 53.5%; Average loss: 3.0933
Iteration: 2142; Percent complete: 53.5%; Average loss: 3.0125
Iteration: 2143; Percent complete: 53.6%; Average loss: 3.0328
Iteration: 2144; Percent complete: 53.6%; Average loss: 3.0783
Iteration: 2145; Percent complete: 53.6%; Average loss: 3.0843
Iteration: 2146; Percent complete: 53.6%; Average loss: 3.1724
Iteration: 2147; Percent complete: 53.7%; Average loss: 3.2402
Iteration: 2148; Percent complete: 53.7%; Average loss: 2.9745
Iteration: 2149; Percent complete: 53.7%; Average loss: 3.1805
Iteration: 2150; Percent complete: 53.8%; Average loss: 3.0659
Iteration: 2151; Percent complete: 53.8%; Average loss: 3.1741
Iteration: 2152; Percent complete: 53.8%; Average loss: 3.2166
Iteration: 2153; Percent complete: 53.8%; Average loss: 3.0210
Iteration: 2154; Percent complete: 53.8%; Average loss: 3.1465
Iteration: 2155; Percent complete: 53.9%; Average loss: 3.1798
Iteration: 2156; Percent complete: 53.9%; Average loss: 2.9233
Iteration: 2157; Percent complete: 53.9%; Average loss: 3.2107
Iteration: 2158; Percent complete: 53.9%; Average loss: 3.1495
Iteration: 2159; Percent complete: 54.0%; Average loss: 3.0062
Iteration: 2160; Percent complete: 54.0%; Average loss: 3.3022
Iteration: 2161; Percent complete: 54.0%; Average loss: 3.0096
Iteration: 2162; Percent complete: 54.0%; Average loss: 3.1913
Iteration: 2163; Percent complete: 54.1%; Average loss: 3.1533
Iteration: 2164; Percent complete: 54.1%; Average loss: 3.0350
Iteration: 2165; Percent complete: 54.1%; Average loss: 3.1658
Iteration: 2166; Percent complete: 54.1%; Average loss: 3.2373
Iteration: 2167; Percent complete: 54.2%; Average loss: 3.2373
Iteration: 2168; Percent complete: 54.2%; Average loss: 3.0903
Iteration: 2169; Percent complete: 54.2%; Average loss: 3.0564
Iteration: 2170; Percent complete: 54.2%; Average loss: 2.9251
Iteration: 2171; Percent complete: 54.3%; Average loss: 3.2464
Iteration: 2172; Percent complete: 54.3%; Average loss: 3.3825
Iteration: 2173; Percent complete: 54.3%; Average loss: 3.3807
Iteration: 2174; Percent complete: 54.4%; Average loss: 3.2350
Iteration: 2175; Percent complete: 54.4%; Average loss: 2.9695
Iteration: 2176; Percent complete: 54.4%; Average loss: 2.9056
Iteration: 2177; Percent complete: 54.4%; Average loss: 3.0504
Iteration: 2178; Percent complete: 54.4%; Average loss: 3.0704
Iteration: 2179; Percent complete: 54.5%; Average loss: 3.2341
Iteration: 2180; Percent complete: 54.5%; Average loss: 2.9216
Iteration: 2181; Percent complete: 54.5%; Average loss: 2.9119
Iteration: 2182; Percent complete: 54.5%; Average loss: 3.1728
Iteration: 2183; Percent complete: 54.6%; Average loss: 3.2105
Iteration: 2184; Percent complete: 54.6%; Average loss: 2.9327
Iteration: 2185; Percent complete: 54.6%; Average loss: 3.1283
Iteration: 2186; Percent complete: 54.6%; Average loss: 3.2375
Iteration: 2187; Percent complete: 54.7%; Average loss: 3.1406
Iteration: 2188; Percent complete: 54.7%; Average loss: 3.2877
Iteration: 2189; Percent complete: 54.7%; Average loss: 3.0811
Iteration: 2190; Percent complete: 54.8%; Average loss: 2.9333
Iteration: 2191; Percent complete: 54.8%; Average loss: 3.0446
Iteration: 2192; Percent complete: 54.8%; Average loss: 3.1889
Iteration: 2193; Percent complete: 54.8%; Average loss: 3.2479
Iteration: 2194; Percent complete: 54.9%; Average loss: 3.2153
Iteration: 2195; Percent complete: 54.9%; Average loss: 3.3670
Iteration: 2196; Percent complete: 54.9%; Average loss: 2.9339
Iteration: 2197; Percent complete: 54.9%; Average loss: 3.0430
Iteration: 2198; Percent complete: 54.9%; Average loss: 3.0997
Iteration: 2199; Percent complete: 55.0%; Average loss: 3.2640
Iteration: 2200; Percent complete: 55.0%; Average loss: 3.1570
Iteration: 2201; Percent complete: 55.0%; Average loss: 3.0195
Iteration: 2202; Percent complete: 55.0%; Average loss: 3.0562
Iteration: 2203; Percent complete: 55.1%; Average loss: 2.9020
Iteration: 2204; Percent complete: 55.1%; Average loss: 3.1450
Iteration: 2205; Percent complete: 55.1%; Average loss: 3.1889
Iteration: 2206; Percent complete: 55.1%; Average loss: 3.1361
Iteration: 2207; Percent complete: 55.2%; Average loss: 3.2197
Iteration: 2208; Percent complete: 55.2%; Average loss: 3.1695
Iteration: 2209; Percent complete: 55.2%; Average loss: 3.1391
Iteration: 2210; Percent complete: 55.2%; Average loss: 3.0901
Iteration: 2211; Percent complete: 55.3%; Average loss: 3.1009
Iteration: 2212; Percent complete: 55.3%; Average loss: 3.0961
Iteration: 2213; Percent complete: 55.3%; Average loss: 3.1071
Iteration: 2214; Percent complete: 55.4%; Average loss: 3.3075
Iteration: 2215; Percent complete: 55.4%; Average loss: 3.1559
Iteration: 2216; Percent complete: 55.4%; Average loss: 3.0978
Iteration: 2217; Percent complete: 55.4%; Average loss: 3.2209
Iteration: 2218; Percent complete: 55.5%; Average loss: 2.8812
Iteration: 2219; Percent complete: 55.5%; Average loss: 3.0528
Iteration: 2220; Percent complete: 55.5%; Average loss: 2.9478
Iteration: 2221; Percent complete: 55.5%; Average loss: 3.2022
Iteration: 2222; Percent complete: 55.5%; Average loss: 3.0690
Iteration: 2223; Percent complete: 55.6%; Average loss: 3.0582
Iteration: 2224; Percent complete: 55.6%; Average loss: 2.9638
Iteration: 2225; Percent complete: 55.6%; Average loss: 3.2415
Iteration: 2226; Percent complete: 55.6%; Average loss: 3.5418
Iteration: 2227; Percent complete: 55.7%; Average loss: 2.9684
Iteration: 2228; Percent complete: 55.7%; Average loss: 2.9168
Iteration: 2229; Percent complete: 55.7%; Average loss: 3.0042
Iteration: 2230; Percent complete: 55.8%; Average loss: 3.3420
Iteration: 2231; Percent complete: 55.8%; Average loss: 2.7673
Iteration: 2232; Percent complete: 55.8%; Average loss: 3.3168
Iteration: 2233; Percent complete: 55.8%; Average loss: 3.0216
Iteration: 2234; Percent complete: 55.9%; Average loss: 3.1888
Iteration: 2235; Percent complete: 55.9%; Average loss: 2.9440
Iteration: 2236; Percent complete: 55.9%; Average loss: 3.0129
Iteration: 2237; Percent complete: 55.9%; Average loss: 3.2862
Iteration: 2238; Percent complete: 56.0%; Average loss: 3.3401
Iteration: 2239; Percent complete: 56.0%; Average loss: 3.0064
Iteration: 2240; Percent complete: 56.0%; Average loss: 2.9815
Iteration: 2241; Percent complete: 56.0%; Average loss: 3.1758
Iteration: 2242; Percent complete: 56.0%; Average loss: 3.1954
Iteration: 2243; Percent complete: 56.1%; Average loss: 3.0948
Iteration: 2244; Percent complete: 56.1%; Average loss: 3.2322
Iteration: 2245; Percent complete: 56.1%; Average loss: 2.8999
Iteration: 2246; Percent complete: 56.1%; Average loss: 3.1323
Iteration: 2247; Percent complete: 56.2%; Average loss: 3.2062
Iteration: 2248; Percent complete: 56.2%; Average loss: 3.2596
Iteration: 2249; Percent complete: 56.2%; Average loss: 3.0669
Iteration: 2250; Percent complete: 56.2%; Average loss: 3.1418
Iteration: 2251; Percent complete: 56.3%; Average loss: 3.0625
Iteration: 2252; Percent complete: 56.3%; Average loss: 2.8354
Iteration: 2253; Percent complete: 56.3%; Average loss: 3.0152
Iteration: 2254; Percent complete: 56.4%; Average loss: 3.3499
Iteration: 2255; Percent complete: 56.4%; Average loss: 3.1123
Iteration: 2256; Percent complete: 56.4%; Average loss: 3.2161
Iteration: 2257; Percent complete: 56.4%; Average loss: 2.9346
Iteration: 2258; Percent complete: 56.5%; Average loss: 3.4500
Iteration: 2259; Percent complete: 56.5%; Average loss: 2.9546
Iteration: 2260; Percent complete: 56.5%; Average loss: 3.2969
Iteration: 2261; Percent complete: 56.5%; Average loss: 3.0232
Iteration: 2262; Percent complete: 56.5%; Average loss: 3.0781
Iteration: 2263; Percent complete: 56.6%; Average loss: 2.6908
Iteration: 2264; Percent complete: 56.6%; Average loss: 3.0890
Iteration: 2265; Percent complete: 56.6%; Average loss: 3.0787
Iteration: 2266; Percent complete: 56.6%; Average loss: 2.8231
Iteration: 2267; Percent complete: 56.7%; Average loss: 2.8522
Iteration: 2268; Percent complete: 56.7%; Average loss: 2.8848
Iteration: 2269; Percent complete: 56.7%; Average loss: 3.2179
Iteration: 2270; Percent complete: 56.8%; Average loss: 2.9336
Iteration: 2271; Percent complete: 56.8%; Average loss: 3.1807
Iteration: 2272; Percent complete: 56.8%; Average loss: 3.2438
Iteration: 2273; Percent complete: 56.8%; Average loss: 3.1505
Iteration: 2274; Percent complete: 56.9%; Average loss: 2.8398
Iteration: 2275; Percent complete: 56.9%; Average loss: 3.0749
Iteration: 2276; Percent complete: 56.9%; Average loss: 3.2126
Iteration: 2277; Percent complete: 56.9%; Average loss: 3.1575
Iteration: 2278; Percent complete: 57.0%; Average loss: 3.0908
Iteration: 2279; Percent complete: 57.0%; Average loss: 3.2163
Iteration: 2280; Percent complete: 57.0%; Average loss: 3.0343
Iteration: 2281; Percent complete: 57.0%; Average loss: 3.1416
Iteration: 2282; Percent complete: 57.0%; Average loss: 3.2054
Iteration: 2283; Percent complete: 57.1%; Average loss: 2.8907
Iteration: 2284; Percent complete: 57.1%; Average loss: 3.1890
Iteration: 2285; Percent complete: 57.1%; Average loss: 3.0720
Iteration: 2286; Percent complete: 57.1%; Average loss: 2.8727
Iteration: 2287; Percent complete: 57.2%; Average loss: 3.2101
Iteration: 2288; Percent complete: 57.2%; Average loss: 2.7598
Iteration: 2289; Percent complete: 57.2%; Average loss: 3.0717
Iteration: 2290; Percent complete: 57.2%; Average loss: 2.9435
Iteration: 2291; Percent complete: 57.3%; Average loss: 2.9831
Iteration: 2292; Percent complete: 57.3%; Average loss: 3.0723
Iteration: 2293; Percent complete: 57.3%; Average loss: 3.0705
Iteration: 2294; Percent complete: 57.4%; Average loss: 2.9697
Iteration: 2295; Percent complete: 57.4%; Average loss: 2.8842
Iteration: 2296; Percent complete: 57.4%; Average loss: 3.1654
Iteration: 2297; Percent complete: 57.4%; Average loss: 3.0200
Iteration: 2298; Percent complete: 57.5%; Average loss: 3.0309
Iteration: 2299; Percent complete: 57.5%; Average loss: 3.1875
Iteration: 2300; Percent complete: 57.5%; Average loss: 3.1743
Iteration: 2301; Percent complete: 57.5%; Average loss: 3.1811
Iteration: 2302; Percent complete: 57.6%; Average loss: 3.0401
Iteration: 2303; Percent complete: 57.6%; Average loss: 2.9408
Iteration: 2304; Percent complete: 57.6%; Average loss: 3.2218
Iteration: 2305; Percent complete: 57.6%; Average loss: 3.1464
Iteration: 2306; Percent complete: 57.6%; Average loss: 2.9642
Iteration: 2307; Percent complete: 57.7%; Average loss: 2.8079
Iteration: 2308; Percent complete: 57.7%; Average loss: 2.9996
Iteration: 2309; Percent complete: 57.7%; Average loss: 3.0726
Iteration: 2310; Percent complete: 57.8%; Average loss: 3.3397
Iteration: 2311; Percent complete: 57.8%; Average loss: 3.1430
Iteration: 2312; Percent complete: 57.8%; Average loss: 3.3286
Iteration: 2313; Percent complete: 57.8%; Average loss: 2.9164
Iteration: 2314; Percent complete: 57.9%; Average loss: 3.2109
Iteration: 2315; Percent complete: 57.9%; Average loss: 3.3330
Iteration: 2316; Percent complete: 57.9%; Average loss: 3.1248
Iteration: 2317; Percent complete: 57.9%; Average loss: 3.0707
Iteration: 2318; Percent complete: 58.0%; Average loss: 3.0612
Iteration: 2319; Percent complete: 58.0%; Average loss: 2.8473
Iteration: 2320; Percent complete: 58.0%; Average loss: 3.2078
Iteration: 2321; Percent complete: 58.0%; Average loss: 3.0709
Iteration: 2322; Percent complete: 58.1%; Average loss: 3.0883
Iteration: 2323; Percent complete: 58.1%; Average loss: 3.0236
Iteration: 2324; Percent complete: 58.1%; Average loss: 2.9528
Iteration: 2325; Percent complete: 58.1%; Average loss: 3.2924
Iteration: 2326; Percent complete: 58.1%; Average loss: 2.7107
Iteration: 2327; Percent complete: 58.2%; Average loss: 2.9690
Iteration: 2328; Percent complete: 58.2%; Average loss: 3.0400
Iteration: 2329; Percent complete: 58.2%; Average loss: 2.9971
Iteration: 2330; Percent complete: 58.2%; Average loss: 3.0223
Iteration: 2331; Percent complete: 58.3%; Average loss: 3.2198
Iteration: 2332; Percent complete: 58.3%; Average loss: 3.0670
Iteration: 2333; Percent complete: 58.3%; Average loss: 2.8575
Iteration: 2334; Percent complete: 58.4%; Average loss: 3.1535
Iteration: 2335; Percent complete: 58.4%; Average loss: 2.8534
Iteration: 2336; Percent complete: 58.4%; Average loss: 3.1788
Iteration: 2337; Percent complete: 58.4%; Average loss: 3.3014
Iteration: 2338; Percent complete: 58.5%; Average loss: 3.1429
Iteration: 2339; Percent complete: 58.5%; Average loss: 3.0781
Iteration: 2340; Percent complete: 58.5%; Average loss: 2.9397
Iteration: 2341; Percent complete: 58.5%; Average loss: 2.9744
Iteration: 2342; Percent complete: 58.6%; Average loss: 3.0321
Iteration: 2343; Percent complete: 58.6%; Average loss: 3.1011
Iteration: 2344; Percent complete: 58.6%; Average loss: 3.0047
Iteration: 2345; Percent complete: 58.6%; Average loss: 2.8201
Iteration: 2346; Percent complete: 58.7%; Average loss: 3.3459
Iteration: 2347; Percent complete: 58.7%; Average loss: 3.0558
Iteration: 2348; Percent complete: 58.7%; Average loss: 2.8595
Iteration: 2349; Percent complete: 58.7%; Average loss: 2.9791
Iteration: 2350; Percent complete: 58.8%; Average loss: 2.9259
Iteration: 2351; Percent complete: 58.8%; Average loss: 2.9089
Iteration: 2352; Percent complete: 58.8%; Average loss: 2.8353
Iteration: 2353; Percent complete: 58.8%; Average loss: 3.2003
Iteration: 2354; Percent complete: 58.9%; Average loss: 3.3361
Iteration: 2355; Percent complete: 58.9%; Average loss: 3.0339
Iteration: 2356; Percent complete: 58.9%; Average loss: 3.0604
Iteration: 2357; Percent complete: 58.9%; Average loss: 3.0730
Iteration: 2358; Percent complete: 59.0%; Average loss: 3.0103
Iteration: 2359; Percent complete: 59.0%; Average loss: 3.1640
Iteration: 2360; Percent complete: 59.0%; Average loss: 2.8391
Iteration: 2361; Percent complete: 59.0%; Average loss: 3.0450
Iteration: 2362; Percent complete: 59.1%; Average loss: 3.2066
Iteration: 2363; Percent complete: 59.1%; Average loss: 3.0557
Iteration: 2364; Percent complete: 59.1%; Average loss: 3.1378
Iteration: 2365; Percent complete: 59.1%; Average loss: 3.0553
Iteration: 2366; Percent complete: 59.2%; Average loss: 2.9069
Iteration: 2367; Percent complete: 59.2%; Average loss: 2.8837
Iteration: 2368; Percent complete: 59.2%; Average loss: 3.0619
Iteration: 2369; Percent complete: 59.2%; Average loss: 3.2510
Iteration: 2370; Percent complete: 59.2%; Average loss: 3.3935
Iteration: 2371; Percent complete: 59.3%; Average loss: 2.9976
Iteration: 2372; Percent complete: 59.3%; Average loss: 3.2120
Iteration: 2373; Percent complete: 59.3%; Average loss: 2.9935
Iteration: 2374; Percent complete: 59.4%; Average loss: 2.9533
Iteration: 2375; Percent complete: 59.4%; Average loss: 2.8794
Iteration: 2376; Percent complete: 59.4%; Average loss: 3.0426
Iteration: 2377; Percent complete: 59.4%; Average loss: 2.9115
Iteration: 2378; Percent complete: 59.5%; Average loss: 2.9513
Iteration: 2379; Percent complete: 59.5%; Average loss: 3.1140
Iteration: 2380; Percent complete: 59.5%; Average loss: 2.8890
Iteration: 2381; Percent complete: 59.5%; Average loss: 3.1780
Iteration: 2382; Percent complete: 59.6%; Average loss: 2.7383
Iteration: 2383; Percent complete: 59.6%; Average loss: 2.9650
Iteration: 2384; Percent complete: 59.6%; Average loss: 2.8571
Iteration: 2385; Percent complete: 59.6%; Average loss: 3.1086
Iteration: 2386; Percent complete: 59.7%; Average loss: 3.0087
Iteration: 2387; Percent complete: 59.7%; Average loss: 3.0994
Iteration: 2388; Percent complete: 59.7%; Average loss: 3.2480
Iteration: 2389; Percent complete: 59.7%; Average loss: 3.1875
Iteration: 2390; Percent complete: 59.8%; Average loss: 2.8676
Iteration: 2391; Percent complete: 59.8%; Average loss: 2.9218
Iteration: 2392; Percent complete: 59.8%; Average loss: 3.1167
Iteration: 2393; Percent complete: 59.8%; Average loss: 2.9529
Iteration: 2394; Percent complete: 59.9%; Average loss: 2.8647
Iteration: 2395; Percent complete: 59.9%; Average loss: 3.0451
Iteration: 2396; Percent complete: 59.9%; Average loss: 3.0807
Iteration: 2397; Percent complete: 59.9%; Average loss: 3.1120
Iteration: 2398; Percent complete: 60.0%; Average loss: 2.9271
Iteration: 2399; Percent complete: 60.0%; Average loss: 3.1743
Iteration: 2400; Percent complete: 60.0%; Average loss: 3.0094
Iteration: 2401; Percent complete: 60.0%; Average loss: 2.8377
Iteration: 2402; Percent complete: 60.1%; Average loss: 3.2161
Iteration: 2403; Percent complete: 60.1%; Average loss: 2.9270
Iteration: 2404; Percent complete: 60.1%; Average loss: 2.8011
Iteration: 2405; Percent complete: 60.1%; Average loss: 3.1273
Iteration: 2406; Percent complete: 60.2%; Average loss: 2.8971
Iteration: 2407; Percent complete: 60.2%; Average loss: 2.8312
Iteration: 2408; Percent complete: 60.2%; Average loss: 3.0171
Iteration: 2409; Percent complete: 60.2%; Average loss: 3.0027
Iteration: 2410; Percent complete: 60.2%; Average loss: 2.6430
Iteration: 2411; Percent complete: 60.3%; Average loss: 2.8715
Iteration: 2412; Percent complete: 60.3%; Average loss: 2.9399
Iteration: 2413; Percent complete: 60.3%; Average loss: 3.1935
Iteration: 2414; Percent complete: 60.4%; Average loss: 2.9675
Iteration: 2415; Percent complete: 60.4%; Average loss: 3.0074
Iteration: 2416; Percent complete: 60.4%; Average loss: 2.9939
Iteration: 2417; Percent complete: 60.4%; Average loss: 3.3969
Iteration: 2418; Percent complete: 60.5%; Average loss: 2.9387
Iteration: 2419; Percent complete: 60.5%; Average loss: 2.7750
Iteration: 2420; Percent complete: 60.5%; Average loss: 3.0736
Iteration: 2421; Percent complete: 60.5%; Average loss: 2.9006
Iteration: 2422; Percent complete: 60.6%; Average loss: 3.1029
Iteration: 2423; Percent complete: 60.6%; Average loss: 3.0998
Iteration: 2424; Percent complete: 60.6%; Average loss: 3.1131
Iteration: 2425; Percent complete: 60.6%; Average loss: 3.0232
Iteration: 2426; Percent complete: 60.7%; Average loss: 3.0732
Iteration: 2427; Percent complete: 60.7%; Average loss: 3.3019
Iteration: 2428; Percent complete: 60.7%; Average loss: 3.2356
Iteration: 2429; Percent complete: 60.7%; Average loss: 3.0353
Iteration: 2430; Percent complete: 60.8%; Average loss: 3.2755
Iteration: 2431; Percent complete: 60.8%; Average loss: 3.0375
Iteration: 2432; Percent complete: 60.8%; Average loss: 3.1990
Iteration: 2433; Percent complete: 60.8%; Average loss: 3.0002
Iteration: 2434; Percent complete: 60.9%; Average loss: 3.0500
Iteration: 2435; Percent complete: 60.9%; Average loss: 3.2308
Iteration: 2436; Percent complete: 60.9%; Average loss: 2.9852
Iteration: 2437; Percent complete: 60.9%; Average loss: 3.0617
Iteration: 2438; Percent complete: 61.0%; Average loss: 2.8609
Iteration: 2439; Percent complete: 61.0%; Average loss: 2.9806
Iteration: 2440; Percent complete: 61.0%; Average loss: 3.3641
Iteration: 2441; Percent complete: 61.0%; Average loss: 2.9916
Iteration: 2442; Percent complete: 61.1%; Average loss: 3.3711
Iteration: 2443; Percent complete: 61.1%; Average loss: 3.1155
Iteration: 2444; Percent complete: 61.1%; Average loss: 3.0574
Iteration: 2445; Percent complete: 61.1%; Average loss: 3.1826
Iteration: 2446; Percent complete: 61.2%; Average loss: 2.9566
Iteration: 2447; Percent complete: 61.2%; Average loss: 2.7333
Iteration: 2448; Percent complete: 61.2%; Average loss: 3.1451
Iteration: 2449; Percent complete: 61.2%; Average loss: 3.0976
Iteration: 2450; Percent complete: 61.3%; Average loss: 3.0232
Iteration: 2451; Percent complete: 61.3%; Average loss: 3.0026
Iteration: 2452; Percent complete: 61.3%; Average loss: 3.1373
Iteration: 2453; Percent complete: 61.3%; Average loss: 2.9024
Iteration: 2454; Percent complete: 61.4%; Average loss: 3.0756
Iteration: 2455; Percent complete: 61.4%; Average loss: 3.1137
Iteration: 2456; Percent complete: 61.4%; Average loss: 2.9910
Iteration: 2457; Percent complete: 61.4%; Average loss: 3.0233
Iteration: 2458; Percent complete: 61.5%; Average loss: 3.1661
Iteration: 2459; Percent complete: 61.5%; Average loss: 2.8733
Iteration: 2460; Percent complete: 61.5%; Average loss: 3.1447
Iteration: 2461; Percent complete: 61.5%; Average loss: 3.0538
Iteration: 2462; Percent complete: 61.6%; Average loss: 2.9841
Iteration: 2463; Percent complete: 61.6%; Average loss: 2.9982
Iteration: 2464; Percent complete: 61.6%; Average loss: 2.8630
Iteration: 2465; Percent complete: 61.6%; Average loss: 2.9709
Iteration: 2466; Percent complete: 61.7%; Average loss: 3.0314
Iteration: 2467; Percent complete: 61.7%; Average loss: 3.1028
Iteration: 2468; Percent complete: 61.7%; Average loss: 3.0246
Iteration: 2469; Percent complete: 61.7%; Average loss: 3.0360
Iteration: 2470; Percent complete: 61.8%; Average loss: 2.8862
Iteration: 2471; Percent complete: 61.8%; Average loss: 2.9073
Iteration: 2472; Percent complete: 61.8%; Average loss: 3.1232
Iteration: 2473; Percent complete: 61.8%; Average loss: 2.7387
Iteration: 2474; Percent complete: 61.9%; Average loss: 3.0861
Iteration: 2475; Percent complete: 61.9%; Average loss: 2.9135
Iteration: 2476; Percent complete: 61.9%; Average loss: 3.0335
Iteration: 2477; Percent complete: 61.9%; Average loss: 3.3159
Iteration: 2478; Percent complete: 62.0%; Average loss: 2.8861
Iteration: 2479; Percent complete: 62.0%; Average loss: 3.0872
Iteration: 2480; Percent complete: 62.0%; Average loss: 3.0781
Iteration: 2481; Percent complete: 62.0%; Average loss: 2.9064
Iteration: 2482; Percent complete: 62.1%; Average loss: 3.0143
Iteration: 2483; Percent complete: 62.1%; Average loss: 3.2912
Iteration: 2484; Percent complete: 62.1%; Average loss: 3.0475
Iteration: 2485; Percent complete: 62.1%; Average loss: 3.0903
Iteration: 2486; Percent complete: 62.2%; Average loss: 2.8582
Iteration: 2487; Percent complete: 62.2%; Average loss: 2.8165
Iteration: 2488; Percent complete: 62.2%; Average loss: 2.8773
Iteration: 2489; Percent complete: 62.2%; Average loss: 3.0432
Iteration: 2490; Percent complete: 62.3%; Average loss: 3.1007
Iteration: 2491; Percent complete: 62.3%; Average loss: 2.7417
Iteration: 2492; Percent complete: 62.3%; Average loss: 3.0895
Iteration: 2493; Percent complete: 62.3%; Average loss: 3.1848
Iteration: 2494; Percent complete: 62.4%; Average loss: 3.1985
Iteration: 2495; Percent complete: 62.4%; Average loss: 2.9238
Iteration: 2496; Percent complete: 62.4%; Average loss: 2.7739
Iteration: 2497; Percent complete: 62.4%; Average loss: 2.9055
Iteration: 2498; Percent complete: 62.5%; Average loss: 3.3160
Iteration: 2499; Percent complete: 62.5%; Average loss: 3.3404
Iteration: 2500; Percent complete: 62.5%; Average loss: 2.6977
Iteration: 2501; Percent complete: 62.5%; Average loss: 2.9520
Iteration: 2502; Percent complete: 62.5%; Average loss: 3.0517
Iteration: 2503; Percent complete: 62.6%; Average loss: 2.8219
Iteration: 2504; Percent complete: 62.6%; Average loss: 2.8717
Iteration: 2505; Percent complete: 62.6%; Average loss: 3.0007
Iteration: 2506; Percent complete: 62.6%; Average loss: 3.0205
Iteration: 2507; Percent complete: 62.7%; Average loss: 2.9983
Iteration: 2508; Percent complete: 62.7%; Average loss: 3.1072
Iteration: 2509; Percent complete: 62.7%; Average loss: 3.0707
Iteration: 2510; Percent complete: 62.7%; Average loss: 3.0299
Iteration: 2511; Percent complete: 62.8%; Average loss: 3.0130
Iteration: 2512; Percent complete: 62.8%; Average loss: 3.1158
Iteration: 2513; Percent complete: 62.8%; Average loss: 2.9568
Iteration: 2514; Percent complete: 62.8%; Average loss: 2.8753
Iteration: 2515; Percent complete: 62.9%; Average loss: 2.9292
Iteration: 2516; Percent complete: 62.9%; Average loss: 3.1571
Iteration: 2517; Percent complete: 62.9%; Average loss: 3.0246
Iteration: 2518; Percent complete: 62.9%; Average loss: 2.8949
Iteration: 2519; Percent complete: 63.0%; Average loss: 2.8011
Iteration: 2520; Percent complete: 63.0%; Average loss: 2.9427
Iteration: 2521; Percent complete: 63.0%; Average loss: 2.9891
Iteration: 2522; Percent complete: 63.0%; Average loss: 3.0095
Iteration: 2523; Percent complete: 63.1%; Average loss: 3.0353
Iteration: 2524; Percent complete: 63.1%; Average loss: 3.2749
Iteration: 2525; Percent complete: 63.1%; Average loss: 2.9561
Iteration: 2526; Percent complete: 63.1%; Average loss: 3.2806
Iteration: 2527; Percent complete: 63.2%; Average loss: 3.0784
Iteration: 2528; Percent complete: 63.2%; Average loss: 3.1953
Iteration: 2529; Percent complete: 63.2%; Average loss: 2.8902
Iteration: 2530; Percent complete: 63.2%; Average loss: 2.9831
Iteration: 2531; Percent complete: 63.3%; Average loss: 3.0368
Iteration: 2532; Percent complete: 63.3%; Average loss: 2.9397
Iteration: 2533; Percent complete: 63.3%; Average loss: 3.1506
Iteration: 2534; Percent complete: 63.3%; Average loss: 2.8618
Iteration: 2535; Percent complete: 63.4%; Average loss: 3.0549
Iteration: 2536; Percent complete: 63.4%; Average loss: 3.1664
Iteration: 2537; Percent complete: 63.4%; Average loss: 2.7106
Iteration: 2538; Percent complete: 63.4%; Average loss: 3.1740
Iteration: 2539; Percent complete: 63.5%; Average loss: 3.0385
Iteration: 2540; Percent complete: 63.5%; Average loss: 3.2137
Iteration: 2541; Percent complete: 63.5%; Average loss: 3.0837
Iteration: 2542; Percent complete: 63.5%; Average loss: 3.0074
Iteration: 2543; Percent complete: 63.6%; Average loss: 2.6155
Iteration: 2544; Percent complete: 63.6%; Average loss: 3.2696
Iteration: 2545; Percent complete: 63.6%; Average loss: 2.8813
Iteration: 2546; Percent complete: 63.6%; Average loss: 3.0037
Iteration: 2547; Percent complete: 63.7%; Average loss: 2.9897
Iteration: 2548; Percent complete: 63.7%; Average loss: 3.0735
Iteration: 2549; Percent complete: 63.7%; Average loss: 3.0650
Iteration: 2550; Percent complete: 63.7%; Average loss: 3.1080
Iteration: 2551; Percent complete: 63.8%; Average loss: 2.8200
Iteration: 2552; Percent complete: 63.8%; Average loss: 2.9047
Iteration: 2553; Percent complete: 63.8%; Average loss: 3.2542
Iteration: 2554; Percent complete: 63.8%; Average loss: 2.9342
Iteration: 2555; Percent complete: 63.9%; Average loss: 3.0346
Iteration: 2556; Percent complete: 63.9%; Average loss: 2.8772
Iteration: 2557; Percent complete: 63.9%; Average loss: 3.1414
Iteration: 2558; Percent complete: 63.9%; Average loss: 3.0918
Iteration: 2559; Percent complete: 64.0%; Average loss: 3.0061
Iteration: 2560; Percent complete: 64.0%; Average loss: 2.8662
Iteration: 2561; Percent complete: 64.0%; Average loss: 3.2474
Iteration: 2562; Percent complete: 64.0%; Average loss: 3.0393
Iteration: 2563; Percent complete: 64.1%; Average loss: 3.0809
Iteration: 2564; Percent complete: 64.1%; Average loss: 2.8227
Iteration: 2565; Percent complete: 64.1%; Average loss: 3.2218
Iteration: 2566; Percent complete: 64.1%; Average loss: 2.9595
Iteration: 2567; Percent complete: 64.2%; Average loss: 2.9379
Iteration: 2568; Percent complete: 64.2%; Average loss: 2.9421
Iteration: 2569; Percent complete: 64.2%; Average loss: 3.0058
Iteration: 2570; Percent complete: 64.2%; Average loss: 3.0435
Iteration: 2571; Percent complete: 64.3%; Average loss: 3.0110
Iteration: 2572; Percent complete: 64.3%; Average loss: 2.8702
Iteration: 2573; Percent complete: 64.3%; Average loss: 2.9370
Iteration: 2574; Percent complete: 64.3%; Average loss: 3.0840
Iteration: 2575; Percent complete: 64.4%; Average loss: 2.9967
Iteration: 2576; Percent complete: 64.4%; Average loss: 3.0738
Iteration: 2577; Percent complete: 64.4%; Average loss: 2.9509
Iteration: 2578; Percent complete: 64.5%; Average loss: 2.8010
Iteration: 2579; Percent complete: 64.5%; Average loss: 2.8671
Iteration: 2580; Percent complete: 64.5%; Average loss: 3.0494
Iteration: 2581; Percent complete: 64.5%; Average loss: 3.2313
Iteration: 2582; Percent complete: 64.5%; Average loss: 2.8002
Iteration: 2583; Percent complete: 64.6%; Average loss: 2.8280
Iteration: 2584; Percent complete: 64.6%; Average loss: 2.7542
Iteration: 2585; Percent complete: 64.6%; Average loss: 3.0977
Iteration: 2586; Percent complete: 64.6%; Average loss: 3.0219
Iteration: 2587; Percent complete: 64.7%; Average loss: 3.0494
Iteration: 2588; Percent complete: 64.7%; Average loss: 2.8396
Iteration: 2589; Percent complete: 64.7%; Average loss: 3.1587
Iteration: 2590; Percent complete: 64.8%; Average loss: 3.2286
Iteration: 2591; Percent complete: 64.8%; Average loss: 3.1454
Iteration: 2592; Percent complete: 64.8%; Average loss: 2.9944
Iteration: 2593; Percent complete: 64.8%; Average loss: 3.0932
Iteration: 2594; Percent complete: 64.8%; Average loss: 2.8769
Iteration: 2595; Percent complete: 64.9%; Average loss: 2.9790
Iteration: 2596; Percent complete: 64.9%; Average loss: 3.0198
Iteration: 2597; Percent complete: 64.9%; Average loss: 2.8238
Iteration: 2598; Percent complete: 65.0%; Average loss: 3.0501
Iteration: 2599; Percent complete: 65.0%; Average loss: 3.1038
Iteration: 2600; Percent complete: 65.0%; Average loss: 2.9094
Iteration: 2601; Percent complete: 65.0%; Average loss: 2.9104
Iteration: 2602; Percent complete: 65.0%; Average loss: 3.0460
Iteration: 2603; Percent complete: 65.1%; Average loss: 3.0343
Iteration: 2604; Percent complete: 65.1%; Average loss: 3.2776
Iteration: 2605; Percent complete: 65.1%; Average loss: 3.1113
Iteration: 2606; Percent complete: 65.1%; Average loss: 3.2273
Iteration: 2607; Percent complete: 65.2%; Average loss: 3.2279
Iteration: 2608; Percent complete: 65.2%; Average loss: 3.0195
Iteration: 2609; Percent complete: 65.2%; Average loss: 3.2257
Iteration: 2610; Percent complete: 65.2%; Average loss: 3.0352
Iteration: 2611; Percent complete: 65.3%; Average loss: 3.0399
Iteration: 2612; Percent complete: 65.3%; Average loss: 2.9857
Iteration: 2613; Percent complete: 65.3%; Average loss: 2.9917
Iteration: 2614; Percent complete: 65.3%; Average loss: 3.0639
Iteration: 2615; Percent complete: 65.4%; Average loss: 2.6918
Iteration: 2616; Percent complete: 65.4%; Average loss: 3.0844
Iteration: 2617; Percent complete: 65.4%; Average loss: 2.8985
Iteration: 2618; Percent complete: 65.5%; Average loss: 2.9986
Iteration: 2619; Percent complete: 65.5%; Average loss: 2.8861
Iteration: 2620; Percent complete: 65.5%; Average loss: 3.1542
Iteration: 2621; Percent complete: 65.5%; Average loss: 2.9965
Iteration: 2622; Percent complete: 65.5%; Average loss: 2.7434
Iteration: 2623; Percent complete: 65.6%; Average loss: 3.2364
Iteration: 2624; Percent complete: 65.6%; Average loss: 3.0255
Iteration: 2625; Percent complete: 65.6%; Average loss: 2.9321
Iteration: 2626; Percent complete: 65.6%; Average loss: 3.0955
Iteration: 2627; Percent complete: 65.7%; Average loss: 2.8944
Iteration: 2628; Percent complete: 65.7%; Average loss: 3.0704
Iteration: 2629; Percent complete: 65.7%; Average loss: 2.8900
Iteration: 2630; Percent complete: 65.8%; Average loss: 2.8992
Iteration: 2631; Percent complete: 65.8%; Average loss: 2.8165
Iteration: 2632; Percent complete: 65.8%; Average loss: 2.9634
Iteration: 2633; Percent complete: 65.8%; Average loss: 3.3636
Iteration: 2634; Percent complete: 65.8%; Average loss: 3.0438
Iteration: 2635; Percent complete: 65.9%; Average loss: 2.8746
Iteration: 2636; Percent complete: 65.9%; Average loss: 3.1292
Iteration: 2637; Percent complete: 65.9%; Average loss: 2.8750
Iteration: 2638; Percent complete: 66.0%; Average loss: 2.7047
Iteration: 2639; Percent complete: 66.0%; Average loss: 2.9193
Iteration: 2640; Percent complete: 66.0%; Average loss: 2.9350
Iteration: 2641; Percent complete: 66.0%; Average loss: 2.8696
Iteration: 2642; Percent complete: 66.0%; Average loss: 3.2226
Iteration: 2643; Percent complete: 66.1%; Average loss: 2.8267
Iteration: 2644; Percent complete: 66.1%; Average loss: 2.9209
Iteration: 2645; Percent complete: 66.1%; Average loss: 3.1970
Iteration: 2646; Percent complete: 66.1%; Average loss: 2.9289
Iteration: 2647; Percent complete: 66.2%; Average loss: 2.9753
Iteration: 2648; Percent complete: 66.2%; Average loss: 3.0245
Iteration: 2649; Percent complete: 66.2%; Average loss: 2.7824
Iteration: 2650; Percent complete: 66.2%; Average loss: 2.8589
Iteration: 2651; Percent complete: 66.3%; Average loss: 3.2010
Iteration: 2652; Percent complete: 66.3%; Average loss: 3.1796
Iteration: 2653; Percent complete: 66.3%; Average loss: 3.1595
Iteration: 2654; Percent complete: 66.3%; Average loss: 2.9745
Iteration: 2655; Percent complete: 66.4%; Average loss: 3.2552
Iteration: 2656; Percent complete: 66.4%; Average loss: 3.0038
Iteration: 2657; Percent complete: 66.4%; Average loss: 2.9362
Iteration: 2658; Percent complete: 66.5%; Average loss: 2.8249
Iteration: 2659; Percent complete: 66.5%; Average loss: 3.0315
Iteration: 2660; Percent complete: 66.5%; Average loss: 3.2433
Iteration: 2661; Percent complete: 66.5%; Average loss: 2.8465
Iteration: 2662; Percent complete: 66.5%; Average loss: 3.0026
Iteration: 2663; Percent complete: 66.6%; Average loss: 2.7188
Iteration: 2664; Percent complete: 66.6%; Average loss: 2.9460
Iteration: 2665; Percent complete: 66.6%; Average loss: 3.0766
Iteration: 2666; Percent complete: 66.6%; Average loss: 2.9762
Iteration: 2667; Percent complete: 66.7%; Average loss: 2.9885
Iteration: 2668; Percent complete: 66.7%; Average loss: 2.9136
Iteration: 2669; Percent complete: 66.7%; Average loss: 2.9530
Iteration: 2670; Percent complete: 66.8%; Average loss: 3.1029
Iteration: 2671; Percent complete: 66.8%; Average loss: 2.9581
Iteration: 2672; Percent complete: 66.8%; Average loss: 2.8751
Iteration: 2673; Percent complete: 66.8%; Average loss: 3.0228
Iteration: 2674; Percent complete: 66.8%; Average loss: 2.9980
Iteration: 2675; Percent complete: 66.9%; Average loss: 3.1778
Iteration: 2676; Percent complete: 66.9%; Average loss: 3.0110
Iteration: 2677; Percent complete: 66.9%; Average loss: 2.9999
Iteration: 2678; Percent complete: 67.0%; Average loss: 2.8637
Iteration: 2679; Percent complete: 67.0%; Average loss: 2.7602
Iteration: 2680; Percent complete: 67.0%; Average loss: 2.9435
Iteration: 2681; Percent complete: 67.0%; Average loss: 2.9638
Iteration: 2682; Percent complete: 67.0%; Average loss: 2.9956
Iteration: 2683; Percent complete: 67.1%; Average loss: 2.6761
Iteration: 2684; Percent complete: 67.1%; Average loss: 2.9263
Iteration: 2685; Percent complete: 67.1%; Average loss: 2.8377
Iteration: 2686; Percent complete: 67.2%; Average loss: 2.8527
Iteration: 2687; Percent complete: 67.2%; Average loss: 2.9922
Iteration: 2688; Percent complete: 67.2%; Average loss: 2.9717
Iteration: 2689; Percent complete: 67.2%; Average loss: 3.0173
Iteration: 2690; Percent complete: 67.2%; Average loss: 2.9750
Iteration: 2691; Percent complete: 67.3%; Average loss: 3.0568
Iteration: 2692; Percent complete: 67.3%; Average loss: 2.9489
Iteration: 2693; Percent complete: 67.3%; Average loss: 3.0926
Iteration: 2694; Percent complete: 67.3%; Average loss: 2.7249
Iteration: 2695; Percent complete: 67.4%; Average loss: 3.0322
Iteration: 2696; Percent complete: 67.4%; Average loss: 2.9278
Iteration: 2697; Percent complete: 67.4%; Average loss: 2.8933
Iteration: 2698; Percent complete: 67.5%; Average loss: 3.0013
Iteration: 2699; Percent complete: 67.5%; Average loss: 2.9842
Iteration: 2700; Percent complete: 67.5%; Average loss: 2.8574
Iteration: 2701; Percent complete: 67.5%; Average loss: 2.9005
Iteration: 2702; Percent complete: 67.5%; Average loss: 2.7546
Iteration: 2703; Percent complete: 67.6%; Average loss: 2.9167
Iteration: 2704; Percent complete: 67.6%; Average loss: 2.9049
Iteration: 2705; Percent complete: 67.6%; Average loss: 3.0145
Iteration: 2706; Percent complete: 67.7%; Average loss: 2.8532
Iteration: 2707; Percent complete: 67.7%; Average loss: 3.0567
Iteration: 2708; Percent complete: 67.7%; Average loss: 3.2285
Iteration: 2709; Percent complete: 67.7%; Average loss: 2.8267
Iteration: 2710; Percent complete: 67.8%; Average loss: 3.0982
Iteration: 2711; Percent complete: 67.8%; Average loss: 2.8877
Iteration: 2712; Percent complete: 67.8%; Average loss: 2.9180
Iteration: 2713; Percent complete: 67.8%; Average loss: 3.1380
Iteration: 2714; Percent complete: 67.8%; Average loss: 2.7569
Iteration: 2715; Percent complete: 67.9%; Average loss: 3.1548
Iteration: 2716; Percent complete: 67.9%; Average loss: 2.8605
Iteration: 2717; Percent complete: 67.9%; Average loss: 3.0388
Iteration: 2718; Percent complete: 68.0%; Average loss: 2.8688
Iteration: 2719; Percent complete: 68.0%; Average loss: 2.9155
Iteration: 2720; Percent complete: 68.0%; Average loss: 2.9601
Iteration: 2721; Percent complete: 68.0%; Average loss: 3.1063
Iteration: 2722; Percent complete: 68.0%; Average loss: 2.9606
Iteration: 2723; Percent complete: 68.1%; Average loss: 3.0425
Iteration: 2724; Percent complete: 68.1%; Average loss: 3.0224
Iteration: 2725; Percent complete: 68.1%; Average loss: 2.9436
Iteration: 2726; Percent complete: 68.2%; Average loss: 2.9152
Iteration: 2727; Percent complete: 68.2%; Average loss: 2.7811
Iteration: 2728; Percent complete: 68.2%; Average loss: 2.9955
Iteration: 2729; Percent complete: 68.2%; Average loss: 3.0642
Iteration: 2730; Percent complete: 68.2%; Average loss: 2.7513
Iteration: 2731; Percent complete: 68.3%; Average loss: 2.9346
Iteration: 2732; Percent complete: 68.3%; Average loss: 2.8338
Iteration: 2733; Percent complete: 68.3%; Average loss: 2.9069
Iteration: 2734; Percent complete: 68.3%; Average loss: 3.0149
Iteration: 2735; Percent complete: 68.4%; Average loss: 3.1079
Iteration: 2736; Percent complete: 68.4%; Average loss: 2.9963
Iteration: 2737; Percent complete: 68.4%; Average loss: 2.9656
Iteration: 2738; Percent complete: 68.5%; Average loss: 3.0681
Iteration: 2739; Percent complete: 68.5%; Average loss: 2.9722
Iteration: 2740; Percent complete: 68.5%; Average loss: 2.9638
Iteration: 2741; Percent complete: 68.5%; Average loss: 3.0068
Iteration: 2742; Percent complete: 68.5%; Average loss: 2.8337
Iteration: 2743; Percent complete: 68.6%; Average loss: 2.9058
Iteration: 2744; Percent complete: 68.6%; Average loss: 3.0459
Iteration: 2745; Percent complete: 68.6%; Average loss: 2.5502
Iteration: 2746; Percent complete: 68.7%; Average loss: 3.0915
Iteration: 2747; Percent complete: 68.7%; Average loss: 2.9855
Iteration: 2748; Percent complete: 68.7%; Average loss: 2.9987
Iteration: 2749; Percent complete: 68.7%; Average loss: 2.8893
Iteration: 2750; Percent complete: 68.8%; Average loss: 2.7861
Iteration: 2751; Percent complete: 68.8%; Average loss: 3.0766
Iteration: 2752; Percent complete: 68.8%; Average loss: 2.8763
Iteration: 2753; Percent complete: 68.8%; Average loss: 3.0608
Iteration: 2754; Percent complete: 68.8%; Average loss: 2.8179
Iteration: 2755; Percent complete: 68.9%; Average loss: 2.5586
Iteration: 2756; Percent complete: 68.9%; Average loss: 2.8236
Iteration: 2757; Percent complete: 68.9%; Average loss: 3.0911
Iteration: 2758; Percent complete: 69.0%; Average loss: 3.0338
Iteration: 2759; Percent complete: 69.0%; Average loss: 2.9071
Iteration: 2760; Percent complete: 69.0%; Average loss: 3.0498
Iteration: 2761; Percent complete: 69.0%; Average loss: 2.9858
Iteration: 2762; Percent complete: 69.0%; Average loss: 2.8432
Iteration: 2763; Percent complete: 69.1%; Average loss: 2.9627
Iteration: 2764; Percent complete: 69.1%; Average loss: 3.0823
Iteration: 2765; Percent complete: 69.1%; Average loss: 2.8110
Iteration: 2766; Percent complete: 69.2%; Average loss: 3.2241
Iteration: 2767; Percent complete: 69.2%; Average loss: 2.8854
Iteration: 2768; Percent complete: 69.2%; Average loss: 2.9809
Iteration: 2769; Percent complete: 69.2%; Average loss: 2.6752
Iteration: 2770; Percent complete: 69.2%; Average loss: 2.7684
Iteration: 2771; Percent complete: 69.3%; Average loss: 2.6598
Iteration: 2772; Percent complete: 69.3%; Average loss: 3.0240
Iteration: 2773; Percent complete: 69.3%; Average loss: 2.9068
Iteration: 2774; Percent complete: 69.3%; Average loss: 2.9570
Iteration: 2775; Percent complete: 69.4%; Average loss: 2.9165
Iteration: 2776; Percent complete: 69.4%; Average loss: 2.9637
Iteration: 2777; Percent complete: 69.4%; Average loss: 3.2319
Iteration: 2778; Percent complete: 69.5%; Average loss: 3.2035
Iteration: 2779; Percent complete: 69.5%; Average loss: 2.9817
Iteration: 2780; Percent complete: 69.5%; Average loss: 2.9455
Iteration: 2781; Percent complete: 69.5%; Average loss: 2.9081
Iteration: 2782; Percent complete: 69.5%; Average loss: 2.9686
Iteration: 2783; Percent complete: 69.6%; Average loss: 2.8813
Iteration: 2784; Percent complete: 69.6%; Average loss: 3.0033
Iteration: 2785; Percent complete: 69.6%; Average loss: 2.8987
Iteration: 2786; Percent complete: 69.7%; Average loss: 3.1428
Iteration: 2787; Percent complete: 69.7%; Average loss: 2.8328
Iteration: 2788; Percent complete: 69.7%; Average loss: 2.8144
Iteration: 2789; Percent complete: 69.7%; Average loss: 3.1540
Iteration: 2790; Percent complete: 69.8%; Average loss: 3.1196
Iteration: 2791; Percent complete: 69.8%; Average loss: 3.0040
Iteration: 2792; Percent complete: 69.8%; Average loss: 3.0600
Iteration: 2793; Percent complete: 69.8%; Average loss: 2.9361
Iteration: 2794; Percent complete: 69.8%; Average loss: 3.0799
Iteration: 2795; Percent complete: 69.9%; Average loss: 3.0402
Iteration: 2796; Percent complete: 69.9%; Average loss: 2.7618
Iteration: 2797; Percent complete: 69.9%; Average loss: 2.7964
Iteration: 2798; Percent complete: 70.0%; Average loss: 2.8617
Iteration: 2799; Percent complete: 70.0%; Average loss: 2.9878
Iteration: 2800; Percent complete: 70.0%; Average loss: 2.7504
Iteration: 2801; Percent complete: 70.0%; Average loss: 2.8580
Iteration: 2802; Percent complete: 70.0%; Average loss: 2.8685
Iteration: 2803; Percent complete: 70.1%; Average loss: 2.7516
Iteration: 2804; Percent complete: 70.1%; Average loss: 2.7257
Iteration: 2805; Percent complete: 70.1%; Average loss: 2.9471
Iteration: 2806; Percent complete: 70.2%; Average loss: 2.8982
Iteration: 2807; Percent complete: 70.2%; Average loss: 2.9259
Iteration: 2808; Percent complete: 70.2%; Average loss: 3.0059
Iteration: 2809; Percent complete: 70.2%; Average loss: 2.7959
Iteration: 2810; Percent complete: 70.2%; Average loss: 2.6522
Iteration: 2811; Percent complete: 70.3%; Average loss: 2.9103
Iteration: 2812; Percent complete: 70.3%; Average loss: 3.0946
Iteration: 2813; Percent complete: 70.3%; Average loss: 2.8603
Iteration: 2814; Percent complete: 70.3%; Average loss: 2.8077
Iteration: 2815; Percent complete: 70.4%; Average loss: 2.9464
Iteration: 2816; Percent complete: 70.4%; Average loss: 2.8726
Iteration: 2817; Percent complete: 70.4%; Average loss: 3.2415
Iteration: 2818; Percent complete: 70.5%; Average loss: 2.9972
Iteration: 2819; Percent complete: 70.5%; Average loss: 2.8944
Iteration: 2820; Percent complete: 70.5%; Average loss: 3.0370
Iteration: 2821; Percent complete: 70.5%; Average loss: 2.8722
Iteration: 2822; Percent complete: 70.5%; Average loss: 3.0710
Iteration: 2823; Percent complete: 70.6%; Average loss: 2.7812
Iteration: 2824; Percent complete: 70.6%; Average loss: 2.9932
Iteration: 2825; Percent complete: 70.6%; Average loss: 3.0553
Iteration: 2826; Percent complete: 70.7%; Average loss: 3.1295
Iteration: 2827; Percent complete: 70.7%; Average loss: 3.0668
Iteration: 2828; Percent complete: 70.7%; Average loss: 3.0899
Iteration: 2829; Percent complete: 70.7%; Average loss: 2.7914
Iteration: 2830; Percent complete: 70.8%; Average loss: 2.8199
Iteration: 2831; Percent complete: 70.8%; Average loss: 2.9094
Iteration: 2832; Percent complete: 70.8%; Average loss: 2.9319
Iteration: 2833; Percent complete: 70.8%; Average loss: 3.0194
Iteration: 2834; Percent complete: 70.9%; Average loss: 2.6769
Iteration: 2835; Percent complete: 70.9%; Average loss: 2.8394
Iteration: 2836; Percent complete: 70.9%; Average loss: 3.0220
Iteration: 2837; Percent complete: 70.9%; Average loss: 2.8327
Iteration: 2838; Percent complete: 71.0%; Average loss: 2.7978
Iteration: 2839; Percent complete: 71.0%; Average loss: 2.8368
Iteration: 2840; Percent complete: 71.0%; Average loss: 2.9183
Iteration: 2841; Percent complete: 71.0%; Average loss: 3.0764
Iteration: 2842; Percent complete: 71.0%; Average loss: 2.9923
Iteration: 2843; Percent complete: 71.1%; Average loss: 2.9126
Iteration: 2844; Percent complete: 71.1%; Average loss: 2.7952
Iteration: 2845; Percent complete: 71.1%; Average loss: 3.1327
Iteration: 2846; Percent complete: 71.2%; Average loss: 2.9010
Iteration: 2847; Percent complete: 71.2%; Average loss: 2.7796
Iteration: 2848; Percent complete: 71.2%; Average loss: 3.0164
Iteration: 2849; Percent complete: 71.2%; Average loss: 2.9841
Iteration: 2850; Percent complete: 71.2%; Average loss: 2.9307
Iteration: 2851; Percent complete: 71.3%; Average loss: 2.9832
Iteration: 2852; Percent complete: 71.3%; Average loss: 2.9146
Iteration: 2853; Percent complete: 71.3%; Average loss: 2.8312
Iteration: 2854; Percent complete: 71.4%; Average loss: 2.9593
Iteration: 2855; Percent complete: 71.4%; Average loss: 2.7897
Iteration: 2856; Percent complete: 71.4%; Average loss: 2.8131
Iteration: 2857; Percent complete: 71.4%; Average loss: 2.8403
Iteration: 2858; Percent complete: 71.5%; Average loss: 2.7888
Iteration: 2859; Percent complete: 71.5%; Average loss: 2.8594
Iteration: 2860; Percent complete: 71.5%; Average loss: 2.8666
Iteration: 2861; Percent complete: 71.5%; Average loss: 2.9241
Iteration: 2862; Percent complete: 71.5%; Average loss: 2.7550
Iteration: 2863; Percent complete: 71.6%; Average loss: 2.7998
Iteration: 2864; Percent complete: 71.6%; Average loss: 3.0297
Iteration: 2865; Percent complete: 71.6%; Average loss: 2.8772
Iteration: 2866; Percent complete: 71.7%; Average loss: 3.0335
Iteration: 2867; Percent complete: 71.7%; Average loss: 2.9476
Iteration: 2868; Percent complete: 71.7%; Average loss: 3.0405
Iteration: 2869; Percent complete: 71.7%; Average loss: 2.7103
Iteration: 2870; Percent complete: 71.8%; Average loss: 2.8975
Iteration: 2871; Percent complete: 71.8%; Average loss: 3.2626
Iteration: 2872; Percent complete: 71.8%; Average loss: 2.6274
Iteration: 2873; Percent complete: 71.8%; Average loss: 3.0785
Iteration: 2874; Percent complete: 71.9%; Average loss: 3.1895
Iteration: 2875; Percent complete: 71.9%; Average loss: 2.9454
Iteration: 2876; Percent complete: 71.9%; Average loss: 2.9936
Iteration: 2877; Percent complete: 71.9%; Average loss: 2.6867
Iteration: 2878; Percent complete: 72.0%; Average loss: 2.8551
Iteration: 2879; Percent complete: 72.0%; Average loss: 3.0061
Iteration: 2880; Percent complete: 72.0%; Average loss: 2.8279
Iteration: 2881; Percent complete: 72.0%; Average loss: 2.9551
Iteration: 2882; Percent complete: 72.0%; Average loss: 3.1905
Iteration: 2883; Percent complete: 72.1%; Average loss: 2.7876
Iteration: 2884; Percent complete: 72.1%; Average loss: 2.7943
Iteration: 2885; Percent complete: 72.1%; Average loss: 3.0628
Iteration: 2886; Percent complete: 72.2%; Average loss: 2.9779
Iteration: 2887; Percent complete: 72.2%; Average loss: 3.0812
Iteration: 2888; Percent complete: 72.2%; Average loss: 2.9815
Iteration: 2889; Percent complete: 72.2%; Average loss: 2.8360
Iteration: 2890; Percent complete: 72.2%; Average loss: 2.7097
Iteration: 2891; Percent complete: 72.3%; Average loss: 2.9955
Iteration: 2892; Percent complete: 72.3%; Average loss: 2.9710
Iteration: 2893; Percent complete: 72.3%; Average loss: 2.9034
Iteration: 2894; Percent complete: 72.4%; Average loss: 2.8781
Iteration: 2895; Percent complete: 72.4%; Average loss: 2.9648
Iteration: 2896; Percent complete: 72.4%; Average loss: 2.8218
Iteration: 2897; Percent complete: 72.4%; Average loss: 2.9542
Iteration: 2898; Percent complete: 72.5%; Average loss: 2.8892
Iteration: 2899; Percent complete: 72.5%; Average loss: 3.1090
Iteration: 2900; Percent complete: 72.5%; Average loss: 2.9539
Iteration: 2901; Percent complete: 72.5%; Average loss: 2.9935
Iteration: 2902; Percent complete: 72.5%; Average loss: 2.6629
Iteration: 2903; Percent complete: 72.6%; Average loss: 2.9627
Iteration: 2904; Percent complete: 72.6%; Average loss: 2.8354
Iteration: 2905; Percent complete: 72.6%; Average loss: 2.9876
Iteration: 2906; Percent complete: 72.7%; Average loss: 2.8022
Iteration: 2907; Percent complete: 72.7%; Average loss: 3.1498
Iteration: 2908; Percent complete: 72.7%; Average loss: 2.9000
Iteration: 2909; Percent complete: 72.7%; Average loss: 3.0950
Iteration: 2910; Percent complete: 72.8%; Average loss: 2.7017
Iteration: 2911; Percent complete: 72.8%; Average loss: 2.8750
Iteration: 2912; Percent complete: 72.8%; Average loss: 2.9354
Iteration: 2913; Percent complete: 72.8%; Average loss: 3.0945
Iteration: 2914; Percent complete: 72.9%; Average loss: 3.0731
Iteration: 2915; Percent complete: 72.9%; Average loss: 2.9518
Iteration: 2916; Percent complete: 72.9%; Average loss: 2.8993
Iteration: 2917; Percent complete: 72.9%; Average loss: 2.7700
Iteration: 2918; Percent complete: 73.0%; Average loss: 2.9766
Iteration: 2919; Percent complete: 73.0%; Average loss: 2.7121
Iteration: 2920; Percent complete: 73.0%; Average loss: 2.9228
Iteration: 2921; Percent complete: 73.0%; Average loss: 3.0062
Iteration: 2922; Percent complete: 73.0%; Average loss: 2.9907
Iteration: 2923; Percent complete: 73.1%; Average loss: 2.7383
Iteration: 2924; Percent complete: 73.1%; Average loss: 2.8742
Iteration: 2925; Percent complete: 73.1%; Average loss: 2.7704
Iteration: 2926; Percent complete: 73.2%; Average loss: 3.0878
Iteration: 2927; Percent complete: 73.2%; Average loss: 3.0383
Iteration: 2928; Percent complete: 73.2%; Average loss: 3.0491
Iteration: 2929; Percent complete: 73.2%; Average loss: 2.8969
Iteration: 2930; Percent complete: 73.2%; Average loss: 2.8859
Iteration: 2931; Percent complete: 73.3%; Average loss: 2.8969
Iteration: 2932; Percent complete: 73.3%; Average loss: 3.0829
Iteration: 2933; Percent complete: 73.3%; Average loss: 2.8648
Iteration: 2934; Percent complete: 73.4%; Average loss: 2.7475
Iteration: 2935; Percent complete: 73.4%; Average loss: 3.4020
Iteration: 2936; Percent complete: 73.4%; Average loss: 2.7819
Iteration: 2937; Percent complete: 73.4%; Average loss: 2.9354
Iteration: 2938; Percent complete: 73.5%; Average loss: 2.8160
Iteration: 2939; Percent complete: 73.5%; Average loss: 2.7817
Iteration: 2940; Percent complete: 73.5%; Average loss: 2.6799
Iteration: 2941; Percent complete: 73.5%; Average loss: 2.9063
Iteration: 2942; Percent complete: 73.6%; Average loss: 3.0292
Iteration: 2943; Percent complete: 73.6%; Average loss: 2.6448
Iteration: 2944; Percent complete: 73.6%; Average loss: 3.1482
Iteration: 2945; Percent complete: 73.6%; Average loss: 2.8997
Iteration: 2946; Percent complete: 73.7%; Average loss: 2.9586
Iteration: 2947; Percent complete: 73.7%; Average loss: 2.8941
Iteration: 2948; Percent complete: 73.7%; Average loss: 2.8444
Iteration: 2949; Percent complete: 73.7%; Average loss: 3.0576
Iteration: 2950; Percent complete: 73.8%; Average loss: 3.0507
Iteration: 2951; Percent complete: 73.8%; Average loss: 2.7605
Iteration: 2952; Percent complete: 73.8%; Average loss: 3.0545
Iteration: 2953; Percent complete: 73.8%; Average loss: 2.6876
Iteration: 2954; Percent complete: 73.9%; Average loss: 2.8494
Iteration: 2955; Percent complete: 73.9%; Average loss: 3.0959
Iteration: 2956; Percent complete: 73.9%; Average loss: 2.7438
Iteration: 2957; Percent complete: 73.9%; Average loss: 2.7802
Iteration: 2958; Percent complete: 74.0%; Average loss: 2.5535
Iteration: 2959; Percent complete: 74.0%; Average loss: 3.0118
Iteration: 2960; Percent complete: 74.0%; Average loss: 2.7613
Iteration: 2961; Percent complete: 74.0%; Average loss: 2.8696
Iteration: 2962; Percent complete: 74.1%; Average loss: 2.9938
Iteration: 2963; Percent complete: 74.1%; Average loss: 2.9215
Iteration: 2964; Percent complete: 74.1%; Average loss: 2.9053
Iteration: 2965; Percent complete: 74.1%; Average loss: 3.0314
Iteration: 2966; Percent complete: 74.2%; Average loss: 2.9922
Iteration: 2967; Percent complete: 74.2%; Average loss: 3.0080
Iteration: 2968; Percent complete: 74.2%; Average loss: 2.8914
Iteration: 2969; Percent complete: 74.2%; Average loss: 2.7259
Iteration: 2970; Percent complete: 74.2%; Average loss: 2.7464
Iteration: 2971; Percent complete: 74.3%; Average loss: 3.0716
Iteration: 2972; Percent complete: 74.3%; Average loss: 3.0906
Iteration: 2973; Percent complete: 74.3%; Average loss: 3.0441
Iteration: 2974; Percent complete: 74.4%; Average loss: 2.9178
Iteration: 2975; Percent complete: 74.4%; Average loss: 2.8915
Iteration: 2976; Percent complete: 74.4%; Average loss: 2.6867
Iteration: 2977; Percent complete: 74.4%; Average loss: 2.9638
Iteration: 2978; Percent complete: 74.5%; Average loss: 2.7096
Iteration: 2979; Percent complete: 74.5%; Average loss: 2.7930
Iteration: 2980; Percent complete: 74.5%; Average loss: 3.0344
Iteration: 2981; Percent complete: 74.5%; Average loss: 2.9932
Iteration: 2982; Percent complete: 74.6%; Average loss: 2.9410
Iteration: 2983; Percent complete: 74.6%; Average loss: 2.6728
Iteration: 2984; Percent complete: 74.6%; Average loss: 2.6244
Iteration: 2985; Percent complete: 74.6%; Average loss: 2.9220
Iteration: 2986; Percent complete: 74.7%; Average loss: 2.8158
Iteration: 2987; Percent complete: 74.7%; Average loss: 3.0462
Iteration: 2988; Percent complete: 74.7%; Average loss: 2.8750
Iteration: 2989; Percent complete: 74.7%; Average loss: 2.6633
Iteration: 2990; Percent complete: 74.8%; Average loss: 3.1500
Iteration: 2991; Percent complete: 74.8%; Average loss: 2.8159
Iteration: 2992; Percent complete: 74.8%; Average loss: 2.7437
Iteration: 2993; Percent complete: 74.8%; Average loss: 2.9685
Iteration: 2994; Percent complete: 74.9%; Average loss: 2.9506
Iteration: 2995; Percent complete: 74.9%; Average loss: 3.0977
Iteration: 2996; Percent complete: 74.9%; Average loss: 3.0489
Iteration: 2997; Percent complete: 74.9%; Average loss: 3.0138
Iteration: 2998; Percent complete: 75.0%; Average loss: 2.9891
Iteration: 2999; Percent complete: 75.0%; Average loss: 2.9733
Iteration: 3000; Percent complete: 75.0%; Average loss: 2.9357
Iteration: 3001; Percent complete: 75.0%; Average loss: 2.8092
Iteration: 3002; Percent complete: 75.0%; Average loss: 2.8705
Iteration: 3003; Percent complete: 75.1%; Average loss: 2.9608
Iteration: 3004; Percent complete: 75.1%; Average loss: 2.8782
Iteration: 3005; Percent complete: 75.1%; Average loss: 2.9570
Iteration: 3006; Percent complete: 75.1%; Average loss: 2.8634
Iteration: 3007; Percent complete: 75.2%; Average loss: 2.8878
Iteration: 3008; Percent complete: 75.2%; Average loss: 2.8199
Iteration: 3009; Percent complete: 75.2%; Average loss: 3.0053
Iteration: 3010; Percent complete: 75.2%; Average loss: 2.9324
Iteration: 3011; Percent complete: 75.3%; Average loss: 2.9002
Iteration: 3012; Percent complete: 75.3%; Average loss: 3.0516
Iteration: 3013; Percent complete: 75.3%; Average loss: 2.9159
Iteration: 3014; Percent complete: 75.3%; Average loss: 2.9177
Iteration: 3015; Percent complete: 75.4%; Average loss: 2.9223
Iteration: 3016; Percent complete: 75.4%; Average loss: 2.9078
Iteration: 3017; Percent complete: 75.4%; Average loss: 3.0318
Iteration: 3018; Percent complete: 75.4%; Average loss: 2.9476
Iteration: 3019; Percent complete: 75.5%; Average loss: 2.8563
Iteration: 3020; Percent complete: 75.5%; Average loss: 2.9087
Iteration: 3021; Percent complete: 75.5%; Average loss: 3.1355
Iteration: 3022; Percent complete: 75.5%; Average loss: 2.9919
Iteration: 3023; Percent complete: 75.6%; Average loss: 2.8395
Iteration: 3024; Percent complete: 75.6%; Average loss: 2.9893
Iteration: 3025; Percent complete: 75.6%; Average loss: 3.2082
Iteration: 3026; Percent complete: 75.6%; Average loss: 2.8194
Iteration: 3027; Percent complete: 75.7%; Average loss: 2.6423
Iteration: 3028; Percent complete: 75.7%; Average loss: 2.9622
Iteration: 3029; Percent complete: 75.7%; Average loss: 2.7897
Iteration: 3030; Percent complete: 75.8%; Average loss: 2.7169
Iteration: 3031; Percent complete: 75.8%; Average loss: 2.7786
Iteration: 3032; Percent complete: 75.8%; Average loss: 2.8090
Iteration: 3033; Percent complete: 75.8%; Average loss: 3.0403
Iteration: 3034; Percent complete: 75.8%; Average loss: 2.6833
Iteration: 3035; Percent complete: 75.9%; Average loss: 2.7725
Iteration: 3036; Percent complete: 75.9%; Average loss: 2.7909
Iteration: 3037; Percent complete: 75.9%; Average loss: 2.9215
Iteration: 3038; Percent complete: 75.9%; Average loss: 2.8880
Iteration: 3039; Percent complete: 76.0%; Average loss: 3.1363
Iteration: 3040; Percent complete: 76.0%; Average loss: 2.9210
Iteration: 3041; Percent complete: 76.0%; Average loss: 3.1818
Iteration: 3042; Percent complete: 76.0%; Average loss: 2.8843
Iteration: 3043; Percent complete: 76.1%; Average loss: 2.6322
Iteration: 3044; Percent complete: 76.1%; Average loss: 2.8226
Iteration: 3045; Percent complete: 76.1%; Average loss: 2.7612
Iteration: 3046; Percent complete: 76.1%; Average loss: 2.8491
Iteration: 3047; Percent complete: 76.2%; Average loss: 2.8252
Iteration: 3048; Percent complete: 76.2%; Average loss: 2.8672
Iteration: 3049; Percent complete: 76.2%; Average loss: 3.0725
Iteration: 3050; Percent complete: 76.2%; Average loss: 2.6355
Iteration: 3051; Percent complete: 76.3%; Average loss: 3.0267
Iteration: 3052; Percent complete: 76.3%; Average loss: 2.7756
Iteration: 3053; Percent complete: 76.3%; Average loss: 2.8999
Iteration: 3054; Percent complete: 76.3%; Average loss: 2.7297
Iteration: 3055; Percent complete: 76.4%; Average loss: 2.9017
Iteration: 3056; Percent complete: 76.4%; Average loss: 2.8614
Iteration: 3057; Percent complete: 76.4%; Average loss: 2.9103
Iteration: 3058; Percent complete: 76.4%; Average loss: 2.9228
Iteration: 3059; Percent complete: 76.5%; Average loss: 2.9057
Iteration: 3060; Percent complete: 76.5%; Average loss: 2.8180
Iteration: 3061; Percent complete: 76.5%; Average loss: 2.8708
Iteration: 3062; Percent complete: 76.5%; Average loss: 2.8121
Iteration: 3063; Percent complete: 76.6%; Average loss: 2.8796
Iteration: 3064; Percent complete: 76.6%; Average loss: 2.8903
Iteration: 3065; Percent complete: 76.6%; Average loss: 2.9282
Iteration: 3066; Percent complete: 76.6%; Average loss: 2.7941
Iteration: 3067; Percent complete: 76.7%; Average loss: 2.8311
Iteration: 3068; Percent complete: 76.7%; Average loss: 2.7265
Iteration: 3069; Percent complete: 76.7%; Average loss: 2.8328
Iteration: 3070; Percent complete: 76.8%; Average loss: 2.6190
Iteration: 3071; Percent complete: 76.8%; Average loss: 2.8061
Iteration: 3072; Percent complete: 76.8%; Average loss: 2.8321
Iteration: 3073; Percent complete: 76.8%; Average loss: 3.2295
Iteration: 3074; Percent complete: 76.8%; Average loss: 2.9620
Iteration: 3075; Percent complete: 76.9%; Average loss: 2.6073
Iteration: 3076; Percent complete: 76.9%; Average loss: 2.6937
Iteration: 3077; Percent complete: 76.9%; Average loss: 2.7879
Iteration: 3078; Percent complete: 77.0%; Average loss: 2.7540
Iteration: 3079; Percent complete: 77.0%; Average loss: 2.8467
Iteration: 3080; Percent complete: 77.0%; Average loss: 2.9061
Iteration: 3081; Percent complete: 77.0%; Average loss: 2.8316
Iteration: 3082; Percent complete: 77.0%; Average loss: 2.6672
Iteration: 3083; Percent complete: 77.1%; Average loss: 2.8990
Iteration: 3084; Percent complete: 77.1%; Average loss: 2.7778
Iteration: 3085; Percent complete: 77.1%; Average loss: 2.9400
Iteration: 3086; Percent complete: 77.1%; Average loss: 2.6451
Iteration: 3087; Percent complete: 77.2%; Average loss: 2.6923
Iteration: 3088; Percent complete: 77.2%; Average loss: 2.8971
Iteration: 3089; Percent complete: 77.2%; Average loss: 2.7108
Iteration: 3090; Percent complete: 77.2%; Average loss: 2.9312
Iteration: 3091; Percent complete: 77.3%; Average loss: 2.8065
Iteration: 3092; Percent complete: 77.3%; Average loss: 2.6729
Iteration: 3093; Percent complete: 77.3%; Average loss: 2.7770
Iteration: 3094; Percent complete: 77.3%; Average loss: 2.9211
Iteration: 3095; Percent complete: 77.4%; Average loss: 2.8852
Iteration: 3096; Percent complete: 77.4%; Average loss: 2.7850
Iteration: 3097; Percent complete: 77.4%; Average loss: 2.7957
Iteration: 3098; Percent complete: 77.5%; Average loss: 2.7621
Iteration: 3099; Percent complete: 77.5%; Average loss: 2.8050
Iteration: 3100; Percent complete: 77.5%; Average loss: 2.7617
Iteration: 3101; Percent complete: 77.5%; Average loss: 2.9096
Iteration: 3102; Percent complete: 77.5%; Average loss: 2.7998
Iteration: 3103; Percent complete: 77.6%; Average loss: 2.7245
Iteration: 3104; Percent complete: 77.6%; Average loss: 2.9060
Iteration: 3105; Percent complete: 77.6%; Average loss: 2.7620
Iteration: 3106; Percent complete: 77.6%; Average loss: 3.0188
Iteration: 3107; Percent complete: 77.7%; Average loss: 2.6290
Iteration: 3108; Percent complete: 77.7%; Average loss: 2.7612
Iteration: 3109; Percent complete: 77.7%; Average loss: 2.8992
Iteration: 3110; Percent complete: 77.8%; Average loss: 3.1011
Iteration: 3111; Percent complete: 77.8%; Average loss: 2.8471
Iteration: 3112; Percent complete: 77.8%; Average loss: 2.8335
Iteration: 3113; Percent complete: 77.8%; Average loss: 2.9999
Iteration: 3114; Percent complete: 77.8%; Average loss: 3.1837
Iteration: 3115; Percent complete: 77.9%; Average loss: 2.7911
Iteration: 3116; Percent complete: 77.9%; Average loss: 2.8297
Iteration: 3117; Percent complete: 77.9%; Average loss: 2.8399
Iteration: 3118; Percent complete: 78.0%; Average loss: 2.7940
Iteration: 3119; Percent complete: 78.0%; Average loss: 2.6527
Iteration: 3120; Percent complete: 78.0%; Average loss: 2.9140
Iteration: 3121; Percent complete: 78.0%; Average loss: 2.6164
Iteration: 3122; Percent complete: 78.0%; Average loss: 3.0743
Iteration: 3123; Percent complete: 78.1%; Average loss: 2.6136
Iteration: 3124; Percent complete: 78.1%; Average loss: 2.7134
Iteration: 3125; Percent complete: 78.1%; Average loss: 2.7049
Iteration: 3126; Percent complete: 78.1%; Average loss: 2.8565
Iteration: 3127; Percent complete: 78.2%; Average loss: 2.9278
Iteration: 3128; Percent complete: 78.2%; Average loss: 2.7772
Iteration: 3129; Percent complete: 78.2%; Average loss: 2.8107
Iteration: 3130; Percent complete: 78.2%; Average loss: 2.8486
Iteration: 3131; Percent complete: 78.3%; Average loss: 2.7065
Iteration: 3132; Percent complete: 78.3%; Average loss: 2.6814
Iteration: 3133; Percent complete: 78.3%; Average loss: 2.8686
Iteration: 3134; Percent complete: 78.3%; Average loss: 3.1442
Iteration: 3135; Percent complete: 78.4%; Average loss: 2.8334
Iteration: 3136; Percent complete: 78.4%; Average loss: 2.7880
Iteration: 3137; Percent complete: 78.4%; Average loss: 2.9166
Iteration: 3138; Percent complete: 78.5%; Average loss: 2.7405
Iteration: 3139; Percent complete: 78.5%; Average loss: 3.0819
Iteration: 3140; Percent complete: 78.5%; Average loss: 2.9483
Iteration: 3141; Percent complete: 78.5%; Average loss: 2.7527
Iteration: 3142; Percent complete: 78.5%; Average loss: 2.8566
Iteration: 3143; Percent complete: 78.6%; Average loss: 2.8262
Iteration: 3144; Percent complete: 78.6%; Average loss: 2.8635
Iteration: 3145; Percent complete: 78.6%; Average loss: 2.7892
Iteration: 3146; Percent complete: 78.6%; Average loss: 2.7291
Iteration: 3147; Percent complete: 78.7%; Average loss: 2.9425
Iteration: 3148; Percent complete: 78.7%; Average loss: 2.6925
Iteration: 3149; Percent complete: 78.7%; Average loss: 2.6886
Iteration: 3150; Percent complete: 78.8%; Average loss: 2.8279
Iteration: 3151; Percent complete: 78.8%; Average loss: 2.7638
Iteration: 3152; Percent complete: 78.8%; Average loss: 2.7155
Iteration: 3153; Percent complete: 78.8%; Average loss: 2.6350
Iteration: 3154; Percent complete: 78.8%; Average loss: 3.0614
Iteration: 3155; Percent complete: 78.9%; Average loss: 2.8030
Iteration: 3156; Percent complete: 78.9%; Average loss: 2.8196
Iteration: 3157; Percent complete: 78.9%; Average loss: 2.8922
Iteration: 3158; Percent complete: 79.0%; Average loss: 2.8866
Iteration: 3159; Percent complete: 79.0%; Average loss: 2.8563
Iteration: 3160; Percent complete: 79.0%; Average loss: 2.7527
Iteration: 3161; Percent complete: 79.0%; Average loss: 2.8517
Iteration: 3162; Percent complete: 79.0%; Average loss: 2.5093
Iteration: 3163; Percent complete: 79.1%; Average loss: 3.0855
Iteration: 3164; Percent complete: 79.1%; Average loss: 2.8036
Iteration: 3165; Percent complete: 79.1%; Average loss: 3.0055
Iteration: 3166; Percent complete: 79.1%; Average loss: 2.7585
Iteration: 3167; Percent complete: 79.2%; Average loss: 2.9728
Iteration: 3168; Percent complete: 79.2%; Average loss: 2.8109
Iteration: 3169; Percent complete: 79.2%; Average loss: 2.6927
Iteration: 3170; Percent complete: 79.2%; Average loss: 2.9480
Iteration: 3171; Percent complete: 79.3%; Average loss: 2.8216
Iteration: 3172; Percent complete: 79.3%; Average loss: 2.7929
Iteration: 3173; Percent complete: 79.3%; Average loss: 2.9728
Iteration: 3174; Percent complete: 79.3%; Average loss: 2.7576
Iteration: 3175; Percent complete: 79.4%; Average loss: 2.8780
Iteration: 3176; Percent complete: 79.4%; Average loss: 2.8704
Iteration: 3177; Percent complete: 79.4%; Average loss: 2.7198
Iteration: 3178; Percent complete: 79.5%; Average loss: 3.0021
Iteration: 3179; Percent complete: 79.5%; Average loss: 2.7362
Iteration: 3180; Percent complete: 79.5%; Average loss: 2.8147
Iteration: 3181; Percent complete: 79.5%; Average loss: 2.6615
Iteration: 3182; Percent complete: 79.5%; Average loss: 2.7305
Iteration: 3183; Percent complete: 79.6%; Average loss: 2.7453
Iteration: 3184; Percent complete: 79.6%; Average loss: 2.8089
Iteration: 3185; Percent complete: 79.6%; Average loss: 2.9210
Iteration: 3186; Percent complete: 79.7%; Average loss: 2.5632
Iteration: 3187; Percent complete: 79.7%; Average loss: 2.9106
Iteration: 3188; Percent complete: 79.7%; Average loss: 3.0693
Iteration: 3189; Percent complete: 79.7%; Average loss: 2.7385
Iteration: 3190; Percent complete: 79.8%; Average loss: 3.0175
Iteration: 3191; Percent complete: 79.8%; Average loss: 2.9093
Iteration: 3192; Percent complete: 79.8%; Average loss: 2.8568
Iteration: 3193; Percent complete: 79.8%; Average loss: 2.7054
Iteration: 3194; Percent complete: 79.8%; Average loss: 2.9797
Iteration: 3195; Percent complete: 79.9%; Average loss: 2.9680
Iteration: 3196; Percent complete: 79.9%; Average loss: 2.8626
Iteration: 3197; Percent complete: 79.9%; Average loss: 2.7779
Iteration: 3198; Percent complete: 80.0%; Average loss: 3.0058
Iteration: 3199; Percent complete: 80.0%; Average loss: 2.7425
Iteration: 3200; Percent complete: 80.0%; Average loss: 2.8558
Iteration: 3201; Percent complete: 80.0%; Average loss: 3.0021
Iteration: 3202; Percent complete: 80.0%; Average loss: 2.8705
Iteration: 3203; Percent complete: 80.1%; Average loss: 2.8362
Iteration: 3204; Percent complete: 80.1%; Average loss: 2.8670
Iteration: 3205; Percent complete: 80.1%; Average loss: 2.6027
Iteration: 3206; Percent complete: 80.2%; Average loss: 2.7443
Iteration: 3207; Percent complete: 80.2%; Average loss: 2.8722
Iteration: 3208; Percent complete: 80.2%; Average loss: 3.0095
Iteration: 3209; Percent complete: 80.2%; Average loss: 3.0660
Iteration: 3210; Percent complete: 80.2%; Average loss: 2.9273
Iteration: 3211; Percent complete: 80.3%; Average loss: 2.8697
Iteration: 3212; Percent complete: 80.3%; Average loss: 2.9546
Iteration: 3213; Percent complete: 80.3%; Average loss: 2.8880
Iteration: 3214; Percent complete: 80.3%; Average loss: 2.8072
Iteration: 3215; Percent complete: 80.4%; Average loss: 2.9747
Iteration: 3216; Percent complete: 80.4%; Average loss: 2.7713
Iteration: 3217; Percent complete: 80.4%; Average loss: 2.8272
Iteration: 3218; Percent complete: 80.5%; Average loss: 2.8666
Iteration: 3219; Percent complete: 80.5%; Average loss: 2.6254
Iteration: 3220; Percent complete: 80.5%; Average loss: 2.8812
Iteration: 3221; Percent complete: 80.5%; Average loss: 2.9393
Iteration: 3222; Percent complete: 80.5%; Average loss: 2.8712
Iteration: 3223; Percent complete: 80.6%; Average loss: 2.7416
Iteration: 3224; Percent complete: 80.6%; Average loss: 2.6847
Iteration: 3225; Percent complete: 80.6%; Average loss: 2.8571
Iteration: 3226; Percent complete: 80.7%; Average loss: 2.6676
Iteration: 3227; Percent complete: 80.7%; Average loss: 2.7207
Iteration: 3228; Percent complete: 80.7%; Average loss: 2.8412
Iteration: 3229; Percent complete: 80.7%; Average loss: 2.7326
Iteration: 3230; Percent complete: 80.8%; Average loss: 2.7722
Iteration: 3231; Percent complete: 80.8%; Average loss: 2.6594
Iteration: 3232; Percent complete: 80.8%; Average loss: 2.7645
Iteration: 3233; Percent complete: 80.8%; Average loss: 2.6013
Iteration: 3234; Percent complete: 80.8%; Average loss: 2.7871
Iteration: 3235; Percent complete: 80.9%; Average loss: 2.9532
Iteration: 3236; Percent complete: 80.9%; Average loss: 2.7464
Iteration: 3237; Percent complete: 80.9%; Average loss: 2.7431
Iteration: 3238; Percent complete: 81.0%; Average loss: 2.6413
Iteration: 3239; Percent complete: 81.0%; Average loss: 2.8418
Iteration: 3240; Percent complete: 81.0%; Average loss: 2.6745
Iteration: 3241; Percent complete: 81.0%; Average loss: 2.7505
Iteration: 3242; Percent complete: 81.0%; Average loss: 2.7462
Iteration: 3243; Percent complete: 81.1%; Average loss: 2.7887
Iteration: 3244; Percent complete: 81.1%; Average loss: 2.6753
Iteration: 3245; Percent complete: 81.1%; Average loss: 2.7981
Iteration: 3246; Percent complete: 81.2%; Average loss: 2.9032
Iteration: 3247; Percent complete: 81.2%; Average loss: 2.7923
Iteration: 3248; Percent complete: 81.2%; Average loss: 2.5653
Iteration: 3249; Percent complete: 81.2%; Average loss: 2.5290
Iteration: 3250; Percent complete: 81.2%; Average loss: 2.6837
Iteration: 3251; Percent complete: 81.3%; Average loss: 2.8940
Iteration: 3252; Percent complete: 81.3%; Average loss: 2.6113
Iteration: 3253; Percent complete: 81.3%; Average loss: 2.9735
Iteration: 3254; Percent complete: 81.3%; Average loss: 2.8283
Iteration: 3255; Percent complete: 81.4%; Average loss: 2.9087
Iteration: 3256; Percent complete: 81.4%; Average loss: 2.8201
Iteration: 3257; Percent complete: 81.4%; Average loss: 2.8443
Iteration: 3258; Percent complete: 81.5%; Average loss: 2.8271
Iteration: 3259; Percent complete: 81.5%; Average loss: 2.8334
Iteration: 3260; Percent complete: 81.5%; Average loss: 2.8674
Iteration: 3261; Percent complete: 81.5%; Average loss: 2.7372
Iteration: 3262; Percent complete: 81.5%; Average loss: 2.9765
Iteration: 3263; Percent complete: 81.6%; Average loss: 2.9380
Iteration: 3264; Percent complete: 81.6%; Average loss: 2.5100
Iteration: 3265; Percent complete: 81.6%; Average loss: 2.6849
Iteration: 3266; Percent complete: 81.7%; Average loss: 2.6896
Iteration: 3267; Percent complete: 81.7%; Average loss: 2.8358
Iteration: 3268; Percent complete: 81.7%; Average loss: 2.6404
Iteration: 3269; Percent complete: 81.7%; Average loss: 2.6790
Iteration: 3270; Percent complete: 81.8%; Average loss: 2.8264
Iteration: 3271; Percent complete: 81.8%; Average loss: 2.7410
Iteration: 3272; Percent complete: 81.8%; Average loss: 2.5339
Iteration: 3273; Percent complete: 81.8%; Average loss: 2.7553
Iteration: 3274; Percent complete: 81.8%; Average loss: 3.1105
Iteration: 3275; Percent complete: 81.9%; Average loss: 2.7710
Iteration: 3276; Percent complete: 81.9%; Average loss: 2.7339
Iteration: 3277; Percent complete: 81.9%; Average loss: 2.6729
Iteration: 3278; Percent complete: 82.0%; Average loss: 2.6078
Iteration: 3279; Percent complete: 82.0%; Average loss: 2.9693
Iteration: 3280; Percent complete: 82.0%; Average loss: 3.0510
Iteration: 3281; Percent complete: 82.0%; Average loss: 3.0771
Iteration: 3282; Percent complete: 82.0%; Average loss: 2.5780
Iteration: 3283; Percent complete: 82.1%; Average loss: 2.8725
Iteration: 3284; Percent complete: 82.1%; Average loss: 2.6944
Iteration: 3285; Percent complete: 82.1%; Average loss: 2.6317
Iteration: 3286; Percent complete: 82.2%; Average loss: 2.7456
Iteration: 3287; Percent complete: 82.2%; Average loss: 2.7917
Iteration: 3288; Percent complete: 82.2%; Average loss: 2.6939
Iteration: 3289; Percent complete: 82.2%; Average loss: 2.7856
Iteration: 3290; Percent complete: 82.2%; Average loss: 2.9281
Iteration: 3291; Percent complete: 82.3%; Average loss: 2.8005
Iteration: 3292; Percent complete: 82.3%; Average loss: 2.7644
Iteration: 3293; Percent complete: 82.3%; Average loss: 2.7684
Iteration: 3294; Percent complete: 82.3%; Average loss: 2.8228
Iteration: 3295; Percent complete: 82.4%; Average loss: 2.8956
Iteration: 3296; Percent complete: 82.4%; Average loss: 2.7739
Iteration: 3297; Percent complete: 82.4%; Average loss: 2.7526
Iteration: 3298; Percent complete: 82.5%; Average loss: 2.6945
Iteration: 3299; Percent complete: 82.5%; Average loss: 2.8824
Iteration: 3300; Percent complete: 82.5%; Average loss: 2.8571
Iteration: 3301; Percent complete: 82.5%; Average loss: 2.6536
Iteration: 3302; Percent complete: 82.5%; Average loss: 2.9658
Iteration: 3303; Percent complete: 82.6%; Average loss: 2.9487
Iteration: 3304; Percent complete: 82.6%; Average loss: 2.9744
Iteration: 3305; Percent complete: 82.6%; Average loss: 2.7595
Iteration: 3306; Percent complete: 82.7%; Average loss: 2.8810
Iteration: 3307; Percent complete: 82.7%; Average loss: 2.6865
Iteration: 3308; Percent complete: 82.7%; Average loss: 2.7194
Iteration: 3309; Percent complete: 82.7%; Average loss: 2.7930
Iteration: 3310; Percent complete: 82.8%; Average loss: 2.7958
Iteration: 3311; Percent complete: 82.8%; Average loss: 2.8537
Iteration: 3312; Percent complete: 82.8%; Average loss: 2.7709
Iteration: 3313; Percent complete: 82.8%; Average loss: 2.8710
Iteration: 3314; Percent complete: 82.8%; Average loss: 2.5551
Iteration: 3315; Percent complete: 82.9%; Average loss: 2.7790
Iteration: 3316; Percent complete: 82.9%; Average loss: 2.6726
Iteration: 3317; Percent complete: 82.9%; Average loss: 2.7081
Iteration: 3318; Percent complete: 83.0%; Average loss: 2.9245
Iteration: 3319; Percent complete: 83.0%; Average loss: 2.7310
Iteration: 3320; Percent complete: 83.0%; Average loss: 2.6148
Iteration: 3321; Percent complete: 83.0%; Average loss: 2.7530
Iteration: 3322; Percent complete: 83.0%; Average loss: 2.9128
Iteration: 3323; Percent complete: 83.1%; Average loss: 2.7109
Iteration: 3324; Percent complete: 83.1%; Average loss: 2.9249
Iteration: 3325; Percent complete: 83.1%; Average loss: 2.7082
Iteration: 3326; Percent complete: 83.2%; Average loss: 2.8828
Iteration: 3327; Percent complete: 83.2%; Average loss: 2.6030
Iteration: 3328; Percent complete: 83.2%; Average loss: 2.6886
Iteration: 3329; Percent complete: 83.2%; Average loss: 2.6496
Iteration: 3330; Percent complete: 83.2%; Average loss: 2.7618
Iteration: 3331; Percent complete: 83.3%; Average loss: 2.5974
Iteration: 3332; Percent complete: 83.3%; Average loss: 2.6716
Iteration: 3333; Percent complete: 83.3%; Average loss: 2.7390
Iteration: 3334; Percent complete: 83.4%; Average loss: 2.9533
Iteration: 3335; Percent complete: 83.4%; Average loss: 2.9526
Iteration: 3336; Percent complete: 83.4%; Average loss: 2.9923
Iteration: 3337; Percent complete: 83.4%; Average loss: 2.5338
Iteration: 3338; Percent complete: 83.5%; Average loss: 2.8745
Iteration: 3339; Percent complete: 83.5%; Average loss: 2.9527
Iteration: 3340; Percent complete: 83.5%; Average loss: 2.5320
Iteration: 3341; Percent complete: 83.5%; Average loss: 2.9352
Iteration: 3342; Percent complete: 83.5%; Average loss: 2.9315
Iteration: 3343; Percent complete: 83.6%; Average loss: 2.6279
Iteration: 3344; Percent complete: 83.6%; Average loss: 2.7975
Iteration: 3345; Percent complete: 83.6%; Average loss: 2.8075
Iteration: 3346; Percent complete: 83.7%; Average loss: 2.8766
Iteration: 3347; Percent complete: 83.7%; Average loss: 2.7700
Iteration: 3348; Percent complete: 83.7%; Average loss: 2.6275
Iteration: 3349; Percent complete: 83.7%; Average loss: 2.8007
Iteration: 3350; Percent complete: 83.8%; Average loss: 2.7294
Iteration: 3351; Percent complete: 83.8%; Average loss: 2.8822
Iteration: 3352; Percent complete: 83.8%; Average loss: 2.6167
Iteration: 3353; Percent complete: 83.8%; Average loss: 2.6327
Iteration: 3354; Percent complete: 83.9%; Average loss: 2.8003
Iteration: 3355; Percent complete: 83.9%; Average loss: 2.7561
Iteration: 3356; Percent complete: 83.9%; Average loss: 2.7685
Iteration: 3357; Percent complete: 83.9%; Average loss: 2.5949
Iteration: 3358; Percent complete: 84.0%; Average loss: 2.7598
Iteration: 3359; Percent complete: 84.0%; Average loss: 2.8619
Iteration: 3360; Percent complete: 84.0%; Average loss: 2.6904
Iteration: 3361; Percent complete: 84.0%; Average loss: 2.4538
Iteration: 3362; Percent complete: 84.0%; Average loss: 2.9059
Iteration: 3363; Percent complete: 84.1%; Average loss: 2.6084
Iteration: 3364; Percent complete: 84.1%; Average loss: 2.8299
Iteration: 3365; Percent complete: 84.1%; Average loss: 2.7287
Iteration: 3366; Percent complete: 84.2%; Average loss: 2.9264
Iteration: 3367; Percent complete: 84.2%; Average loss: 2.7792
Iteration: 3368; Percent complete: 84.2%; Average loss: 2.9716
Iteration: 3369; Percent complete: 84.2%; Average loss: 2.7664
Iteration: 3370; Percent complete: 84.2%; Average loss: 2.6142
Iteration: 3371; Percent complete: 84.3%; Average loss: 2.9256
Iteration: 3372; Percent complete: 84.3%; Average loss: 2.7091
Iteration: 3373; Percent complete: 84.3%; Average loss: 2.8418
Iteration: 3374; Percent complete: 84.4%; Average loss: 2.5683
Iteration: 3375; Percent complete: 84.4%; Average loss: 2.8068
Iteration: 3376; Percent complete: 84.4%; Average loss: 2.4184
Iteration: 3377; Percent complete: 84.4%; Average loss: 2.6956
Iteration: 3378; Percent complete: 84.5%; Average loss: 2.7040
Iteration: 3379; Percent complete: 84.5%; Average loss: 2.7223
Iteration: 3380; Percent complete: 84.5%; Average loss: 2.8612
Iteration: 3381; Percent complete: 84.5%; Average loss: 2.6737
Iteration: 3382; Percent complete: 84.5%; Average loss: 2.9890
Iteration: 3383; Percent complete: 84.6%; Average loss: 2.9207
Iteration: 3384; Percent complete: 84.6%; Average loss: 2.9666
Iteration: 3385; Percent complete: 84.6%; Average loss: 2.5636
Iteration: 3386; Percent complete: 84.7%; Average loss: 2.8154
Iteration: 3387; Percent complete: 84.7%; Average loss: 2.7641
Iteration: 3388; Percent complete: 84.7%; Average loss: 2.7416
Iteration: 3389; Percent complete: 84.7%; Average loss: 2.6718
Iteration: 3390; Percent complete: 84.8%; Average loss: 2.9008
Iteration: 3391; Percent complete: 84.8%; Average loss: 2.7515
Iteration: 3392; Percent complete: 84.8%; Average loss: 2.6745
Iteration: 3393; Percent complete: 84.8%; Average loss: 3.0190
Iteration: 3394; Percent complete: 84.9%; Average loss: 2.7931
Iteration: 3395; Percent complete: 84.9%; Average loss: 2.8146
Iteration: 3396; Percent complete: 84.9%; Average loss: 2.6754
Iteration: 3397; Percent complete: 84.9%; Average loss: 2.6769
Iteration: 3398; Percent complete: 85.0%; Average loss: 2.6979
Iteration: 3399; Percent complete: 85.0%; Average loss: 2.8452
Iteration: 3400; Percent complete: 85.0%; Average loss: 2.7310
Iteration: 3401; Percent complete: 85.0%; Average loss: 2.8965
Iteration: 3402; Percent complete: 85.0%; Average loss: 3.0748
Iteration: 3403; Percent complete: 85.1%; Average loss: 2.8856
Iteration: 3404; Percent complete: 85.1%; Average loss: 2.5991
Iteration: 3405; Percent complete: 85.1%; Average loss: 2.6787
Iteration: 3406; Percent complete: 85.2%; Average loss: 2.6551
Iteration: 3407; Percent complete: 85.2%; Average loss: 2.7821
Iteration: 3408; Percent complete: 85.2%; Average loss: 2.8974
Iteration: 3409; Percent complete: 85.2%; Average loss: 2.9564
Iteration: 3410; Percent complete: 85.2%; Average loss: 2.6635
Iteration: 3411; Percent complete: 85.3%; Average loss: 2.7594
Iteration: 3412; Percent complete: 85.3%; Average loss: 2.6535
Iteration: 3413; Percent complete: 85.3%; Average loss: 2.8085
Iteration: 3414; Percent complete: 85.4%; Average loss: 2.9373
Iteration: 3415; Percent complete: 85.4%; Average loss: 2.6269
Iteration: 3416; Percent complete: 85.4%; Average loss: 2.8624
Iteration: 3417; Percent complete: 85.4%; Average loss: 2.9595
Iteration: 3418; Percent complete: 85.5%; Average loss: 2.5837
Iteration: 3419; Percent complete: 85.5%; Average loss: 2.7707
Iteration: 3420; Percent complete: 85.5%; Average loss: 2.8404
Iteration: 3421; Percent complete: 85.5%; Average loss: 2.7980
Iteration: 3422; Percent complete: 85.5%; Average loss: 3.0416
Iteration: 3423; Percent complete: 85.6%; Average loss: 2.7625
Iteration: 3424; Percent complete: 85.6%; Average loss: 2.8074
Iteration: 3425; Percent complete: 85.6%; Average loss: 2.7947
Iteration: 3426; Percent complete: 85.7%; Average loss: 2.7064
Iteration: 3427; Percent complete: 85.7%; Average loss: 3.0650
Iteration: 3428; Percent complete: 85.7%; Average loss: 2.8685
Iteration: 3429; Percent complete: 85.7%; Average loss: 2.8462
Iteration: 3430; Percent complete: 85.8%; Average loss: 2.7492
Iteration: 3431; Percent complete: 85.8%; Average loss: 2.7039
Iteration: 3432; Percent complete: 85.8%; Average loss: 2.8781
Iteration: 3433; Percent complete: 85.8%; Average loss: 2.6406
Iteration: 3434; Percent complete: 85.9%; Average loss: 2.8070
Iteration: 3435; Percent complete: 85.9%; Average loss: 2.5837
Iteration: 3436; Percent complete: 85.9%; Average loss: 2.9474
Iteration: 3437; Percent complete: 85.9%; Average loss: 2.7634
Iteration: 3438; Percent complete: 86.0%; Average loss: 2.8880
Iteration: 3439; Percent complete: 86.0%; Average loss: 2.8609
Iteration: 3440; Percent complete: 86.0%; Average loss: 2.7405
Iteration: 3441; Percent complete: 86.0%; Average loss: 2.6488
Iteration: 3442; Percent complete: 86.1%; Average loss: 2.8511
Iteration: 3443; Percent complete: 86.1%; Average loss: 2.6294
Iteration: 3444; Percent complete: 86.1%; Average loss: 2.6564
Iteration: 3445; Percent complete: 86.1%; Average loss: 2.6365
Iteration: 3446; Percent complete: 86.2%; Average loss: 2.5888
Iteration: 3447; Percent complete: 86.2%; Average loss: 2.7230
Iteration: 3448; Percent complete: 86.2%; Average loss: 2.4821
Iteration: 3449; Percent complete: 86.2%; Average loss: 3.0519
Iteration: 3450; Percent complete: 86.2%; Average loss: 2.8094
Iteration: 3451; Percent complete: 86.3%; Average loss: 2.8395
Iteration: 3452; Percent complete: 86.3%; Average loss: 2.8834
Iteration: 3453; Percent complete: 86.3%; Average loss: 2.8109
Iteration: 3454; Percent complete: 86.4%; Average loss: 2.8134
Iteration: 3455; Percent complete: 86.4%; Average loss: 2.7765
Iteration: 3456; Percent complete: 86.4%; Average loss: 2.5413
Iteration: 3457; Percent complete: 86.4%; Average loss: 2.8004
Iteration: 3458; Percent complete: 86.5%; Average loss: 2.6987
Iteration: 3459; Percent complete: 86.5%; Average loss: 3.0946
Iteration: 3460; Percent complete: 86.5%; Average loss: 2.8857
Iteration: 3461; Percent complete: 86.5%; Average loss: 2.5598
Iteration: 3462; Percent complete: 86.6%; Average loss: 2.8451
Iteration: 3463; Percent complete: 86.6%; Average loss: 2.8108
Iteration: 3464; Percent complete: 86.6%; Average loss: 2.6672
Iteration: 3465; Percent complete: 86.6%; Average loss: 2.6177
Iteration: 3466; Percent complete: 86.7%; Average loss: 2.6013
Iteration: 3467; Percent complete: 86.7%; Average loss: 2.7564
Iteration: 3468; Percent complete: 86.7%; Average loss: 2.6710
Iteration: 3469; Percent complete: 86.7%; Average loss: 2.7328
Iteration: 3470; Percent complete: 86.8%; Average loss: 2.7745
Iteration: 3471; Percent complete: 86.8%; Average loss: 2.5268
Iteration: 3472; Percent complete: 86.8%; Average loss: 2.6877
Iteration: 3473; Percent complete: 86.8%; Average loss: 2.7586
Iteration: 3474; Percent complete: 86.9%; Average loss: 2.9320
Iteration: 3475; Percent complete: 86.9%; Average loss: 2.7742
Iteration: 3476; Percent complete: 86.9%; Average loss: 2.7361
Iteration: 3477; Percent complete: 86.9%; Average loss: 2.6517
Iteration: 3478; Percent complete: 87.0%; Average loss: 2.6035
Iteration: 3479; Percent complete: 87.0%; Average loss: 2.8535
Iteration: 3480; Percent complete: 87.0%; Average loss: 2.7606
Iteration: 3481; Percent complete: 87.0%; Average loss: 2.7198
Iteration: 3482; Percent complete: 87.1%; Average loss: 2.6336
Iteration: 3483; Percent complete: 87.1%; Average loss: 2.5265
Iteration: 3484; Percent complete: 87.1%; Average loss: 2.8110
Iteration: 3485; Percent complete: 87.1%; Average loss: 3.0392
Iteration: 3486; Percent complete: 87.2%; Average loss: 2.7315
Iteration: 3487; Percent complete: 87.2%; Average loss: 2.8930
Iteration: 3488; Percent complete: 87.2%; Average loss: 2.7499
Iteration: 3489; Percent complete: 87.2%; Average loss: 2.9192
Iteration: 3490; Percent complete: 87.2%; Average loss: 2.4131
Iteration: 3491; Percent complete: 87.3%; Average loss: 2.8195
Iteration: 3492; Percent complete: 87.3%; Average loss: 2.6261
Iteration: 3493; Percent complete: 87.3%; Average loss: 2.6877
Iteration: 3494; Percent complete: 87.4%; Average loss: 2.9134
Iteration: 3495; Percent complete: 87.4%; Average loss: 3.0331
Iteration: 3496; Percent complete: 87.4%; Average loss: 2.8177
Iteration: 3497; Percent complete: 87.4%; Average loss: 2.7524
Iteration: 3498; Percent complete: 87.5%; Average loss: 2.8604
Iteration: 3499; Percent complete: 87.5%; Average loss: 2.7868
Iteration: 3500; Percent complete: 87.5%; Average loss: 2.8101
Iteration: 3501; Percent complete: 87.5%; Average loss: 2.6919
Iteration: 3502; Percent complete: 87.5%; Average loss: 2.6421
Iteration: 3503; Percent complete: 87.6%; Average loss: 2.7526
Iteration: 3504; Percent complete: 87.6%; Average loss: 2.8567
Iteration: 3505; Percent complete: 87.6%; Average loss: 2.8137
Iteration: 3506; Percent complete: 87.6%; Average loss: 2.7189
Iteration: 3507; Percent complete: 87.7%; Average loss: 2.8170
Iteration: 3508; Percent complete: 87.7%; Average loss: 2.6677
Iteration: 3509; Percent complete: 87.7%; Average loss: 2.7455
Iteration: 3510; Percent complete: 87.8%; Average loss: 2.6448
Iteration: 3511; Percent complete: 87.8%; Average loss: 2.7990
Iteration: 3512; Percent complete: 87.8%; Average loss: 2.7411
Iteration: 3513; Percent complete: 87.8%; Average loss: 2.8946
Iteration: 3514; Percent complete: 87.8%; Average loss: 2.5619
Iteration: 3515; Percent complete: 87.9%; Average loss: 2.7954
Iteration: 3516; Percent complete: 87.9%; Average loss: 2.7614
Iteration: 3517; Percent complete: 87.9%; Average loss: 2.8210
Iteration: 3518; Percent complete: 87.9%; Average loss: 2.5165
Iteration: 3519; Percent complete: 88.0%; Average loss: 2.9331
Iteration: 3520; Percent complete: 88.0%; Average loss: 2.8895
Iteration: 3521; Percent complete: 88.0%; Average loss: 2.6333
Iteration: 3522; Percent complete: 88.0%; Average loss: 2.7865
Iteration: 3523; Percent complete: 88.1%; Average loss: 2.8571
Iteration: 3524; Percent complete: 88.1%; Average loss: 2.7569
Iteration: 3525; Percent complete: 88.1%; Average loss: 2.7661
Iteration: 3526; Percent complete: 88.1%; Average loss: 2.8462
Iteration: 3527; Percent complete: 88.2%; Average loss: 2.9266
Iteration: 3528; Percent complete: 88.2%; Average loss: 2.5955
Iteration: 3529; Percent complete: 88.2%; Average loss: 2.7975
Iteration: 3530; Percent complete: 88.2%; Average loss: 2.8836
Iteration: 3531; Percent complete: 88.3%; Average loss: 2.7054
Iteration: 3532; Percent complete: 88.3%; Average loss: 2.7210
Iteration: 3533; Percent complete: 88.3%; Average loss: 2.6846
Iteration: 3534; Percent complete: 88.3%; Average loss: 2.6646
Iteration: 3535; Percent complete: 88.4%; Average loss: 2.8414
Iteration: 3536; Percent complete: 88.4%; Average loss: 2.6424
Iteration: 3537; Percent complete: 88.4%; Average loss: 2.8802
Iteration: 3538; Percent complete: 88.4%; Average loss: 2.6915
Iteration: 3539; Percent complete: 88.5%; Average loss: 2.8430
Iteration: 3540; Percent complete: 88.5%; Average loss: 2.9332
Iteration: 3541; Percent complete: 88.5%; Average loss: 2.8082
Iteration: 3542; Percent complete: 88.5%; Average loss: 2.7314
Iteration: 3543; Percent complete: 88.6%; Average loss: 2.5753
Iteration: 3544; Percent complete: 88.6%; Average loss: 2.9192
Iteration: 3545; Percent complete: 88.6%; Average loss: 2.6688
Iteration: 3546; Percent complete: 88.6%; Average loss: 2.6843
Iteration: 3547; Percent complete: 88.7%; Average loss: 2.8122
Iteration: 3548; Percent complete: 88.7%; Average loss: 2.6978
Iteration: 3549; Percent complete: 88.7%; Average loss: 2.7366
Iteration: 3550; Percent complete: 88.8%; Average loss: 2.8988
Iteration: 3551; Percent complete: 88.8%; Average loss: 2.6629
Iteration: 3552; Percent complete: 88.8%; Average loss: 2.8440
Iteration: 3553; Percent complete: 88.8%; Average loss: 2.7957
Iteration: 3554; Percent complete: 88.8%; Average loss: 3.0217
Iteration: 3555; Percent complete: 88.9%; Average loss: 2.7490
Iteration: 3556; Percent complete: 88.9%; Average loss: 2.8884
Iteration: 3557; Percent complete: 88.9%; Average loss: 2.9393
Iteration: 3558; Percent complete: 88.9%; Average loss: 2.8683
Iteration: 3559; Percent complete: 89.0%; Average loss: 2.9548
Iteration: 3560; Percent complete: 89.0%; Average loss: 2.6712
Iteration: 3561; Percent complete: 89.0%; Average loss: 2.6861
Iteration: 3562; Percent complete: 89.0%; Average loss: 2.4478
Iteration: 3563; Percent complete: 89.1%; Average loss: 2.7216
Iteration: 3564; Percent complete: 89.1%; Average loss: 2.7814
Iteration: 3565; Percent complete: 89.1%; Average loss: 2.8691
Iteration: 3566; Percent complete: 89.1%; Average loss: 2.7201
Iteration: 3567; Percent complete: 89.2%; Average loss: 2.8213
Iteration: 3568; Percent complete: 89.2%; Average loss: 2.8969
Iteration: 3569; Percent complete: 89.2%; Average loss: 2.7880
Iteration: 3570; Percent complete: 89.2%; Average loss: 2.8587
Iteration: 3571; Percent complete: 89.3%; Average loss: 2.7468
Iteration: 3572; Percent complete: 89.3%; Average loss: 2.7664
Iteration: 3573; Percent complete: 89.3%; Average loss: 2.6145
Iteration: 3574; Percent complete: 89.3%; Average loss: 2.7998
Iteration: 3575; Percent complete: 89.4%; Average loss: 2.9450
Iteration: 3576; Percent complete: 89.4%; Average loss: 2.7980
Iteration: 3577; Percent complete: 89.4%; Average loss: 2.8749
Iteration: 3578; Percent complete: 89.5%; Average loss: 2.6039
Iteration: 3579; Percent complete: 89.5%; Average loss: 2.7327
Iteration: 3580; Percent complete: 89.5%; Average loss: 2.7637
Iteration: 3581; Percent complete: 89.5%; Average loss: 2.5677
Iteration: 3582; Percent complete: 89.5%; Average loss: 2.6359
Iteration: 3583; Percent complete: 89.6%; Average loss: 2.8550
Iteration: 3584; Percent complete: 89.6%; Average loss: 2.7408
Iteration: 3585; Percent complete: 89.6%; Average loss: 2.6209
Iteration: 3586; Percent complete: 89.6%; Average loss: 2.7148
Iteration: 3587; Percent complete: 89.7%; Average loss: 2.8705
Iteration: 3588; Percent complete: 89.7%; Average loss: 2.9079
Iteration: 3589; Percent complete: 89.7%; Average loss: 2.8261
Iteration: 3590; Percent complete: 89.8%; Average loss: 2.7317
Iteration: 3591; Percent complete: 89.8%; Average loss: 2.5852
Iteration: 3592; Percent complete: 89.8%; Average loss: 2.5261
Iteration: 3593; Percent complete: 89.8%; Average loss: 2.8429
Iteration: 3594; Percent complete: 89.8%; Average loss: 2.8071
Iteration: 3595; Percent complete: 89.9%; Average loss: 2.5704
Iteration: 3596; Percent complete: 89.9%; Average loss: 2.6715
Iteration: 3597; Percent complete: 89.9%; Average loss: 2.7706
Iteration: 3598; Percent complete: 90.0%; Average loss: 2.7544
Iteration: 3599; Percent complete: 90.0%; Average loss: 2.6661
Iteration: 3600; Percent complete: 90.0%; Average loss: 2.7660
Iteration: 3601; Percent complete: 90.0%; Average loss: 2.6500
Iteration: 3602; Percent complete: 90.0%; Average loss: 2.6437
Iteration: 3603; Percent complete: 90.1%; Average loss: 2.6360
Iteration: 3604; Percent complete: 90.1%; Average loss: 2.7389
Iteration: 3605; Percent complete: 90.1%; Average loss: 2.8445
Iteration: 3606; Percent complete: 90.1%; Average loss: 2.6361
Iteration: 3607; Percent complete: 90.2%; Average loss: 2.7655
Iteration: 3608; Percent complete: 90.2%; Average loss: 2.5592
Iteration: 3609; Percent complete: 90.2%; Average loss: 2.7872
Iteration: 3610; Percent complete: 90.2%; Average loss: 2.5624
Iteration: 3611; Percent complete: 90.3%; Average loss: 2.7962
Iteration: 3612; Percent complete: 90.3%; Average loss: 2.7204
Iteration: 3613; Percent complete: 90.3%; Average loss: 2.7911
Iteration: 3614; Percent complete: 90.3%; Average loss: 2.6070
Iteration: 3615; Percent complete: 90.4%; Average loss: 2.7184
Iteration: 3616; Percent complete: 90.4%; Average loss: 2.7040
Iteration: 3617; Percent complete: 90.4%; Average loss: 2.6034
Iteration: 3618; Percent complete: 90.5%; Average loss: 2.5909
Iteration: 3619; Percent complete: 90.5%; Average loss: 2.7891
Iteration: 3620; Percent complete: 90.5%; Average loss: 2.7941
Iteration: 3621; Percent complete: 90.5%; Average loss: 2.7387
Iteration: 3622; Percent complete: 90.5%; Average loss: 2.8106
Iteration: 3623; Percent complete: 90.6%; Average loss: 2.8161
Iteration: 3624; Percent complete: 90.6%; Average loss: 2.6772
Iteration: 3625; Percent complete: 90.6%; Average loss: 2.8693
Iteration: 3626; Percent complete: 90.6%; Average loss: 2.6609
Iteration: 3627; Percent complete: 90.7%; Average loss: 2.2583
Iteration: 3628; Percent complete: 90.7%; Average loss: 2.5550
Iteration: 3629; Percent complete: 90.7%; Average loss: 2.6270
Iteration: 3630; Percent complete: 90.8%; Average loss: 2.6131
Iteration: 3631; Percent complete: 90.8%; Average loss: 2.7411
Iteration: 3632; Percent complete: 90.8%; Average loss: 2.8756
Iteration: 3633; Percent complete: 90.8%; Average loss: 2.7440
Iteration: 3634; Percent complete: 90.8%; Average loss: 2.6204
Iteration: 3635; Percent complete: 90.9%; Average loss: 2.7481
Iteration: 3636; Percent complete: 90.9%; Average loss: 2.4324
Iteration: 3637; Percent complete: 90.9%; Average loss: 2.6290
Iteration: 3638; Percent complete: 91.0%; Average loss: 2.5657
Iteration: 3639; Percent complete: 91.0%; Average loss: 2.7277
Iteration: 3640; Percent complete: 91.0%; Average loss: 2.8937
Iteration: 3641; Percent complete: 91.0%; Average loss: 2.3627
Iteration: 3642; Percent complete: 91.0%; Average loss: 2.7155
Iteration: 3643; Percent complete: 91.1%; Average loss: 2.7697
Iteration: 3644; Percent complete: 91.1%; Average loss: 2.8136
Iteration: 3645; Percent complete: 91.1%; Average loss: 2.8715
Iteration: 3646; Percent complete: 91.1%; Average loss: 2.6719
Iteration: 3647; Percent complete: 91.2%; Average loss: 2.6676
Iteration: 3648; Percent complete: 91.2%; Average loss: 2.5993
Iteration: 3649; Percent complete: 91.2%; Average loss: 2.7738
Iteration: 3650; Percent complete: 91.2%; Average loss: 2.8756
Iteration: 3651; Percent complete: 91.3%; Average loss: 2.8535
Iteration: 3652; Percent complete: 91.3%; Average loss: 2.7332
Iteration: 3653; Percent complete: 91.3%; Average loss: 2.9763
Iteration: 3654; Percent complete: 91.3%; Average loss: 2.6252
Iteration: 3655; Percent complete: 91.4%; Average loss: 2.6605
Iteration: 3656; Percent complete: 91.4%; Average loss: 2.6673
Iteration: 3657; Percent complete: 91.4%; Average loss: 2.7444
Iteration: 3658; Percent complete: 91.5%; Average loss: 2.5654
Iteration: 3659; Percent complete: 91.5%; Average loss: 2.3670
Iteration: 3660; Percent complete: 91.5%; Average loss: 2.9667
Iteration: 3661; Percent complete: 91.5%; Average loss: 2.7091
Iteration: 3662; Percent complete: 91.5%; Average loss: 2.6378
Iteration: 3663; Percent complete: 91.6%; Average loss: 2.5715
Iteration: 3664; Percent complete: 91.6%; Average loss: 2.4035
Iteration: 3665; Percent complete: 91.6%; Average loss: 2.6691
Iteration: 3666; Percent complete: 91.6%; Average loss: 2.8368
Iteration: 3667; Percent complete: 91.7%; Average loss: 2.5952
Iteration: 3668; Percent complete: 91.7%; Average loss: 3.0498
Iteration: 3669; Percent complete: 91.7%; Average loss: 2.6671
Iteration: 3670; Percent complete: 91.8%; Average loss: 2.7720
Iteration: 3671; Percent complete: 91.8%; Average loss: 2.7269
Iteration: 3672; Percent complete: 91.8%; Average loss: 2.8617
Iteration: 3673; Percent complete: 91.8%; Average loss: 2.6387
Iteration: 3674; Percent complete: 91.8%; Average loss: 2.7025
Iteration: 3675; Percent complete: 91.9%; Average loss: 2.7508
Iteration: 3676; Percent complete: 91.9%; Average loss: 2.7247
Iteration: 3677; Percent complete: 91.9%; Average loss: 3.0049
Iteration: 3678; Percent complete: 92.0%; Average loss: 2.6115
Iteration: 3679; Percent complete: 92.0%; Average loss: 2.7385
Iteration: 3680; Percent complete: 92.0%; Average loss: 2.5090
Iteration: 3681; Percent complete: 92.0%; Average loss: 2.7184
Iteration: 3682; Percent complete: 92.0%; Average loss: 2.5811
Iteration: 3683; Percent complete: 92.1%; Average loss: 2.5861
Iteration: 3684; Percent complete: 92.1%; Average loss: 2.6464
Iteration: 3685; Percent complete: 92.1%; Average loss: 2.4681
Iteration: 3686; Percent complete: 92.2%; Average loss: 2.7413
Iteration: 3687; Percent complete: 92.2%; Average loss: 2.6202
Iteration: 3688; Percent complete: 92.2%; Average loss: 2.6965
Iteration: 3689; Percent complete: 92.2%; Average loss: 2.5116
Iteration: 3690; Percent complete: 92.2%; Average loss: 2.7124
Iteration: 3691; Percent complete: 92.3%; Average loss: 2.8565
Iteration: 3692; Percent complete: 92.3%; Average loss: 2.8379
Iteration: 3693; Percent complete: 92.3%; Average loss: 2.7236
Iteration: 3694; Percent complete: 92.3%; Average loss: 2.9059
Iteration: 3695; Percent complete: 92.4%; Average loss: 2.6912
Iteration: 3696; Percent complete: 92.4%; Average loss: 2.7722
Iteration: 3697; Percent complete: 92.4%; Average loss: 2.7468
Iteration: 3698; Percent complete: 92.5%; Average loss: 2.7573
Iteration: 3699; Percent complete: 92.5%; Average loss: 2.5251
Iteration: 3700; Percent complete: 92.5%; Average loss: 2.5587
Iteration: 3701; Percent complete: 92.5%; Average loss: 2.8658
Iteration: 3702; Percent complete: 92.5%; Average loss: 2.7020
Iteration: 3703; Percent complete: 92.6%; Average loss: 2.6218
Iteration: 3704; Percent complete: 92.6%; Average loss: 2.7208
Iteration: 3705; Percent complete: 92.6%; Average loss: 2.5934
Iteration: 3706; Percent complete: 92.7%; Average loss: 2.8950
Iteration: 3707; Percent complete: 92.7%; Average loss: 2.4966
Iteration: 3708; Percent complete: 92.7%; Average loss: 2.5085
Iteration: 3709; Percent complete: 92.7%; Average loss: 2.7147
Iteration: 3710; Percent complete: 92.8%; Average loss: 2.5579
Iteration: 3711; Percent complete: 92.8%; Average loss: 2.8422
Iteration: 3712; Percent complete: 92.8%; Average loss: 2.6113
Iteration: 3713; Percent complete: 92.8%; Average loss: 2.6717
Iteration: 3714; Percent complete: 92.8%; Average loss: 2.5341
Iteration: 3715; Percent complete: 92.9%; Average loss: 2.6218
Iteration: 3716; Percent complete: 92.9%; Average loss: 2.6519
Iteration: 3717; Percent complete: 92.9%; Average loss: 2.4849
Iteration: 3718; Percent complete: 93.0%; Average loss: 2.6524
Iteration: 3719; Percent complete: 93.0%; Average loss: 2.4739
Iteration: 3720; Percent complete: 93.0%; Average loss: 2.7633
Iteration: 3721; Percent complete: 93.0%; Average loss: 2.4064
Iteration: 3722; Percent complete: 93.0%; Average loss: 2.7249
Iteration: 3723; Percent complete: 93.1%; Average loss: 2.5313
Iteration: 3724; Percent complete: 93.1%; Average loss: 2.6195
Iteration: 3725; Percent complete: 93.1%; Average loss: 2.7112
Iteration: 3726; Percent complete: 93.2%; Average loss: 2.9204
Iteration: 3727; Percent complete: 93.2%; Average loss: 2.6928
Iteration: 3728; Percent complete: 93.2%; Average loss: 2.6325
Iteration: 3729; Percent complete: 93.2%; Average loss: 2.6230
Iteration: 3730; Percent complete: 93.2%; Average loss: 2.7028
Iteration: 3731; Percent complete: 93.3%; Average loss: 2.7023
Iteration: 3732; Percent complete: 93.3%; Average loss: 2.7067
Iteration: 3733; Percent complete: 93.3%; Average loss: 2.7280
Iteration: 3734; Percent complete: 93.3%; Average loss: 2.6250
Iteration: 3735; Percent complete: 93.4%; Average loss: 2.3916
Iteration: 3736; Percent complete: 93.4%; Average loss: 2.4029
Iteration: 3737; Percent complete: 93.4%; Average loss: 2.4762
Iteration: 3738; Percent complete: 93.5%; Average loss: 2.6466
Iteration: 3739; Percent complete: 93.5%; Average loss: 2.6428
Iteration: 3740; Percent complete: 93.5%; Average loss: 2.6117
Iteration: 3741; Percent complete: 93.5%; Average loss: 2.5662
Iteration: 3742; Percent complete: 93.5%; Average loss: 2.6060
Iteration: 3743; Percent complete: 93.6%; Average loss: 2.6598
Iteration: 3744; Percent complete: 93.6%; Average loss: 2.6716
Iteration: 3745; Percent complete: 93.6%; Average loss: 2.6250
Iteration: 3746; Percent complete: 93.7%; Average loss: 2.5523
Iteration: 3747; Percent complete: 93.7%; Average loss: 2.6362
Iteration: 3748; Percent complete: 93.7%; Average loss: 2.7859
Iteration: 3749; Percent complete: 93.7%; Average loss: 2.7047
Iteration: 3750; Percent complete: 93.8%; Average loss: 2.6173
Iteration: 3751; Percent complete: 93.8%; Average loss: 2.5027
Iteration: 3752; Percent complete: 93.8%; Average loss: 2.6918
Iteration: 3753; Percent complete: 93.8%; Average loss: 3.0341
Iteration: 3754; Percent complete: 93.8%; Average loss: 2.6777
Iteration: 3755; Percent complete: 93.9%; Average loss: 2.5369
Iteration: 3756; Percent complete: 93.9%; Average loss: 2.6036
Iteration: 3757; Percent complete: 93.9%; Average loss: 2.8520
Iteration: 3758; Percent complete: 94.0%; Average loss: 2.6455
Iteration: 3759; Percent complete: 94.0%; Average loss: 2.7668
Iteration: 3760; Percent complete: 94.0%; Average loss: 2.4912
Iteration: 3761; Percent complete: 94.0%; Average loss: 2.7218
Iteration: 3762; Percent complete: 94.0%; Average loss: 2.9150
Iteration: 3763; Percent complete: 94.1%; Average loss: 2.8503
Iteration: 3764; Percent complete: 94.1%; Average loss: 2.8348
Iteration: 3765; Percent complete: 94.1%; Average loss: 2.6052
Iteration: 3766; Percent complete: 94.2%; Average loss: 2.5784
Iteration: 3767; Percent complete: 94.2%; Average loss: 2.6383
Iteration: 3768; Percent complete: 94.2%; Average loss: 2.5500
Iteration: 3769; Percent complete: 94.2%; Average loss: 2.4657
Iteration: 3770; Percent complete: 94.2%; Average loss: 2.6778
Iteration: 3771; Percent complete: 94.3%; Average loss: 2.5614
Iteration: 3772; Percent complete: 94.3%; Average loss: 2.7583
Iteration: 3773; Percent complete: 94.3%; Average loss: 2.5660
Iteration: 3774; Percent complete: 94.3%; Average loss: 2.6402
Iteration: 3775; Percent complete: 94.4%; Average loss: 2.6572
Iteration: 3776; Percent complete: 94.4%; Average loss: 2.7160
Iteration: 3777; Percent complete: 94.4%; Average loss: 2.7647
Iteration: 3778; Percent complete: 94.5%; Average loss: 2.7438
Iteration: 3779; Percent complete: 94.5%; Average loss: 2.6466
Iteration: 3780; Percent complete: 94.5%; Average loss: 2.5878
Iteration: 3781; Percent complete: 94.5%; Average loss: 2.4100
Iteration: 3782; Percent complete: 94.5%; Average loss: 2.6912
Iteration: 3783; Percent complete: 94.6%; Average loss: 2.6480
Iteration: 3784; Percent complete: 94.6%; Average loss: 2.7136
Iteration: 3785; Percent complete: 94.6%; Average loss: 2.5206
Iteration: 3786; Percent complete: 94.7%; Average loss: 2.6126
Iteration: 3787; Percent complete: 94.7%; Average loss: 2.7213
Iteration: 3788; Percent complete: 94.7%; Average loss: 2.4696
Iteration: 3789; Percent complete: 94.7%; Average loss: 2.7261
Iteration: 3790; Percent complete: 94.8%; Average loss: 2.6836
Iteration: 3791; Percent complete: 94.8%; Average loss: 2.7555
Iteration: 3792; Percent complete: 94.8%; Average loss: 2.8439
Iteration: 3793; Percent complete: 94.8%; Average loss: 2.4288
Iteration: 3794; Percent complete: 94.8%; Average loss: 2.8137
Iteration: 3795; Percent complete: 94.9%; Average loss: 2.8264
Iteration: 3796; Percent complete: 94.9%; Average loss: 2.4946
Iteration: 3797; Percent complete: 94.9%; Average loss: 2.8219
Iteration: 3798; Percent complete: 95.0%; Average loss: 2.5639
Iteration: 3799; Percent complete: 95.0%; Average loss: 2.8002
Iteration: 3800; Percent complete: 95.0%; Average loss: 2.6819
Iteration: 3801; Percent complete: 95.0%; Average loss: 2.7871
Iteration: 3802; Percent complete: 95.0%; Average loss: 2.5175
Iteration: 3803; Percent complete: 95.1%; Average loss: 2.5202
Iteration: 3804; Percent complete: 95.1%; Average loss: 2.6545
Iteration: 3805; Percent complete: 95.1%; Average loss: 2.6961
Iteration: 3806; Percent complete: 95.2%; Average loss: 2.5205
Iteration: 3807; Percent complete: 95.2%; Average loss: 2.6703
Iteration: 3808; Percent complete: 95.2%; Average loss: 2.6644
Iteration: 3809; Percent complete: 95.2%; Average loss: 2.5069
Iteration: 3810; Percent complete: 95.2%; Average loss: 2.6489
Iteration: 3811; Percent complete: 95.3%; Average loss: 2.9117
Iteration: 3812; Percent complete: 95.3%; Average loss: 2.7419
Iteration: 3813; Percent complete: 95.3%; Average loss: 2.6565
Iteration: 3814; Percent complete: 95.3%; Average loss: 2.6431
Iteration: 3815; Percent complete: 95.4%; Average loss: 2.4354
Iteration: 3816; Percent complete: 95.4%; Average loss: 2.3492
Iteration: 3817; Percent complete: 95.4%; Average loss: 2.5975
Iteration: 3818; Percent complete: 95.5%; Average loss: 2.8249
Iteration: 3819; Percent complete: 95.5%; Average loss: 2.8128
Iteration: 3820; Percent complete: 95.5%; Average loss: 2.6498
Iteration: 3821; Percent complete: 95.5%; Average loss: 2.7396
Iteration: 3822; Percent complete: 95.5%; Average loss: 2.6298
Iteration: 3823; Percent complete: 95.6%; Average loss: 2.7593
Iteration: 3824; Percent complete: 95.6%; Average loss: 2.4111
Iteration: 3825; Percent complete: 95.6%; Average loss: 2.5952
Iteration: 3826; Percent complete: 95.7%; Average loss: 2.5228
Iteration: 3827; Percent complete: 95.7%; Average loss: 2.5803
Iteration: 3828; Percent complete: 95.7%; Average loss: 2.5323
Iteration: 3829; Percent complete: 95.7%; Average loss: 2.5916
Iteration: 3830; Percent complete: 95.8%; Average loss: 2.5924
Iteration: 3831; Percent complete: 95.8%; Average loss: 2.4993
Iteration: 3832; Percent complete: 95.8%; Average loss: 2.8020
Iteration: 3833; Percent complete: 95.8%; Average loss: 2.9128
Iteration: 3834; Percent complete: 95.9%; Average loss: 2.6753
Iteration: 3835; Percent complete: 95.9%; Average loss: 2.5177
Iteration: 3836; Percent complete: 95.9%; Average loss: 2.6892
Iteration: 3837; Percent complete: 95.9%; Average loss: 2.5647
Iteration: 3838; Percent complete: 96.0%; Average loss: 2.7260
Iteration: 3839; Percent complete: 96.0%; Average loss: 2.7435
Iteration: 3840; Percent complete: 96.0%; Average loss: 2.6940
Iteration: 3841; Percent complete: 96.0%; Average loss: 2.7308
Iteration: 3842; Percent complete: 96.0%; Average loss: 2.4757
Iteration: 3843; Percent complete: 96.1%; Average loss: 2.9029
Iteration: 3844; Percent complete: 96.1%; Average loss: 2.4683
Iteration: 3845; Percent complete: 96.1%; Average loss: 2.4489
Iteration: 3846; Percent complete: 96.2%; Average loss: 2.6549
Iteration: 3847; Percent complete: 96.2%; Average loss: 2.6728
Iteration: 3848; Percent complete: 96.2%; Average loss: 2.6637
Iteration: 3849; Percent complete: 96.2%; Average loss: 2.5499
Iteration: 3850; Percent complete: 96.2%; Average loss: 2.4308
Iteration: 3851; Percent complete: 96.3%; Average loss: 2.6762
Iteration: 3852; Percent complete: 96.3%; Average loss: 2.5643
Iteration: 3853; Percent complete: 96.3%; Average loss: 2.7090
Iteration: 3854; Percent complete: 96.4%; Average loss: 2.5415
Iteration: 3855; Percent complete: 96.4%; Average loss: 2.7316
Iteration: 3856; Percent complete: 96.4%; Average loss: 2.8635
Iteration: 3857; Percent complete: 96.4%; Average loss: 2.5885
Iteration: 3858; Percent complete: 96.5%; Average loss: 2.5194
Iteration: 3859; Percent complete: 96.5%; Average loss: 2.5740
Iteration: 3860; Percent complete: 96.5%; Average loss: 2.4419
Iteration: 3861; Percent complete: 96.5%; Average loss: 2.7156
Iteration: 3862; Percent complete: 96.5%; Average loss: 2.7024
Iteration: 3863; Percent complete: 96.6%; Average loss: 2.5988
Iteration: 3864; Percent complete: 96.6%; Average loss: 2.5606
Iteration: 3865; Percent complete: 96.6%; Average loss: 2.6909
Iteration: 3866; Percent complete: 96.7%; Average loss: 2.7817
Iteration: 3867; Percent complete: 96.7%; Average loss: 2.4599
Iteration: 3868; Percent complete: 96.7%; Average loss: 2.5629
Iteration: 3869; Percent complete: 96.7%; Average loss: 2.6829
Iteration: 3870; Percent complete: 96.8%; Average loss: 2.5845
Iteration: 3871; Percent complete: 96.8%; Average loss: 2.7375
Iteration: 3872; Percent complete: 96.8%; Average loss: 2.6716
Iteration: 3873; Percent complete: 96.8%; Average loss: 2.6114
Iteration: 3874; Percent complete: 96.9%; Average loss: 2.6976
Iteration: 3875; Percent complete: 96.9%; Average loss: 2.5456
Iteration: 3876; Percent complete: 96.9%; Average loss: 2.7874
Iteration: 3877; Percent complete: 96.9%; Average loss: 2.8360
Iteration: 3878; Percent complete: 97.0%; Average loss: 2.7745
Iteration: 3879; Percent complete: 97.0%; Average loss: 2.9649
Iteration: 3880; Percent complete: 97.0%; Average loss: 2.6135
Iteration: 3881; Percent complete: 97.0%; Average loss: 2.5413
Iteration: 3882; Percent complete: 97.0%; Average loss: 2.9062
Iteration: 3883; Percent complete: 97.1%; Average loss: 2.7175
Iteration: 3884; Percent complete: 97.1%; Average loss: 2.9140
Iteration: 3885; Percent complete: 97.1%; Average loss: 2.7382
Iteration: 3886; Percent complete: 97.2%; Average loss: 2.4912
Iteration: 3887; Percent complete: 97.2%; Average loss: 2.7386
Iteration: 3888; Percent complete: 97.2%; Average loss: 2.6930
Iteration: 3889; Percent complete: 97.2%; Average loss: 2.5423
Iteration: 3890; Percent complete: 97.2%; Average loss: 2.6614
Iteration: 3891; Percent complete: 97.3%; Average loss: 2.8658
Iteration: 3892; Percent complete: 97.3%; Average loss: 2.7217
Iteration: 3893; Percent complete: 97.3%; Average loss: 2.5815
Iteration: 3894; Percent complete: 97.4%; Average loss: 2.4968
Iteration: 3895; Percent complete: 97.4%; Average loss: 2.6752
Iteration: 3896; Percent complete: 97.4%; Average loss: 2.7592
Iteration: 3897; Percent complete: 97.4%; Average loss: 2.7382
Iteration: 3898; Percent complete: 97.5%; Average loss: 2.7946
Iteration: 3899; Percent complete: 97.5%; Average loss: 2.6107
Iteration: 3900; Percent complete: 97.5%; Average loss: 2.6435
Iteration: 3901; Percent complete: 97.5%; Average loss: 2.6608
Iteration: 3902; Percent complete: 97.5%; Average loss: 2.5030
Iteration: 3903; Percent complete: 97.6%; Average loss: 2.6083
Iteration: 3904; Percent complete: 97.6%; Average loss: 2.5874
Iteration: 3905; Percent complete: 97.6%; Average loss: 2.6836
Iteration: 3906; Percent complete: 97.7%; Average loss: 2.6553
Iteration: 3907; Percent complete: 97.7%; Average loss: 2.7086
Iteration: 3908; Percent complete: 97.7%; Average loss: 2.5893
Iteration: 3909; Percent complete: 97.7%; Average loss: 2.6382
Iteration: 3910; Percent complete: 97.8%; Average loss: 2.5498
Iteration: 3911; Percent complete: 97.8%; Average loss: 2.4810
Iteration: 3912; Percent complete: 97.8%; Average loss: 2.6768
Iteration: 3913; Percent complete: 97.8%; Average loss: 2.4554
Iteration: 3914; Percent complete: 97.9%; Average loss: 2.6367
Iteration: 3915; Percent complete: 97.9%; Average loss: 2.5724
Iteration: 3916; Percent complete: 97.9%; Average loss: 2.6495
Iteration: 3917; Percent complete: 97.9%; Average loss: 2.4811
Iteration: 3918; Percent complete: 98.0%; Average loss: 2.4607
Iteration: 3919; Percent complete: 98.0%; Average loss: 2.8312
Iteration: 3920; Percent complete: 98.0%; Average loss: 2.7209
Iteration: 3921; Percent complete: 98.0%; Average loss: 2.7160
Iteration: 3922; Percent complete: 98.0%; Average loss: 2.8307
Iteration: 3923; Percent complete: 98.1%; Average loss: 2.7633
Iteration: 3924; Percent complete: 98.1%; Average loss: 2.9466
Iteration: 3925; Percent complete: 98.1%; Average loss: 2.5440
Iteration: 3926; Percent complete: 98.2%; Average loss: 2.4603
Iteration: 3927; Percent complete: 98.2%; Average loss: 2.5636
Iteration: 3928; Percent complete: 98.2%; Average loss: 2.9691
Iteration: 3929; Percent complete: 98.2%; Average loss: 2.6196
Iteration: 3930; Percent complete: 98.2%; Average loss: 2.5323
Iteration: 3931; Percent complete: 98.3%; Average loss: 2.3430
Iteration: 3932; Percent complete: 98.3%; Average loss: 2.9050
Iteration: 3933; Percent complete: 98.3%; Average loss: 2.5606
Iteration: 3934; Percent complete: 98.4%; Average loss: 2.7507
Iteration: 3935; Percent complete: 98.4%; Average loss: 2.7561
Iteration: 3936; Percent complete: 98.4%; Average loss: 2.6409
Iteration: 3937; Percent complete: 98.4%; Average loss: 2.8119
Iteration: 3938; Percent complete: 98.5%; Average loss: 2.5014
Iteration: 3939; Percent complete: 98.5%; Average loss: 2.7470
Iteration: 3940; Percent complete: 98.5%; Average loss: 2.6831
Iteration: 3941; Percent complete: 98.5%; Average loss: 2.6739
Iteration: 3942; Percent complete: 98.6%; Average loss: 2.5648
Iteration: 3943; Percent complete: 98.6%; Average loss: 2.7060
Iteration: 3944; Percent complete: 98.6%; Average loss: 2.7278
Iteration: 3945; Percent complete: 98.6%; Average loss: 2.7134
Iteration: 3946; Percent complete: 98.7%; Average loss: 2.5586
Iteration: 3947; Percent complete: 98.7%; Average loss: 2.6631
Iteration: 3948; Percent complete: 98.7%; Average loss: 2.4562
Iteration: 3949; Percent complete: 98.7%; Average loss: 2.6221
Iteration: 3950; Percent complete: 98.8%; Average loss: 2.5409
Iteration: 3951; Percent complete: 98.8%; Average loss: 2.7434
Iteration: 3952; Percent complete: 98.8%; Average loss: 2.6169
Iteration: 3953; Percent complete: 98.8%; Average loss: 2.3909
Iteration: 3954; Percent complete: 98.9%; Average loss: 2.6249
Iteration: 3955; Percent complete: 98.9%; Average loss: 2.7647
Iteration: 3956; Percent complete: 98.9%; Average loss: 2.6571
Iteration: 3957; Percent complete: 98.9%; Average loss: 2.4930
Iteration: 3958; Percent complete: 99.0%; Average loss: 2.4042
Iteration: 3959; Percent complete: 99.0%; Average loss: 2.9591
Iteration: 3960; Percent complete: 99.0%; Average loss: 2.6932
Iteration: 3961; Percent complete: 99.0%; Average loss: 2.5330
Iteration: 3962; Percent complete: 99.1%; Average loss: 2.7501
Iteration: 3963; Percent complete: 99.1%; Average loss: 2.7160
Iteration: 3964; Percent complete: 99.1%; Average loss: 2.5589
Iteration: 3965; Percent complete: 99.1%; Average loss: 2.5702
Iteration: 3966; Percent complete: 99.2%; Average loss: 2.7847
Iteration: 3967; Percent complete: 99.2%; Average loss: 2.6711
Iteration: 3968; Percent complete: 99.2%; Average loss: 2.6549
Iteration: 3969; Percent complete: 99.2%; Average loss: 2.7142
Iteration: 3970; Percent complete: 99.2%; Average loss: 2.5803
Iteration: 3971; Percent complete: 99.3%; Average loss: 2.5902
Iteration: 3972; Percent complete: 99.3%; Average loss: 2.5613
Iteration: 3973; Percent complete: 99.3%; Average loss: 2.6055
Iteration: 3974; Percent complete: 99.4%; Average loss: 2.6768
Iteration: 3975; Percent complete: 99.4%; Average loss: 2.5305
Iteration: 3976; Percent complete: 99.4%; Average loss: 2.8735
Iteration: 3977; Percent complete: 99.4%; Average loss: 2.6139
Iteration: 3978; Percent complete: 99.5%; Average loss: 2.5500
Iteration: 3979; Percent complete: 99.5%; Average loss: 2.3924
Iteration: 3980; Percent complete: 99.5%; Average loss: 2.4942
Iteration: 3981; Percent complete: 99.5%; Average loss: 2.5486
Iteration: 3982; Percent complete: 99.6%; Average loss: 2.6154
Iteration: 3983; Percent complete: 99.6%; Average loss: 2.5299
Iteration: 3984; Percent complete: 99.6%; Average loss: 2.5456
Iteration: 3985; Percent complete: 99.6%; Average loss: 2.6009
Iteration: 3986; Percent complete: 99.7%; Average loss: 2.8824
Iteration: 3987; Percent complete: 99.7%; Average loss: 2.6323
Iteration: 3988; Percent complete: 99.7%; Average loss: 2.5172
Iteration: 3989; Percent complete: 99.7%; Average loss: 2.6723
Iteration: 3990; Percent complete: 99.8%; Average loss: 2.7110
Iteration: 3991; Percent complete: 99.8%; Average loss: 2.7118
Iteration: 3992; Percent complete: 99.8%; Average loss: 2.4058
Iteration: 3993; Percent complete: 99.8%; Average loss: 2.3599
Iteration: 3994; Percent complete: 99.9%; Average loss: 2.4839
Iteration: 3995; Percent complete: 99.9%; Average loss: 2.4875
Iteration: 3996; Percent complete: 99.9%; Average loss: 2.5872
Iteration: 3997; Percent complete: 99.9%; Average loss: 2.6011
Iteration: 3998; Percent complete: 100.0%; Average loss: 2.6643
Iteration: 3999; Percent complete: 100.0%; Average loss: 2.6179
Iteration: 4000; Percent complete: 100.0%; Average loss: 2.4628
</pre></div>
</td></tr></table>

<h4 id="运行评估">运行评估<a class="headerlink" href="#运行评估" title="Permanent link">&para;</a></h4>
<p>要和这个模型聊天，运行下面的代码块。</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3
4
5
6
7
8
9</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Set dropout layers to eval mode</span>
<span class="n">encoder</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">decoder</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># Initialize search module</span>
<span class="n">searcher</span> <span class="o">=</span> <span class="n">GreedySearchDecoder</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">)</span>

<span class="c1"># Begin chatting (uncomment and run the following line to begin)</span>
<span class="c1"># evaluateInput(encoder, decoder, searcher, voc)</span>
</pre></div>
</td></tr></table>

<h3 id="结论">结论<a class="headerlink" href="#结论" title="Permanent link">&para;</a></h3>
<p>伙计们，这就是这一切。恭喜你，现在知道了构建生成聊天机器人模型的基础知识！如果有兴趣，可以尝试通过调整模型和训练参数以及自定义训练模型的数据来定制聊天机器人的行为。</p>
<p>查看其他教程，了解PyTorch中更酷的深度学习应用程序！</p>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../transfer_learning_tutorial/" title="迁移学习教程" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  后退
                </span>
                迁移学习教程
              </span>
            </div>
          </a>
        
        
          <a href="../../intermediate/char_rnn_classification_tutorial/" title="使用字符级RNN对名字进行分类" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  前进
                </span>
                使用字符级RNN对名字进行分类
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../assets/javascripts/application.267712eb.js"></script>
      
        
        
          
          <script src="../../assets/javascripts/lunr/lunr.stemmer.support.js"></script>
          
            
              
                <script src="../../assets/javascripts/lunr/tinyseg.js"></script>
              
              
                <script src="../../assets/javascripts/lunr/lunr.ja.js"></script>
              
            
          
          
        
      
      <script>app.initialize({version:"1.0.4",url:{base:"../.."}})</script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
      
    
  </body>
</html>