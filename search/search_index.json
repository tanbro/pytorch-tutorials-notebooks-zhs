{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"pytorch-tutorials-notebooks-zhs PyTorch tutorials \u7684\u4e2d\u6587\u7ffb\u8bd1\u7b14\u8bb0\u3002 \u70b9\u51fb\u8fd9\u4e2a\u94fe\u63a5\u76f4\u63a5\u8bbf\u95ee\u8fd9\u4e2a\u9879\u76ee\u751f\u6210\u7684Web\u6587\u6863\u7ad9\u70b9\uff1a https://tanbro.github.io/pytorch-tutorials-notebooks-zhs/ \u5b98\u65b9\u6559\u7a0b\u4ee5 \u5e26\u6709 Sphinx-Doc \u6269\u5c55 reST \u683c\u5f0f\u6ce8\u91ca\u7684Python\u6e90\u4ee3\u7801 \u7684\u5f62\u5f0f\u5236\u4f5c\u4e86Web\u6587\u6863\u5e76\u5bfc\u51fa Jupyter \u7b14\u8bb0\u3002 \u800c\u8fd9\u4e2a\u9879\u76ee\u9664\u4e86\u7ffb\u8bd1\uff0c\u4fee\u6539\u8fd8\u6709: \u4f7f\u7528 Jupyter \u7b14\u8bb0\u6587\u4ef6\u4fdd\u5b58\u8bf4\u660e\u548c\u4ee3\u7801\u7247\u6bb5 \u4ece Jupyter \u7b14\u8bb0\u5bfc\u51fa Markdown \u6587\u4ef6\uff0c\u5e76\u5728\u5176\u57fa\u7840\u4e0a\u6784\u5efaWeb\u6587\u6863\u7ad9\u70b9 \u8fd9\u662f\u4e2a\u4eba\u7684\u7ffb\u8bd1\u7b14\u8bb0\uff0c\u7528\u4e8e\u4e2a\u4eba\u5b66\u4e60 PyTorch\u3002 \u76ee\u524d\u53ea\u7ffb\u8bd1\u548c\u8f6c\u6362\u4e86\u90e8\u5206 *.py \u6587\u4ef6(\u5b98\u65b9\u4f7f\u7528Sphinx-Gallery\u5c06\u5b83\u4eec\u8f6c\u6210 Sphinx-Doc \u6587\u6863\u5e76\u5bfc\u51fa Jupyter \u7b14\u8bb0) \uff0c\u6ca1\u6709\u5904\u7406\u4efb\u4f55 *.rst (\u5355\u7eaf\u7684 Sphinx-Doc \u6587\u6863) \u73af\u5883 \u8fd9\u4e2a\u5de5\u7a0b\u91c7\u5728Ubuntu1604,1804\u4e0b\uff0c\u4f7f\u7528Python3.6\u8fd0\u884c\u548c\u6784\u5efa\uff0c\u6ca1\u6709\u6d4b\u8bd5\u8fc7\u5176\u5b83\u73af\u5883\u3002 \u5f3a\u70c8 \u5efa\u8bae\u4e3a\u8fd9\u4e2a\u9879\u76ee\u5355\u72ec\u65b0\u5efa\u4e00\u4e2a\u4e13\u7528\u7684 venv \u6216 Conda \u73af\u5883 pip : 1 pip install -r requirements.txt Pipenv : 1 pipenv install Conda : 1 2 3 4 conda install -y jupyter nbconvert tqdm pandas matplotlib scikit-image pygments conda install -y -c conda-forge jupyterlab conda install -y pytorch torchvision cudatoolkit=9.0 -c pytorch pip install mkdocs pymdown-extensions mkdocs-material mkdocs-pdf-export-plugin \u8fd0\u884c Jupyter Lab/Notebook \u5165\u95e8\u6307\u5357\u7684\u7ffb\u8bd1\u4ee5 Jupyter \u7b14\u8bb0\u7684\u5f62\u5f0f\u4fdd\u5b58\u5728 notebooks \u76ee\u5f55\uff0c\u542f\u52a8 Jupyter lab/notebook: 1 jupyter-lab \u6216\u8005 1 jupyter-notebook \u5728Web\u754c\u9762\u4e2d\u8fdb\u5165 notebooks \u76ee\u5f55\u53ef\u4ee5\u627e\u5230\u6240\u6709\u7b14\u8bb0\u3002 \u6784\u5efaWeb\u6587\u6863\u7ad9\u70b9 \u8fd9\u4e2a\u9879\u76ee\u4f7f\u7528 MkDocs \u5c06\u591a\u4e2a\u7531\u7b14\u8bb0\u5bfc\u51fa\u7684Markdown\u6587\u4ef6\u5408\u5e76\u751f\u6210\u4e00\u4e2aWeb\u6587\u6863\u7ad9\u70b9\u3002 \u628a\u7b14\u8bb0\u5bfc\u51fa\u4e3aMarkdown\u6587\u4ef6: 1 python tools/nbtomd.py \u6784\u5efaWeb\u6587\u6863 1 mkdocs build \u8fd0\u884cWeb\u6587\u6863\u670d\u52a1\u5668\uff0c\u8bbf\u95ee http://localhost:8000/ \u67e5\u770b 1 mkdocs serve Jupyter Notebook \u65e0\u6cd5\u5bfc\u51fa\u542b\u6709\u4e2d\u6587 LaTeX/PDF \u7684\u95ee\u9898 \u91c7\u7528\u6765\u81ea https://github.com/jupyter/notebook/issues/2848#issuecomment-372736199 \u7684\u65b9\u6cd5\uff1a \u5b89\u88c5 texlive \u548c pandoc ```console sudo apt install texlive-xetex pandoc \u4fee\u6539 Article \u7684 LaTeX \u6a21\u677f \u627e\u5230\u6587\u4ef6 site-packages/nbconvert/templates/latex/article.tplx , \u5728 \\documentclass[11pt]{article} \u4e4b\u540e\u52a0\u4e0a\u4e2d\u6587\u5b57\u4f53\u5b9a\u4e49: 1 2 3 4 5 6 7 ((* block docclass *)) \\documentclass [11pt] { article } % \u4e2d\u6587\u95ee\u9898: https://github.com/jupyter/notebook/issues/2848#issuecomment-372736199 \\usepackage { xeCJK } \\setCJKmainfont { Noto Sans CJK SC } \\setCJKmonofont { Noto Sans Mono CJK SC } ((* endblock docclass *)) \u5177\u4f53\u91c7\u7528\u54ea\u4e2a\u7684\u5b57\u4f53\u5e94\u4ee5\u5b9e\u9645\u60c5\u51b5\u4e3a\u51c6","title":"README"},{"location":"#pytorch-tutorials-notebooks-zhs","text":"PyTorch tutorials \u7684\u4e2d\u6587\u7ffb\u8bd1\u7b14\u8bb0\u3002 \u70b9\u51fb\u8fd9\u4e2a\u94fe\u63a5\u76f4\u63a5\u8bbf\u95ee\u8fd9\u4e2a\u9879\u76ee\u751f\u6210\u7684Web\u6587\u6863\u7ad9\u70b9\uff1a https://tanbro.github.io/pytorch-tutorials-notebooks-zhs/ \u5b98\u65b9\u6559\u7a0b\u4ee5 \u5e26\u6709 Sphinx-Doc \u6269\u5c55 reST \u683c\u5f0f\u6ce8\u91ca\u7684Python\u6e90\u4ee3\u7801 \u7684\u5f62\u5f0f\u5236\u4f5c\u4e86Web\u6587\u6863\u5e76\u5bfc\u51fa Jupyter \u7b14\u8bb0\u3002 \u800c\u8fd9\u4e2a\u9879\u76ee\u9664\u4e86\u7ffb\u8bd1\uff0c\u4fee\u6539\u8fd8\u6709: \u4f7f\u7528 Jupyter \u7b14\u8bb0\u6587\u4ef6\u4fdd\u5b58\u8bf4\u660e\u548c\u4ee3\u7801\u7247\u6bb5 \u4ece Jupyter \u7b14\u8bb0\u5bfc\u51fa Markdown \u6587\u4ef6\uff0c\u5e76\u5728\u5176\u57fa\u7840\u4e0a\u6784\u5efaWeb\u6587\u6863\u7ad9\u70b9 \u8fd9\u662f\u4e2a\u4eba\u7684\u7ffb\u8bd1\u7b14\u8bb0\uff0c\u7528\u4e8e\u4e2a\u4eba\u5b66\u4e60 PyTorch\u3002 \u76ee\u524d\u53ea\u7ffb\u8bd1\u548c\u8f6c\u6362\u4e86\u90e8\u5206 *.py \u6587\u4ef6(\u5b98\u65b9\u4f7f\u7528Sphinx-Gallery\u5c06\u5b83\u4eec\u8f6c\u6210 Sphinx-Doc \u6587\u6863\u5e76\u5bfc\u51fa Jupyter \u7b14\u8bb0) \uff0c\u6ca1\u6709\u5904\u7406\u4efb\u4f55 *.rst (\u5355\u7eaf\u7684 Sphinx-Doc \u6587\u6863)","title":"pytorch-tutorials-notebooks-zhs"},{"location":"#_1","text":"\u8fd9\u4e2a\u5de5\u7a0b\u91c7\u5728Ubuntu1604,1804\u4e0b\uff0c\u4f7f\u7528Python3.6\u8fd0\u884c\u548c\u6784\u5efa\uff0c\u6ca1\u6709\u6d4b\u8bd5\u8fc7\u5176\u5b83\u73af\u5883\u3002 \u5f3a\u70c8 \u5efa\u8bae\u4e3a\u8fd9\u4e2a\u9879\u76ee\u5355\u72ec\u65b0\u5efa\u4e00\u4e2a\u4e13\u7528\u7684 venv \u6216 Conda \u73af\u5883 pip : 1 pip install -r requirements.txt Pipenv : 1 pipenv install Conda : 1 2 3 4 conda install -y jupyter nbconvert tqdm pandas matplotlib scikit-image pygments conda install -y -c conda-forge jupyterlab conda install -y pytorch torchvision cudatoolkit=9.0 -c pytorch pip install mkdocs pymdown-extensions mkdocs-material mkdocs-pdf-export-plugin","title":"\u73af\u5883"},{"location":"#jupyter-labnotebook","text":"\u5165\u95e8\u6307\u5357\u7684\u7ffb\u8bd1\u4ee5 Jupyter \u7b14\u8bb0\u7684\u5f62\u5f0f\u4fdd\u5b58\u5728 notebooks \u76ee\u5f55\uff0c\u542f\u52a8 Jupyter lab/notebook: 1 jupyter-lab \u6216\u8005 1 jupyter-notebook \u5728Web\u754c\u9762\u4e2d\u8fdb\u5165 notebooks \u76ee\u5f55\u53ef\u4ee5\u627e\u5230\u6240\u6709\u7b14\u8bb0\u3002","title":"\u8fd0\u884c Jupyter Lab/Notebook"},{"location":"#web","text":"\u8fd9\u4e2a\u9879\u76ee\u4f7f\u7528 MkDocs \u5c06\u591a\u4e2a\u7531\u7b14\u8bb0\u5bfc\u51fa\u7684Markdown\u6587\u4ef6\u5408\u5e76\u751f\u6210\u4e00\u4e2aWeb\u6587\u6863\u7ad9\u70b9\u3002 \u628a\u7b14\u8bb0\u5bfc\u51fa\u4e3aMarkdown\u6587\u4ef6: 1 python tools/nbtomd.py \u6784\u5efaWeb\u6587\u6863 1 mkdocs build \u8fd0\u884cWeb\u6587\u6863\u670d\u52a1\u5668\uff0c\u8bbf\u95ee http://localhost:8000/ \u67e5\u770b 1 mkdocs serve","title":"\u6784\u5efaWeb\u6587\u6863\u7ad9\u70b9"},{"location":"#jupyter-notebook-latexpdf","text":"\u91c7\u7528\u6765\u81ea https://github.com/jupyter/notebook/issues/2848#issuecomment-372736199 \u7684\u65b9\u6cd5\uff1a \u5b89\u88c5 texlive \u548c pandoc ```console sudo apt install texlive-xetex pandoc \u4fee\u6539 Article \u7684 LaTeX \u6a21\u677f \u627e\u5230\u6587\u4ef6 site-packages/nbconvert/templates/latex/article.tplx , \u5728 \\documentclass[11pt]{article} \u4e4b\u540e\u52a0\u4e0a\u4e2d\u6587\u5b57\u4f53\u5b9a\u4e49: 1 2 3 4 5 6 7 ((* block docclass *)) \\documentclass [11pt] { article } % \u4e2d\u6587\u95ee\u9898: https://github.com/jupyter/notebook/issues/2848#issuecomment-372736199 \\usepackage { xeCJK } \\setCJKmainfont { Noto Sans CJK SC } \\setCJKmonofont { Noto Sans Mono CJK SC } ((* endblock docclass *)) \u5177\u4f53\u91c7\u7528\u54ea\u4e2a\u7684\u5b57\u4f53\u5e94\u4ee5\u5b9e\u9645\u60c5\u51b5\u4e3a\u51c6","title":"Jupyter Notebook \u65e0\u6cd5\u5bfc\u51fa\u542b\u6709\u4e2d\u6587 LaTeX/PDF \u7684\u95ee\u9898"},{"location":"beginner/data_loading_tutorial/","text":"\u6570\u636e\u52a0\u8f7d\u548c\u5904\u7406\u6559\u7a0b \u4f5c\u8005 \uff1a Sasank Chilamkurthy \u5bf9\u4e8e\u4efb\u4f55\u673a\u5668\u5b66\u4e60\u7684\u95ee\u9898\uff0c\u90fd\u4f1a\u6709\u5927\u91cf\u7684\u52aa\u529b\u88ab\u7528\u5728\u51c6\u5907\u6570\u636e\u4e0a\u9762\u3002 PyTorch\u63d0\u4f9b\u4e86\u8bb8\u591a\u5de5\u5177\u6765\u7b80\u5316\u6570\u636e\u52a0\u8f7d\u3001\u589e\u5f3a\u4ee3\u7801\u53ef\u8bfb\u6027\u3002 \u7740\u8fd9\u4e2a\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u770b\u5230\u5982\u4f55\u5728\u4e00\u4e2a\u975e\u51e1\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u52a0\u8f7d\u3001\u9884\u5904\u7406\u548c\u6269\u5145\u3002 \u8981\u8fd0\u884c\u8fd9\u4e2a\u6559\u7a0b\uff0c\u786e\u5b9a\u5b89\u88c5\u4e86\u4ee5\u4e0b\u5305\uff1a scikit-image \uff1a\u7528\u4e8e\u56fe\u50cfIO\u548c\u53d8\u6362 pandas \uff1a\u7528\u4e8e\u7b80\u5316CVS\u89e3\u6790 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from __future__ import print_function , division import os import torch import pandas as pd from skimage import io , transform import numpy as np import matplotlib.pyplot as plt from torch.utils.data import Dataset , DataLoader from torchvision import transforms , utils # Ignore warnings import warnings warnings . filterwarnings ( \"ignore\" ) plt . ion () # interactive mode \u6211\u4eec\u8981\u5904\u7406\u7684\u6570\u636e\u96c6\u662f\u4eba\u8138\u59ff\u6001\u3002 \u9762\u90e8\u6807\u6ce8\u5c31\u50cf\u8fd9\u6837\uff1a \u603b\u4e4b\uff0c\u6bcf\u4e2a\u9762\u90e8\u90fd\u6ce8\u91ca\u4e8668\u4e2a\u4e0d\u540c\u7684\u5173\u952e\u7279\u5f81\u70b9(landmark)\u3002 \u6ce8\u610f \uff1a \u4ece https://download.pytorch.org/tutorial/faces.zip \u4e0b\u8f7d\u8fd9\u4e2a\u6570\u636e\u96c6\uff0c\u56fe\u7247\u5728\u76ee\u5f55 data/faces/ \u3002 \u8fd9\u4e2a\u6570\u636e\u96c6\u5b9e\u9645\u4e0a\u662f\u901a\u8fc7\u5bf9\u6765\u81eaimagenet\u6807\u6ce8\u4e3a\u201dface\u201d\u7684\u4e00\u4e9b\u56fe\u50cf\u5e94\u7528 dlib \u8fd9\u4e00\u4f18\u79c0\u7684\u59ff\u6001\u4f30\u8ba1\u6cd5\u5f97\u6765\u7684\u3002 \u8fd9\u4e2a\u6570\u636e\u96c6\u91cc\u6709\u4e00\u4e2a\u5e26\u6709\u6807\u6ce8\u7684CSV\u6587\u4ef6\uff0c\u5b83\u770b\u8d77\u6765\u662f\u8fd9\u6837\u7684\uff1a image_name part_0_x part_0_y part_1_x part_1_y part_2_x \u2026 part_67_x part_67_y 0805personali01.jpg 27 83 27 98 \u2026 \u2026 84 134 1084239450_e76e00b7e7.jpg 70 236 71 257 \u2026 \u2026 128 312 \u8bfb\u53d6CSV\u5e76\u628a\u6807\u6ce8\u6570\u636e\u653e\u5230(N,2)\u6570\u7ec4\u4e2d\uff0c\u5176\u4e2dN\u662f\u5173\u952e\u7279\u5f81\u70b9\u7684\u6570\u91cf\u3002 1 2 3 4 5 6 7 8 9 10 landmarks_frame = pd . read_csv ( 'data/faces/face_landmarks.csv' ) n = 65 img_name = landmarks_frame . iloc [ n , 0 ] landmarks = landmarks_frame . iloc [ n , 1 :] . as_matrix () landmarks = landmarks . astype ( 'float' ) . reshape ( - 1 , 2 ) print ( 'Image name: {}' . format ( img_name )) print ( 'Landmarks shape: {}' . format ( landmarks . shape )) print ( 'First 4 Landmarks: {}' . format ( landmarks [: 4 ])) 1 2 3 4 5 6 Image name: person-7.jpg Landmarks shape: (68, 2) First 4 Landmarks: [[32. 65.] [33. 76.] [34. 86.] [34. 97.]] \u7f16\u5199\u4e00\u4e2a\u53ef\u4ee5\u663e\u793a\u56fe\u50cf\u53ca\u5176\u5173\u952e\u7279\u5f81\u70b9\u7684\u7b80\u5355\u8f85\u52a9\u51fd\u6570\uff0c\u7528\u5b83\u663e\u793a\u6837\u672c\u3002 1 2 3 4 5 6 7 8 9 def show_landmarks ( image , landmarks ): \"\"\"Show image with landmarks\"\"\" plt . imshow ( image ) plt . scatter ( landmarks [:, 0 ], landmarks [:, 1 ], s = 10 , marker = '.' , c = 'r' ) plt . pause ( 0.001 ) # pause a bit so that plots are updated plt . figure () show_landmarks ( io . imread ( os . path . join ( 'data/faces/' , img_name )), landmarks ) plt . show () Dataset \u7c7b torch.utils.data.Dataset \u662f\u9488\u5bf9\u6570\u636e\u96c6\u7684\u62bd\u8c61\u7c7b. \u6211\u4eec\u81ea\u5df1\u7684\u6570\u636e\u96c6\u7c7b\u5e94\u8be5\u7ee7\u627f\u81ea Dataset \uff0c\u8986\u76d6\u4ee5\u4e0b\u65b9\u6cd5\uff1a __len__ \uff1a\u7528 len(dataset) \u53ef\u4ee5\u8fd4\u56de\u6570\u636e\u96c6\u7684\u5927\u5c0f\u3002 __getitem__ \uff1a\u4f7f\u5f97\u50cf dataset[i] \u8fd9\u6837\u7684\u7d22\u5f15\u64cd\u4f5c\u53ef\u4ee5\u7528\u4e8e\u8bbf\u95ee\u7b2c i i \u4e2a\u6837\u672c \u73b0\u5728\uff0c\u4e3a\u6211\u4eec\u81ea\u5df1\u7684\u9762\u90e8\u5173\u952e\u7279\u5f81\u70b9\u6570\u636e\u96c6\u521b\u5efa\u4e00\u4e2a Dataset \u7c7b\u3002 \u6211\u4eec\u5728 __init__ \u8bfb\u53d6CSV\uff0c\u4f46\u662f\u628a\u8bfb\u53d6\u56fe\u7247\u7684\u5de5\u4f5c\u7559\u7ed9 __getitem__ \u3002 \u8fd9\u6837\u53ef\u4ee5\u8282\u7701\u5185\u5b58\uff0c\u56e0\u4e3a\u56fe\u7247\u4e0d\u662f\u4e00\u6b21\u6027\u7684\u5168\u90e8\u8bfb\u8fdb\u5185\u5b58\uff0c\u800c\u662f\u9700\u8981\u65f6\u624d\u8bfb\u53d6\u3002 \u6211\u4eec\u7684\u6570\u636e\u96c6\u4e2d\u7684\u6837\u672c\u5c06\u662f\u4e00\u4e2a dict \uff1a 1 { 'image' : image , 'landmarks' : landmarks } \u6211\u4eec\u7684\u6570\u636e\u96c6\u5c06\u63d0\u4f9b\u4e00\u4e2a\u53ef\u9009\u7684\u53c2\u6570 transform \uff0c\u8fd9\u6837\u4efb\u4f55\u9700\u8981\u7684\u5904\u7406\u8fc7\u7a0b\u53ef\u4ee5\u5728\u6837\u672c\u4e0a\u6267\u884c\u3002 \u5728\u4e0b\u4e00\u8282\u53ef\u4ee5\u770b\u5230 transform \u7684\u4f5c\u7528\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class FaceLandmarksDataset ( Dataset ): \"\"\"Face Landmarks dataset.\"\"\" def __init__ ( self , csv_file , root_dir , transform = None ): \"\"\" Args: csv_file (string): Path to the csv file with annotations. root_dir (string): Directory with all the images. transform (callable, optional): Optional transform to be applied on a sample. \"\"\" self . landmarks_frame = pd . read_csv ( csv_file ) self . root_dir = root_dir self . transform = transform def __len__ ( self ): return len ( self . landmarks_frame ) def __getitem__ ( self , idx ): img_name = os . path . join ( self . root_dir , self . landmarks_frame . iloc [ idx , 0 ]) image = io . imread ( img_name ) landmarks = self . landmarks_frame . iloc [ idx , 1 :] . as_matrix () landmarks = landmarks . astype ( 'float' ) . reshape ( - 1 , 2 ) sample = { 'image' : image , 'landmarks' : landmarks } if self . transform : sample = self . transform ( sample ) return sample \u5b9e\u4f8b\u5316\u8fd9\u4e2a\u7c7b\u5e76\u4e14\u904d\u5386\u6570\u636e\u6837\u672c\uff0c\u6253\u5370\u59344\u4e2a\u6837\u672c\u7684\u5927\u5c0f\u548c\u5173\u952e\u7279\u5f81\u70b9\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 face_dataset = FaceLandmarksDataset ( csv_file = 'data/faces/face_landmarks.csv' , root_dir = 'data/faces/' ) fig = plt . figure () for i , sample in enumerate ( face_dataset ): print ( i , sample [ 'image' ] . shape , sample [ 'landmarks' ] . shape ) ax = plt . subplot ( 1 , 4 , i + 1 ) plt . tight_layout () ax . set_title ( 'Sample #{}' . format ( i )) ax . axis ( 'off' ) show_landmarks ( ** sample ) if i == 3 : plt . show () break 1 0 (324, 215, 3) (68, 2) 1 1 (500, 333, 3) (68, 2) 1 2 (250, 258, 3) (68, 2) 1 3 (434, 290, 3) (68, 2) \u53d8\u6362 \u89c2\u5bdf\u4e0a\u9762\u7684\u6837\u672c\uff0c\u6211\u4eec\u53d1\u73b0\u4e00\u4e2a\u95ee\u9898\u2014\u2014\u56fe\u50cf\u4e0d\u662f\u540c\u6837\u7684\u5927\u5c0f\u3002 \u5927\u591a\u6570\u795e\u7ecf\u7f51\u7edc\u90fd\u671f\u5f85\u56fa\u5b9a\u5927\u5c0f\u7684\u56fe\u50cf\u3002 \u56e0\u6b64\uff0c\u9700\u8981\u7f16\u5199\u4e00\u4e9b\u9884\u5904\u7406\u4ee3\u7801\u3002 \u6211\u4eec\u521b\u5efa\u4e09\u79cd\u53d8\u6362\uff1a Rescale : \u7f29\u653e\u56fe\u50cf RandomCrop : \u968f\u673a\u88c1\u526a\u56fe\u50cf\u3002\u8fd9\u7b97\u662f\u6570\u636e\u589e\u52a0\u6269\u5145\u3002 ToTensor : \u5c06numpy\u56fe\u50cf\u8f6c\u4e3atorch\u56fe\u50cf\uff08\u9700\u8981\u4ea4\u6362\u5750\u6807\u8f74\uff09\u3002 \u628a\u4ed6\u4eec\u7f16\u5199\u4e3a\u53ef\u8c03\u7528\u7c7b\uff0c\u800c\u4e0d\u662f\u51fd\u6570\uff0c\u8fd9\u6837\uff0c\u6bcf\u6b21\u8c03\u7528\u5c31\u4e0d\u7528\u4f20\u9012\u53d8\u6362\u7684\u53c2\u6570\u3002 \u4e3a\u6b64\uff0c\u6211\u4eec\u9700\u8981\u5b9e\u73b0 __call__ \u65b9\u6cd5\uff0c\u5982\u679c\u6709\u5fc5\u8981\uff0c\u8fd8\u6709 __init__ \u65b9\u6cd5\u3002 \u7136\u540e\u50cf\u8fd9\u6837\u8fdb\u884c\u53d8\u6362\uff1a 1 2 tsfm = Transform ( params ) transformed_sample = tsfm ( sample ) \u8bf7\u6ce8\u610f\u4ee5\u4e0b\u8fd9\u4e9b\u53d8\u6362\u5982\u4f55\u88ab\u5e94\u7528\u4e8e\u56fe\u50cf\u548c\u5173\u952e\u7279\u5f81\u70b9\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 class Rescale ( object ): \"\"\"Rescale the image in a sample to a given size. Args: output_size (tuple or int): Desired output size. If tuple, output is matched to output_size. If int, smaller of image edges is matched to output_size keeping aspect ratio the same. \"\"\" def __init__ ( self , output_size ): assert isinstance ( output_size , ( int , tuple )) self . output_size = output_size def __call__ ( self , sample ): image , landmarks = sample [ 'image' ], sample [ 'landmarks' ] h , w = image . shape [: 2 ] if isinstance ( self . output_size , int ): if h > w : new_h , new_w = self . output_size * h / w , self . output_size else : new_h , new_w = self . output_size , self . output_size * w / h else : new_h , new_w = self . output_size new_h , new_w = int ( new_h ), int ( new_w ) img = transform . resize ( image , ( new_h , new_w )) # h and w are swapped for landmarks because for images, # x and y axes are axis 1 and 0 respectively landmarks = landmarks * [ new_w / w , new_h / h ] return { 'image' : img , 'landmarks' : landmarks } class RandomCrop ( object ): \"\"\"Crop randomly the image in a sample. Args: output_size (tuple or int): Desired output size. If int, square crop is made. \"\"\" def __init__ ( self , output_size ): assert isinstance ( output_size , ( int , tuple )) if isinstance ( output_size , int ): self . output_size = ( output_size , output_size ) else : assert len ( output_size ) == 2 self . output_size = output_size def __call__ ( self , sample ): image , landmarks = sample [ 'image' ], sample [ 'landmarks' ] h , w = image . shape [: 2 ] new_h , new_w = self . output_size top = np . random . randint ( 0 , h - new_h ) left = np . random . randint ( 0 , w - new_w ) image = image [ top : top + new_h , left : left + new_w ] landmarks = landmarks - [ left , top ] return { 'image' : image , 'landmarks' : landmarks } class ToTensor ( object ): \"\"\"Convert ndarrays in sample to Tensors.\"\"\" def __call__ ( self , sample ): image , landmarks = sample [ 'image' ], sample [ 'landmarks' ] # swap color axis because # numpy image: H x W x C # torch image: C X H X W image = image . transpose (( 2 , 0 , 1 )) return { 'image' : torch . from_numpy ( image ), 'landmarks' : torch . from_numpy ( landmarks )} \u7ec4\u5408\u8f6c\u6362 \u4e0b\u9762\uff0c\u5728\u6837\u672c\u4e0a\u5e94\u7528\u8f6c\u6362\u3002 \u5047\u8bbe\u6211\u4eec\u60f3\u8981\u5c06\u56fe\u50cf\u7684\u77ed\u8fb9\u91cd\u65b0\u7f29\u653e\u5230256\uff0c\u7136\u540e\u4ece\u4e2d\u968f\u673a\u88c1\u526a\u4e00\u4e2a224\u7684\u6b63\u65b9\u5f62\u3002 \u4e5f\u5c31\u662f\u8bf4\uff0c\u6211\u4eec\u9700\u8981\u7ec4\u5408 Rescale \u548c RandomCrop \u8f6c\u6362\u3002 torchvision.transforms.Compose \u662f\u4e00\u4e2a\u7b80\u5355\u7684\u53ef\u8c03\u7528\u7c7b\uff0c\u5b83\u7528\u4e8e\u5b8c\u6210\u8fd9\u6837\u7684\u5de5\u4f5c\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 scale = Rescale ( 256 ) crop = RandomCrop ( 128 ) composed = transforms . Compose ([ Rescale ( 256 ), RandomCrop ( 224 )]) # Apply each of the above transforms on sample. fig = plt . figure () sample = face_dataset [ 65 ] for i , tsfrm in enumerate ([ scale , crop , composed ]): transformed_sample = tsfrm ( sample ) ax = plt . subplot ( 1 , 3 , i + 1 ) plt . tight_layout () ax . set_title ( type ( tsfrm ) . __name__ ) show_landmarks ( ** transformed_sample ) plt . show () \u904d\u5386\u6570\u636e\u96c6 \u628a\u4e0a\u9762\u7684\u4f60\u540c\u653e\u5728\u4e00\u8d77\uff0c\u521b\u5efa\u4e00\u4e2a\u5e26\u6709\u7ec4\u5408\u53d8\u6362\u7684\u6570\u636e\u96c6\u3002 \u603b\u800c\u8a00\u4e4b\uff0c\u6bcf\u6b21\u4ece\u8fd9\u4e2a\u6570\u636e\u96c6\u91c7\u7528\u7684\u65f6\u5019\uff1a \u4ece\u6587\u4ef6\u4e2d\u5373\u65f6\u7684\u8bfb\u53d6\u56fe\u50cf \u5bf9\u8bfb\u53d6\u7684\u56fe\u50cf\u8fdb\u884c\u53d8\u6362 \u7531\u4e8e\u5176\u4e2d\u4e00\u4e2a\u53d8\u5316\u662f\u968f\u673a\u7684\uff0c\u6570\u636e\u5728\u91c7\u6837\u65f6\u662f\u6269\u5145\u7684 \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u4e4b\u524d\u7528\u8fc7\u7684 for ... in \u5faa\u73af\u904d\u5386\u6570\u636e\u96c6\u3002 1 2 3 4 5 6 7 8 9 10 11 12 transformed_dataset = FaceLandmarksDataset ( csv_file = 'data/faces/face_landmarks.csv' , root_dir = 'data/faces/' , transform = transforms . Compose ([ Rescale ( 256 ), RandomCrop ( 224 ), ToTensor () ])) for i , sample in enumerate ( transformed_dataset ): print ( i , sample [ 'image' ] . size (), sample [ 'landmarks' ] . size ()) if i == 3 : break 1 2 3 4 0 torch.Size([3, 224, 224]) torch.Size([68, 2]) 1 torch.Size([3, 224, 224]) torch.Size([68, 2]) 2 torch.Size([3, 224, 224]) torch.Size([68, 2]) 3 torch.Size([3, 224, 224]) torch.Size([68, 2]) \u4e0d\u8fc7\uff0c\u7b80\u5355\u7684 for \u5faa\u73af\u904d\u5386\u6570\u636e\u8ba9\u6211\u4eec\u4e22\u5931\u4e86\u4e0d\u5c11\u7279\u6027\uff0c\u7279\u522b\u662f\uff1a \u6570\u636e\u7684\u6279\u5904\u7406 \u4e71\u5e8f\u6392\u5217\u6570\u636e \u4f7f\u7528 multiprocessing \u5e76\u884c\u52a0\u8f7d\u6570\u636e\u3002 torch.utils.data.DataLoader \u662f\u4e00\u4e2a\u8fed\u4ee3\u5668\uff0c\u5b83\u63d0\u4f9b\u4e86\u6240\u6709\u8fd9\u4e9b\u7279\u6027\u3002 \u4e0b\u9762\u7684\u53c2\u6570\u5e94\u8be5\u6e05\u9664\u7684\u8bf4\u660e\u4e86\u95ee\u9898\u3002 \u5176\u4e2d\u4e00\u4e2a\u6709\u8da3\u7684\u53c2\u6570\u662f collate_fn \uff0c\u53ef\u4ee5\u6307\u5b9a\u54ea\u4e9b\u5177\u4f53\u7684\u6837\u672c\u9700\u8981\u5728\u6279\u6b21\u4e2d\u4f7f\u7528 collate_fn \u8fdb\u884c\u6574\u7406\u3002 \u5f53\u7136\uff0c\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u9ed8\u8ba4\u7684\u6574\u7406(collat)\u5c31\u53ef\u4ee5\u4e86\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 dataloader = DataLoader ( transformed_dataset , batch_size = 4 , shuffle = True , num_workers = 4 ) # Helper function to show a batch def show_landmarks_batch ( sample_batched ): \"\"\"Show image with landmarks for a batch of samples.\"\"\" images_batch , landmarks_batch = \\ sample_batched [ 'image' ], sample_batched [ 'landmarks' ] batch_size = len ( images_batch ) im_size = images_batch . size ( 2 ) grid = utils . make_grid ( images_batch ) plt . imshow ( grid . numpy () . transpose (( 1 , 2 , 0 ))) for i in range ( batch_size ): plt . scatter ( landmarks_batch [ i , :, 0 ] . numpy () + i * im_size , landmarks_batch [ i , :, 1 ] . numpy (), s = 10 , marker = '.' , c = 'r' ) plt . title ( 'Batch from dataloader' ) for i_batch , sample_batched in enumerate ( dataloader ): print ( i_batch , sample_batched [ 'image' ] . size (), sample_batched [ 'landmarks' ] . size ()) # observe 4th batch and stop. if i_batch == 3 : plt . figure () show_landmarks_batch ( sample_batched ) plt . axis ( 'off' ) plt . ioff () plt . show () break 1 2 3 4 0 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2]) 1 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2]) 2 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2]) 3 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2]) \u540e\u8bb0\uff1atorchvision \u5728\u8fd9\u4e2a\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5df2\u7ecf\u770b\u5230\u4e86\u5982\u4f55\u7f16\u5199\u548c\u4f7f\u7528 Dataset , transform \u548c Dataloader \u3002 torchvision \u5305\u63d0\u4f9b\u4e86\u4e00\u4e9b\u5e38\u7528\u7684\u6570\u636e\u96c6\u548c\u53d8\u6362\u65b9\u6cd5\uff0c\u7528\u5b83\uff0c\u751a\u81f3\u53ef\u80fd\u4e0d\u7528\u7f16\u5199\u81ea\u5df1\u7684\u7c7b\u3002 torchvision \u4e2d\u8fd8\u6709\u66f4\u901a\u7528\u7684\u6570\u636e\u96c6\u7c7b\u578b\uff0c\u5176\u4e2d\u4e00\u4e2a\u662f ImageFolder \u3002 \u5b83\u5047\u5b9a\u56fe\u50cf\u6309\u4ee5\u4e0b\u65b9\u5f0f\u7ec4\u7ec7\uff1a 1 2 3 4 5 6 7 8 9 root/ants/xxx.png root/ants/xxy.jpeg root/ants/xxz.png . . . root/bees/123.jpg root/bees/nsdf3.png root/bees/asd932_.png \u5176\u4e2d\uff0c\u2019ants\u2019\u3001\u2019bees\u2019\u7b49\u662f\u5206\u7c7b\u6807\u7b7e\u3002 \u5b83\u5177\u5907\u7c7b\u4f3c PIL.Image \u7684\u901a\u7528\u53d8\u6362\u65b9\u6cd5\uff0c\u50cf RandomHorizontalFlip \u3001 Scale \u3002 \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u5b83\u4eec\u6765\u7f16\u5199\u50cf\u8fd9\u6837\u7684 Dataloader \uff1a 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import torch from torchvision import transforms , datasets data_transform = transforms . Compose ([ transforms . RandomSizedCrop ( 224 ), transforms . RandomHorizontalFlip (), transforms . ToTensor (), transforms . Normalize ( mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ]) ]) hymenoptera_dataset = datasets . ImageFolder ( root = 'hymenoptera_data/train' , transform = data_transform ) dataset_loader = torch . utils . data . DataLoader ( hymenoptera_dataset , batch_size = 4 , shuffle = True , num_workers = 4 ) \u6709\u5173\u6559\u7a0b\u7684\u4ee3\u7801\u793a\u4f8b\uff0c\u8bf7\u53c2\u9605 \u8fc1\u79fb\u5b66\u4e60\u6559\u7a0b \u3002","title":"\u6570\u636e\u52a0\u8f7d\u548c\u5904\u7406\u6559\u7a0b"},{"location":"beginner/data_loading_tutorial/#_1","text":"\u4f5c\u8005 \uff1a Sasank Chilamkurthy \u5bf9\u4e8e\u4efb\u4f55\u673a\u5668\u5b66\u4e60\u7684\u95ee\u9898\uff0c\u90fd\u4f1a\u6709\u5927\u91cf\u7684\u52aa\u529b\u88ab\u7528\u5728\u51c6\u5907\u6570\u636e\u4e0a\u9762\u3002 PyTorch\u63d0\u4f9b\u4e86\u8bb8\u591a\u5de5\u5177\u6765\u7b80\u5316\u6570\u636e\u52a0\u8f7d\u3001\u589e\u5f3a\u4ee3\u7801\u53ef\u8bfb\u6027\u3002 \u7740\u8fd9\u4e2a\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u770b\u5230\u5982\u4f55\u5728\u4e00\u4e2a\u975e\u51e1\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u52a0\u8f7d\u3001\u9884\u5904\u7406\u548c\u6269\u5145\u3002 \u8981\u8fd0\u884c\u8fd9\u4e2a\u6559\u7a0b\uff0c\u786e\u5b9a\u5b89\u88c5\u4e86\u4ee5\u4e0b\u5305\uff1a scikit-image \uff1a\u7528\u4e8e\u56fe\u50cfIO\u548c\u53d8\u6362 pandas \uff1a\u7528\u4e8e\u7b80\u5316CVS\u89e3\u6790 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from __future__ import print_function , division import os import torch import pandas as pd from skimage import io , transform import numpy as np import matplotlib.pyplot as plt from torch.utils.data import Dataset , DataLoader from torchvision import transforms , utils # Ignore warnings import warnings warnings . filterwarnings ( \"ignore\" ) plt . ion () # interactive mode \u6211\u4eec\u8981\u5904\u7406\u7684\u6570\u636e\u96c6\u662f\u4eba\u8138\u59ff\u6001\u3002 \u9762\u90e8\u6807\u6ce8\u5c31\u50cf\u8fd9\u6837\uff1a \u603b\u4e4b\uff0c\u6bcf\u4e2a\u9762\u90e8\u90fd\u6ce8\u91ca\u4e8668\u4e2a\u4e0d\u540c\u7684\u5173\u952e\u7279\u5f81\u70b9(landmark)\u3002 \u6ce8\u610f \uff1a \u4ece https://download.pytorch.org/tutorial/faces.zip \u4e0b\u8f7d\u8fd9\u4e2a\u6570\u636e\u96c6\uff0c\u56fe\u7247\u5728\u76ee\u5f55 data/faces/ \u3002 \u8fd9\u4e2a\u6570\u636e\u96c6\u5b9e\u9645\u4e0a\u662f\u901a\u8fc7\u5bf9\u6765\u81eaimagenet\u6807\u6ce8\u4e3a\u201dface\u201d\u7684\u4e00\u4e9b\u56fe\u50cf\u5e94\u7528 dlib \u8fd9\u4e00\u4f18\u79c0\u7684\u59ff\u6001\u4f30\u8ba1\u6cd5\u5f97\u6765\u7684\u3002 \u8fd9\u4e2a\u6570\u636e\u96c6\u91cc\u6709\u4e00\u4e2a\u5e26\u6709\u6807\u6ce8\u7684CSV\u6587\u4ef6\uff0c\u5b83\u770b\u8d77\u6765\u662f\u8fd9\u6837\u7684\uff1a image_name part_0_x part_0_y part_1_x part_1_y part_2_x \u2026 part_67_x part_67_y 0805personali01.jpg 27 83 27 98 \u2026 \u2026 84 134 1084239450_e76e00b7e7.jpg 70 236 71 257 \u2026 \u2026 128 312 \u8bfb\u53d6CSV\u5e76\u628a\u6807\u6ce8\u6570\u636e\u653e\u5230(N,2)\u6570\u7ec4\u4e2d\uff0c\u5176\u4e2dN\u662f\u5173\u952e\u7279\u5f81\u70b9\u7684\u6570\u91cf\u3002 1 2 3 4 5 6 7 8 9 10 landmarks_frame = pd . read_csv ( 'data/faces/face_landmarks.csv' ) n = 65 img_name = landmarks_frame . iloc [ n , 0 ] landmarks = landmarks_frame . iloc [ n , 1 :] . as_matrix () landmarks = landmarks . astype ( 'float' ) . reshape ( - 1 , 2 ) print ( 'Image name: {}' . format ( img_name )) print ( 'Landmarks shape: {}' . format ( landmarks . shape )) print ( 'First 4 Landmarks: {}' . format ( landmarks [: 4 ])) 1 2 3 4 5 6 Image name: person-7.jpg Landmarks shape: (68, 2) First 4 Landmarks: [[32. 65.] [33. 76.] [34. 86.] [34. 97.]] \u7f16\u5199\u4e00\u4e2a\u53ef\u4ee5\u663e\u793a\u56fe\u50cf\u53ca\u5176\u5173\u952e\u7279\u5f81\u70b9\u7684\u7b80\u5355\u8f85\u52a9\u51fd\u6570\uff0c\u7528\u5b83\u663e\u793a\u6837\u672c\u3002 1 2 3 4 5 6 7 8 9 def show_landmarks ( image , landmarks ): \"\"\"Show image with landmarks\"\"\" plt . imshow ( image ) plt . scatter ( landmarks [:, 0 ], landmarks [:, 1 ], s = 10 , marker = '.' , c = 'r' ) plt . pause ( 0.001 ) # pause a bit so that plots are updated plt . figure () show_landmarks ( io . imread ( os . path . join ( 'data/faces/' , img_name )), landmarks ) plt . show ()","title":"\u6570\u636e\u52a0\u8f7d\u548c\u5904\u7406\u6559\u7a0b"},{"location":"beginner/data_loading_tutorial/#dataset","text":"torch.utils.data.Dataset \u662f\u9488\u5bf9\u6570\u636e\u96c6\u7684\u62bd\u8c61\u7c7b. \u6211\u4eec\u81ea\u5df1\u7684\u6570\u636e\u96c6\u7c7b\u5e94\u8be5\u7ee7\u627f\u81ea Dataset \uff0c\u8986\u76d6\u4ee5\u4e0b\u65b9\u6cd5\uff1a __len__ \uff1a\u7528 len(dataset) \u53ef\u4ee5\u8fd4\u56de\u6570\u636e\u96c6\u7684\u5927\u5c0f\u3002 __getitem__ \uff1a\u4f7f\u5f97\u50cf dataset[i] \u8fd9\u6837\u7684\u7d22\u5f15\u64cd\u4f5c\u53ef\u4ee5\u7528\u4e8e\u8bbf\u95ee\u7b2c i i \u4e2a\u6837\u672c \u73b0\u5728\uff0c\u4e3a\u6211\u4eec\u81ea\u5df1\u7684\u9762\u90e8\u5173\u952e\u7279\u5f81\u70b9\u6570\u636e\u96c6\u521b\u5efa\u4e00\u4e2a Dataset \u7c7b\u3002 \u6211\u4eec\u5728 __init__ \u8bfb\u53d6CSV\uff0c\u4f46\u662f\u628a\u8bfb\u53d6\u56fe\u7247\u7684\u5de5\u4f5c\u7559\u7ed9 __getitem__ \u3002 \u8fd9\u6837\u53ef\u4ee5\u8282\u7701\u5185\u5b58\uff0c\u56e0\u4e3a\u56fe\u7247\u4e0d\u662f\u4e00\u6b21\u6027\u7684\u5168\u90e8\u8bfb\u8fdb\u5185\u5b58\uff0c\u800c\u662f\u9700\u8981\u65f6\u624d\u8bfb\u53d6\u3002 \u6211\u4eec\u7684\u6570\u636e\u96c6\u4e2d\u7684\u6837\u672c\u5c06\u662f\u4e00\u4e2a dict \uff1a 1 { 'image' : image , 'landmarks' : landmarks } \u6211\u4eec\u7684\u6570\u636e\u96c6\u5c06\u63d0\u4f9b\u4e00\u4e2a\u53ef\u9009\u7684\u53c2\u6570 transform \uff0c\u8fd9\u6837\u4efb\u4f55\u9700\u8981\u7684\u5904\u7406\u8fc7\u7a0b\u53ef\u4ee5\u5728\u6837\u672c\u4e0a\u6267\u884c\u3002 \u5728\u4e0b\u4e00\u8282\u53ef\u4ee5\u770b\u5230 transform \u7684\u4f5c\u7528\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class FaceLandmarksDataset ( Dataset ): \"\"\"Face Landmarks dataset.\"\"\" def __init__ ( self , csv_file , root_dir , transform = None ): \"\"\" Args: csv_file (string): Path to the csv file with annotations. root_dir (string): Directory with all the images. transform (callable, optional): Optional transform to be applied on a sample. \"\"\" self . landmarks_frame = pd . read_csv ( csv_file ) self . root_dir = root_dir self . transform = transform def __len__ ( self ): return len ( self . landmarks_frame ) def __getitem__ ( self , idx ): img_name = os . path . join ( self . root_dir , self . landmarks_frame . iloc [ idx , 0 ]) image = io . imread ( img_name ) landmarks = self . landmarks_frame . iloc [ idx , 1 :] . as_matrix () landmarks = landmarks . astype ( 'float' ) . reshape ( - 1 , 2 ) sample = { 'image' : image , 'landmarks' : landmarks } if self . transform : sample = self . transform ( sample ) return sample \u5b9e\u4f8b\u5316\u8fd9\u4e2a\u7c7b\u5e76\u4e14\u904d\u5386\u6570\u636e\u6837\u672c\uff0c\u6253\u5370\u59344\u4e2a\u6837\u672c\u7684\u5927\u5c0f\u548c\u5173\u952e\u7279\u5f81\u70b9\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 face_dataset = FaceLandmarksDataset ( csv_file = 'data/faces/face_landmarks.csv' , root_dir = 'data/faces/' ) fig = plt . figure () for i , sample in enumerate ( face_dataset ): print ( i , sample [ 'image' ] . shape , sample [ 'landmarks' ] . shape ) ax = plt . subplot ( 1 , 4 , i + 1 ) plt . tight_layout () ax . set_title ( 'Sample #{}' . format ( i )) ax . axis ( 'off' ) show_landmarks ( ** sample ) if i == 3 : plt . show () break 1 0 (324, 215, 3) (68, 2) 1 1 (500, 333, 3) (68, 2) 1 2 (250, 258, 3) (68, 2) 1 3 (434, 290, 3) (68, 2)","title":"Dataset \u7c7b"},{"location":"beginner/data_loading_tutorial/#_2","text":"\u89c2\u5bdf\u4e0a\u9762\u7684\u6837\u672c\uff0c\u6211\u4eec\u53d1\u73b0\u4e00\u4e2a\u95ee\u9898\u2014\u2014\u56fe\u50cf\u4e0d\u662f\u540c\u6837\u7684\u5927\u5c0f\u3002 \u5927\u591a\u6570\u795e\u7ecf\u7f51\u7edc\u90fd\u671f\u5f85\u56fa\u5b9a\u5927\u5c0f\u7684\u56fe\u50cf\u3002 \u56e0\u6b64\uff0c\u9700\u8981\u7f16\u5199\u4e00\u4e9b\u9884\u5904\u7406\u4ee3\u7801\u3002 \u6211\u4eec\u521b\u5efa\u4e09\u79cd\u53d8\u6362\uff1a Rescale : \u7f29\u653e\u56fe\u50cf RandomCrop : \u968f\u673a\u88c1\u526a\u56fe\u50cf\u3002\u8fd9\u7b97\u662f\u6570\u636e\u589e\u52a0\u6269\u5145\u3002 ToTensor : \u5c06numpy\u56fe\u50cf\u8f6c\u4e3atorch\u56fe\u50cf\uff08\u9700\u8981\u4ea4\u6362\u5750\u6807\u8f74\uff09\u3002 \u628a\u4ed6\u4eec\u7f16\u5199\u4e3a\u53ef\u8c03\u7528\u7c7b\uff0c\u800c\u4e0d\u662f\u51fd\u6570\uff0c\u8fd9\u6837\uff0c\u6bcf\u6b21\u8c03\u7528\u5c31\u4e0d\u7528\u4f20\u9012\u53d8\u6362\u7684\u53c2\u6570\u3002 \u4e3a\u6b64\uff0c\u6211\u4eec\u9700\u8981\u5b9e\u73b0 __call__ \u65b9\u6cd5\uff0c\u5982\u679c\u6709\u5fc5\u8981\uff0c\u8fd8\u6709 __init__ \u65b9\u6cd5\u3002 \u7136\u540e\u50cf\u8fd9\u6837\u8fdb\u884c\u53d8\u6362\uff1a 1 2 tsfm = Transform ( params ) transformed_sample = tsfm ( sample ) \u8bf7\u6ce8\u610f\u4ee5\u4e0b\u8fd9\u4e9b\u53d8\u6362\u5982\u4f55\u88ab\u5e94\u7528\u4e8e\u56fe\u50cf\u548c\u5173\u952e\u7279\u5f81\u70b9\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 class Rescale ( object ): \"\"\"Rescale the image in a sample to a given size. Args: output_size (tuple or int): Desired output size. If tuple, output is matched to output_size. If int, smaller of image edges is matched to output_size keeping aspect ratio the same. \"\"\" def __init__ ( self , output_size ): assert isinstance ( output_size , ( int , tuple )) self . output_size = output_size def __call__ ( self , sample ): image , landmarks = sample [ 'image' ], sample [ 'landmarks' ] h , w = image . shape [: 2 ] if isinstance ( self . output_size , int ): if h > w : new_h , new_w = self . output_size * h / w , self . output_size else : new_h , new_w = self . output_size , self . output_size * w / h else : new_h , new_w = self . output_size new_h , new_w = int ( new_h ), int ( new_w ) img = transform . resize ( image , ( new_h , new_w )) # h and w are swapped for landmarks because for images, # x and y axes are axis 1 and 0 respectively landmarks = landmarks * [ new_w / w , new_h / h ] return { 'image' : img , 'landmarks' : landmarks } class RandomCrop ( object ): \"\"\"Crop randomly the image in a sample. Args: output_size (tuple or int): Desired output size. If int, square crop is made. \"\"\" def __init__ ( self , output_size ): assert isinstance ( output_size , ( int , tuple )) if isinstance ( output_size , int ): self . output_size = ( output_size , output_size ) else : assert len ( output_size ) == 2 self . output_size = output_size def __call__ ( self , sample ): image , landmarks = sample [ 'image' ], sample [ 'landmarks' ] h , w = image . shape [: 2 ] new_h , new_w = self . output_size top = np . random . randint ( 0 , h - new_h ) left = np . random . randint ( 0 , w - new_w ) image = image [ top : top + new_h , left : left + new_w ] landmarks = landmarks - [ left , top ] return { 'image' : image , 'landmarks' : landmarks } class ToTensor ( object ): \"\"\"Convert ndarrays in sample to Tensors.\"\"\" def __call__ ( self , sample ): image , landmarks = sample [ 'image' ], sample [ 'landmarks' ] # swap color axis because # numpy image: H x W x C # torch image: C X H X W image = image . transpose (( 2 , 0 , 1 )) return { 'image' : torch . from_numpy ( image ), 'landmarks' : torch . from_numpy ( landmarks )}","title":"\u53d8\u6362"},{"location":"beginner/data_loading_tutorial/#_3","text":"\u4e0b\u9762\uff0c\u5728\u6837\u672c\u4e0a\u5e94\u7528\u8f6c\u6362\u3002 \u5047\u8bbe\u6211\u4eec\u60f3\u8981\u5c06\u56fe\u50cf\u7684\u77ed\u8fb9\u91cd\u65b0\u7f29\u653e\u5230256\uff0c\u7136\u540e\u4ece\u4e2d\u968f\u673a\u88c1\u526a\u4e00\u4e2a224\u7684\u6b63\u65b9\u5f62\u3002 \u4e5f\u5c31\u662f\u8bf4\uff0c\u6211\u4eec\u9700\u8981\u7ec4\u5408 Rescale \u548c RandomCrop \u8f6c\u6362\u3002 torchvision.transforms.Compose \u662f\u4e00\u4e2a\u7b80\u5355\u7684\u53ef\u8c03\u7528\u7c7b\uff0c\u5b83\u7528\u4e8e\u5b8c\u6210\u8fd9\u6837\u7684\u5de5\u4f5c\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 scale = Rescale ( 256 ) crop = RandomCrop ( 128 ) composed = transforms . Compose ([ Rescale ( 256 ), RandomCrop ( 224 )]) # Apply each of the above transforms on sample. fig = plt . figure () sample = face_dataset [ 65 ] for i , tsfrm in enumerate ([ scale , crop , composed ]): transformed_sample = tsfrm ( sample ) ax = plt . subplot ( 1 , 3 , i + 1 ) plt . tight_layout () ax . set_title ( type ( tsfrm ) . __name__ ) show_landmarks ( ** transformed_sample ) plt . show ()","title":"\u7ec4\u5408\u8f6c\u6362"},{"location":"beginner/data_loading_tutorial/#_4","text":"\u628a\u4e0a\u9762\u7684\u4f60\u540c\u653e\u5728\u4e00\u8d77\uff0c\u521b\u5efa\u4e00\u4e2a\u5e26\u6709\u7ec4\u5408\u53d8\u6362\u7684\u6570\u636e\u96c6\u3002 \u603b\u800c\u8a00\u4e4b\uff0c\u6bcf\u6b21\u4ece\u8fd9\u4e2a\u6570\u636e\u96c6\u91c7\u7528\u7684\u65f6\u5019\uff1a \u4ece\u6587\u4ef6\u4e2d\u5373\u65f6\u7684\u8bfb\u53d6\u56fe\u50cf \u5bf9\u8bfb\u53d6\u7684\u56fe\u50cf\u8fdb\u884c\u53d8\u6362 \u7531\u4e8e\u5176\u4e2d\u4e00\u4e2a\u53d8\u5316\u662f\u968f\u673a\u7684\uff0c\u6570\u636e\u5728\u91c7\u6837\u65f6\u662f\u6269\u5145\u7684 \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u4e4b\u524d\u7528\u8fc7\u7684 for ... in \u5faa\u73af\u904d\u5386\u6570\u636e\u96c6\u3002 1 2 3 4 5 6 7 8 9 10 11 12 transformed_dataset = FaceLandmarksDataset ( csv_file = 'data/faces/face_landmarks.csv' , root_dir = 'data/faces/' , transform = transforms . Compose ([ Rescale ( 256 ), RandomCrop ( 224 ), ToTensor () ])) for i , sample in enumerate ( transformed_dataset ): print ( i , sample [ 'image' ] . size (), sample [ 'landmarks' ] . size ()) if i == 3 : break 1 2 3 4 0 torch.Size([3, 224, 224]) torch.Size([68, 2]) 1 torch.Size([3, 224, 224]) torch.Size([68, 2]) 2 torch.Size([3, 224, 224]) torch.Size([68, 2]) 3 torch.Size([3, 224, 224]) torch.Size([68, 2]) \u4e0d\u8fc7\uff0c\u7b80\u5355\u7684 for \u5faa\u73af\u904d\u5386\u6570\u636e\u8ba9\u6211\u4eec\u4e22\u5931\u4e86\u4e0d\u5c11\u7279\u6027\uff0c\u7279\u522b\u662f\uff1a \u6570\u636e\u7684\u6279\u5904\u7406 \u4e71\u5e8f\u6392\u5217\u6570\u636e \u4f7f\u7528 multiprocessing \u5e76\u884c\u52a0\u8f7d\u6570\u636e\u3002 torch.utils.data.DataLoader \u662f\u4e00\u4e2a\u8fed\u4ee3\u5668\uff0c\u5b83\u63d0\u4f9b\u4e86\u6240\u6709\u8fd9\u4e9b\u7279\u6027\u3002 \u4e0b\u9762\u7684\u53c2\u6570\u5e94\u8be5\u6e05\u9664\u7684\u8bf4\u660e\u4e86\u95ee\u9898\u3002 \u5176\u4e2d\u4e00\u4e2a\u6709\u8da3\u7684\u53c2\u6570\u662f collate_fn \uff0c\u53ef\u4ee5\u6307\u5b9a\u54ea\u4e9b\u5177\u4f53\u7684\u6837\u672c\u9700\u8981\u5728\u6279\u6b21\u4e2d\u4f7f\u7528 collate_fn \u8fdb\u884c\u6574\u7406\u3002 \u5f53\u7136\uff0c\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u9ed8\u8ba4\u7684\u6574\u7406(collat)\u5c31\u53ef\u4ee5\u4e86\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 dataloader = DataLoader ( transformed_dataset , batch_size = 4 , shuffle = True , num_workers = 4 ) # Helper function to show a batch def show_landmarks_batch ( sample_batched ): \"\"\"Show image with landmarks for a batch of samples.\"\"\" images_batch , landmarks_batch = \\ sample_batched [ 'image' ], sample_batched [ 'landmarks' ] batch_size = len ( images_batch ) im_size = images_batch . size ( 2 ) grid = utils . make_grid ( images_batch ) plt . imshow ( grid . numpy () . transpose (( 1 , 2 , 0 ))) for i in range ( batch_size ): plt . scatter ( landmarks_batch [ i , :, 0 ] . numpy () + i * im_size , landmarks_batch [ i , :, 1 ] . numpy (), s = 10 , marker = '.' , c = 'r' ) plt . title ( 'Batch from dataloader' ) for i_batch , sample_batched in enumerate ( dataloader ): print ( i_batch , sample_batched [ 'image' ] . size (), sample_batched [ 'landmarks' ] . size ()) # observe 4th batch and stop. if i_batch == 3 : plt . figure () show_landmarks_batch ( sample_batched ) plt . axis ( 'off' ) plt . ioff () plt . show () break 1 2 3 4 0 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2]) 1 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2]) 2 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2]) 3 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])","title":"\u904d\u5386\u6570\u636e\u96c6"},{"location":"beginner/data_loading_tutorial/#torchvision","text":"\u5728\u8fd9\u4e2a\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5df2\u7ecf\u770b\u5230\u4e86\u5982\u4f55\u7f16\u5199\u548c\u4f7f\u7528 Dataset , transform \u548c Dataloader \u3002 torchvision \u5305\u63d0\u4f9b\u4e86\u4e00\u4e9b\u5e38\u7528\u7684\u6570\u636e\u96c6\u548c\u53d8\u6362\u65b9\u6cd5\uff0c\u7528\u5b83\uff0c\u751a\u81f3\u53ef\u80fd\u4e0d\u7528\u7f16\u5199\u81ea\u5df1\u7684\u7c7b\u3002 torchvision \u4e2d\u8fd8\u6709\u66f4\u901a\u7528\u7684\u6570\u636e\u96c6\u7c7b\u578b\uff0c\u5176\u4e2d\u4e00\u4e2a\u662f ImageFolder \u3002 \u5b83\u5047\u5b9a\u56fe\u50cf\u6309\u4ee5\u4e0b\u65b9\u5f0f\u7ec4\u7ec7\uff1a 1 2 3 4 5 6 7 8 9 root/ants/xxx.png root/ants/xxy.jpeg root/ants/xxz.png . . . root/bees/123.jpg root/bees/nsdf3.png root/bees/asd932_.png \u5176\u4e2d\uff0c\u2019ants\u2019\u3001\u2019bees\u2019\u7b49\u662f\u5206\u7c7b\u6807\u7b7e\u3002 \u5b83\u5177\u5907\u7c7b\u4f3c PIL.Image \u7684\u901a\u7528\u53d8\u6362\u65b9\u6cd5\uff0c\u50cf RandomHorizontalFlip \u3001 Scale \u3002 \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u5b83\u4eec\u6765\u7f16\u5199\u50cf\u8fd9\u6837\u7684 Dataloader \uff1a 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import torch from torchvision import transforms , datasets data_transform = transforms . Compose ([ transforms . RandomSizedCrop ( 224 ), transforms . RandomHorizontalFlip (), transforms . ToTensor (), transforms . Normalize ( mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ]) ]) hymenoptera_dataset = datasets . ImageFolder ( root = 'hymenoptera_data/train' , transform = data_transform ) dataset_loader = torch . utils . data . DataLoader ( hymenoptera_dataset , batch_size = 4 , shuffle = True , num_workers = 4 ) \u6709\u5173\u6559\u7a0b\u7684\u4ee3\u7801\u793a\u4f8b\uff0c\u8bf7\u53c2\u9605 \u8fc1\u79fb\u5b66\u4e60\u6559\u7a0b \u3002","title":"\u540e\u8bb0\uff1atorchvision"},{"location":"beginner/blitz/autograd_tutorial/","text":"Autograd: \u81ea\u52a8\u5fae\u5206(Automatic differentiation) autograd \u5305 \u662f PyTorch \u4e2d\u6240\u6709\u795e\u7ecf\u7f51\u7edc\u7684\u4e2d\u5fc3\u3002 \u6211\u4eec\u9996\u5148\u7b80\u8981\u67e5\u770b\uff0c\u7136\u540e\u8bad\u7ec3\u7b2c\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u3002 autograd \u5305\u4e3a Tensor \u4e0a\u7684\u6240\u6709\u8fd0\u7b97\u63d0\u4f9b\u81ea\u52a8\u5fae\u5206(automatic differentiation)\u3002 \u5b83\u662f\u4e00\u4e2a\u201c\u8fd0\u884c\u4e2d\u5b9a\u4e49\u201d(define-by-run)\u7684\u6846\u67b6\uff0c\u8fd9\u610f\u5473\u7740\u53cd\u5411\u4f20\u64ad(backpropagation)\u7b97\u6cd5\u7531\u4ee3\u7801\u5728\u8fd0\u884c\u65f6\u5b9a\u4e49\uff0c\u5e76\u4e14\u6bcf\u4e2a\u8fed\u4ee3\u90fd\u53ef\u4ee5\u4e0d\u540c\u3002 \u6211\u4eec\u901a\u8fc7\u66f4\u7b80\u5355\u7684\u672f\u8bed\u7684\u4f8b\u5b50\u6765\u770b\u770b\u8fd9\u4e9b\u3002 \u5f20\u91cf(Tensor) torch.Tensor \u662f\u8fd9\u4e2a\u5305\u7684\u4e2d\u5fc3\u7c7b\u3002 \u5c06\u5b83\u7684 .requires_grad \u5c5e\u6027\u8bbe\u7f6e\u4e3a True \uff0c\u5373\u5f00\u59cb\u8ddf\u8e2a\u5176\u4e0a\u7684\u6240\u6709\u8fd0\u7b97\u3002 \u5b8c\u6210\u8ba1\u7b97\u540e\u8c03\u7528 .backward() \u5373\u53ef\u81ea\u52a8\u5b8c\u6210\u68af\u5ea6\u8ba1\u7b97\u3002 \u8be5 tensor \u68af\u5ea6\u7684\u7d2f\u8ba1\u548c\u8bb0\u5f55\u5728 .grad \u5c5e\u6027\u3002 \u8981\u505c\u6b62 tensor \u7684\u5386\u53f2\u8ddf\u8e2a\uff0c\u53ef\u8c03\u7528 .detach() \uff0c\u5c06\u5b83\u4ece\u8ba1\u7b97\u5386\u53f2\u4e2d\u5206\u79bb\u51fa\u6765\uff0c\u9632\u6b62\u5b83\u5728\u5c06\u6765\u7684\u8ba1\u7b97\u4e2d\u88ab\u8ddf\u8e2a\u3002 \u8981\u9632\u6b62\u5386\u53f2\u8ddf\u8e2a\uff08\u548c\u5185\u5b58\u7684\u4f7f\u7528\uff09\uff0c\u8fd8\u53ef\u4ee5\u5c06\u4ee3\u7801\u5757\u653e\u7f6e\u5728 with torch.no_grad(): \u4e2d\u3002 \u8fd9\u5728\u8bc4\u4f30\u6a21\u578b\u65f6\u5c24\u5176\u6709\u7528\uff0c\u56e0\u4e3a\u6a21\u578b\u53ef\u80fd\u5177\u6709\u53ef\u8bad\u7ec3\u7684\u53c2\u6570 requires_grad=True \uff0c\u4f46\u8fdb\u884c\u8bc4\u4f30\u65f6\u4e0d\u9700\u8981\u68af\u5ea6\u3002 \u8fd8\u6709\u4e00\u4e2a\u7c7b\u5bf9\u4e8e autograd \u5b9e\u73b0\u975e\u5e38\u91cd\u8981\u2014\u2014 Function \u3002 Tensor \u548c Function \u4e92\u76f8\u8fde\u63a5\u5e76\u6784\u5efa\u4e00\u4e2a\u975e\u5faa\u73af\u56fe\uff0c\u5b83\u5c06\u5b8c\u6574\u7684\u8ba1\u7b97\u5386\u53f2\u8fdb\u884c\u4e86\u7f16\u7801\u8bb0\u5f55\u3002 \u6bcf\u4e2a tensor \u90fd\u6709\u4e00\u4e2a .grad_fn \u5c5e\u6027\uff0c\u8be5\u5c5e\u6027\u6307\u5411\u521b\u5efa\u8fd9\u4e2a Tensor \u7684 Function \uff08\u9664\u4e86\u7528\u6237\u521b\u5efa\u7684 Tensor\u2014\u2014\u4ed6\u4eec\u7684 grand_fn \u662f None \uff09 \u5982\u9700\u8ba1\u7b97\u5bfc\u6570(derivatives)\uff0c\u53ef\u4ee5\u8c03\u7528 Tensor \u7684 .backward() \u65b9\u6cd5\u3002 \u5982\u679c Tensor \u662f\u6807\u91cf\uff08\u5373\u5b83\u5305\u53ea\u542b\u5355\u5143\u7d20\u6570\u636e\uff09\uff0c\u5219 backward() \u4e0d\u9700\u8981\u6307\u5b9a\u53c2\u6570\uff0c\u4f46\u662f\u5982\u679c\u5b83\u6709\u591a\u4e2a\u5143\u7d20\uff0c\u5219\u9700\u8981\u6307\u5b9a\u4e00\u4e2a\u5f62\u72b6\u5339\u914d\u7684 tensor \u4f5c\u4e3a gradient \u53c2\u6570\u3002 1 import torch \u521b\u5efa\u4e00\u4e2a tensor\uff0c\u8bbe\u7f6e requires_grad=True \u8ddf\u8e2a\u5b83\u7684\u8ba1\u7b97 1 2 x = torch . ones ( 2 , 2 , requires_grad = True ) print ( x ) 1 2 tensor([[1., 1.], [1., 1.]], requires_grad=True) \u8fdb\u884c\u4e00\u6b21 tensor \u8fd0\u7b97\uff1a 1 2 y = x + 2 print ( y ) 1 2 tensor([[3., 3.], [3., 3.]], grad_fn=<AddBackward0>) y \u662f\u8fd9\u4e2a\u8fd0\u7b97\u7684\u7ed3\u679c\u5f20\u91cf\uff0c\u6240\u4ee5\u5b83\u4e5f\u6709 grad_fn \u3002 1 print ( y . grad_fn ) 1 <AddBackward0 object at 0x7f10a069dd68> \u5bf9 y \u505a\u66f4\u591a\u8fd0\u7b97 1 2 3 4 z = y * y * 3 out = z . mean () print ( z , out ) 1 2 tensor([[27., 27.], [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward1>) .requires_grad_( ... ) \u539f\u4f4d\u6539\u53d8\u4e86\u5df2\u6709 Tensor \u7684 requires_grad \u6807\u5fd7\u3002\u8be5\u8f93\u5165\u6807\u5fd7\u7684\u9ed8\u8ba4\u503c\u662f False \u3002 1 2 3 4 5 6 7 a = torch . randn ( 2 , 2 ) a = (( a * 3 ) / ( a - 1 )) print ( a . requires_grad ) a . requires_grad_ ( True ) print ( a . requires_grad ) b = ( a * a ) . sum () print ( b . grad_fn ) 1 2 3 False True <SumBackward0 object at 0x7f0ff5fc2240> \u68af\u5ea6(Gradients) \u73b0\u5728\u8fdb\u884c\u53cd\u5411\u4f20\u64ad(backprop)\u3002 \u7531\u4e8e out \u5305\u542b\u5355\u4e2a\u6807\u91cf\uff0c out.backward() \u76f8\u5f53\u4e8e out.backward(torch.tensor(1.)) \u3002 1 out . backward () \u8f93\u51fa\u68af\u5ea6 d(out)/dx 1 print ( x . grad ) 1 2 tensor([[4.5000, 4.5000], [4.5000, 4.5000]]) \u5f97\u5230\u4e00\u4e2a 4.5 \u77e9\u9635\u3002 \u5c06 out Tensor \u8bbe\u4e3a \u201c o o \u201d\u3002 \u6211\u4eec\u6709 o = \\frac{1}{4}\\sum_i z_i o = \\frac{1}{4}\\sum_i z_i z_i = 3(x_i+2)^2 z_i = 3(x_i+2)^2 \u4ee5\u53ca z_i\\bigr\\rvert_{x_i=1} = 27 z_i\\bigr\\rvert_{x_i=1} = 27 \u6240\u4ee5\uff0c \\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2) \\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2) \u90a3\u4e48 \\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5 \\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5 \u5728\u6570\u5b66\u4e0a\uff0c\u82e5\u6709\u5411\u91cf\u503c\u51fd\u6570 \\vec{y}=f(\\vec{x}) \\vec{y}=f(\\vec{x}) \uff0c \u5219 \\vec{y} \\vec{y} \u7684\u68af\u5ea6\u662f\u5173\u4e8e \\vec{x} \\vec{x} \u7684\u96c5\u53ef\u6bd4\u77e9\u9635\uff1a \\begin{align}J=\\left(\\begin{array}{ccc} \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}} \\end{array}\\right)\\end{align} \\begin{align}J=\\left(\\begin{array}{ccc} \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}} \\end{array}\\right)\\end{align} \u4e00\u822c\u6765\u8bf4\uff0c torch.autograd \u5c31\u662f\u4e00\u4e2a\u8ba1\u7b97\u5411\u91cf\u96c5\u53ef\u6bd4\u79ef\u7684\u5f15\u64ce\u3002 \u4e5f\u5c31\u662f\u8bf4\uff0c\u5bf9\u7ed9\u5b9a\u7684\u4efb\u610f\u5411\u91cf v=\\left(\\begin{array}{cccc} v_{1} & v_{2} & \\cdots & v_{m}\\end{array}\\right)^{T} v=\\left(\\begin{array}{cccc} v_{1} & v_{2} & \\cdots & v_{m}\\end{array}\\right)^{T} \uff0c \u8ba1\u7b97 v^{T}\\cdot J v^{T}\\cdot J \u7684\u79ef\u3002 \u5982\u679c v v \u6070\u597d\u662f\u6807\u91cf\u51fd\u6570 l=g\\left(\\vec{y}\\right) l=g\\left(\\vec{y}\\right) \u7684\u68af\u5ea6\uff0c\u6362\u800c\u8a00\u4e4b\uff0c v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T} v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T} \uff0c \u90a3\u4e48\uff0c\u7531\u94fe\u89c4\u5219(chain rule)\u53ef\u77e5\uff0c\u5411\u91cf\u96c5\u53ef\u6bd4\u79ef\u662f l l \u5173\u4e8e \\vec{x} \\vec{x} \u7684\u68af\u5ea6\uff1a \\begin{align}J^{T}\\cdot v=\\left(\\begin{array}{ccc} \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}} \\end{array}\\right)\\left(\\begin{array}{c} \\frac{\\partial l}{\\partial y_{1}}\\\\ \\vdots\\\\ \\frac{\\partial l}{\\partial y_{m}} \\end{array}\\right)=\\left(\\begin{array}{c} \\frac{\\partial l}{\\partial x_{1}}\\\\ \\vdots\\\\ \\frac{\\partial l}{\\partial x_{n}} \\end{array}\\right)\\end{align} \\begin{align}J^{T}\\cdot v=\\left(\\begin{array}{ccc} \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}} \\end{array}\\right)\\left(\\begin{array}{c} \\frac{\\partial l}{\\partial y_{1}}\\\\ \\vdots\\\\ \\frac{\\partial l}{\\partial y_{m}} \\end{array}\\right)=\\left(\\begin{array}{c} \\frac{\\partial l}{\\partial x_{1}}\\\\ \\vdots\\\\ \\frac{\\partial l}{\\partial x_{n}} \\end{array}\\right)\\end{align} (\u6ce8\u610f v^{T}\\cdot J v^{T}\\cdot J \u7ed9\u51fa\u4e86\u4e00\u4e2a\u884c\u5411\u91cf\uff0c\u5b83\u53ef\u88ab\u89c6\u4f5c J^{T}\\cdot v J^{T}\\cdot v \u5217\u5411\u91cf\u3002) \u5411\u91cf\u96c5\u53ef\u6bd4\u79ef\u7684\u8fd9\u4e9b\u6027\u8d28\u4f7f\u5f97\u5c06\u5916\u90e8\u68af\u5ea6\u9001\u5230\u975e\u6807\u91cf\u8f93\u51fa\u6a21\u578b\u4e2d\u53d8\u5f97\u975e\u5e38\u65b9\u4fbf\u3002 \u73b0\u5728\uff0c\u6211\u4eec\u770b\u4e00\u4e2a\u5411\u91cf\u96c5\u53ef\u6bd4\u79ef\u7684\u4f8b\u5b50\uff1a 1 2 3 4 5 6 7 x = torch . randn ( 3 , requires_grad = True ) y = x * 2 while y . data . norm () < 1000 : y = y * 2 print ( y ) 1 tensor([-977.1336, 264.0834, -116.1632], grad_fn=<MulBackward0>) \u73b0\u5728\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c y \u4e0d\u518d\u662f\u6807\u91cf\u4e86\u3002 torch.autograd \u65e0\u6cd5\u76f4\u63a5\u8ba1\u7b97\u5b8c\u6574\u7684\u5411\u91cf\u96c5\u53ef\u6bd4\u884c\u5217\u5f0f\uff0c \u4f46\u662f\u5982\u679c\u6211\u4eec\u53ea\u662f\u9700\u8981\u5411\u91cf-\u5411\u91cf-\u96c5\u53ef\u6bd4\u79ef, \u53ea\u8981\u628a\u5411\u91cf\u4f20\u7ed9\u53c2\u6570 backward : 1 2 3 4 v = torch . tensor ([ 0.1 , 1.0 , 0.0001 ], dtype = torch . float ) y . backward ( v ) print ( x . grad ) 1 tensor([2.0480e+02, 2.0480e+03, 2.0480e-01]) \u53e6\u5916\uff0c\u7528 .requires_grad=True: \u5305\u542b\u4ee3\u7801\u5757\u53ef\u4ee5\u8ba9 autograd \u505c\u6b62\u8ddf\u8e2a Tensor \u7684\u5386\u53f2 1 2 3 4 5 print ( x . requires_grad ) print (( x ** 2 ) . requires_grad ) with torch . no_grad (): print (( x ** 2 ) . requires_grad ) 1 2 3 True True False \u5ef6\u540e\u9605\u8bfb: autograd \u548c Function \u7684\u6587\u6863\u5728 https://pytorch.org/docs/autograd","title":"Autograd\uff1a\u81ea\u52a8\u5fae\u5206"},{"location":"beginner/blitz/autograd_tutorial/#autograd-automatic-differentiation","text":"autograd \u5305 \u662f PyTorch \u4e2d\u6240\u6709\u795e\u7ecf\u7f51\u7edc\u7684\u4e2d\u5fc3\u3002 \u6211\u4eec\u9996\u5148\u7b80\u8981\u67e5\u770b\uff0c\u7136\u540e\u8bad\u7ec3\u7b2c\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u3002 autograd \u5305\u4e3a Tensor \u4e0a\u7684\u6240\u6709\u8fd0\u7b97\u63d0\u4f9b\u81ea\u52a8\u5fae\u5206(automatic differentiation)\u3002 \u5b83\u662f\u4e00\u4e2a\u201c\u8fd0\u884c\u4e2d\u5b9a\u4e49\u201d(define-by-run)\u7684\u6846\u67b6\uff0c\u8fd9\u610f\u5473\u7740\u53cd\u5411\u4f20\u64ad(backpropagation)\u7b97\u6cd5\u7531\u4ee3\u7801\u5728\u8fd0\u884c\u65f6\u5b9a\u4e49\uff0c\u5e76\u4e14\u6bcf\u4e2a\u8fed\u4ee3\u90fd\u53ef\u4ee5\u4e0d\u540c\u3002 \u6211\u4eec\u901a\u8fc7\u66f4\u7b80\u5355\u7684\u672f\u8bed\u7684\u4f8b\u5b50\u6765\u770b\u770b\u8fd9\u4e9b\u3002","title":"Autograd: \u81ea\u52a8\u5fae\u5206(Automatic differentiation)"},{"location":"beginner/blitz/autograd_tutorial/#tensor","text":"torch.Tensor \u662f\u8fd9\u4e2a\u5305\u7684\u4e2d\u5fc3\u7c7b\u3002 \u5c06\u5b83\u7684 .requires_grad \u5c5e\u6027\u8bbe\u7f6e\u4e3a True \uff0c\u5373\u5f00\u59cb\u8ddf\u8e2a\u5176\u4e0a\u7684\u6240\u6709\u8fd0\u7b97\u3002 \u5b8c\u6210\u8ba1\u7b97\u540e\u8c03\u7528 .backward() \u5373\u53ef\u81ea\u52a8\u5b8c\u6210\u68af\u5ea6\u8ba1\u7b97\u3002 \u8be5 tensor \u68af\u5ea6\u7684\u7d2f\u8ba1\u548c\u8bb0\u5f55\u5728 .grad \u5c5e\u6027\u3002 \u8981\u505c\u6b62 tensor \u7684\u5386\u53f2\u8ddf\u8e2a\uff0c\u53ef\u8c03\u7528 .detach() \uff0c\u5c06\u5b83\u4ece\u8ba1\u7b97\u5386\u53f2\u4e2d\u5206\u79bb\u51fa\u6765\uff0c\u9632\u6b62\u5b83\u5728\u5c06\u6765\u7684\u8ba1\u7b97\u4e2d\u88ab\u8ddf\u8e2a\u3002 \u8981\u9632\u6b62\u5386\u53f2\u8ddf\u8e2a\uff08\u548c\u5185\u5b58\u7684\u4f7f\u7528\uff09\uff0c\u8fd8\u53ef\u4ee5\u5c06\u4ee3\u7801\u5757\u653e\u7f6e\u5728 with torch.no_grad(): \u4e2d\u3002 \u8fd9\u5728\u8bc4\u4f30\u6a21\u578b\u65f6\u5c24\u5176\u6709\u7528\uff0c\u56e0\u4e3a\u6a21\u578b\u53ef\u80fd\u5177\u6709\u53ef\u8bad\u7ec3\u7684\u53c2\u6570 requires_grad=True \uff0c\u4f46\u8fdb\u884c\u8bc4\u4f30\u65f6\u4e0d\u9700\u8981\u68af\u5ea6\u3002 \u8fd8\u6709\u4e00\u4e2a\u7c7b\u5bf9\u4e8e autograd \u5b9e\u73b0\u975e\u5e38\u91cd\u8981\u2014\u2014 Function \u3002 Tensor \u548c Function \u4e92\u76f8\u8fde\u63a5\u5e76\u6784\u5efa\u4e00\u4e2a\u975e\u5faa\u73af\u56fe\uff0c\u5b83\u5c06\u5b8c\u6574\u7684\u8ba1\u7b97\u5386\u53f2\u8fdb\u884c\u4e86\u7f16\u7801\u8bb0\u5f55\u3002 \u6bcf\u4e2a tensor \u90fd\u6709\u4e00\u4e2a .grad_fn \u5c5e\u6027\uff0c\u8be5\u5c5e\u6027\u6307\u5411\u521b\u5efa\u8fd9\u4e2a Tensor \u7684 Function \uff08\u9664\u4e86\u7528\u6237\u521b\u5efa\u7684 Tensor\u2014\u2014\u4ed6\u4eec\u7684 grand_fn \u662f None \uff09 \u5982\u9700\u8ba1\u7b97\u5bfc\u6570(derivatives)\uff0c\u53ef\u4ee5\u8c03\u7528 Tensor \u7684 .backward() \u65b9\u6cd5\u3002 \u5982\u679c Tensor \u662f\u6807\u91cf\uff08\u5373\u5b83\u5305\u53ea\u542b\u5355\u5143\u7d20\u6570\u636e\uff09\uff0c\u5219 backward() \u4e0d\u9700\u8981\u6307\u5b9a\u53c2\u6570\uff0c\u4f46\u662f\u5982\u679c\u5b83\u6709\u591a\u4e2a\u5143\u7d20\uff0c\u5219\u9700\u8981\u6307\u5b9a\u4e00\u4e2a\u5f62\u72b6\u5339\u914d\u7684 tensor \u4f5c\u4e3a gradient \u53c2\u6570\u3002 1 import torch \u521b\u5efa\u4e00\u4e2a tensor\uff0c\u8bbe\u7f6e requires_grad=True \u8ddf\u8e2a\u5b83\u7684\u8ba1\u7b97 1 2 x = torch . ones ( 2 , 2 , requires_grad = True ) print ( x ) 1 2 tensor([[1., 1.], [1., 1.]], requires_grad=True) \u8fdb\u884c\u4e00\u6b21 tensor \u8fd0\u7b97\uff1a 1 2 y = x + 2 print ( y ) 1 2 tensor([[3., 3.], [3., 3.]], grad_fn=<AddBackward0>) y \u662f\u8fd9\u4e2a\u8fd0\u7b97\u7684\u7ed3\u679c\u5f20\u91cf\uff0c\u6240\u4ee5\u5b83\u4e5f\u6709 grad_fn \u3002 1 print ( y . grad_fn ) 1 <AddBackward0 object at 0x7f10a069dd68> \u5bf9 y \u505a\u66f4\u591a\u8fd0\u7b97 1 2 3 4 z = y * y * 3 out = z . mean () print ( z , out ) 1 2 tensor([[27., 27.], [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward1>) .requires_grad_( ... ) \u539f\u4f4d\u6539\u53d8\u4e86\u5df2\u6709 Tensor \u7684 requires_grad \u6807\u5fd7\u3002\u8be5\u8f93\u5165\u6807\u5fd7\u7684\u9ed8\u8ba4\u503c\u662f False \u3002 1 2 3 4 5 6 7 a = torch . randn ( 2 , 2 ) a = (( a * 3 ) / ( a - 1 )) print ( a . requires_grad ) a . requires_grad_ ( True ) print ( a . requires_grad ) b = ( a * a ) . sum () print ( b . grad_fn ) 1 2 3 False True <SumBackward0 object at 0x7f0ff5fc2240>","title":"\u5f20\u91cf(Tensor)"},{"location":"beginner/blitz/autograd_tutorial/#gradients","text":"\u73b0\u5728\u8fdb\u884c\u53cd\u5411\u4f20\u64ad(backprop)\u3002 \u7531\u4e8e out \u5305\u542b\u5355\u4e2a\u6807\u91cf\uff0c out.backward() \u76f8\u5f53\u4e8e out.backward(torch.tensor(1.)) \u3002 1 out . backward () \u8f93\u51fa\u68af\u5ea6 d(out)/dx 1 print ( x . grad ) 1 2 tensor([[4.5000, 4.5000], [4.5000, 4.5000]]) \u5f97\u5230\u4e00\u4e2a 4.5 \u77e9\u9635\u3002 \u5c06 out Tensor \u8bbe\u4e3a \u201c o o \u201d\u3002 \u6211\u4eec\u6709 o = \\frac{1}{4}\\sum_i z_i o = \\frac{1}{4}\\sum_i z_i z_i = 3(x_i+2)^2 z_i = 3(x_i+2)^2 \u4ee5\u53ca z_i\\bigr\\rvert_{x_i=1} = 27 z_i\\bigr\\rvert_{x_i=1} = 27 \u6240\u4ee5\uff0c \\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2) \\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2) \u90a3\u4e48 \\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5 \\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5 \u5728\u6570\u5b66\u4e0a\uff0c\u82e5\u6709\u5411\u91cf\u503c\u51fd\u6570 \\vec{y}=f(\\vec{x}) \\vec{y}=f(\\vec{x}) \uff0c \u5219 \\vec{y} \\vec{y} \u7684\u68af\u5ea6\u662f\u5173\u4e8e \\vec{x} \\vec{x} \u7684\u96c5\u53ef\u6bd4\u77e9\u9635\uff1a \\begin{align}J=\\left(\\begin{array}{ccc} \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}} \\end{array}\\right)\\end{align} \\begin{align}J=\\left(\\begin{array}{ccc} \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}} \\end{array}\\right)\\end{align} \u4e00\u822c\u6765\u8bf4\uff0c torch.autograd \u5c31\u662f\u4e00\u4e2a\u8ba1\u7b97\u5411\u91cf\u96c5\u53ef\u6bd4\u79ef\u7684\u5f15\u64ce\u3002 \u4e5f\u5c31\u662f\u8bf4\uff0c\u5bf9\u7ed9\u5b9a\u7684\u4efb\u610f\u5411\u91cf v=\\left(\\begin{array}{cccc} v_{1} & v_{2} & \\cdots & v_{m}\\end{array}\\right)^{T} v=\\left(\\begin{array}{cccc} v_{1} & v_{2} & \\cdots & v_{m}\\end{array}\\right)^{T} \uff0c \u8ba1\u7b97 v^{T}\\cdot J v^{T}\\cdot J \u7684\u79ef\u3002 \u5982\u679c v v \u6070\u597d\u662f\u6807\u91cf\u51fd\u6570 l=g\\left(\\vec{y}\\right) l=g\\left(\\vec{y}\\right) \u7684\u68af\u5ea6\uff0c\u6362\u800c\u8a00\u4e4b\uff0c v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T} v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T} \uff0c \u90a3\u4e48\uff0c\u7531\u94fe\u89c4\u5219(chain rule)\u53ef\u77e5\uff0c\u5411\u91cf\u96c5\u53ef\u6bd4\u79ef\u662f l l \u5173\u4e8e \\vec{x} \\vec{x} \u7684\u68af\u5ea6\uff1a \\begin{align}J^{T}\\cdot v=\\left(\\begin{array}{ccc} \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}} \\end{array}\\right)\\left(\\begin{array}{c} \\frac{\\partial l}{\\partial y_{1}}\\\\ \\vdots\\\\ \\frac{\\partial l}{\\partial y_{m}} \\end{array}\\right)=\\left(\\begin{array}{c} \\frac{\\partial l}{\\partial x_{1}}\\\\ \\vdots\\\\ \\frac{\\partial l}{\\partial x_{n}} \\end{array}\\right)\\end{align} \\begin{align}J^{T}\\cdot v=\\left(\\begin{array}{ccc} \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}} \\end{array}\\right)\\left(\\begin{array}{c} \\frac{\\partial l}{\\partial y_{1}}\\\\ \\vdots\\\\ \\frac{\\partial l}{\\partial y_{m}} \\end{array}\\right)=\\left(\\begin{array}{c} \\frac{\\partial l}{\\partial x_{1}}\\\\ \\vdots\\\\ \\frac{\\partial l}{\\partial x_{n}} \\end{array}\\right)\\end{align} (\u6ce8\u610f v^{T}\\cdot J v^{T}\\cdot J \u7ed9\u51fa\u4e86\u4e00\u4e2a\u884c\u5411\u91cf\uff0c\u5b83\u53ef\u88ab\u89c6\u4f5c J^{T}\\cdot v J^{T}\\cdot v \u5217\u5411\u91cf\u3002) \u5411\u91cf\u96c5\u53ef\u6bd4\u79ef\u7684\u8fd9\u4e9b\u6027\u8d28\u4f7f\u5f97\u5c06\u5916\u90e8\u68af\u5ea6\u9001\u5230\u975e\u6807\u91cf\u8f93\u51fa\u6a21\u578b\u4e2d\u53d8\u5f97\u975e\u5e38\u65b9\u4fbf\u3002 \u73b0\u5728\uff0c\u6211\u4eec\u770b\u4e00\u4e2a\u5411\u91cf\u96c5\u53ef\u6bd4\u79ef\u7684\u4f8b\u5b50\uff1a 1 2 3 4 5 6 7 x = torch . randn ( 3 , requires_grad = True ) y = x * 2 while y . data . norm () < 1000 : y = y * 2 print ( y ) 1 tensor([-977.1336, 264.0834, -116.1632], grad_fn=<MulBackward0>) \u73b0\u5728\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c y \u4e0d\u518d\u662f\u6807\u91cf\u4e86\u3002 torch.autograd \u65e0\u6cd5\u76f4\u63a5\u8ba1\u7b97\u5b8c\u6574\u7684\u5411\u91cf\u96c5\u53ef\u6bd4\u884c\u5217\u5f0f\uff0c \u4f46\u662f\u5982\u679c\u6211\u4eec\u53ea\u662f\u9700\u8981\u5411\u91cf-\u5411\u91cf-\u96c5\u53ef\u6bd4\u79ef, \u53ea\u8981\u628a\u5411\u91cf\u4f20\u7ed9\u53c2\u6570 backward : 1 2 3 4 v = torch . tensor ([ 0.1 , 1.0 , 0.0001 ], dtype = torch . float ) y . backward ( v ) print ( x . grad ) 1 tensor([2.0480e+02, 2.0480e+03, 2.0480e-01]) \u53e6\u5916\uff0c\u7528 .requires_grad=True: \u5305\u542b\u4ee3\u7801\u5757\u53ef\u4ee5\u8ba9 autograd \u505c\u6b62\u8ddf\u8e2a Tensor \u7684\u5386\u53f2 1 2 3 4 5 print ( x . requires_grad ) print (( x ** 2 ) . requires_grad ) with torch . no_grad (): print (( x ** 2 ) . requires_grad ) 1 2 3 True True False \u5ef6\u540e\u9605\u8bfb: autograd \u548c Function \u7684\u6587\u6863\u5728 https://pytorch.org/docs/autograd","title":"\u68af\u5ea6(Gradients)"},{"location":"beginner/blitz/cifar10_tutorial/","text":"\u8bad\u7ec3\u5206\u7c7b\u5668 \u6211\u4eec\u5df2\u7ecf\u4e86\u89e3\u4e86\u5982\u4f55\u5b9a\u4e49\u795e\u7ecf\u7f51\u7edc\uff0c\u8ba1\u7b97\u635f\u8017\u5e76\u66f4\u65b0\u7f51\u7edc\u6743\u91cd\u3002 \u73b0\u5728\u4f60\u53ef\u80fd\u5728\u60f3\uff1a \u6709\u5173\u6570\u636e\u7684\u95ee\u9898 \u901a\u5e38\uff0c\u5f53\u5904\u7406\u56fe\u50cf\u3001\u6587\u5b57\u3001\u97f3\u9891\u6216\u8005\u89c6\u9891\u6570\u636e\u7684\u65f6\u5019\uff0c\u53ef\u4ee5\u4f7f\u7528\u6807\u51c6\u7684python\u5305\u5c06\u6570\u636e\u52a0\u8f7d\u5230numpy\u6570\u7ec4\u4e2d\u3002\u7136\u540e\u53ef\u4ee5\u5c06\u8fd9\u4e2a\u6570\u7ec4\u8f6c\u6362\u6210\u4e00\u4e2a torch.*Tensor \u3002 \u5bf9\u4e8e\u56fe\u50cf\uff0cPillow\uff0cOpenCV\u7b49\u8f6f\u4ef6\u5305\u5f88\u6709\u7528 \u5bf9\u4e8e\u97f3\u9891\uff0cscipy\uff0clibrosa\u7b49\u5305\u5f88\u6709\u7528 \u5bf9\u4e8e\u6587\u5b57\uff0c\u76f4\u63a5\u4f7f\u7528Python\u6216\u8005Cython\u52a0\u8f7d\uff0c\u6216\u8005NLTK\u548cSpaCY\u90fd\u53ef\u884c \u89c6\u89c9\u5f88\u7279\u6b8a\uff0c\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a torchvision \u7684\u5305\uff0c\u5b83\u63d0\u4f9b\u4e86\u6570\u636e\u52a0\u8f7d\u5668 torchvision.datasets \uff0c\u53ef\u4ee5\u4ece\u5e38\u89c1\u7684\u6570\u636e\u96c6\u52a0\u8f7d\u6570\u636e\uff0c\u5982Imagenet, CIFAR10, MNIST \u7b49\uff1b\u8fd8\u63d0\u4f9b\u7528\u4e8e\u56fe\u50cf\u7684\u6570\u636e\u8f6c\u6362\u5668 torch.utils.data.DataLoader \u3002 \u8fd9\u63d0\u4f9b\u4e86\u6781\u5927\u7684\u4fbf\u5229\u800c\u4e14\u80fd\u591f\u907f\u514d\u7f16\u5199\u6837\u677f\u4ee3\u7801\u3002 \u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u4f7f\u7528CIFAR10\u6570\u636e\u96c6\u3002\u5b83\u6709\u7c7b\uff1a\u2018airplane\u2019, \u2018automobile\u2019, \u2018bird\u2019, \u2018cat\u2019, \u2018deer\u2019, \u2018dog\u2019, \u2018frog\u2019, \u2018horse\u2019, \u2018ship\u2019,\u2018truck\u2019\u3002 CIFAR-10\u4e2d\u7684\u56fe\u50cf\u5c3a\u5bf8\u4e3a3x32x32\uff0c\u5373\u5c3a\u5bf8\u4e3a32x32\u50cf\u7d20\u76843\u901a\u9053\u5f69\u8272\u56fe\u50cf\u3002 cifar10 \u8bad\u7ec3\u56fe\u50cf\u5206\u7c7b\u5668 \u6211\u4eec\u5c06\u6309\u987a\u5e8f\u6267\u884c\u4ee5\u4e0b\u6b65\u9aa4\uff1a \u7528 torchvision \u52a0\u8f7d\u5e76\u89c4\u683c\u5316CIFAR10\u7684\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u96c6 \u5b9a\u4e49\u5377\u53ca\u795e\u7ecf\u7f51\u7edc(CNN) \u5b9a\u4e49\u635f\u5931\u51fd\u6570 \u4f7f\u7528\u8bad\u7ec3\u6570\u636e\u8bad\u7ec3\u7f51\u7edc \u4f7f\u7528\u6d4b\u8bd5\u6570\u636e\u6d4b\u8bd5\u7f51\u7edc 1. \u52a0\u8f7d\u5e76\u89c4\u683c\u5316CIFAR10 \u7528 torchvision \u52a0\u8f7dCIFAR10\u975e\u5e38\u5bb9\u6613\u3002 1 2 3 4 5 6 7 from time import time import torch import torchvision import torchvision.transforms as transforms from tqdm import tqdm torchvision\u6570\u636e\u96c6\u7684\u8f93\u51fa\u662f [0,1] \u533a\u95f4PILImage\u56fe\u50cf\u3002\u6211\u4eec\u628a\u5b83\u4eec\u8f6c\u6362\u4e3a\u89c4\u683c\u5316 [-1, 1] \u533a\u95f4\u7684\u5f20\u91cf\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.5 , 0.5 , 0.5 ), ( 0.5 , 0.5 , 0.5 ))]) trainset = torchvision . datasets . CIFAR10 ( root = './data' , train = True , download = True , transform = transform ) trainloader = torch . utils . data . DataLoader ( trainset , batch_size = 4 , shuffle = True , num_workers = 2 ) testset = torchvision . datasets . CIFAR10 ( root = './data' , train = False , download = True , transform = transform ) testloader = torch . utils . data . DataLoader ( testset , batch_size = 4 , shuffle = False , num_workers = 2 ) classes = ( 'plane' , 'car' , 'bird' , 'cat' , 'deer' , 'dog' , 'frog' , 'horse' , 'ship' , 'truck' ) 1 2 Files already downloaded and verified Files already downloaded and verified \u6211\u4eec\u6765\u5c55\u793a\u4e00\u4e9b\u8bad\u7ec3\u56fe\u50cf\uff0c\u633a\u6709\u8da3\u7684\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import matplotlib.pyplot as plt import numpy as np # functions to show an image def imshow ( img ): img = img / 2 + 0.5 # unnormalize npimg = img . numpy () plt . imshow ( np . transpose ( npimg , ( 1 , 2 , 0 ))) # get some random training images dataiter = iter ( trainloader ) images , labels = dataiter . next () # show images imshow ( torchvision . utils . make_grid ( images )) # print labels print ( ' ' . join ( ' %5s ' % classes [ labels [ j ]] for j in range ( 4 ))) 1 truck horse dog deer 2. \u5b9a\u4e49\u5377\u53ca\u795e\u7ecf\u7f51\u7edc(CNN) \u4ece\u524d\u6587\u7684\u795e\u7ecf\u7f51\u7edc\u7ae0\u8282\u590d\u5236\u4ee3\u7801\uff0c\u4fee\u6539\uff0c\u8ba9\u5b83\u91c7\u75283\u901a\u9053\u56fe\u50cf(\u800c\u4e0d\u662f\u4e4b\u524d\u5b9a\u4e49\u76841\u901a\u9053) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import torch.nn as nn import torch.nn.functional as F class Net ( nn . Module ): def __init__ ( self ): super ( Net , self ) . __init__ () self . conv1 = nn . Conv2d ( 3 , 6 , 5 ) self . pool = nn . MaxPool2d ( 2 , 2 ) self . conv2 = nn . Conv2d ( 6 , 16 , 5 ) self . fc1 = nn . Linear ( 16 * 5 * 5 , 120 ) self . fc2 = nn . Linear ( 120 , 84 ) self . fc3 = nn . Linear ( 84 , 10 ) def forward ( self , x ): x = self . pool ( F . relu ( self . conv1 ( x ))) x = self . pool ( F . relu ( self . conv2 ( x ))) x = x . view ( - 1 , 16 * 5 * 5 ) x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x net = Net () 3. \u5b9a\u4e49\u635f\u5931\u51fd\u6570 \u4f7f\u7528\u4ea4\u53c9\u71b5\u635f\u5931\u548c\u5e26\u6709\u52a8\u91cf\u7684SGD\u3002 1 2 3 4 import torch.optim as optim criterion = nn . CrossEntropyLoss () optimizer = optim . SGD ( net . parameters (), lr = 0.001 , momentum = 0.9 ) 4. \u8bad\u7ec3\u7f51\u7edc \u4e8b\u60c5\u5f00\u59cb\u53d8\u5f97\u6709\u8da3\u4e86\u3002 \u6211\u4eec\u53ea\u9700\u5faa\u73af\u904d\u5386\u6570\u636e\u8fed\u4ee3\u5668\uff0c\u5e76\u5c06\u8f93\u5165\u63d0\u4f9b\u7ed9\u7f51\u7edc\u5e76\u8fdb\u884c\u4f18\u5316\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 for epoch in range ( 2 ): # loop over the dataset multiple times ts = time () running_loss = 0.0 for i , data in enumerate ( trainloader , 0 ): # get the inputs inputs , labels = data # zero the parameter gradients optimizer . zero_grad () # forward + backward + optimize outputs = net ( inputs ) loss = criterion ( outputs , labels ) loss . backward () optimizer . step () # print statistics running_loss += loss . item () if i % 2000 == 1999 : # print every 2000 mini-batches print ( '[ %d , %5d ] loss: %.3f ' % ( epoch + 1 , i + 1 , running_loss / 2000 )) running_loss = 0.0 print ( 'Epoch {0} finished in {1:.3f} second(s)' . format ( epoch , time () - ts )) print ( 'Finished Training' ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 [1, 2000] loss: 2.237 [1, 4000] loss: 1.909 [1, 6000] loss: 1.723 [1, 8000] loss: 1.601 [1, 10000] loss: 1.531 [1, 12000] loss: 1.477 Epoch 0 finished in 75.642 second(s) [2, 2000] loss: 1.404 [2, 4000] loss: 1.372 [2, 6000] loss: 1.347 [2, 8000] loss: 1.353 [2, 10000] loss: 1.296 [2, 12000] loss: 1.263 Epoch 1 finished in 72.981 second(s) Finished Training 5. \u4f7f\u7528\u6d4b\u8bd5\u6570\u636e\u5bf9\u7f51\u7edc\u8fdb\u884c\u6d4b\u8bd5 \u5728\u8bad\u7ec3\u6570\u636e\u96c6\u4e0a\u5df2\u7ecf\u5bf9\u7f51\u7edc\u8fdb\u884c\u4e86\u4e24\u4e2a\u7eaa\u5143(epoch)\u7684\u8bad\u7ec3\u3002 \u4f46\u662f\u6211\u4eec\u8fd8\u8981\u8fdb\u884c\u68c0\u67e5\uff0c\u770b\u770b\u8fd9\u4e2a\u7f51\u7edc\u662f\u5426\u771f\u7684\u5b66\u4f1a\u4e86\u4ec0\u4e48\u3002 \u7528\u8fd9\u4e2a\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u9884\u6d4b\uff0c\u68c0\u67e5\u5b83\u8f93\u51fa\u7684\u5206\u7c7b\u6807\u7b7e\u662f\u5426\u548c\u5b9e\u9645\u503c\u4e00\u76f4\u3002\u5982\u679c\u9884\u6d4b\u6b63\u786e\uff0c\u628a\u6837\u672c\u52a0\u5230\u6b63\u786e\u9884\u6d4b\u5217\u8868\u3002 \u597d\u4e86\uff0c\u7b2c\u4e00\u6b65\u5148\u4ece\u6d4b\u8bd5\u96c6\u663e\u793a\u4e00\u4e2a\u56fe\u50cf\uff0c\u719f\u6089\u4e00\u4e0b\u3002 1 2 3 4 5 6 dataiter = iter ( testloader ) images , labels = dataiter . next () # print images imshow ( torchvision . utils . make_grid ( images )) print ( 'GroundTruth: ' , ' ' . join ( ' %5s ' % classes [ labels [ j ]] for j in range ( 4 ))) 1 GroundTruth : cat ship ship plane \u597d\u7684\uff0c\u73b0\u5728\u8ba9\u6211\u4eec\u770b\u770b\u795e\u7ecf\u7f51\u7edc\u8ba4\u4e3a\u4e0a\u9762\u8fd9\u4e9b\u4f8b\u5b50\u662f\u4ec0\u4e48\uff1a 1 outputs = net ( images ) \u8f93\u51fa\u503c\u662f10\u91cd\u7c7b\u578b\u7684\u80fd\u91cf\u503c\u3002 \u4e00\u4e2a\u7c7b\u578b\u7684\u80fd\u91cf\u503c\u8d8a\u9ad8\uff0c\u5c31\u8868\u660e\u7f51\u8def\u8d8a\u503e\u5411\u4e8e\u8ba4\u4e3a\u56fe\u7247\u662f\u8fd9\u79cd\u7c7b\u578b\u7684\u3002 \u90a3\u4e48\uff0c\u8ba9\u6211\u4eec\u5f97\u5230\u6700\u9ad8\u80fd\u91cf\u503c\u7684\u5e8f\u53f7\uff1a 1 2 3 4 _ , predicted = torch . max ( outputs , 1 ) print ( 'Predicted: ' , ' ' . join ( ' %5s ' % classes [ predicted [ j ]] for j in range ( 4 ))) 1 Predicted : cat ship ship ship \u7ed3\u679c\u4f3c\u4e4e\u8fd8\u884c\u3002 \u8ba9\u6211\u4eec\u770b\u770b\u8fd9\u4e2a\u7f51\u7edc\u5982\u4f55\u9884\u6d4b\u6574\u4e2a\u6d4b\u8bd5\u6570\u636e\u96c6\u3002 1 2 3 4 5 6 7 8 9 10 11 12 correct = 0 total = 0 with torch . no_grad (): for data in tqdm ( testloader ): images , labels = data outputs = net ( images ) _ , predicted = torch . max ( outputs . data , 1 ) total += labels . size ( 0 ) correct += ( predicted == labels ) . sum () . item () print ( 'Accuracy of the network on the 10000 test images: %d %% ' % ( 100 * correct / total )) 1 2 3 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [00:05<00:00, 420.29it/s] Accuracy of the network on the 10000 test images: 53 % \u770b\u8d77\u6765\u6bd4\u968f\u4fbf\u731c(\u4ece10\u4e2a\u7c7b\u578b\u4e2d\u968f\u673a\u9009\u4e00\u4e2a\u7684\u51c6\u786e\u5ea6\u662f10%)\u5f3a\u3002 \u8fd9\u4e2a\u7f51\u7edc\u4f3c\u4e4e\u5b66\u4f1a\u4e86\u4e00\u4e9b\u4e1c\u897f\u3002 \u55ef\uff0c\u54ea\u4e9b\u7c7b\u578b\u8868\u73b0\u597d\uff0c\u54ea\u4e9b\u4e0d\u597d\uff1a 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class_correct = list ( 0. for i in range ( 10 )) class_total = list ( 0. for i in range ( 10 )) with torch . no_grad (): for data in tqdm ( testloader ): images , labels = data outputs = net ( images ) _ , predicted = torch . max ( outputs , 1 ) c = ( predicted == labels ) . squeeze () for i in range ( 4 ): label = labels [ i ] class_correct [ label ] += c [ i ] . item () class_total [ label ] += 1 for i in range ( 10 ): print ( 'Accuracy of %5s : %2d %% ' % ( classes [ i ], 100 * class_correct [ i ] / class_total [ i ])) 1 2 3 4 5 6 7 8 9 10 11 12 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [00:05<00:00, 418.22it/s] Accuracy of plane : 29 % Accuracy of car : 68 % Accuracy of bird : 28 % Accuracy of cat : 42 % Accuracy of deer : 43 % Accuracy of dog : 45 % Accuracy of frog : 70 % Accuracy of horse : 56 % Accuracy of ship : 81 % Accuracy of truck : 73 % \u597d\u7684\uff0c\u63a5\u4e0b\u6765\u5462\uff1f \u6211\u4eec\u5982\u4f55\u5728GPU\u4e0a\u8fd0\u884c\u8fd9\u4e9b\u795e\u7ecf\u7f51\u7edc\uff1f \u5728GPU\u4e0a\u8bad\u7ec3 \u5c31\u50cf\u5c06Tensor\u8f6c\u5230GPU\u4e00\u6837\uff0c\u53ef\u5c06\u795e\u7ecf\u7f51\u7edc\u8f6c\u5230GPU\u3002 \u5982\u679c\u6709\u53ef\u7528\u7684CUDA\uff0c\u6211\u4eec\u9996\u5148\u83b7\u53d6\u7b2c\u4e00\u4e2a\u53ef\u89c1\u7684cuda\u8bbe\u5907\uff1a 1 2 3 4 5 device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) # Assuming that we are on a CUDA machine, this should print a CUDA device: print ( device ) 1 cuda : 0 \u672c\u8282\u7684\u5176\u4f59\u90e8\u5206\u5047\u5b9a device \u662f\u4e00\u4e2aCUDA\u8bbe\u5907\u3002 \u7136\u540e\u8fd9\u4e9b\u65b9\u6cd5\u5c06\u9012\u5f52\u904d\u5386\u6240\u6709\u6a21\u5757\u5e76\u5c06\u5176\u53c2\u6570\u548c\u7f13\u51b2\u533a\u8f6c\u6362\u4e3aCUDA\u5f20\u91cf\uff1a 1 net . to ( device ) \u8bb0\u4f4f\u8fd8\u5f97\u5c06\u6bcf\u4e00\u6b65\u7684\u8f93\u5165\u548c\u76ee\u6807\u53d1\u9001\u5230GPU\uff1a 1 inputs , labels = inputs . to ( device ), labels . to ( device ) \u53ef\u80fd\u89c2\u5bdf\u4e0d\u5230\u4e0eCPU\u76f8\u6bd4\u7684\u6709\u660e\u663e\u52a0\u901f\uff0c\u90a3\u662f\u56e0\u4e3a\u7684\u7f51\u7edc\u592a\u5c0f\u3002 \u7ec3\u4e60 \u5c1d\u8bd5\u589e\u52a0\u7f51\u7edc\u7684\u5bbd\u5ea6\uff08\u7b2c\u4e00\u4e2a nn.Conv2d \u7684\u7b2c\u4e8c\u4e2a\u53c2\u6570\uff0c\u7b2c\u4e8c\u4e2a nn.Conv2d \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u2014\u2014\u5b83\u4eec\u5e94\u662f\u76f8\u540c\u7684\u6570\u503c\uff09\uff0c\u770b\u770b\u80fd\u83b7\u5f97\u4ec0\u4e48\u6837\u7684\u52a0\u901f\u3002 \u8fbe\u5230\u7684\u76ee\u6807\uff1a \u5728\u9ad8\u5c42\u6b21\u4e0a\u7406\u89e3PyTorch\u7684Tensor\u5e93\u548c\u795e\u7ecf\u7f51\u7edc\u3002 \u8bad\u7ec3\u4e00\u4e2a\u5c0f\u578b\u7684\u56fe\u50cf\u5206\u7c7b\u795e\u7ecf\u7f51\u7edc 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 class Net2 ( nn . Module ): def __init__ ( self ): super () . __init__ () self . conv1 = nn . Conv2d ( 3 , 12 , 5 ) self . pool = nn . MaxPool2d ( 2 , 2 ) self . conv2 = nn . Conv2d ( 12 , 16 , 5 ) self . fc1 = nn . Linear ( 16 * 5 * 5 , 120 ) self . fc2 = nn . Linear ( 120 , 84 ) self . fc3 = nn . Linear ( 84 , 10 ) def forward ( self , x ): x = self . pool ( F . relu ( self . conv1 ( x ))) x = self . pool ( F . relu ( self . conv2 ( x ))) x = x . view ( - 1 , 16 * 5 * 5 ) x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x net2 = Net2 () net2 . to ( device ) for epoch in range ( 2 ): # loop over the dataset multiple times ts = time () running_loss = 0.0 for i , data in enumerate ( trainloader , 0 ): # get the inputs inputs , labels = data inputs , labels = inputs . to ( device ), labels . to ( device ) # zero the parameter gradients optimizer . zero_grad () # forward + backward + optimize outputs = net2 ( inputs ) loss = criterion ( outputs , labels ) loss . backward () optimizer . step () # print statistics running_loss += loss . item () if i % 2000 == 1999 : # print every 2000 mini-batches print ( '[ %d , %5d ] loss: %.3f ' % ( epoch + 1 , i + 1 , running_loss / 2000 )) running_loss = 0.0 print ( 'Epoch {0} finished in {1:.3f} second(s)' . format ( epoch , time () - ts )) print ( 'Finished Training' ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 [1, 2000] loss: 2.305 [1, 4000] loss: 2.304 [1, 6000] loss: 2.304 [1, 8000] loss: 2.304 [1, 10000] loss: 2.303 [1, 12000] loss: 2.305 Epoch 0 finished in 72.346 second(s) [2, 2000] loss: 2.304 [2, 4000] loss: 2.304 [2, 6000] loss: 2.304 [2, 8000] loss: 2.303 [2, 10000] loss: 2.304 [2, 12000] loss: 2.305 Epoch 1 finished in 63.276 second(s) Finished Training \u5728\u591a\u4e2aGPU\u4e0a\u8fdb\u884c\u8bad\u7ec3 \u5982\u679c\u60f3\u4f7f\u7528\u6240\u6709\u7684GPU\u6765\u5927\u5e45\u52a0\u901f\uff0c\u8bf7\u53c2\u8003 data_parallel_tutorial\u3002 \u4e0b\u4e00\u6b65\u600e\u4e48\u529e\uff1f :doc: Train neural nets to play video games </intermediate/reinforcement_q_learning> Train a state-of-the-art ResNet network on imagenet _ Train a face generator using Generative Adversarial Networks _ Train a word-level language model using Recurrent LSTM networks _ More examples _ More tutorials _ Discuss PyTorch on the Forums _ Chat with other users on Slack _ 1","title":"\u8bad\u7ec3\u5206\u7c7b\u5668"},{"location":"beginner/blitz/cifar10_tutorial/#_1","text":"\u6211\u4eec\u5df2\u7ecf\u4e86\u89e3\u4e86\u5982\u4f55\u5b9a\u4e49\u795e\u7ecf\u7f51\u7edc\uff0c\u8ba1\u7b97\u635f\u8017\u5e76\u66f4\u65b0\u7f51\u7edc\u6743\u91cd\u3002 \u73b0\u5728\u4f60\u53ef\u80fd\u5728\u60f3\uff1a","title":"\u8bad\u7ec3\u5206\u7c7b\u5668"},{"location":"beginner/blitz/cifar10_tutorial/#_2","text":"\u901a\u5e38\uff0c\u5f53\u5904\u7406\u56fe\u50cf\u3001\u6587\u5b57\u3001\u97f3\u9891\u6216\u8005\u89c6\u9891\u6570\u636e\u7684\u65f6\u5019\uff0c\u53ef\u4ee5\u4f7f\u7528\u6807\u51c6\u7684python\u5305\u5c06\u6570\u636e\u52a0\u8f7d\u5230numpy\u6570\u7ec4\u4e2d\u3002\u7136\u540e\u53ef\u4ee5\u5c06\u8fd9\u4e2a\u6570\u7ec4\u8f6c\u6362\u6210\u4e00\u4e2a torch.*Tensor \u3002 \u5bf9\u4e8e\u56fe\u50cf\uff0cPillow\uff0cOpenCV\u7b49\u8f6f\u4ef6\u5305\u5f88\u6709\u7528 \u5bf9\u4e8e\u97f3\u9891\uff0cscipy\uff0clibrosa\u7b49\u5305\u5f88\u6709\u7528 \u5bf9\u4e8e\u6587\u5b57\uff0c\u76f4\u63a5\u4f7f\u7528Python\u6216\u8005Cython\u52a0\u8f7d\uff0c\u6216\u8005NLTK\u548cSpaCY\u90fd\u53ef\u884c \u89c6\u89c9\u5f88\u7279\u6b8a\uff0c\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a torchvision \u7684\u5305\uff0c\u5b83\u63d0\u4f9b\u4e86\u6570\u636e\u52a0\u8f7d\u5668 torchvision.datasets \uff0c\u53ef\u4ee5\u4ece\u5e38\u89c1\u7684\u6570\u636e\u96c6\u52a0\u8f7d\u6570\u636e\uff0c\u5982Imagenet, CIFAR10, MNIST \u7b49\uff1b\u8fd8\u63d0\u4f9b\u7528\u4e8e\u56fe\u50cf\u7684\u6570\u636e\u8f6c\u6362\u5668 torch.utils.data.DataLoader \u3002 \u8fd9\u63d0\u4f9b\u4e86\u6781\u5927\u7684\u4fbf\u5229\u800c\u4e14\u80fd\u591f\u907f\u514d\u7f16\u5199\u6837\u677f\u4ee3\u7801\u3002 \u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u4f7f\u7528CIFAR10\u6570\u636e\u96c6\u3002\u5b83\u6709\u7c7b\uff1a\u2018airplane\u2019, \u2018automobile\u2019, \u2018bird\u2019, \u2018cat\u2019, \u2018deer\u2019, \u2018dog\u2019, \u2018frog\u2019, \u2018horse\u2019, \u2018ship\u2019,\u2018truck\u2019\u3002 CIFAR-10\u4e2d\u7684\u56fe\u50cf\u5c3a\u5bf8\u4e3a3x32x32\uff0c\u5373\u5c3a\u5bf8\u4e3a32x32\u50cf\u7d20\u76843\u901a\u9053\u5f69\u8272\u56fe\u50cf\u3002 cifar10","title":"\u6709\u5173\u6570\u636e\u7684\u95ee\u9898"},{"location":"beginner/blitz/cifar10_tutorial/#_3","text":"\u6211\u4eec\u5c06\u6309\u987a\u5e8f\u6267\u884c\u4ee5\u4e0b\u6b65\u9aa4\uff1a \u7528 torchvision \u52a0\u8f7d\u5e76\u89c4\u683c\u5316CIFAR10\u7684\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u96c6 \u5b9a\u4e49\u5377\u53ca\u795e\u7ecf\u7f51\u7edc(CNN) \u5b9a\u4e49\u635f\u5931\u51fd\u6570 \u4f7f\u7528\u8bad\u7ec3\u6570\u636e\u8bad\u7ec3\u7f51\u7edc \u4f7f\u7528\u6d4b\u8bd5\u6570\u636e\u6d4b\u8bd5\u7f51\u7edc","title":"\u8bad\u7ec3\u56fe\u50cf\u5206\u7c7b\u5668"},{"location":"beginner/blitz/cifar10_tutorial/#1-cifar10","text":"\u7528 torchvision \u52a0\u8f7dCIFAR10\u975e\u5e38\u5bb9\u6613\u3002 1 2 3 4 5 6 7 from time import time import torch import torchvision import torchvision.transforms as transforms from tqdm import tqdm torchvision\u6570\u636e\u96c6\u7684\u8f93\u51fa\u662f [0,1] \u533a\u95f4PILImage\u56fe\u50cf\u3002\u6211\u4eec\u628a\u5b83\u4eec\u8f6c\u6362\u4e3a\u89c4\u683c\u5316 [-1, 1] \u533a\u95f4\u7684\u5f20\u91cf\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.5 , 0.5 , 0.5 ), ( 0.5 , 0.5 , 0.5 ))]) trainset = torchvision . datasets . CIFAR10 ( root = './data' , train = True , download = True , transform = transform ) trainloader = torch . utils . data . DataLoader ( trainset , batch_size = 4 , shuffle = True , num_workers = 2 ) testset = torchvision . datasets . CIFAR10 ( root = './data' , train = False , download = True , transform = transform ) testloader = torch . utils . data . DataLoader ( testset , batch_size = 4 , shuffle = False , num_workers = 2 ) classes = ( 'plane' , 'car' , 'bird' , 'cat' , 'deer' , 'dog' , 'frog' , 'horse' , 'ship' , 'truck' ) 1 2 Files already downloaded and verified Files already downloaded and verified \u6211\u4eec\u6765\u5c55\u793a\u4e00\u4e9b\u8bad\u7ec3\u56fe\u50cf\uff0c\u633a\u6709\u8da3\u7684\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import matplotlib.pyplot as plt import numpy as np # functions to show an image def imshow ( img ): img = img / 2 + 0.5 # unnormalize npimg = img . numpy () plt . imshow ( np . transpose ( npimg , ( 1 , 2 , 0 ))) # get some random training images dataiter = iter ( trainloader ) images , labels = dataiter . next () # show images imshow ( torchvision . utils . make_grid ( images )) # print labels print ( ' ' . join ( ' %5s ' % classes [ labels [ j ]] for j in range ( 4 ))) 1 truck horse dog deer","title":"1. \u52a0\u8f7d\u5e76\u89c4\u683c\u5316CIFAR10"},{"location":"beginner/blitz/cifar10_tutorial/#2-cnn","text":"\u4ece\u524d\u6587\u7684\u795e\u7ecf\u7f51\u7edc\u7ae0\u8282\u590d\u5236\u4ee3\u7801\uff0c\u4fee\u6539\uff0c\u8ba9\u5b83\u91c7\u75283\u901a\u9053\u56fe\u50cf(\u800c\u4e0d\u662f\u4e4b\u524d\u5b9a\u4e49\u76841\u901a\u9053) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import torch.nn as nn import torch.nn.functional as F class Net ( nn . Module ): def __init__ ( self ): super ( Net , self ) . __init__ () self . conv1 = nn . Conv2d ( 3 , 6 , 5 ) self . pool = nn . MaxPool2d ( 2 , 2 ) self . conv2 = nn . Conv2d ( 6 , 16 , 5 ) self . fc1 = nn . Linear ( 16 * 5 * 5 , 120 ) self . fc2 = nn . Linear ( 120 , 84 ) self . fc3 = nn . Linear ( 84 , 10 ) def forward ( self , x ): x = self . pool ( F . relu ( self . conv1 ( x ))) x = self . pool ( F . relu ( self . conv2 ( x ))) x = x . view ( - 1 , 16 * 5 * 5 ) x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x net = Net ()","title":"2. \u5b9a\u4e49\u5377\u53ca\u795e\u7ecf\u7f51\u7edc(CNN)"},{"location":"beginner/blitz/cifar10_tutorial/#3","text":"\u4f7f\u7528\u4ea4\u53c9\u71b5\u635f\u5931\u548c\u5e26\u6709\u52a8\u91cf\u7684SGD\u3002 1 2 3 4 import torch.optim as optim criterion = nn . CrossEntropyLoss () optimizer = optim . SGD ( net . parameters (), lr = 0.001 , momentum = 0.9 )","title":"3. \u5b9a\u4e49\u635f\u5931\u51fd\u6570"},{"location":"beginner/blitz/cifar10_tutorial/#4","text":"\u4e8b\u60c5\u5f00\u59cb\u53d8\u5f97\u6709\u8da3\u4e86\u3002 \u6211\u4eec\u53ea\u9700\u5faa\u73af\u904d\u5386\u6570\u636e\u8fed\u4ee3\u5668\uff0c\u5e76\u5c06\u8f93\u5165\u63d0\u4f9b\u7ed9\u7f51\u7edc\u5e76\u8fdb\u884c\u4f18\u5316\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 for epoch in range ( 2 ): # loop over the dataset multiple times ts = time () running_loss = 0.0 for i , data in enumerate ( trainloader , 0 ): # get the inputs inputs , labels = data # zero the parameter gradients optimizer . zero_grad () # forward + backward + optimize outputs = net ( inputs ) loss = criterion ( outputs , labels ) loss . backward () optimizer . step () # print statistics running_loss += loss . item () if i % 2000 == 1999 : # print every 2000 mini-batches print ( '[ %d , %5d ] loss: %.3f ' % ( epoch + 1 , i + 1 , running_loss / 2000 )) running_loss = 0.0 print ( 'Epoch {0} finished in {1:.3f} second(s)' . format ( epoch , time () - ts )) print ( 'Finished Training' ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 [1, 2000] loss: 2.237 [1, 4000] loss: 1.909 [1, 6000] loss: 1.723 [1, 8000] loss: 1.601 [1, 10000] loss: 1.531 [1, 12000] loss: 1.477 Epoch 0 finished in 75.642 second(s) [2, 2000] loss: 1.404 [2, 4000] loss: 1.372 [2, 6000] loss: 1.347 [2, 8000] loss: 1.353 [2, 10000] loss: 1.296 [2, 12000] loss: 1.263 Epoch 1 finished in 72.981 second(s) Finished Training","title":"4. \u8bad\u7ec3\u7f51\u7edc"},{"location":"beginner/blitz/cifar10_tutorial/#5","text":"\u5728\u8bad\u7ec3\u6570\u636e\u96c6\u4e0a\u5df2\u7ecf\u5bf9\u7f51\u7edc\u8fdb\u884c\u4e86\u4e24\u4e2a\u7eaa\u5143(epoch)\u7684\u8bad\u7ec3\u3002 \u4f46\u662f\u6211\u4eec\u8fd8\u8981\u8fdb\u884c\u68c0\u67e5\uff0c\u770b\u770b\u8fd9\u4e2a\u7f51\u7edc\u662f\u5426\u771f\u7684\u5b66\u4f1a\u4e86\u4ec0\u4e48\u3002 \u7528\u8fd9\u4e2a\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u9884\u6d4b\uff0c\u68c0\u67e5\u5b83\u8f93\u51fa\u7684\u5206\u7c7b\u6807\u7b7e\u662f\u5426\u548c\u5b9e\u9645\u503c\u4e00\u76f4\u3002\u5982\u679c\u9884\u6d4b\u6b63\u786e\uff0c\u628a\u6837\u672c\u52a0\u5230\u6b63\u786e\u9884\u6d4b\u5217\u8868\u3002 \u597d\u4e86\uff0c\u7b2c\u4e00\u6b65\u5148\u4ece\u6d4b\u8bd5\u96c6\u663e\u793a\u4e00\u4e2a\u56fe\u50cf\uff0c\u719f\u6089\u4e00\u4e0b\u3002 1 2 3 4 5 6 dataiter = iter ( testloader ) images , labels = dataiter . next () # print images imshow ( torchvision . utils . make_grid ( images )) print ( 'GroundTruth: ' , ' ' . join ( ' %5s ' % classes [ labels [ j ]] for j in range ( 4 ))) 1 GroundTruth : cat ship ship plane \u597d\u7684\uff0c\u73b0\u5728\u8ba9\u6211\u4eec\u770b\u770b\u795e\u7ecf\u7f51\u7edc\u8ba4\u4e3a\u4e0a\u9762\u8fd9\u4e9b\u4f8b\u5b50\u662f\u4ec0\u4e48\uff1a 1 outputs = net ( images ) \u8f93\u51fa\u503c\u662f10\u91cd\u7c7b\u578b\u7684\u80fd\u91cf\u503c\u3002 \u4e00\u4e2a\u7c7b\u578b\u7684\u80fd\u91cf\u503c\u8d8a\u9ad8\uff0c\u5c31\u8868\u660e\u7f51\u8def\u8d8a\u503e\u5411\u4e8e\u8ba4\u4e3a\u56fe\u7247\u662f\u8fd9\u79cd\u7c7b\u578b\u7684\u3002 \u90a3\u4e48\uff0c\u8ba9\u6211\u4eec\u5f97\u5230\u6700\u9ad8\u80fd\u91cf\u503c\u7684\u5e8f\u53f7\uff1a 1 2 3 4 _ , predicted = torch . max ( outputs , 1 ) print ( 'Predicted: ' , ' ' . join ( ' %5s ' % classes [ predicted [ j ]] for j in range ( 4 ))) 1 Predicted : cat ship ship ship \u7ed3\u679c\u4f3c\u4e4e\u8fd8\u884c\u3002 \u8ba9\u6211\u4eec\u770b\u770b\u8fd9\u4e2a\u7f51\u7edc\u5982\u4f55\u9884\u6d4b\u6574\u4e2a\u6d4b\u8bd5\u6570\u636e\u96c6\u3002 1 2 3 4 5 6 7 8 9 10 11 12 correct = 0 total = 0 with torch . no_grad (): for data in tqdm ( testloader ): images , labels = data outputs = net ( images ) _ , predicted = torch . max ( outputs . data , 1 ) total += labels . size ( 0 ) correct += ( predicted == labels ) . sum () . item () print ( 'Accuracy of the network on the 10000 test images: %d %% ' % ( 100 * correct / total )) 1 2 3 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [00:05<00:00, 420.29it/s] Accuracy of the network on the 10000 test images: 53 % \u770b\u8d77\u6765\u6bd4\u968f\u4fbf\u731c(\u4ece10\u4e2a\u7c7b\u578b\u4e2d\u968f\u673a\u9009\u4e00\u4e2a\u7684\u51c6\u786e\u5ea6\u662f10%)\u5f3a\u3002 \u8fd9\u4e2a\u7f51\u7edc\u4f3c\u4e4e\u5b66\u4f1a\u4e86\u4e00\u4e9b\u4e1c\u897f\u3002 \u55ef\uff0c\u54ea\u4e9b\u7c7b\u578b\u8868\u73b0\u597d\uff0c\u54ea\u4e9b\u4e0d\u597d\uff1a 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class_correct = list ( 0. for i in range ( 10 )) class_total = list ( 0. for i in range ( 10 )) with torch . no_grad (): for data in tqdm ( testloader ): images , labels = data outputs = net ( images ) _ , predicted = torch . max ( outputs , 1 ) c = ( predicted == labels ) . squeeze () for i in range ( 4 ): label = labels [ i ] class_correct [ label ] += c [ i ] . item () class_total [ label ] += 1 for i in range ( 10 ): print ( 'Accuracy of %5s : %2d %% ' % ( classes [ i ], 100 * class_correct [ i ] / class_total [ i ])) 1 2 3 4 5 6 7 8 9 10 11 12 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [00:05<00:00, 418.22it/s] Accuracy of plane : 29 % Accuracy of car : 68 % Accuracy of bird : 28 % Accuracy of cat : 42 % Accuracy of deer : 43 % Accuracy of dog : 45 % Accuracy of frog : 70 % Accuracy of horse : 56 % Accuracy of ship : 81 % Accuracy of truck : 73 % \u597d\u7684\uff0c\u63a5\u4e0b\u6765\u5462\uff1f \u6211\u4eec\u5982\u4f55\u5728GPU\u4e0a\u8fd0\u884c\u8fd9\u4e9b\u795e\u7ecf\u7f51\u7edc\uff1f","title":"5. \u4f7f\u7528\u6d4b\u8bd5\u6570\u636e\u5bf9\u7f51\u7edc\u8fdb\u884c\u6d4b\u8bd5"},{"location":"beginner/blitz/cifar10_tutorial/#gpu","text":"\u5c31\u50cf\u5c06Tensor\u8f6c\u5230GPU\u4e00\u6837\uff0c\u53ef\u5c06\u795e\u7ecf\u7f51\u7edc\u8f6c\u5230GPU\u3002 \u5982\u679c\u6709\u53ef\u7528\u7684CUDA\uff0c\u6211\u4eec\u9996\u5148\u83b7\u53d6\u7b2c\u4e00\u4e2a\u53ef\u89c1\u7684cuda\u8bbe\u5907\uff1a 1 2 3 4 5 device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) # Assuming that we are on a CUDA machine, this should print a CUDA device: print ( device ) 1 cuda : 0 \u672c\u8282\u7684\u5176\u4f59\u90e8\u5206\u5047\u5b9a device \u662f\u4e00\u4e2aCUDA\u8bbe\u5907\u3002 \u7136\u540e\u8fd9\u4e9b\u65b9\u6cd5\u5c06\u9012\u5f52\u904d\u5386\u6240\u6709\u6a21\u5757\u5e76\u5c06\u5176\u53c2\u6570\u548c\u7f13\u51b2\u533a\u8f6c\u6362\u4e3aCUDA\u5f20\u91cf\uff1a 1 net . to ( device ) \u8bb0\u4f4f\u8fd8\u5f97\u5c06\u6bcf\u4e00\u6b65\u7684\u8f93\u5165\u548c\u76ee\u6807\u53d1\u9001\u5230GPU\uff1a 1 inputs , labels = inputs . to ( device ), labels . to ( device ) \u53ef\u80fd\u89c2\u5bdf\u4e0d\u5230\u4e0eCPU\u76f8\u6bd4\u7684\u6709\u660e\u663e\u52a0\u901f\uff0c\u90a3\u662f\u56e0\u4e3a\u7684\u7f51\u7edc\u592a\u5c0f\u3002","title":"\u5728GPU\u4e0a\u8bad\u7ec3"},{"location":"beginner/blitz/cifar10_tutorial/#_4","text":"\u5c1d\u8bd5\u589e\u52a0\u7f51\u7edc\u7684\u5bbd\u5ea6\uff08\u7b2c\u4e00\u4e2a nn.Conv2d \u7684\u7b2c\u4e8c\u4e2a\u53c2\u6570\uff0c\u7b2c\u4e8c\u4e2a nn.Conv2d \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u2014\u2014\u5b83\u4eec\u5e94\u662f\u76f8\u540c\u7684\u6570\u503c\uff09\uff0c\u770b\u770b\u80fd\u83b7\u5f97\u4ec0\u4e48\u6837\u7684\u52a0\u901f\u3002 \u8fbe\u5230\u7684\u76ee\u6807\uff1a \u5728\u9ad8\u5c42\u6b21\u4e0a\u7406\u89e3PyTorch\u7684Tensor\u5e93\u548c\u795e\u7ecf\u7f51\u7edc\u3002 \u8bad\u7ec3\u4e00\u4e2a\u5c0f\u578b\u7684\u56fe\u50cf\u5206\u7c7b\u795e\u7ecf\u7f51\u7edc 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 class Net2 ( nn . Module ): def __init__ ( self ): super () . __init__ () self . conv1 = nn . Conv2d ( 3 , 12 , 5 ) self . pool = nn . MaxPool2d ( 2 , 2 ) self . conv2 = nn . Conv2d ( 12 , 16 , 5 ) self . fc1 = nn . Linear ( 16 * 5 * 5 , 120 ) self . fc2 = nn . Linear ( 120 , 84 ) self . fc3 = nn . Linear ( 84 , 10 ) def forward ( self , x ): x = self . pool ( F . relu ( self . conv1 ( x ))) x = self . pool ( F . relu ( self . conv2 ( x ))) x = x . view ( - 1 , 16 * 5 * 5 ) x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x net2 = Net2 () net2 . to ( device ) for epoch in range ( 2 ): # loop over the dataset multiple times ts = time () running_loss = 0.0 for i , data in enumerate ( trainloader , 0 ): # get the inputs inputs , labels = data inputs , labels = inputs . to ( device ), labels . to ( device ) # zero the parameter gradients optimizer . zero_grad () # forward + backward + optimize outputs = net2 ( inputs ) loss = criterion ( outputs , labels ) loss . backward () optimizer . step () # print statistics running_loss += loss . item () if i % 2000 == 1999 : # print every 2000 mini-batches print ( '[ %d , %5d ] loss: %.3f ' % ( epoch + 1 , i + 1 , running_loss / 2000 )) running_loss = 0.0 print ( 'Epoch {0} finished in {1:.3f} second(s)' . format ( epoch , time () - ts )) print ( 'Finished Training' ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 [1, 2000] loss: 2.305 [1, 4000] loss: 2.304 [1, 6000] loss: 2.304 [1, 8000] loss: 2.304 [1, 10000] loss: 2.303 [1, 12000] loss: 2.305 Epoch 0 finished in 72.346 second(s) [2, 2000] loss: 2.304 [2, 4000] loss: 2.304 [2, 6000] loss: 2.304 [2, 8000] loss: 2.303 [2, 10000] loss: 2.304 [2, 12000] loss: 2.305 Epoch 1 finished in 63.276 second(s) Finished Training","title":"\u7ec3\u4e60"},{"location":"beginner/blitz/cifar10_tutorial/#gpu_1","text":"\u5982\u679c\u60f3\u4f7f\u7528\u6240\u6709\u7684GPU\u6765\u5927\u5e45\u52a0\u901f\uff0c\u8bf7\u53c2\u8003 data_parallel_tutorial\u3002","title":"\u5728\u591a\u4e2aGPU\u4e0a\u8fdb\u884c\u8bad\u7ec3"},{"location":"beginner/blitz/cifar10_tutorial/#_5","text":":doc: Train neural nets to play video games </intermediate/reinforcement_q_learning> Train a state-of-the-art ResNet network on imagenet _ Train a face generator using Generative Adversarial Networks _ Train a word-level language model using Recurrent LSTM networks _ More examples _ More tutorials _ Discuss PyTorch on the Forums _ Chat with other users on Slack _ 1","title":"\u4e0b\u4e00\u6b65\u600e\u4e48\u529e\uff1f"},{"location":"beginner/blitz/data_parallel_tutorial/","text":"\u53ef\u9009\uff1a\u6570\u636e\u5e76\u884c \u4f5c\u8005 \uff1a Sung Kim \u4e0e Jenny Kang \u5728\u8fd9\u4e2a\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u5b66\u4e60\u5982\u4f55\u901a\u8fc7 DataParallel \u4f7f\u7528\u591aGPU\u3002 It\u2019s very easy to use GPUs with PyTorch. You can put the model on a GPU: \u5728PyTorch\u4e2d\u4f7f\u7528GPU\u975e\u5e38\u7b80\u5355\u3002\u53ef\u4ee5\u628a\u6a21\u578b\u653e\u5230GPU\uff1a 1 2 device = torch . device ( \"cuda:0\" ) model . to ( device ) \u7136\u540e\uff0c\u590d\u5236\u5f20\u91cf\u5230GPU\uff1a 1 mytensor = my_tensor . to ( device ) \u6ce8\u610f\uff1a\u8c03\u7528 my_tensor.to(device) \u8fd4\u56de\u4e86 my_tensor \u7684\u65b0\u62f7\u8d1d\uff0c\u800c\u4e0d\u662f\u6539\u5199 my_tensor \u3002 \u9700\u8981\u5c06\u5b83\u590d\u5236\u5230\u65b0\u7684\u5f20\u91cf\u5bf9\u8c61\uff0c\u800c\u540e\u5728GPU\u4e0a\u4f7f\u7528\u8fd9\u4e2a\u65b0\u5f20\u91cf\u3002 \u5728\u591aGPU\u4e2d\u6267\u884c\u6b63\u5411\u3001\u53cd\u5411\u4f20\u64ad\u5f88\u7b80\u5355\u3002\u4e0d\u8fc7\uff0cPyTorch\u9ed8\u8ba4\u53ea\u7528\u4e00\u4e2aGPU\u3002 \u4f7f\u7528 DataParallel \u53ef\u4ee5\u7b80\u5355\u5730\u8ba9\u6a21\u578b\u5728\u591a\u4e2aGPU\u4e0a\u8fdb\u884c\u8fd0\u7b97\uff1a 1 model = nn . DataParallel ( model ) \u8fd9\u662f\u672c\u6559\u7a0b\u80cc\u540e\u7684\u6838\u5fc3\u3002\u6211\u4eec\u5c06\u5728\u4e0b\u9762\u66f4\u8be6\u7ec6\u5730\u63a2\u8ba8\u5b83\u3002 \u5bfc\u5165\u548c\u53c2\u6570 \u5bfc\u5165PyTorch\u6a21\u5757\u5e76\u5b9a\u4e49\u53c2\u6570\u3002 1 2 3 4 5 6 7 8 9 10 11 12 from tqdm import tqdm import torch import torch.nn as nn from torch.utils.data import Dataset , DataLoader # Parameters and DataLoaders input_size = 5 output_size = 2 batch_size = 30 data_size = 100 \u8bbe\u5907 1 device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) \u5047\u6570\u636e\u96c6 \u5236\u4f5c\u4e00\u4e2a\u5047\uff08\u968f\u673a\uff09\u6570\u636e\u96c6\u3002 \u53ea\u9700\u8981\u5b9e\u73b0getitem 1 2 3 4 5 6 7 8 9 10 11 12 13 14 class RandomDataset ( Dataset ): def __init__ ( self , size , length ): self . len = length self . data = torch . randn ( length , size ) def __getitem__ ( self , index ): return self . data [ index ] def __len__ ( self ): return self . len rand_loader = DataLoader ( dataset = RandomDataset ( input_size , data_size ), batch_size = batch_size , shuffle = True ) \u7b80\u5355\u6a21\u578b For the demo, our model just gets an input, performs a linear operation, and gives an output. However, you can use DataParallel on any model (CNN, RNN, Capsule Net etc.) \u5728\u8fd9\u4e2a\u6f14\u793a\u4e2d\uff0c\u6211\u4eec\u7684\u6a21\u578b\u53ea\u8981\u8f93\u5165\uff0c\u6267\u884c\u4e00\u6b21\u73b0\u884c\u8fd0\u7b97\uff0c\u5e76\u7ed9\u51fa\u8f93\u51fa\u3002 \u53e6\u5916\uff0c DataParallel \u5728\u4efb\u4f55\u6a21\u578b(CNN\uff0cRNN\uff0cCapsule Net \u7b49)\u4e0a\u90fd\u53ef\u7528 \u6211\u4eec\u5728\u6a21\u578b\u4e2d\u653e\u7f6e\u4e86\u4e00\u4e2aprint\u8bed\u53e5\u6765\u76d1\u89c6\u8f93\u5165\u548c\u8f93\u51fa\u5f20\u91cf\u7684\u5927\u5c0f\u3002 \u8bf7\u6ce8\u610f\u6279\u6b210\u4e2d\u6253\u5370\u7684\u5185\u5bb9\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 class Model ( nn . Module ): # Our model def __init__ ( self , input_size , output_size ): super ( Model , self ) . __init__ () self . fc = nn . Linear ( input_size , output_size ) def forward ( self , input ): output = self . fc ( input ) print ( \" \\t In Model: input size\" , input . size (), \"output size\" , output . size ()) return output \u521b\u5efa\u6a21\u578b\u548c\u6570\u636e\u5e76\u53d1 \u8fd9\u662f\u672c\u6559\u7a0b\u7684\u6838\u5fc3\u90e8\u5206\u3002 \u9996\u5148\uff0c\u6211\u4eec\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u6a21\u578b\u5b9e\u4f8b\u5e76\u68c0\u67e5\u6211\u4eec\u662f\u5426\u6709\u591a\u4e2aGPU\u3002\u5982\u679c\u6211\u4eec\u6709\u591a\u4e2aGPU\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528 nn.DataParallel \u5305\u88c5\u6211\u4eec\u7684\u6a21\u578b\u3002 \u7136\u540e\u6211\u4eec\u53ef\u4ee5\u7528 model.to(device) \u5c06\u6211\u4eec\u7684\u6a21\u578b\u653e\u5728GPU\u4e0a 1 2 3 4 5 6 7 model = Model ( input_size , output_size ) if torch . cuda . device_count () > 1 : print ( \"Let's use\" , torch . cuda . device_count (), \"GPUs!\" ) # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs model = nn . DataParallel ( model ) model . to ( device ) 1 2 3 4 5 6 7 8 9 10 11 Let's use 3 GPUs! DataParallel( (module): Model( (fc): Linear(in_features=5, out_features=2, bias=True) ) ) \u8fd0\u884c\u6a21\u578b \u73b0\u5728\u53ef\u4ee5\u770b\u770b\u8f93\u5165\u548c\u8f93\u51fa\u5f20\u91cf\u7684\u5927\u5c0f\u3002 1 2 3 4 5 for data in tqdm ( rand_loader ): input = data . to ( device ) output = model ( input ) print ( \"Outside: input size\" , input . size (), \"output_size\" , output . size ()) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:07<00:00, 1.93s/it] In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2]) \u7ed3\u679c \u5982\u679c\u6ca1\u6709GPU\u6216\u53ea\u6709\u4e00\u4e2aGPU\uff0c\u5f53\u6211\u4eec\u6279\u6b21\u6267\u884c30\u4e2a\u8f93\u5165\u548c30\u4e2a\u8f93\u51fa\u7684\u65f6\u5019\uff0c\u6a21\u578b\u5f97\u5230\u4e8630\u4e2a\u8f93\u5165\u5e76\u671f\u5f8530\u4e2a\u8f93\u51fa\u3002 \u5982\u679c\u6709\u591a\u4e2aGPU\uff0c\u5c31\u53ef\u4ee5\u5f97\u5230\u50cf\u8fd9\u6837\u7684\u7ed3\u679c\u3002 2 GPU \u5982\u679c\u67092\u4e2aGPU\uff0c\u5c31\u4f1a\u770b\u5230\uff1a 1 2 3 4 5 6 7 8 9 10 11 12 13 Let's use 2 GPUs! In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2]) In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2]) 3 GPU \u5982\u679c\u67093\u4e2aGPU\uff0c\u5c31\u4f1a\u770b\u5230\uff1a 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Let's use 3 GPUs! In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2]) 8 GPU \u5982\u679c\u67098\u4e2aGPU\uff0c\u5c31\u4f1a\u770b\u5230 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 Let's use 8 GPUs! In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2]) \u603b\u7ed3 DataParallel\u81ea\u52a8\u62c6\u5206\u6570\u636e\u5e76\u5c06\u4f5c\u4e1a\u53d1\u9001\u5230\u591a\u4e2aGPU\u4e0a\u7684\u591a\u4e2a\u6a21\u578b\u3002 \u5728\u6bcf\u4e2a\u6a21\u578b\u5b8c\u6210\u5176\u5de5\u4f5c\u540e\uff0cDataParallel\u5c31\u4f1a\u5728\u51fd\u6570\u8fd4\u56de\u4e4b\u524d\u6536\u96c6\u5e76\u4e14\u5408\u5e76\u7ed3\u679c\u3002 \u6709\u5173\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u67e5\u770b https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html \u3002","title":"\u6570\u636e\u5e76\u884c"},{"location":"beginner/blitz/data_parallel_tutorial/#_1","text":"\u4f5c\u8005 \uff1a Sung Kim \u4e0e Jenny Kang \u5728\u8fd9\u4e2a\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u5b66\u4e60\u5982\u4f55\u901a\u8fc7 DataParallel \u4f7f\u7528\u591aGPU\u3002 It\u2019s very easy to use GPUs with PyTorch. You can put the model on a GPU: \u5728PyTorch\u4e2d\u4f7f\u7528GPU\u975e\u5e38\u7b80\u5355\u3002\u53ef\u4ee5\u628a\u6a21\u578b\u653e\u5230GPU\uff1a 1 2 device = torch . device ( \"cuda:0\" ) model . to ( device ) \u7136\u540e\uff0c\u590d\u5236\u5f20\u91cf\u5230GPU\uff1a 1 mytensor = my_tensor . to ( device ) \u6ce8\u610f\uff1a\u8c03\u7528 my_tensor.to(device) \u8fd4\u56de\u4e86 my_tensor \u7684\u65b0\u62f7\u8d1d\uff0c\u800c\u4e0d\u662f\u6539\u5199 my_tensor \u3002 \u9700\u8981\u5c06\u5b83\u590d\u5236\u5230\u65b0\u7684\u5f20\u91cf\u5bf9\u8c61\uff0c\u800c\u540e\u5728GPU\u4e0a\u4f7f\u7528\u8fd9\u4e2a\u65b0\u5f20\u91cf\u3002 \u5728\u591aGPU\u4e2d\u6267\u884c\u6b63\u5411\u3001\u53cd\u5411\u4f20\u64ad\u5f88\u7b80\u5355\u3002\u4e0d\u8fc7\uff0cPyTorch\u9ed8\u8ba4\u53ea\u7528\u4e00\u4e2aGPU\u3002 \u4f7f\u7528 DataParallel \u53ef\u4ee5\u7b80\u5355\u5730\u8ba9\u6a21\u578b\u5728\u591a\u4e2aGPU\u4e0a\u8fdb\u884c\u8fd0\u7b97\uff1a 1 model = nn . DataParallel ( model ) \u8fd9\u662f\u672c\u6559\u7a0b\u80cc\u540e\u7684\u6838\u5fc3\u3002\u6211\u4eec\u5c06\u5728\u4e0b\u9762\u66f4\u8be6\u7ec6\u5730\u63a2\u8ba8\u5b83\u3002","title":"\u53ef\u9009\uff1a\u6570\u636e\u5e76\u884c"},{"location":"beginner/blitz/data_parallel_tutorial/#_2","text":"\u5bfc\u5165PyTorch\u6a21\u5757\u5e76\u5b9a\u4e49\u53c2\u6570\u3002 1 2 3 4 5 6 7 8 9 10 11 12 from tqdm import tqdm import torch import torch.nn as nn from torch.utils.data import Dataset , DataLoader # Parameters and DataLoaders input_size = 5 output_size = 2 batch_size = 30 data_size = 100 \u8bbe\u5907 1 device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" )","title":"\u5bfc\u5165\u548c\u53c2\u6570"},{"location":"beginner/blitz/data_parallel_tutorial/#_3","text":"\u5236\u4f5c\u4e00\u4e2a\u5047\uff08\u968f\u673a\uff09\u6570\u636e\u96c6\u3002 \u53ea\u9700\u8981\u5b9e\u73b0getitem 1 2 3 4 5 6 7 8 9 10 11 12 13 14 class RandomDataset ( Dataset ): def __init__ ( self , size , length ): self . len = length self . data = torch . randn ( length , size ) def __getitem__ ( self , index ): return self . data [ index ] def __len__ ( self ): return self . len rand_loader = DataLoader ( dataset = RandomDataset ( input_size , data_size ), batch_size = batch_size , shuffle = True )","title":"\u5047\u6570\u636e\u96c6"},{"location":"beginner/blitz/data_parallel_tutorial/#_4","text":"For the demo, our model just gets an input, performs a linear operation, and gives an output. However, you can use DataParallel on any model (CNN, RNN, Capsule Net etc.) \u5728\u8fd9\u4e2a\u6f14\u793a\u4e2d\uff0c\u6211\u4eec\u7684\u6a21\u578b\u53ea\u8981\u8f93\u5165\uff0c\u6267\u884c\u4e00\u6b21\u73b0\u884c\u8fd0\u7b97\uff0c\u5e76\u7ed9\u51fa\u8f93\u51fa\u3002 \u53e6\u5916\uff0c DataParallel \u5728\u4efb\u4f55\u6a21\u578b(CNN\uff0cRNN\uff0cCapsule Net \u7b49)\u4e0a\u90fd\u53ef\u7528 \u6211\u4eec\u5728\u6a21\u578b\u4e2d\u653e\u7f6e\u4e86\u4e00\u4e2aprint\u8bed\u53e5\u6765\u76d1\u89c6\u8f93\u5165\u548c\u8f93\u51fa\u5f20\u91cf\u7684\u5927\u5c0f\u3002 \u8bf7\u6ce8\u610f\u6279\u6b210\u4e2d\u6253\u5370\u7684\u5185\u5bb9\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 class Model ( nn . Module ): # Our model def __init__ ( self , input_size , output_size ): super ( Model , self ) . __init__ () self . fc = nn . Linear ( input_size , output_size ) def forward ( self , input ): output = self . fc ( input ) print ( \" \\t In Model: input size\" , input . size (), \"output size\" , output . size ()) return output","title":"\u7b80\u5355\u6a21\u578b"},{"location":"beginner/blitz/data_parallel_tutorial/#_5","text":"\u8fd9\u662f\u672c\u6559\u7a0b\u7684\u6838\u5fc3\u90e8\u5206\u3002 \u9996\u5148\uff0c\u6211\u4eec\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u6a21\u578b\u5b9e\u4f8b\u5e76\u68c0\u67e5\u6211\u4eec\u662f\u5426\u6709\u591a\u4e2aGPU\u3002\u5982\u679c\u6211\u4eec\u6709\u591a\u4e2aGPU\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528 nn.DataParallel \u5305\u88c5\u6211\u4eec\u7684\u6a21\u578b\u3002 \u7136\u540e\u6211\u4eec\u53ef\u4ee5\u7528 model.to(device) \u5c06\u6211\u4eec\u7684\u6a21\u578b\u653e\u5728GPU\u4e0a 1 2 3 4 5 6 7 model = Model ( input_size , output_size ) if torch . cuda . device_count () > 1 : print ( \"Let's use\" , torch . cuda . device_count (), \"GPUs!\" ) # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs model = nn . DataParallel ( model ) model . to ( device ) 1 2 3 4 5 6 7 8 9 10 11 Let's use 3 GPUs! DataParallel( (module): Model( (fc): Linear(in_features=5, out_features=2, bias=True) ) )","title":"\u521b\u5efa\u6a21\u578b\u548c\u6570\u636e\u5e76\u53d1"},{"location":"beginner/blitz/data_parallel_tutorial/#_6","text":"\u73b0\u5728\u53ef\u4ee5\u770b\u770b\u8f93\u5165\u548c\u8f93\u51fa\u5f20\u91cf\u7684\u5927\u5c0f\u3002 1 2 3 4 5 for data in tqdm ( rand_loader ): input = data . to ( device ) output = model ( input ) print ( \"Outside: input size\" , input . size (), \"output_size\" , output . size ()) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:07<00:00, 1.93s/it] In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])","title":"\u8fd0\u884c\u6a21\u578b"},{"location":"beginner/blitz/data_parallel_tutorial/#_7","text":"\u5982\u679c\u6ca1\u6709GPU\u6216\u53ea\u6709\u4e00\u4e2aGPU\uff0c\u5f53\u6211\u4eec\u6279\u6b21\u6267\u884c30\u4e2a\u8f93\u5165\u548c30\u4e2a\u8f93\u51fa\u7684\u65f6\u5019\uff0c\u6a21\u578b\u5f97\u5230\u4e8630\u4e2a\u8f93\u5165\u5e76\u671f\u5f8530\u4e2a\u8f93\u51fa\u3002 \u5982\u679c\u6709\u591a\u4e2aGPU\uff0c\u5c31\u53ef\u4ee5\u5f97\u5230\u50cf\u8fd9\u6837\u7684\u7ed3\u679c\u3002","title":"\u7ed3\u679c"},{"location":"beginner/blitz/data_parallel_tutorial/#2-gpu","text":"\u5982\u679c\u67092\u4e2aGPU\uff0c\u5c31\u4f1a\u770b\u5230\uff1a 1 2 3 4 5 6 7 8 9 10 11 12 13 Let's use 2 GPUs! In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2]) In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])","title":"2 GPU"},{"location":"beginner/blitz/data_parallel_tutorial/#3-gpu","text":"\u5982\u679c\u67093\u4e2aGPU\uff0c\u5c31\u4f1a\u770b\u5230\uff1a 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Let's use 3 GPUs! In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])","title":"3 GPU"},{"location":"beginner/blitz/data_parallel_tutorial/#8-gpu","text":"\u5982\u679c\u67098\u4e2aGPU\uff0c\u5c31\u4f1a\u770b\u5230 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 Let's use 8 GPUs! In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])","title":"8 GPU"},{"location":"beginner/blitz/data_parallel_tutorial/#_8","text":"DataParallel\u81ea\u52a8\u62c6\u5206\u6570\u636e\u5e76\u5c06\u4f5c\u4e1a\u53d1\u9001\u5230\u591a\u4e2aGPU\u4e0a\u7684\u591a\u4e2a\u6a21\u578b\u3002 \u5728\u6bcf\u4e2a\u6a21\u578b\u5b8c\u6210\u5176\u5de5\u4f5c\u540e\uff0cDataParallel\u5c31\u4f1a\u5728\u51fd\u6570\u8fd4\u56de\u4e4b\u524d\u6536\u96c6\u5e76\u4e14\u5408\u5e76\u7ed3\u679c\u3002 \u6709\u5173\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u67e5\u770b https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html \u3002","title":"\u603b\u7ed3"},{"location":"beginner/blitz/neural_networks_tutorial/","text":"\u795e\u7ecf\u7f51\u7edc(Neural Networks) \u53ef\u4ee5\u4f7f\u7528 torch.nn \u5305\u6784\u9020\u795e\u7ecf\u7f51\u7edc\u3002 \u73b0\u5728\u6211\u4eec\u5df2\u7ecf\u521d\u6b65\u4e86\u89e3\u4e86 autograd \uff0c nn \u4f9d\u9760 autograd \u5b9a\u4e49\u6a21\u578b\u4ee5\u53ca\u6c42\u5fae\u5206\u3002 \u4e00\u4e2a nn.Module \u5305\u542b\u591a\u4e2a\u5c42\uff0c\u4e00\u4e2a\u8fd4\u56de output \u7684 forward(input) \u65b9\u6cd5\u3002 \u4f8b\u5982\uff0c\u8fd9\u4e2a\u6570\u5b57\u56fe\u50cf\u5206\u7c7b\u7684\u7f51\u7edc\u56fe\uff1a convnet \u5b83\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u524d\u9988\u7f51\u7edc\u3002\u5b83\u63a5\u53d7\u8f93\u5165\uff0c\u4e00\u4e2a\u63a5\u4e00\u4e2a\u5730\u901a\u8fc7\u51e0\u4e2a\u5c42\u8f93\u5165\uff0c\u7136\u540e\u6700\u7ec8\u7ed9\u51fa\u8f93\u51fa\u3002 \u795e\u7ecf\u7f51\u7edc\u7684\u5178\u578b\u8bad\u7ec3\u7a0b\u5e8f\u5982\u4e0b\uff1a \u5b9a\u4e49\u5177\u6709\u4e00\u4e9b\u53ef\u5b66\u4e60\u53c2\u6570\uff08\u6216\u6743\u91cd\uff09\u7684\u795e\u7ecf\u7f51\u7edc \u8fed\u4ee3\u8f93\u5165\u6570\u636e\u96c6 \u901a\u8fc7\u7f51\u7edc\u5904\u7406\u8f93\u5165 \u8ba1\u7b97\u635f\u5931\uff08\u8f93\u51fa\u8ddd\u79bb\u6b63\u786e\u591a\u8fdc\uff09 \u5c06\u6e10\u53d8\u4f20\u64ad\u56de\u7f51\u7edc\u53c2\u6570 \u66f4\u65b0\u7f51\u7edc\u6743\u91cd\uff0c\u901a\u5e38\u4f7f\u7528\u7b80\u5355\u7684\u66f4\u65b0\u89c4\u5219\uff1a weight = weight - learning_rate * gradient \u5b9a\u4e49\u7f51\u7edc \u6211\u4eec\u6765\u5b9a\u4e49\u7f51\u7edc: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 import torch import torch.nn as nn import torch.nn.functional as F class Net ( nn . Module ): def __init__ ( self ): super ( Net , self ) . __init__ () # 1 input image channel, 6 output channels, 5x5 square convolution # kernel self . conv1 = nn . Conv2d ( 1 , 6 , 5 ) self . conv2 = nn . Conv2d ( 6 , 16 , 5 ) # an affine operation: y = Wx + b self . fc1 = nn . Linear ( 16 * 5 * 5 , 120 ) self . fc2 = nn . Linear ( 120 , 84 ) self . fc3 = nn . Linear ( 84 , 10 ) def forward ( self , x ): # Max pooling over a (2, 2) window x = F . max_pool2d ( F . relu ( self . conv1 ( x )), ( 2 , 2 )) # If the size is a square you can only specify a single number x = F . max_pool2d ( F . relu ( self . conv2 ( x )), 2 ) x = x . view ( - 1 , self . num_flat_features ( x )) x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x def num_flat_features ( self , x ): size = x . size ()[ 1 :] # all dimensions except the batch dimension num_features = 1 for s in size : num_features *= s return num_features net = Net () print ( net ) 1 2 3 4 5 6 7 Net( (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)) (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)) (fc1): Linear(in_features=400, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=10, bias=True) ) \u6211\u4eec\u53ea\u9700\u5b9a\u4e49 forward \u51fd\u6570\uff0c backward \u51fd\u6570\uff08\u68af\u5ea6\u5728\u8fd9\u91cc\u88ab\u8ba1\u7b97\uff09\u7531 autograd \u81ea\u52a8\u751f\u6210\u3002 \u5728 forward \u51fd\u6570\u4e2d\u53ef\u4ee5\u4f7f\u7528\u4efb\u4f55\u4e00\u79cd\u5f20\u91cf\u8fd0\u7b97\u3002 \u6a21\u578b\u7684\u53ef\u5b66\u4e60\u53c2\u6570\u7531 net.parameters() \u8fd4\u56de 1 2 3 params = list ( net . parameters ()) print ( len ( params )) print ( params [ 0 ] . size ()) # conv1's .weight 1 2 10 torch.Size([6, 1, 5, 5]) \u8ba9\u6211\u4eec\u5c1d\u8bd5\u4e00\u4e2a\u968f\u673a\u768432x32\u8f93\u5165\u3002\u6ce8\u610f\uff1a\u6b64\u7f51\u7edc\uff08LeNet\uff09\u7684\u9884\u671f\u8f93\u5165\u5927\u5c0f\u4e3a32x32\u3002\u8981\u5728MNIST\u6570\u636e\u96c6\u4e0a\u4f7f\u7528\u6b64\u7f51\u7edc\uff0c\u8bf7\u5c06\u6570\u636e\u96c6\u4e2d\u7684\u56fe\u50cf\u8c03\u6574\u4e3a32x32\u3002 1 2 3 input = torch . randn ( 1 , 1 , 32 , 32 ) out = net ( input ) print ( out ) 1 2 tensor([[ 0.0108, -0.0120, -0.0415, -0.0964, 0.0957, 0.0217, -0.1095, 0.0581, 0.0173, -0.0540]], grad_fn=<AddmmBackward>) \u5c06\u6240\u6709\u53c2\u6570\u548c\u5e26\u6709\u968f\u673a\u68af\u5ea6\u7684\u53cd\u5411\u4f20\u64ad\u7684\u68af\u5ea6\u7f13\u51b2\u533a\u5f52\u96f6\uff1a 1 2 net . zero_grad () out . backward ( torch . randn ( 1 , 10 )) Note\uff1a torch.nn \u4ec5\u652f\u6301\u5c0f\u6279\u6b21\u3002\u6574\u4e2a torch.nn \u5305\u4ec5\u652f\u6301\u5c0f\u6279\u6b21\u7684\u6837\u672c\uff0c\u800c\u4e0d\u662f\u5355\u4e2a\u6837\u672c\u3002 \u4f8b\u5982\uff0c nn.Conv2d \u91c7\u7528 nSamples x nChannels x Height x Width 4\u7ef4\u5f20\u91cf\u3002 \u5982\u679c\u662f\u5355\u4e2a\u6837\u672c\uff0c\u8981\u7528 input.unsqueeze(0) \u628a\u5b83\u52a0\u5230\u4e00\u4e2a\u5047\u7684\u6279\u6b21\u7ef4\u5ea6\u3002 \u5728\u7ee7\u7eed\u4e4b\u524d\uff0c\u8ba9\u6211\u4eec\u56de\u987e\u4e00\u4e0b\u5230\u76ee\u524d\u4e3a\u6b62\u770b\u5230\u7684\u6240\u6709\u8bfe\u7a0b\u3002 \u6982\u62ec\uff1a - torch.Tensor - \u5b83\u662f\u652f\u6301\u50cf backward() \u8fd9\u79cdautograd\u8fd0\u7b97\u7684 \u591a\u7ef4\u6570\u7ec4 \uff0c\u8fd8\u80fd \u4fdd\u5b58 \u5f20\u91cf\u7684 \u68af\u5ea6 \u3002 - nn.Module - \u795e\u7ecf\u7f51\u7edc\u6a21\u5757\u3002\u63d0\u4f9b \u65b9\u4fbf\u7684\u53c2\u6570\u5c01\u88c5\u65b9\u5f0f \uff0c\u79fb\u81f3GPU\u3001\u5bfc\u51fa\u3001\u52a0\u8f7d\u7b49\u8f85\u52a9\u529f\u80fd\u3002 - nn.Parameter - \u4e00\u79cd\u5f20\u91cf\uff0c \u5f53\u8d4b\u503c\u7ed9 Module \u5bf9\u8c61\u7684\u5c5e\u6027\u65f6\uff0c\u5b83\u4f5c\u4e3a\u53c2\u6570\u88ab\u81ea\u52a8\u6ce8\u518c \u3002 - autograd.Function - \u5b9e\u73b0 autograde\u8fd0\u7b97\u7684 forward() \u548c backward() \u5b9a\u4e49 \u3002 \u6bcf\u6b21 Tensor \u8fd0\u7b97\u81f3\u5c11\u521b\u5efa\u4e00\u4e2a Function \u8282\u70b9\uff0c\u8be5\u8282\u70b9\u8fde\u63a5\u5230\u521b\u5efa Tensor \u7684 Function \u5bf9\u8c61\uff0c\u5e76 \u7f16\u7801\u5176\u5386\u53f2 \u3002 \u5728\u8fd9\u4e00\u70b9\u4e0a\uff0c\u6211\u4eec\u6db5\u76d6\u4e86\uff1a - \u5b9a\u4e49\u5ba1\u673a\u6784\u7f51\u7edcDefining a neural network - \u5904\u7406\u8f93\u5165\u8c03\u7528backward \u8fd8\u5269\u4e0b\uff1a - \u8ba1\u7b97\u635f\u5931 - \u66f4\u65b0\u7f51\u7edc\u6743\u91cd \u635f\u5931\u51fd\u6570 \u635f\u5931\u51fd\u6570\u91c7\u7528 output \uff0c target \u8f93\u5165\u5bf9\uff0c\u8ba1\u7b97\u8f93\u51fa\u4e0e\u76ee\u6807\u7684\u8ddd\u79bb\u4f30\u7b97\u503c\u3002 nn \u5305\u4e0b\u6709\u591a\u79cd\u4e0d\u540c\u7684 \u635f\u5931\u51fd\u6570 \u3002 \u6bd4\u5982 nn.MSELoss \u5c31\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u635f\u5931\u51fd\u6570\uff0c\u5b83\u8ba1\u7b97\u8f93\u5165\u548c\u76ee\u6807\u4e4b\u95f4\u7684\u5747\u65b9\u8bef\u5dee\u3002 \u4f8b\u5982\uff1a 1 2 3 4 5 6 7 output = net ( input ) target = torch . randn ( 10 ) # a dummy target, for example target = target . view ( 1 , - 1 ) # make it the same shape as output criterion = nn . MSELoss () loss = criterion ( output , target ) print ( loss ) 1 tensor(1.4750, grad_fn=<MseLossBackward>) \u73b0\u5728\uff0c\u5982\u679c\u6309 loss \u7684\u53cd\u65b9\u5411\uff0c\u4f7f\u7528 .grad_fn \u5c5e\u6027\uff0c\u5c31\u53ef\u770b\u5230\u8fd9\u6837\u7684\u8ba1\u7b97\u56fe\uff1a 1 2 3 4 input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d -> view -> linear -> relu -> linear -> relu -> linear -> MSELoss -> loss \u6240\u4ee5\uff0c\u5f53\u6211\u4eec\u8c03\u7528 loss.backward() \uff0c\u5c31\u4f1a\u6c42\u6574\u4e2a\u56fe\u5173\u4e8e\u635f\u5931\u7684\u5fae\u5206\uff0c\u56fe\u4e2d\u6240\u6709\u5177\u6709 requires_grad=True \u7684 Tensor \u5bf9\u8c61\u7684 .grad \u5f20\u91cf\u5c5e\u6027\u90fd\u4f7f\u7528\u68af\u5ea6\u7d2f\u52a0\u3002 \u4e3a\u4e86\u8bf4\u660e\u8fd9\u4e00\u70b9\uff0c\u6211\u4eec\u8fdb\u884c\u51e0\u6b65\u53cd\u5411\uff1a 1 2 3 print ( loss . grad_fn ) # MSELoss print ( loss . grad_fn . next_functions [ 0 ][ 0 ]) # Linear print ( loss . grad_fn . next_functions [ 0 ][ 0 ] . next_functions [ 0 ][ 0 ]) # ReLU 1 2 3 <MseLossBackward object at 0x7fa1140c1fd0> <AddmmBackward object at 0x7fa1140c6080> <AccumulateGrad object at 0x7fa1140c1fd0> \u53cd\u5411\u4f20\u64ad \u8981\u53cd\u5411\u4f20\u64ad\u8bef\u5dee\uff0c\u6211\u4eec\u9501\u8981\u505a\u7684\u5c31\u662f loss.backward() \u3002 \u4e0d\u8fc7\u6211\u4eec\u9700\u8981\u6e05\u9664\u5df2\u6709\u7684\u68af\u5ea6\uff0c\u5426\u5219\u68af\u5ea6\u5c06\u88ab\u7d2f\u79ef\u5230\u5df2\u6709\u7684\u68af\u5ea6\u4e0a\u3002 \u73b0\u5728\u53ef\u4ee5\u8c03\u7528 loss.backward() \uff0c\u770b\u770b conv1 \u5728\u8c03\u7528\u4e4b\u524d\u548c\u4e4b\u540e\u548c\u504f\u5dee\u68af\u5ea6\u3002 1 2 3 4 5 6 7 8 9 net . zero_grad () # zeroes the gradient buffers of all parameters print ( 'conv1.bias.grad before backward' ) print ( net . conv1 . bias . grad ) loss . backward () print ( 'conv1.bias.grad after backward' ) print ( net . conv1 . bias . grad ) 1 2 3 4 conv1.bias.grad before backward tensor([0., 0., 0., 0., 0., 0.]) conv1.bias.grad after backward tensor([-0.0245, -0.0181, 0.0134, 0.0136, 0.0242, 0.0009]) \u5230\u76ee\u524d\u4f4d\u7f6e, \u6211\u4eec\u5df2\u7ecf\u770b\u5230\u4e86\u5982\u4f55\u4f7f\u7528\u635f\u5931\u51fd\u6570\u3002 \u5ef6\u540e\u9605\u8bfb\uff1a \u795e\u7ecf\u7f51\u7edc\u5305\u5177\u6709\u7531\u4e8e\u6784\u5efa\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u5404\u79cd\u6a21\u5757\u548c\u635f\u5931\u51fd\u6570\u3002\u5e26\u6709\u6587\u6863\u7684\u5b8c\u6574\u5217\u8868\u5728 https://pytorch.org/docs/nn \u8fd8\u5269\u4e0b\u4e00\u4e2a\u8981\u5b66\u4e60\u7684\u662f\uff1a \u66f4\u65b0\u7f51\u7edc\u7684\u6743\u91cd \u66f4\u65b0\u6743\u91cd \u968f\u673a\u68af\u5ea6\u4e0b\u964d\u662f(SGD)\u662f\u6700\u7b80\u5355\u7684\u66f4\u65b0\u89c4\u5219\uff1a 1 weight = weight - learning_rate * gradient \u53ef\u4ee5\u7528\u7b80\u5355\u7684Python\u4ee3\u7801\u5b9e\u73b0\u5b83\uff1a 1 2 3 learning_rate = 0.01 for f in net . parameters (): f . data . sub_ ( f . grad . data * learning_rate ) \u4e0d\u8fc7\uff0c\u5f53\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u65f6\uff0c\u8fd8\u9700\u8981\u4f7f\u7528\u5404\u79cd\u4e0d\u540c\u7684\u66f4\u65b0\u89c4\u5219\uff0c\u4f8b\u5982SGD\uff0cNesterov-SGD\uff0cAdam\uff0cRMSProp\u7b49\u3002 \u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u5c0f\u5305\uff1a torch.optim \uff0c\u5b83\u5b9e\u73b0\u4e86\u6240\u6709\u8fd9\u4e9b\u65b9\u6cd5\u3002\u4f7f\u7528\u5b83\u975e\u5e38\u7b80\u5355\uff1a 1 2 3 4 5 6 7 8 9 10 11 import torch.optim as optim # create your optimizer optimizer = optim . SGD ( net . parameters (), lr = 0.01 ) # in your training loop: optimizer . zero_grad () # zero the gradient buffers output = net ( input ) loss = criterion ( output , target ) loss . backward () optimizer . step () # Does the update Note\uff1a \u89c2\u5bdf\u5982\u4f55\u4f7f\u7528 optimizer.zero_grad() \u624b\u52a8\u5c06\u68af\u5ea6\u7f13\u51b2\u533a\u8bbe\u7f6e\u4e3a\u96f6\u3002\u8fd9\u662f\u56e0\u4e3a\u68af\u5ea6\u7d2f\u79ef\u7684\uff0c\u53c2\u89c1 \u53cd\u5411\u4f20\u64ad \u90e8\u5206\u4e2d\u7684\u8bf4\u660e\u3002","title":"\u795e\u7ecf\u7f51\u7edc"},{"location":"beginner/blitz/neural_networks_tutorial/#neural-networks","text":"\u53ef\u4ee5\u4f7f\u7528 torch.nn \u5305\u6784\u9020\u795e\u7ecf\u7f51\u7edc\u3002 \u73b0\u5728\u6211\u4eec\u5df2\u7ecf\u521d\u6b65\u4e86\u89e3\u4e86 autograd \uff0c nn \u4f9d\u9760 autograd \u5b9a\u4e49\u6a21\u578b\u4ee5\u53ca\u6c42\u5fae\u5206\u3002 \u4e00\u4e2a nn.Module \u5305\u542b\u591a\u4e2a\u5c42\uff0c\u4e00\u4e2a\u8fd4\u56de output \u7684 forward(input) \u65b9\u6cd5\u3002 \u4f8b\u5982\uff0c\u8fd9\u4e2a\u6570\u5b57\u56fe\u50cf\u5206\u7c7b\u7684\u7f51\u7edc\u56fe\uff1a convnet \u5b83\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u524d\u9988\u7f51\u7edc\u3002\u5b83\u63a5\u53d7\u8f93\u5165\uff0c\u4e00\u4e2a\u63a5\u4e00\u4e2a\u5730\u901a\u8fc7\u51e0\u4e2a\u5c42\u8f93\u5165\uff0c\u7136\u540e\u6700\u7ec8\u7ed9\u51fa\u8f93\u51fa\u3002 \u795e\u7ecf\u7f51\u7edc\u7684\u5178\u578b\u8bad\u7ec3\u7a0b\u5e8f\u5982\u4e0b\uff1a \u5b9a\u4e49\u5177\u6709\u4e00\u4e9b\u53ef\u5b66\u4e60\u53c2\u6570\uff08\u6216\u6743\u91cd\uff09\u7684\u795e\u7ecf\u7f51\u7edc \u8fed\u4ee3\u8f93\u5165\u6570\u636e\u96c6 \u901a\u8fc7\u7f51\u7edc\u5904\u7406\u8f93\u5165 \u8ba1\u7b97\u635f\u5931\uff08\u8f93\u51fa\u8ddd\u79bb\u6b63\u786e\u591a\u8fdc\uff09 \u5c06\u6e10\u53d8\u4f20\u64ad\u56de\u7f51\u7edc\u53c2\u6570 \u66f4\u65b0\u7f51\u7edc\u6743\u91cd\uff0c\u901a\u5e38\u4f7f\u7528\u7b80\u5355\u7684\u66f4\u65b0\u89c4\u5219\uff1a weight = weight - learning_rate * gradient","title":"\u795e\u7ecf\u7f51\u7edc(Neural Networks)"},{"location":"beginner/blitz/neural_networks_tutorial/#_1","text":"\u6211\u4eec\u6765\u5b9a\u4e49\u7f51\u7edc: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 import torch import torch.nn as nn import torch.nn.functional as F class Net ( nn . Module ): def __init__ ( self ): super ( Net , self ) . __init__ () # 1 input image channel, 6 output channels, 5x5 square convolution # kernel self . conv1 = nn . Conv2d ( 1 , 6 , 5 ) self . conv2 = nn . Conv2d ( 6 , 16 , 5 ) # an affine operation: y = Wx + b self . fc1 = nn . Linear ( 16 * 5 * 5 , 120 ) self . fc2 = nn . Linear ( 120 , 84 ) self . fc3 = nn . Linear ( 84 , 10 ) def forward ( self , x ): # Max pooling over a (2, 2) window x = F . max_pool2d ( F . relu ( self . conv1 ( x )), ( 2 , 2 )) # If the size is a square you can only specify a single number x = F . max_pool2d ( F . relu ( self . conv2 ( x )), 2 ) x = x . view ( - 1 , self . num_flat_features ( x )) x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x def num_flat_features ( self , x ): size = x . size ()[ 1 :] # all dimensions except the batch dimension num_features = 1 for s in size : num_features *= s return num_features net = Net () print ( net ) 1 2 3 4 5 6 7 Net( (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)) (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)) (fc1): Linear(in_features=400, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=10, bias=True) ) \u6211\u4eec\u53ea\u9700\u5b9a\u4e49 forward \u51fd\u6570\uff0c backward \u51fd\u6570\uff08\u68af\u5ea6\u5728\u8fd9\u91cc\u88ab\u8ba1\u7b97\uff09\u7531 autograd \u81ea\u52a8\u751f\u6210\u3002 \u5728 forward \u51fd\u6570\u4e2d\u53ef\u4ee5\u4f7f\u7528\u4efb\u4f55\u4e00\u79cd\u5f20\u91cf\u8fd0\u7b97\u3002 \u6a21\u578b\u7684\u53ef\u5b66\u4e60\u53c2\u6570\u7531 net.parameters() \u8fd4\u56de 1 2 3 params = list ( net . parameters ()) print ( len ( params )) print ( params [ 0 ] . size ()) # conv1's .weight 1 2 10 torch.Size([6, 1, 5, 5]) \u8ba9\u6211\u4eec\u5c1d\u8bd5\u4e00\u4e2a\u968f\u673a\u768432x32\u8f93\u5165\u3002\u6ce8\u610f\uff1a\u6b64\u7f51\u7edc\uff08LeNet\uff09\u7684\u9884\u671f\u8f93\u5165\u5927\u5c0f\u4e3a32x32\u3002\u8981\u5728MNIST\u6570\u636e\u96c6\u4e0a\u4f7f\u7528\u6b64\u7f51\u7edc\uff0c\u8bf7\u5c06\u6570\u636e\u96c6\u4e2d\u7684\u56fe\u50cf\u8c03\u6574\u4e3a32x32\u3002 1 2 3 input = torch . randn ( 1 , 1 , 32 , 32 ) out = net ( input ) print ( out ) 1 2 tensor([[ 0.0108, -0.0120, -0.0415, -0.0964, 0.0957, 0.0217, -0.1095, 0.0581, 0.0173, -0.0540]], grad_fn=<AddmmBackward>) \u5c06\u6240\u6709\u53c2\u6570\u548c\u5e26\u6709\u968f\u673a\u68af\u5ea6\u7684\u53cd\u5411\u4f20\u64ad\u7684\u68af\u5ea6\u7f13\u51b2\u533a\u5f52\u96f6\uff1a 1 2 net . zero_grad () out . backward ( torch . randn ( 1 , 10 )) Note\uff1a torch.nn \u4ec5\u652f\u6301\u5c0f\u6279\u6b21\u3002\u6574\u4e2a torch.nn \u5305\u4ec5\u652f\u6301\u5c0f\u6279\u6b21\u7684\u6837\u672c\uff0c\u800c\u4e0d\u662f\u5355\u4e2a\u6837\u672c\u3002 \u4f8b\u5982\uff0c nn.Conv2d \u91c7\u7528 nSamples x nChannels x Height x Width 4\u7ef4\u5f20\u91cf\u3002 \u5982\u679c\u662f\u5355\u4e2a\u6837\u672c\uff0c\u8981\u7528 input.unsqueeze(0) \u628a\u5b83\u52a0\u5230\u4e00\u4e2a\u5047\u7684\u6279\u6b21\u7ef4\u5ea6\u3002 \u5728\u7ee7\u7eed\u4e4b\u524d\uff0c\u8ba9\u6211\u4eec\u56de\u987e\u4e00\u4e0b\u5230\u76ee\u524d\u4e3a\u6b62\u770b\u5230\u7684\u6240\u6709\u8bfe\u7a0b\u3002 \u6982\u62ec\uff1a - torch.Tensor - \u5b83\u662f\u652f\u6301\u50cf backward() \u8fd9\u79cdautograd\u8fd0\u7b97\u7684 \u591a\u7ef4\u6570\u7ec4 \uff0c\u8fd8\u80fd \u4fdd\u5b58 \u5f20\u91cf\u7684 \u68af\u5ea6 \u3002 - nn.Module - \u795e\u7ecf\u7f51\u7edc\u6a21\u5757\u3002\u63d0\u4f9b \u65b9\u4fbf\u7684\u53c2\u6570\u5c01\u88c5\u65b9\u5f0f \uff0c\u79fb\u81f3GPU\u3001\u5bfc\u51fa\u3001\u52a0\u8f7d\u7b49\u8f85\u52a9\u529f\u80fd\u3002 - nn.Parameter - \u4e00\u79cd\u5f20\u91cf\uff0c \u5f53\u8d4b\u503c\u7ed9 Module \u5bf9\u8c61\u7684\u5c5e\u6027\u65f6\uff0c\u5b83\u4f5c\u4e3a\u53c2\u6570\u88ab\u81ea\u52a8\u6ce8\u518c \u3002 - autograd.Function - \u5b9e\u73b0 autograde\u8fd0\u7b97\u7684 forward() \u548c backward() \u5b9a\u4e49 \u3002 \u6bcf\u6b21 Tensor \u8fd0\u7b97\u81f3\u5c11\u521b\u5efa\u4e00\u4e2a Function \u8282\u70b9\uff0c\u8be5\u8282\u70b9\u8fde\u63a5\u5230\u521b\u5efa Tensor \u7684 Function \u5bf9\u8c61\uff0c\u5e76 \u7f16\u7801\u5176\u5386\u53f2 \u3002 \u5728\u8fd9\u4e00\u70b9\u4e0a\uff0c\u6211\u4eec\u6db5\u76d6\u4e86\uff1a - \u5b9a\u4e49\u5ba1\u673a\u6784\u7f51\u7edcDefining a neural network - \u5904\u7406\u8f93\u5165\u8c03\u7528backward \u8fd8\u5269\u4e0b\uff1a - \u8ba1\u7b97\u635f\u5931 - \u66f4\u65b0\u7f51\u7edc\u6743\u91cd","title":"\u5b9a\u4e49\u7f51\u7edc"},{"location":"beginner/blitz/neural_networks_tutorial/#_2","text":"\u635f\u5931\u51fd\u6570\u91c7\u7528 output \uff0c target \u8f93\u5165\u5bf9\uff0c\u8ba1\u7b97\u8f93\u51fa\u4e0e\u76ee\u6807\u7684\u8ddd\u79bb\u4f30\u7b97\u503c\u3002 nn \u5305\u4e0b\u6709\u591a\u79cd\u4e0d\u540c\u7684 \u635f\u5931\u51fd\u6570 \u3002 \u6bd4\u5982 nn.MSELoss \u5c31\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u635f\u5931\u51fd\u6570\uff0c\u5b83\u8ba1\u7b97\u8f93\u5165\u548c\u76ee\u6807\u4e4b\u95f4\u7684\u5747\u65b9\u8bef\u5dee\u3002 \u4f8b\u5982\uff1a 1 2 3 4 5 6 7 output = net ( input ) target = torch . randn ( 10 ) # a dummy target, for example target = target . view ( 1 , - 1 ) # make it the same shape as output criterion = nn . MSELoss () loss = criterion ( output , target ) print ( loss ) 1 tensor(1.4750, grad_fn=<MseLossBackward>) \u73b0\u5728\uff0c\u5982\u679c\u6309 loss \u7684\u53cd\u65b9\u5411\uff0c\u4f7f\u7528 .grad_fn \u5c5e\u6027\uff0c\u5c31\u53ef\u770b\u5230\u8fd9\u6837\u7684\u8ba1\u7b97\u56fe\uff1a 1 2 3 4 input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d -> view -> linear -> relu -> linear -> relu -> linear -> MSELoss -> loss \u6240\u4ee5\uff0c\u5f53\u6211\u4eec\u8c03\u7528 loss.backward() \uff0c\u5c31\u4f1a\u6c42\u6574\u4e2a\u56fe\u5173\u4e8e\u635f\u5931\u7684\u5fae\u5206\uff0c\u56fe\u4e2d\u6240\u6709\u5177\u6709 requires_grad=True \u7684 Tensor \u5bf9\u8c61\u7684 .grad \u5f20\u91cf\u5c5e\u6027\u90fd\u4f7f\u7528\u68af\u5ea6\u7d2f\u52a0\u3002 \u4e3a\u4e86\u8bf4\u660e\u8fd9\u4e00\u70b9\uff0c\u6211\u4eec\u8fdb\u884c\u51e0\u6b65\u53cd\u5411\uff1a 1 2 3 print ( loss . grad_fn ) # MSELoss print ( loss . grad_fn . next_functions [ 0 ][ 0 ]) # Linear print ( loss . grad_fn . next_functions [ 0 ][ 0 ] . next_functions [ 0 ][ 0 ]) # ReLU 1 2 3 <MseLossBackward object at 0x7fa1140c1fd0> <AddmmBackward object at 0x7fa1140c6080> <AccumulateGrad object at 0x7fa1140c1fd0>","title":"\u635f\u5931\u51fd\u6570"},{"location":"beginner/blitz/neural_networks_tutorial/#_3","text":"\u8981\u53cd\u5411\u4f20\u64ad\u8bef\u5dee\uff0c\u6211\u4eec\u9501\u8981\u505a\u7684\u5c31\u662f loss.backward() \u3002 \u4e0d\u8fc7\u6211\u4eec\u9700\u8981\u6e05\u9664\u5df2\u6709\u7684\u68af\u5ea6\uff0c\u5426\u5219\u68af\u5ea6\u5c06\u88ab\u7d2f\u79ef\u5230\u5df2\u6709\u7684\u68af\u5ea6\u4e0a\u3002 \u73b0\u5728\u53ef\u4ee5\u8c03\u7528 loss.backward() \uff0c\u770b\u770b conv1 \u5728\u8c03\u7528\u4e4b\u524d\u548c\u4e4b\u540e\u548c\u504f\u5dee\u68af\u5ea6\u3002 1 2 3 4 5 6 7 8 9 net . zero_grad () # zeroes the gradient buffers of all parameters print ( 'conv1.bias.grad before backward' ) print ( net . conv1 . bias . grad ) loss . backward () print ( 'conv1.bias.grad after backward' ) print ( net . conv1 . bias . grad ) 1 2 3 4 conv1.bias.grad before backward tensor([0., 0., 0., 0., 0., 0.]) conv1.bias.grad after backward tensor([-0.0245, -0.0181, 0.0134, 0.0136, 0.0242, 0.0009]) \u5230\u76ee\u524d\u4f4d\u7f6e, \u6211\u4eec\u5df2\u7ecf\u770b\u5230\u4e86\u5982\u4f55\u4f7f\u7528\u635f\u5931\u51fd\u6570\u3002 \u5ef6\u540e\u9605\u8bfb\uff1a \u795e\u7ecf\u7f51\u7edc\u5305\u5177\u6709\u7531\u4e8e\u6784\u5efa\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u5404\u79cd\u6a21\u5757\u548c\u635f\u5931\u51fd\u6570\u3002\u5e26\u6709\u6587\u6863\u7684\u5b8c\u6574\u5217\u8868\u5728 https://pytorch.org/docs/nn \u8fd8\u5269\u4e0b\u4e00\u4e2a\u8981\u5b66\u4e60\u7684\u662f\uff1a \u66f4\u65b0\u7f51\u7edc\u7684\u6743\u91cd","title":"\u53cd\u5411\u4f20\u64ad"},{"location":"beginner/blitz/neural_networks_tutorial/#_4","text":"\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u662f(SGD)\u662f\u6700\u7b80\u5355\u7684\u66f4\u65b0\u89c4\u5219\uff1a 1 weight = weight - learning_rate * gradient \u53ef\u4ee5\u7528\u7b80\u5355\u7684Python\u4ee3\u7801\u5b9e\u73b0\u5b83\uff1a 1 2 3 learning_rate = 0.01 for f in net . parameters (): f . data . sub_ ( f . grad . data * learning_rate ) \u4e0d\u8fc7\uff0c\u5f53\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u65f6\uff0c\u8fd8\u9700\u8981\u4f7f\u7528\u5404\u79cd\u4e0d\u540c\u7684\u66f4\u65b0\u89c4\u5219\uff0c\u4f8b\u5982SGD\uff0cNesterov-SGD\uff0cAdam\uff0cRMSProp\u7b49\u3002 \u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u5c0f\u5305\uff1a torch.optim \uff0c\u5b83\u5b9e\u73b0\u4e86\u6240\u6709\u8fd9\u4e9b\u65b9\u6cd5\u3002\u4f7f\u7528\u5b83\u975e\u5e38\u7b80\u5355\uff1a 1 2 3 4 5 6 7 8 9 10 11 import torch.optim as optim # create your optimizer optimizer = optim . SGD ( net . parameters (), lr = 0.01 ) # in your training loop: optimizer . zero_grad () # zero the gradient buffers output = net ( input ) loss = criterion ( output , target ) loss . backward () optimizer . step () # Does the update Note\uff1a \u89c2\u5bdf\u5982\u4f55\u4f7f\u7528 optimizer.zero_grad() \u624b\u52a8\u5c06\u68af\u5ea6\u7f13\u51b2\u533a\u8bbe\u7f6e\u4e3a\u96f6\u3002\u8fd9\u662f\u56e0\u4e3a\u68af\u5ea6\u7d2f\u79ef\u7684\uff0c\u53c2\u89c1 \u53cd\u5411\u4f20\u64ad \u90e8\u5206\u4e2d\u7684\u8bf4\u660e\u3002","title":"\u66f4\u65b0\u6743\u91cd"},{"location":"beginner/blitz/tensor_tutorial/","text":"\u4ec0\u4e48\u662fPyTorch? \u5b83\u662f\u4e00\u4e2a\u57fa\u4e8ePython\u7684\u79d1\u5b66\u8ba1\u7b97\u8f6f\u4ef6\u5305\uff0c\u9488\u5bf9\u4e24\u7ec4\u53d7\u4f17\uff1a NumPy \u7684\u66ff\u4ee3\u54c1\uff0c\u53ef\u4ee5\u4f7f\u7528 GPU \u7684\u5f3a\u5927\u529f\u80fd \u6df1\u5ea6\u5b66\u4e60\u7814\u7a76\u5e73\u53f0\uff0c\u63d0\u4f9b\u6700\u5927\u7684\u7075\u6d3b\u6027\u548c\u901f\u5ea6 \u5f00\u59cb \u5f20\u91cf(Tensor) Tensor(\u5f20\u91cf)\u7c7b\u4f3c\u4e8eNumPy\u7684ndarrays(\u591a\u7ef4\u6570\u7ec4)\uff0c\u4e0d\u8fc7Tensor\u53ef\u4ee5\u4f7f\u7528GPU\u52a0\u901f\u8ba1\u7b97\u3002 1 2 from __future__ import print_function import torch \u6784\u9020\u4e00\u4e2a\u672a\u521d\u59cb\u5316\u76845x3\u77e9\u9635\uff1a 1 2 x = torch . empty ( 5 , 3 ) print ( x ) 1 2 3 4 5 tensor([[-2.6621e+09, 4.5587e-41, -2.6621e+09], [ 4.5587e-41, 3.3129e-18, 2.6302e+20], [ 6.1949e-04, 2.5640e-09, 1.6408e-07], [ 3.1472e+12, 2.7095e-09, 1.0374e-08], [ 6.4830e-10, 1.2398e+16, 1.8503e+20]]) \u6784\u9020\u4e00\u4e2a\u968f\u673a\u521d\u59cb\u5316\u7684\u77e9\u9635\uff1a 1 2 x = torch . rand ( 5 , 3 ) print ( x ) 1 2 3 4 5 tensor([[0.8017, 0.1650, 0.3771], [0.8411, 0.4406, 0.3873], [0.3508, 0.8021, 0.5265], [0.0255, 0.8830, 0.0428], [0.1457, 0.5150, 0.1314]]) \u6784\u9020\u4e00\u4e2a\u96f6\u586b\u5145\uff0c\u6570\u636e\u7c7b\u578b( dtype )\u4e3a\u957f\u6574\u5f62( long )\u7684\u77e9\u9635: 1 2 x = torch . zeros ( 5 , 3 , dtype = torch . long ) print ( x ) 1 2 3 4 5 tensor([[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]]) \u76f4\u63a5\u4ece\u6570\u636e\u6784\u9020 tensor: 1 2 x = torch . tensor ([ 5.5 , 3 ]) print ( x ) 1 tensor([5.5000, 3.0000]) \u6216\u8005\u5728\u5df2\u6709 tensor \u7684\u57fa\u7840\u4e0a\u521b\u5efa tensor\u3002 \u8fd9\u79cd\u65b9\u6cd5\u5c06\u91cd\u7528\u8f93\u5165 tensor \u7684\u5c5e\u6027\uff0c\u5982 dtype , \u9664\u975e\u63d0\u4f9b\u65b0\u7684\u503c 1 2 3 4 5 x = x . new_ones ( 5 , 3 , dtype = torch . double ) # new_* methods take in sizes print ( x ) x = torch . randn_like ( x , dtype = torch . float ) # override dtype! print ( x ) # result has the same size 1 2 3 4 5 6 7 8 9 10 tensor([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.]], dtype=torch.float64) tensor([[ 0.4672, -2.5856, -0.5848], [-0.0800, -0.0157, -0.1485], [-1.4250, 1.9369, -1.9591], [ 0.2027, -0.4884, -0.9408], [ 1.0423, 0.9927, -0.4951]]) \u83b7\u53d6\u5176\u5927\u5c0f: 1 print ( x . size ()) 1 torch.Size([5, 3]) \u6ce8\u610f : torch.Size \u662f tuple(\u5143\u7ec4)\uff0c\u5b83\u652f\u6301\u6240\u6709 tuple \u64cd\u4f5c\u3002 \u8fd0\u7b97 \u8fd0\u7b97\u6709\u591a\u79cd\u8bed\u6cd5\u3002\u5728\u4e0b\u9762\u7684\u793a\u4f8b\u4e2d\uff0c\u6211\u4eec\u5c06\u770b\u4e00\u770b\u52a0\u6cd5\u8fd0\u7b97 \u52a0\u6cd5: \u8bed\u6cd5 1 1 2 y = torch . rand ( 5 , 3 ) print ( x + y ) 1 2 3 4 5 tensor([[ 1.4545, -1.7998, -0.3246], [-0.0237, 0.9005, 0.3812], [-1.0698, 2.0101, -1.0791], [ 0.5078, 0.4077, -0.0533], [ 1.8212, 1.6021, -0.3893]]) \u52a0\u6cd5: \u8bed\u6cd5 2 1 print ( torch . add ( x , y )) 1 2 3 4 5 tensor([[ 1.4545, -1.7998, -0.3246], [-0.0237, 0.9005, 0.3812], [-1.0698, 2.0101, -1.0791], [ 0.5078, 0.4077, -0.0533], [ 1.8212, 1.6021, -0.3893]]) \u52a0\u6cd5: \u5c06\u8f93\u51fa tensor \u4f5c\u4e3a\u53c2\u6570 1 2 3 result = torch . empty ( 5 , 3 ) torch . add ( x , y , out = result ) print ( result ) 1 2 3 4 5 tensor([[ 1.4545, -1.7998, -0.3246], [-0.0237, 0.9005, 0.3812], [-1.0698, 2.0101, -1.0791], [ 0.5078, 0.4077, -0.0533], [ 1.8212, 1.6021, -0.3893]]) \u52a0\u6cd5: \u539f\u4f4d\u4fee\u6539 1 2 3 # adds x to y y . add_ ( x ) print ( y ) 1 2 3 4 5 tensor([[ 1.4545, -1.7998, -0.3246], [-0.0237, 0.9005, 0.3812], [-1.0698, 2.0101, -1.0791], [ 0.5078, 0.4077, -0.0533], [ 1.8212, 1.6021, -0.3893]]) \u6ce8\u610f : \u4efb\u4f55\u539f\u4f4d\u4fee\u6539 tensor \u7684\u8fd0\u7b97\u65b9\u6cd5\u90fd\u662f\u4ee5 _ \u7ed3\u5c3e\u7684\u3002 \u4f8b\u5982\uff1a x.copy_(y) \uff0c x.t_() \uff0c\u4f1a\u6539\u53d8 x \u53ef\u4ee5\u4f7f\u7528\u7c7b\u4f3c NumPy \u7684\u7d22\u5f15\u8fd0\u7b97\uff01 1 print ( x [:, 1 ]) 1 tensor([-2.5856, -0.0157, 1.9369, -0.4884, 0.9927]) \u8c03\u6574\u5927\u5c0f\uff1a\u5982\u679c\u8981\u5bf9 tensor \u8c03\u6574\u5927\u5c0f/\u6539\u53d8\u5f62\u72b6\uff0c\u53ef\u4ee5\u4f7f\u7528 torch.view : 1 2 3 4 x = torch . randn ( 4 , 4 ) y = x . view ( 16 ) z = x . view ( - 1 , 8 ) # the size -1 is inferred from other dimensions print ( x . size (), y . size (), z . size ()) 1 torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8]) \u5bf9\u4e8e\u53ea\u6709\u4e00\u4e2a\u5143\u7d20\u7684 tensor\uff0c\u53ef\u4ee5\u7528 .item() \u5f97\u5230\u8fd9\u4e2a\u503c\u5bf9\u5e94\u7684 Python \u6570\u503c 1 2 3 x = torch . randn ( 1 ) print ( x ) print ( x . item ()) 1 2 tensor([1.0215]) 1.0214648246765137 \u5ef6\u540e\u9605\u8bfb \uff1a 100+ Tensor \u8fd0\u7b97\uff0c\u5305\u62ec\uff1a\u79fb\u8c03\u3001\u7d22\u5f15\u3001\u5207\u7247\u3001\u6570\u5b66\u8fd0\u7b97\u3001\u7ebf\u6027\u4ee3\u6570\u3001\u968f\u673a\u6570\u7b49\uff0c\u53ef\u53c2\u8003 https://pytorch.org/docs/torch NumPy \u4e92\u8f6c \u5c06 Torch Tensor \u8f6c\u6362\u4e3a NumPy \u6570\u7ec4\uff08\u53cd\u4e4b\u4ea6\u7136\uff09\u662f\u4e00\u4ef6\u8f7b\u800c\u6613\u4e3e\u7684\u4e8b\u3002 Torch Tensor \u548c NumPy \u6570\u7ec4\u5171\u4eab\u5176\u5e95\u5c42\u5185\u5b58\u4f4d\u7f6e\uff0c\u66f4\u6539\u4e00\u4e2a\u4f1a\u5bfc\u81f4\u53e6\u4e00\u4e2a\u7684\u6539\u53d8\u3002 \u5c06 Torch Tensor \u8f6c\u6362\u4e3a NumPy \u6570\u7ec4 1 2 a = torch . ones ( 5 ) print ( a ) 1 tensor([1., 1., 1., 1., 1.]) 1 2 b = a . numpy () print ( b ) 1 [1. 1. 1. 1. 1.] \u4e86\u89e3numpy\u6570\u7ec4\u7684\u503c\u5982\u4f55\u53d8\u5316\u3002 1 2 3 a . add_ ( 1 ) print ( a ) print ( b ) 1 2 tensor([2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2.] \u5c06 NumPy \u6570\u7ec4\u8f6c\u6362\u4e3a Torch Tensor \u4e86\u89e3\u66f4\u6539 NumPy \u6570\u7ec4\u5982\u4f55\u81ea\u52a8\u66f4\u6539 Torch Tensor 1 2 3 4 5 6 import numpy as np a = np . ones ( 5 ) b = torch . from_numpy ( a ) np . add ( a , 1 , out = a ) print ( a ) print ( b ) 1 2 [2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype = torch.float64) \u9664\u4e86 CharTensor \uff0c\u6240\u6709 CPU \u4e0a\u7684 Tensor \u90fd\u53ef\u4ee5\u548c Numpy \u4e92\u76f8\u8f6c\u6362\u3002 CUDA Tensor \u53ef\u4ee5\u4f7f\u7528 .to \u65b9\u6cd5\u79fb\u52a8Tensor\u5bf9\u8c61\u5230\u4efb\u4f55\u8bbe\u5907\u3002 1 2 3 4 5 6 7 8 9 # \u53ea\u6709 CUDA \u53ef\u7528\u65f6\uff0c\u624d\u8fd0\u884c\u8fd9\u4e2a cell # \u4f7f\u7528 ``torch.device`` \u5bf9\u8c61\u5c06 tensor \u79fb\u5165\u4ee5\u53ca\u79fb\u51fa GPU if torch . cuda . is_available (): device = torch . device ( \"cuda\" ) # CUDA \u8bbe\u5907\u5bf9\u8c61 y = torch . ones_like ( x , device = device ) # \u76f4\u63a5\u5728 GPU \u4e0a\u521b\u5efa tensor x = x . to ( device ) # \u6216\u8005\u7528 ``.to(\"cuda\")`` z = x + y print ( z ) print ( z . to ( \"cpu\" , torch . double )) # ``.to`` \u4e5f\u53ef\u4ee5\u540c\u65f6\u66f4\u6539 ``dtype``! 1 2 tensor([2.0215], device='cuda:0') tensor([2.0215], dtype=torch.float64)","title":"\u4ec0\u4e48\u662fPyTorch\uff1f"},{"location":"beginner/blitz/tensor_tutorial/#pytorch","text":"\u5b83\u662f\u4e00\u4e2a\u57fa\u4e8ePython\u7684\u79d1\u5b66\u8ba1\u7b97\u8f6f\u4ef6\u5305\uff0c\u9488\u5bf9\u4e24\u7ec4\u53d7\u4f17\uff1a NumPy \u7684\u66ff\u4ee3\u54c1\uff0c\u53ef\u4ee5\u4f7f\u7528 GPU \u7684\u5f3a\u5927\u529f\u80fd \u6df1\u5ea6\u5b66\u4e60\u7814\u7a76\u5e73\u53f0\uff0c\u63d0\u4f9b\u6700\u5927\u7684\u7075\u6d3b\u6027\u548c\u901f\u5ea6","title":"\u4ec0\u4e48\u662fPyTorch?"},{"location":"beginner/blitz/tensor_tutorial/#_1","text":"","title":"\u5f00\u59cb"},{"location":"beginner/blitz/tensor_tutorial/#tensor","text":"Tensor(\u5f20\u91cf)\u7c7b\u4f3c\u4e8eNumPy\u7684ndarrays(\u591a\u7ef4\u6570\u7ec4)\uff0c\u4e0d\u8fc7Tensor\u53ef\u4ee5\u4f7f\u7528GPU\u52a0\u901f\u8ba1\u7b97\u3002 1 2 from __future__ import print_function import torch \u6784\u9020\u4e00\u4e2a\u672a\u521d\u59cb\u5316\u76845x3\u77e9\u9635\uff1a 1 2 x = torch . empty ( 5 , 3 ) print ( x ) 1 2 3 4 5 tensor([[-2.6621e+09, 4.5587e-41, -2.6621e+09], [ 4.5587e-41, 3.3129e-18, 2.6302e+20], [ 6.1949e-04, 2.5640e-09, 1.6408e-07], [ 3.1472e+12, 2.7095e-09, 1.0374e-08], [ 6.4830e-10, 1.2398e+16, 1.8503e+20]]) \u6784\u9020\u4e00\u4e2a\u968f\u673a\u521d\u59cb\u5316\u7684\u77e9\u9635\uff1a 1 2 x = torch . rand ( 5 , 3 ) print ( x ) 1 2 3 4 5 tensor([[0.8017, 0.1650, 0.3771], [0.8411, 0.4406, 0.3873], [0.3508, 0.8021, 0.5265], [0.0255, 0.8830, 0.0428], [0.1457, 0.5150, 0.1314]]) \u6784\u9020\u4e00\u4e2a\u96f6\u586b\u5145\uff0c\u6570\u636e\u7c7b\u578b( dtype )\u4e3a\u957f\u6574\u5f62( long )\u7684\u77e9\u9635: 1 2 x = torch . zeros ( 5 , 3 , dtype = torch . long ) print ( x ) 1 2 3 4 5 tensor([[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]]) \u76f4\u63a5\u4ece\u6570\u636e\u6784\u9020 tensor: 1 2 x = torch . tensor ([ 5.5 , 3 ]) print ( x ) 1 tensor([5.5000, 3.0000]) \u6216\u8005\u5728\u5df2\u6709 tensor \u7684\u57fa\u7840\u4e0a\u521b\u5efa tensor\u3002 \u8fd9\u79cd\u65b9\u6cd5\u5c06\u91cd\u7528\u8f93\u5165 tensor \u7684\u5c5e\u6027\uff0c\u5982 dtype , \u9664\u975e\u63d0\u4f9b\u65b0\u7684\u503c 1 2 3 4 5 x = x . new_ones ( 5 , 3 , dtype = torch . double ) # new_* methods take in sizes print ( x ) x = torch . randn_like ( x , dtype = torch . float ) # override dtype! print ( x ) # result has the same size 1 2 3 4 5 6 7 8 9 10 tensor([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.]], dtype=torch.float64) tensor([[ 0.4672, -2.5856, -0.5848], [-0.0800, -0.0157, -0.1485], [-1.4250, 1.9369, -1.9591], [ 0.2027, -0.4884, -0.9408], [ 1.0423, 0.9927, -0.4951]]) \u83b7\u53d6\u5176\u5927\u5c0f: 1 print ( x . size ()) 1 torch.Size([5, 3]) \u6ce8\u610f : torch.Size \u662f tuple(\u5143\u7ec4)\uff0c\u5b83\u652f\u6301\u6240\u6709 tuple \u64cd\u4f5c\u3002","title":"\u5f20\u91cf(Tensor)"},{"location":"beginner/blitz/tensor_tutorial/#_2","text":"\u8fd0\u7b97\u6709\u591a\u79cd\u8bed\u6cd5\u3002\u5728\u4e0b\u9762\u7684\u793a\u4f8b\u4e2d\uff0c\u6211\u4eec\u5c06\u770b\u4e00\u770b\u52a0\u6cd5\u8fd0\u7b97 \u52a0\u6cd5: \u8bed\u6cd5 1 1 2 y = torch . rand ( 5 , 3 ) print ( x + y ) 1 2 3 4 5 tensor([[ 1.4545, -1.7998, -0.3246], [-0.0237, 0.9005, 0.3812], [-1.0698, 2.0101, -1.0791], [ 0.5078, 0.4077, -0.0533], [ 1.8212, 1.6021, -0.3893]]) \u52a0\u6cd5: \u8bed\u6cd5 2 1 print ( torch . add ( x , y )) 1 2 3 4 5 tensor([[ 1.4545, -1.7998, -0.3246], [-0.0237, 0.9005, 0.3812], [-1.0698, 2.0101, -1.0791], [ 0.5078, 0.4077, -0.0533], [ 1.8212, 1.6021, -0.3893]]) \u52a0\u6cd5: \u5c06\u8f93\u51fa tensor \u4f5c\u4e3a\u53c2\u6570 1 2 3 result = torch . empty ( 5 , 3 ) torch . add ( x , y , out = result ) print ( result ) 1 2 3 4 5 tensor([[ 1.4545, -1.7998, -0.3246], [-0.0237, 0.9005, 0.3812], [-1.0698, 2.0101, -1.0791], [ 0.5078, 0.4077, -0.0533], [ 1.8212, 1.6021, -0.3893]]) \u52a0\u6cd5: \u539f\u4f4d\u4fee\u6539 1 2 3 # adds x to y y . add_ ( x ) print ( y ) 1 2 3 4 5 tensor([[ 1.4545, -1.7998, -0.3246], [-0.0237, 0.9005, 0.3812], [-1.0698, 2.0101, -1.0791], [ 0.5078, 0.4077, -0.0533], [ 1.8212, 1.6021, -0.3893]]) \u6ce8\u610f : \u4efb\u4f55\u539f\u4f4d\u4fee\u6539 tensor \u7684\u8fd0\u7b97\u65b9\u6cd5\u90fd\u662f\u4ee5 _ \u7ed3\u5c3e\u7684\u3002 \u4f8b\u5982\uff1a x.copy_(y) \uff0c x.t_() \uff0c\u4f1a\u6539\u53d8 x \u53ef\u4ee5\u4f7f\u7528\u7c7b\u4f3c NumPy \u7684\u7d22\u5f15\u8fd0\u7b97\uff01 1 print ( x [:, 1 ]) 1 tensor([-2.5856, -0.0157, 1.9369, -0.4884, 0.9927]) \u8c03\u6574\u5927\u5c0f\uff1a\u5982\u679c\u8981\u5bf9 tensor \u8c03\u6574\u5927\u5c0f/\u6539\u53d8\u5f62\u72b6\uff0c\u53ef\u4ee5\u4f7f\u7528 torch.view : 1 2 3 4 x = torch . randn ( 4 , 4 ) y = x . view ( 16 ) z = x . view ( - 1 , 8 ) # the size -1 is inferred from other dimensions print ( x . size (), y . size (), z . size ()) 1 torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8]) \u5bf9\u4e8e\u53ea\u6709\u4e00\u4e2a\u5143\u7d20\u7684 tensor\uff0c\u53ef\u4ee5\u7528 .item() \u5f97\u5230\u8fd9\u4e2a\u503c\u5bf9\u5e94\u7684 Python \u6570\u503c 1 2 3 x = torch . randn ( 1 ) print ( x ) print ( x . item ()) 1 2 tensor([1.0215]) 1.0214648246765137 \u5ef6\u540e\u9605\u8bfb \uff1a 100+ Tensor \u8fd0\u7b97\uff0c\u5305\u62ec\uff1a\u79fb\u8c03\u3001\u7d22\u5f15\u3001\u5207\u7247\u3001\u6570\u5b66\u8fd0\u7b97\u3001\u7ebf\u6027\u4ee3\u6570\u3001\u968f\u673a\u6570\u7b49\uff0c\u53ef\u53c2\u8003 https://pytorch.org/docs/torch","title":"\u8fd0\u7b97"},{"location":"beginner/blitz/tensor_tutorial/#numpy","text":"\u5c06 Torch Tensor \u8f6c\u6362\u4e3a NumPy \u6570\u7ec4\uff08\u53cd\u4e4b\u4ea6\u7136\uff09\u662f\u4e00\u4ef6\u8f7b\u800c\u6613\u4e3e\u7684\u4e8b\u3002 Torch Tensor \u548c NumPy \u6570\u7ec4\u5171\u4eab\u5176\u5e95\u5c42\u5185\u5b58\u4f4d\u7f6e\uff0c\u66f4\u6539\u4e00\u4e2a\u4f1a\u5bfc\u81f4\u53e6\u4e00\u4e2a\u7684\u6539\u53d8\u3002","title":"NumPy \u4e92\u8f6c"},{"location":"beginner/blitz/tensor_tutorial/#torch-tensor-numpy","text":"1 2 a = torch . ones ( 5 ) print ( a ) 1 tensor([1., 1., 1., 1., 1.]) 1 2 b = a . numpy () print ( b ) 1 [1. 1. 1. 1. 1.] \u4e86\u89e3numpy\u6570\u7ec4\u7684\u503c\u5982\u4f55\u53d8\u5316\u3002 1 2 3 a . add_ ( 1 ) print ( a ) print ( b ) 1 2 tensor([2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2.]","title":"\u5c06 Torch Tensor \u8f6c\u6362\u4e3a NumPy \u6570\u7ec4"},{"location":"beginner/blitz/tensor_tutorial/#numpy-torch-tensor","text":"\u4e86\u89e3\u66f4\u6539 NumPy \u6570\u7ec4\u5982\u4f55\u81ea\u52a8\u66f4\u6539 Torch Tensor 1 2 3 4 5 6 import numpy as np a = np . ones ( 5 ) b = torch . from_numpy ( a ) np . add ( a , 1 , out = a ) print ( a ) print ( b ) 1 2 [2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype = torch.float64) \u9664\u4e86 CharTensor \uff0c\u6240\u6709 CPU \u4e0a\u7684 Tensor \u90fd\u53ef\u4ee5\u548c Numpy \u4e92\u76f8\u8f6c\u6362\u3002","title":"\u5c06 NumPy \u6570\u7ec4\u8f6c\u6362\u4e3a Torch Tensor"},{"location":"beginner/blitz/tensor_tutorial/#cuda-tensor","text":"\u53ef\u4ee5\u4f7f\u7528 .to \u65b9\u6cd5\u79fb\u52a8Tensor\u5bf9\u8c61\u5230\u4efb\u4f55\u8bbe\u5907\u3002 1 2 3 4 5 6 7 8 9 # \u53ea\u6709 CUDA \u53ef\u7528\u65f6\uff0c\u624d\u8fd0\u884c\u8fd9\u4e2a cell # \u4f7f\u7528 ``torch.device`` \u5bf9\u8c61\u5c06 tensor \u79fb\u5165\u4ee5\u53ca\u79fb\u51fa GPU if torch . cuda . is_available (): device = torch . device ( \"cuda\" ) # CUDA \u8bbe\u5907\u5bf9\u8c61 y = torch . ones_like ( x , device = device ) # \u76f4\u63a5\u5728 GPU \u4e0a\u521b\u5efa tensor x = x . to ( device ) # \u6216\u8005\u7528 ``.to(\"cuda\")`` z = x + y print ( z ) print ( z . to ( \"cpu\" , torch . double )) # ``.to`` \u4e5f\u53ef\u4ee5\u540c\u65f6\u66f4\u6539 ``dtype``! 1 2 tensor([2.0215], device='cuda:0') tensor([2.0215], dtype=torch.float64)","title":"CUDA Tensor"}]}