{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"pytorch-tutorials-notebooks-zhs \u00b6 PyTorch tutorials \u7684\u4e2d\u6587\u7ffb\u8bd1\u7b14\u8bb0\u3002 \u70b9\u51fb\u4e0b\u9762\u7684\u94fe\u63a5\u53ef\u76f4\u63a5\u8bbf\u95ee\u8be5\u9879\u76ee\u751f\u6210\u7684 Web \u6587\u6863\u7ad9\u70b9\uff1a https://tanbro.github.io/pytorch-tutorials-notebooks-zhs/ \u6982\u8ff0 \u00b6 \u5b98\u65b9\u6559\u7a0b\u4ee5 \u5e26\u6709 Sphinx-Doc \u6269\u5c55 reST \u683c\u5f0f\u6ce8\u91ca\u7684 Python \u6e90\u4ee3\u7801 \u7684\u5f62\u5f0f\u5236\u4f5c\u4e86Web\u6587\u6863\u5e76\u5bfc\u51fa Jupyter \u7b14\u8bb0\u3002 \u800c\u8fd9\u4e2a\u9879\u76ee\u9664\u4e86\u7ffb\u8bd1\uff0c\u4fee\u6539\u8fd8\u6709: \u4f7f\u7528 Jupyter \u7b14\u8bb0\u6587\u4ef6\u4fdd\u5b58\u8bf4\u660e\u548c\u4ee3\u7801\u7247\u6bb5 \u4ece Jupyter \u7b14\u8bb0\u5bfc\u51fa Markdown \u6587\u4ef6\uff0c\u5e76\u5728\u5176\u57fa\u7840\u4e0a\u6784\u5efaWeb\u6587\u6863\u7ad9\u70b9 \u8fd9\u662f\u4e2a\u4eba\u7684\u7ffb\u8bd1\u7b14\u8bb0\uff0c\u7528\u4e8e\u4e2a\u4eba\u5b66\u4e60 PyTorch\u3002 \u76ee\u524d\u53ea\u7ffb\u8bd1\u548c\u8f6c\u6362\u4e86\u90e8\u5206 *.py \u6587\u4ef6(\u5b98\u65b9\u4f7f\u7528Sphinx-Gallery\u5c06\u5b83\u4eec\u8f6c\u6210 Sphinx-Doc \u6587\u6863\u5e76\u5bfc\u51fa Jupyter \u7b14\u8bb0) \uff0c\u6ca1\u6709\u5904\u7406\u4efb\u4f55 *.rst (\u5355\u7eaf\u7684 Sphinx-Doc \u6587\u6863) \u73af\u5883 \u00b6 \u8fd9\u4e2a\u5de5\u7a0b\u5728 Ubuntu 1604, 1804\u4e0b\uff0c\u4f7f\u7528 Python3.6 \u8fd0\u884c\u548c\u6784\u5efa\uff0c\u6ca1\u6709\u6d4b\u8bd5\u8fc7\u5176\u5b83\u73af\u5883\u3002 \u5f3a\u70c8 \u5efa\u8bae\u4e3a\u8fd9\u4e2a\u9879\u76ee\u4f7f\u7528\u5355\u72ec\u7684 venv \u6216 Conda \u73af\u5883 \u5305\u4f9d\u8d56 \u00b6 pip , Pipenv \u6216 Conda \u7b49\u5305\u7ba1\u7406\u5668\u5747\u53ef\u4e3a\u8fd9\u4e2a\u9879\u76ee\u5b89\u88c5\u6240\u9700\u7684\u5305\uff1a pip 1 2 # pip pip install -r requirements.txt Pipenv 1 2 # Pipenv pipenv install Conda 1 2 3 4 5 # Conda conda install -y jupyter nbconvert tqdm pandas matplotlib scikit-image pillow = 4 .1.1 pygments conda install -y -c conda-forge jupyterlab conda install -y pytorch torchvision cudatoolkit = 9 .0 -c pytorch pip install mkdocs pymdown-extensions mkdocs-material mkdocs-pdf-export-plugin \u8fd0\u884c Jupyter Lab/Notebook \u00b6 \u5165\u95e8\u6307\u5357\u7684\u7ffb\u8bd1\u4ee5 Jupyter \u7b14\u8bb0\u7684\u5f62\u5f0f\u4fdd\u5b58\u5728 notebooks \u5b50\u76ee\u5f55\u4e2d\u3002 \u8981\u542f\u52a8 Jupyter lab/notebook web \u670d\u52a1\u7a0b\u5e8f\uff0c\u53ef\u5206\u522b\u6267\u884c\uff1a JupyterLab 1 2 # JupyterLab jupyter-lab Jupyter Notebook 1 2 # Jupyter Notebook jupyter-notebook \u5728 Web \u754c\u9762\u4e2d\u8fdb\u5165 notebooks \u5b50\u76ee\u5f55\uff0c\u53ef\u4ee5\u627e\u5230\u6240\u6709\u7b14\u8bb0\u3002 \u6784\u5efa Web \u6587\u6863\u7ad9\u70b9 \u00b6 \u8fd9\u4e2a\u9879\u76ee\u4f7f\u7528 MkDocs \u5c06\u591a\u4e2a\u7531\u7b14\u8bb0\u5bfc\u51fa\u7684Markdown\u6587\u4ef6\u5408\u5e76\u751f\u6210\u4e00\u4e2aWeb\u6587\u6863\u7ad9\u70b9\u3002 \u628a\u7b14\u8bb0\u5bfc\u51fa\u4e3aMarkdown\u6587\u4ef6: 1 python tools/nbtomd.py \u6784\u5efa Web \u6587\u6863 1 mkdocs build \u542f\u52a8 Web \u6587\u6863\u670d\u52a1\u5668\uff1a 1 mkdocs serve \u7136\u540e\u5728\u6d4f\u89c8\u5668\u4e2d\u8bbf\u95ee http://localhost:8000/ \u5176\u5b83 \u00b6 Jupyter \u65e0\u6cd5\u4ee5 LaTeX/PDF \u683c\u5f0f\u5bfc\u51fa\u542b\u6709\u4e2d\u6587\u7684\u7b14\u8bb0 \u00b6 \u91c7\u7528\u6765\u81ea https://github.com/jupyter/notebook/issues/2848#issuecomment-372736199 \u7684\u65b9\u6cd5\uff1a \u5b89\u88c5 texlive \u548c pandoc 1 sudo apt install texlive-xetex pandoc \u4fee\u6539 nbconvert Article \u7684 LaTeX \u6a21\u677f\u6587\u4ef6 \u627e\u5230\u6587\u4ef6 site-packages/nbconvert/templates/latex/article.tplx , \u5728 \\documentclass[11pt]{article} \u4e4b\u540e\u52a0\u4e0a\u4e2d\u6587\u5b57\u4f53\u5b9a\u4e49\uff0c\u4fee\u6539\u540e\u7684\u5185\u5bb9\u5f62\u5982: 1 2 3 4 5 6 7 ((* block docclass *)) \\documentclass [11pt] { article } % \u4e2d\u6587\u95ee\u9898: https://github.com/jupyter/notebook/issues/2848#issuecomment-372736199 \\usepackage { xeCJK } \\setCJKmainfont { Noto Sans CJK SC } \\setCJKmonofont { Noto Sans Mono CJK SC } ((* endblock docclass *)) \u5177\u4f53\u91c7\u7528\u54ea\u4e2a\u7684\u5b57\u4f53\u5e94\u4ee5\u5b9e\u9645\u60c5\u51b5\u4e3a\u51c6\u3002","title":"README"},{"location":"#pytorch-tutorials-notebooks-zhs","text":"PyTorch tutorials \u7684\u4e2d\u6587\u7ffb\u8bd1\u7b14\u8bb0\u3002 \u70b9\u51fb\u4e0b\u9762\u7684\u94fe\u63a5\u53ef\u76f4\u63a5\u8bbf\u95ee\u8be5\u9879\u76ee\u751f\u6210\u7684 Web \u6587\u6863\u7ad9\u70b9\uff1a https://tanbro.github.io/pytorch-tutorials-notebooks-zhs/","title":"pytorch-tutorials-notebooks-zhs"},{"location":"#\u6982\u8ff0","text":"\u5b98\u65b9\u6559\u7a0b\u4ee5 \u5e26\u6709 Sphinx-Doc \u6269\u5c55 reST \u683c\u5f0f\u6ce8\u91ca\u7684 Python \u6e90\u4ee3\u7801 \u7684\u5f62\u5f0f\u5236\u4f5c\u4e86Web\u6587\u6863\u5e76\u5bfc\u51fa Jupyter \u7b14\u8bb0\u3002 \u800c\u8fd9\u4e2a\u9879\u76ee\u9664\u4e86\u7ffb\u8bd1\uff0c\u4fee\u6539\u8fd8\u6709: \u4f7f\u7528 Jupyter \u7b14\u8bb0\u6587\u4ef6\u4fdd\u5b58\u8bf4\u660e\u548c\u4ee3\u7801\u7247\u6bb5 \u4ece Jupyter \u7b14\u8bb0\u5bfc\u51fa Markdown \u6587\u4ef6\uff0c\u5e76\u5728\u5176\u57fa\u7840\u4e0a\u6784\u5efaWeb\u6587\u6863\u7ad9\u70b9 \u8fd9\u662f\u4e2a\u4eba\u7684\u7ffb\u8bd1\u7b14\u8bb0\uff0c\u7528\u4e8e\u4e2a\u4eba\u5b66\u4e60 PyTorch\u3002 \u76ee\u524d\u53ea\u7ffb\u8bd1\u548c\u8f6c\u6362\u4e86\u90e8\u5206 *.py \u6587\u4ef6(\u5b98\u65b9\u4f7f\u7528Sphinx-Gallery\u5c06\u5b83\u4eec\u8f6c\u6210 Sphinx-Doc \u6587\u6863\u5e76\u5bfc\u51fa Jupyter \u7b14\u8bb0) \uff0c\u6ca1\u6709\u5904\u7406\u4efb\u4f55 *.rst (\u5355\u7eaf\u7684 Sphinx-Doc \u6587\u6863)","title":"\u6982\u8ff0"},{"location":"#\u73af\u5883","text":"\u8fd9\u4e2a\u5de5\u7a0b\u5728 Ubuntu 1604, 1804\u4e0b\uff0c\u4f7f\u7528 Python3.6 \u8fd0\u884c\u548c\u6784\u5efa\uff0c\u6ca1\u6709\u6d4b\u8bd5\u8fc7\u5176\u5b83\u73af\u5883\u3002 \u5f3a\u70c8 \u5efa\u8bae\u4e3a\u8fd9\u4e2a\u9879\u76ee\u4f7f\u7528\u5355\u72ec\u7684 venv \u6216 Conda \u73af\u5883","title":"\u73af\u5883"},{"location":"#\u5305\u4f9d\u8d56","text":"pip , Pipenv \u6216 Conda \u7b49\u5305\u7ba1\u7406\u5668\u5747\u53ef\u4e3a\u8fd9\u4e2a\u9879\u76ee\u5b89\u88c5\u6240\u9700\u7684\u5305\uff1a pip 1 2 # pip pip install -r requirements.txt Pipenv 1 2 # Pipenv pipenv install Conda 1 2 3 4 5 # Conda conda install -y jupyter nbconvert tqdm pandas matplotlib scikit-image pillow = 4 .1.1 pygments conda install -y -c conda-forge jupyterlab conda install -y pytorch torchvision cudatoolkit = 9 .0 -c pytorch pip install mkdocs pymdown-extensions mkdocs-material mkdocs-pdf-export-plugin","title":"\u5305\u4f9d\u8d56"},{"location":"#\u8fd0\u884c-jupyter-labnotebook","text":"\u5165\u95e8\u6307\u5357\u7684\u7ffb\u8bd1\u4ee5 Jupyter \u7b14\u8bb0\u7684\u5f62\u5f0f\u4fdd\u5b58\u5728 notebooks \u5b50\u76ee\u5f55\u4e2d\u3002 \u8981\u542f\u52a8 Jupyter lab/notebook web \u670d\u52a1\u7a0b\u5e8f\uff0c\u53ef\u5206\u522b\u6267\u884c\uff1a JupyterLab 1 2 # JupyterLab jupyter-lab Jupyter Notebook 1 2 # Jupyter Notebook jupyter-notebook \u5728 Web \u754c\u9762\u4e2d\u8fdb\u5165 notebooks \u5b50\u76ee\u5f55\uff0c\u53ef\u4ee5\u627e\u5230\u6240\u6709\u7b14\u8bb0\u3002","title":"\u8fd0\u884c Jupyter Lab/Notebook"},{"location":"#\u6784\u5efa-web-\u6587\u6863\u7ad9\u70b9","text":"\u8fd9\u4e2a\u9879\u76ee\u4f7f\u7528 MkDocs \u5c06\u591a\u4e2a\u7531\u7b14\u8bb0\u5bfc\u51fa\u7684Markdown\u6587\u4ef6\u5408\u5e76\u751f\u6210\u4e00\u4e2aWeb\u6587\u6863\u7ad9\u70b9\u3002 \u628a\u7b14\u8bb0\u5bfc\u51fa\u4e3aMarkdown\u6587\u4ef6: 1 python tools/nbtomd.py \u6784\u5efa Web \u6587\u6863 1 mkdocs build \u542f\u52a8 Web \u6587\u6863\u670d\u52a1\u5668\uff1a 1 mkdocs serve \u7136\u540e\u5728\u6d4f\u89c8\u5668\u4e2d\u8bbf\u95ee http://localhost:8000/","title":"\u6784\u5efa Web \u6587\u6863\u7ad9\u70b9"},{"location":"#\u5176\u5b83","text":"","title":"\u5176\u5b83"},{"location":"#jupyter-\u65e0\u6cd5\u4ee5-latexpdf-\u683c\u5f0f\u5bfc\u51fa\u542b\u6709\u4e2d\u6587\u7684\u7b14\u8bb0","text":"\u91c7\u7528\u6765\u81ea https://github.com/jupyter/notebook/issues/2848#issuecomment-372736199 \u7684\u65b9\u6cd5\uff1a \u5b89\u88c5 texlive \u548c pandoc 1 sudo apt install texlive-xetex pandoc \u4fee\u6539 nbconvert Article \u7684 LaTeX \u6a21\u677f\u6587\u4ef6 \u627e\u5230\u6587\u4ef6 site-packages/nbconvert/templates/latex/article.tplx , \u5728 \\documentclass[11pt]{article} \u4e4b\u540e\u52a0\u4e0a\u4e2d\u6587\u5b57\u4f53\u5b9a\u4e49\uff0c\u4fee\u6539\u540e\u7684\u5185\u5bb9\u5f62\u5982: 1 2 3 4 5 6 7 ((* block docclass *)) \\documentclass [11pt] { article } % \u4e2d\u6587\u95ee\u9898: https://github.com/jupyter/notebook/issues/2848#issuecomment-372736199 \\usepackage { xeCJK } \\setCJKmainfont { Noto Sans CJK SC } \\setCJKmonofont { Noto Sans Mono CJK SC } ((* endblock docclass *)) \u5177\u4f53\u91c7\u7528\u54ea\u4e2a\u7684\u5b57\u4f53\u5e94\u4ee5\u5b9e\u9645\u60c5\u51b5\u4e3a\u51c6\u3002","title":"Jupyter \u65e0\u6cd5\u4ee5 LaTeX/PDF \u683c\u5f0f\u5bfc\u51fa\u542b\u6709\u4e2d\u6587\u7684\u7b14\u8bb0"},{"location":"beginner/chatbot_tutorial/","text":"\u804a\u5929\u673a\u5668\u4eba\u6559\u7a0b \u00b6 \u4f5c\u8005\uff1a Matthew Inkawhich \u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u63a2\u7d22\u4e00\u4e2a\u6709\u8da3\u7684\u5e8f\u5217\u5230\u5e8f\u5217(Seq2Seq)\u6a21\u578b\u7684\u7528\u4f8b\u3002\u6211\u4eec\u5c06\u4f7f\u7528 Cornell Movie-Dialogs Corpus \u7684\u7535\u5f71\u811a\u672c\u8bad\u7ec3\u4e00\u4e2a\u7b80\u5355\u7684\u804a\u5929\u673a\u5668\u4eba\u3002 \u5bf9\u8bdd\u6a21\u578b\u662f\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u7684\u70ed\u95e8\u8bdd\u9898\u3002 \u804a\u5929\u673a\u5668\u4eba\u5728\u5305\u62ec\u5ba2\u6237\u670d\u52a1\u5e94\u7528\u548c\u5728\u7ebf\u5e2e\u52a9\u53f0\u5728\u5185\u7684\u5404\u79cd\u573a\u666f\u90fd\u7528\u5e94\u7528\u3002 \u8fd9\u4e9b\u673a\u5668\u4eba\u901a\u5e38\u4f7f\u7531\u57fa\u4e8e\u7d22\u5f15\u7684\u6a21\u578b\u9a71\u52a8\u7684\uff0c\u5b83\u4eec\u54cd\u5e94\u7279\u5b9a\u7684\u95ee\u9898\uff0c\u8f93\u51fa\u9884\u5b9a\u4e49\u7684\u54cd\u5e94\u3002\u5728\u50cf\u516c\u53f8IT\u5e2e\u52a9\u53f0\u8fd9\u6837\u9ad8\u5ea6\u53d7\u9650\u7684\u9886\u57df\u4e2d\uff0c\u8fd9\u4e9b\u6a21\u578b\u53ef\u80fd\u662f\u4f4f\u591f\u4e86\uff0c\u4e0d\u8fc7\u5728\u901a\u5e38\u7684\u72b6\u51b5\u4e0b\uff0c\u4ed6\u4eec\u662f\u4e0d\u591f\u5065\u58ee\u7684\u3002 \u6559\u4e00\u53f0\u673a\u5668\u4e0e\u4eba\u7c7b\u5728\u591a\u4e2a\u9886\u57df\u5185\u8fdb\u884c\u6709\u610f\u4e49\u7684\u5bf9\u8bdd\u662f\u4e00\u4e2a\u8fdc\u672a\u89e3\u51b3\u7684\u7814\u7a76\u6027\u95ee\u9898\u3002 \u5728\u6700\u8fd1\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u70ed\u6f6e\u4e2d\uff0c\u50cf Google Neural Conversational Model \u8fd9\u6837\u7684\u5f3a\u5927\u7684\u751f\u6210\u6a21\u578b\u7684\u51fa\u73b0\uff0c\u6807\u5fd7\u7740\u591a\u9886\u57df\u751f\u6210\u5bf9\u8bdd\u6a21\u578b\u7684\u4e00\u5927\u8fdb\u6b65\u3002 \u5728\u8fd9\u4e2a\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u7528 PyTorch \u5b9e\u73b0\u8fd9\u79cd\u6a21\u578b\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 > hello? Bot: hello . > where am I? Bot: you re in a hospital . > who are you? Bot: i m a lawyer . > how are you doing? Bot: i m fine . > are you my friend? Bot: no . > you ' re under arrest Bot: i m trying to help you ! > i ' m just kidding Bot: i m sorry . > where are you from? Bot: san francisco . > it ' s time for me to leave Bot: i know . > goodbye Bot: goodbye . \u6559\u7a0b\u8981\u70b9\uff1a \u52a0\u8f7d\u548c\u9884\u5904\u7406 Cornell Movie-Dialogs Corpus \u6570\u636e\u96c6 \u4f7f\u7528 Luong attention mechanism(s) \u5b9e\u73b0\u4e00\u4e2a\u5e8f\u5217\u5230\u5e8f\u5217\u6a21\u578b \u4f7f\u7528\u5c0f\u6279\u6b21\u8054\u5408\u8bad\u7ec3\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u6a21\u578b \u5b9e\u73b0\u8d2a\u5a6a\u641c\u7d22\u89e3\u7801\u6a21\u5757 \u4e0e\u8bad\u7ec3\u540e\u7684\u804a\u5929\u673a\u5668\u4eba\u4e92\u52a8 \u81f4\u8c22\uff1a \u672c\u6559\u7a0b\u501f\u7528\u4ee5\u4e0b\u6765\u6e90\u7684\u4ee3\u7801\uff1a Yuan-Kuei Wu \u7684 pytorch-chatbot \u5b9e\u73b0: https://github.com/ywk991112/pytorch-chatbot Sean Robertson \u7684 practical-pytorch seq2seq-translation \u4f8b\u5b50: https://github.com/spro/practical-pytorch/tree/master/seq2seq-translation FloydHub \u7684 Cornell Movie Corpus \u9884\u5904\u7406\u4ee3\u7801: https://github.com/floydhub/textutil-preprocess-cornell-movie-corpus \u51c6\u5907\u5de5\u4f5c \u00b6 \u9996\u5148\uff0c\u4ece https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html \u4e0b\u8f7dZIP\u6587\u4ef6\u89e3\u538b\u7f29\u5230\u5f53\u524d\u76ee\u5f55\u7684 data \u5b50\u76ee\u5f55\u4e2d\u3002 \u7136\u540e\uff0c\u6211\u4eec\u8fdb\u884c\u4e00\u4e9b\u5fc5\u8981\u7684\u5f15\u7528\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from __future__ import absolute_import , division , print_function , unicode_literals import torch from torch.jit import script , trace import torch.nn as nn from torch import optim import torch.nn.functional as F import csv import random import re import os import unicodedata import codecs from io import open import itertools import math USE_CUDA = torch . cuda . is_available () device = torch . device ( \"cuda\" if USE_CUDA else \"cpu\" ) print ( device ) 1 cuda \u6570\u636e\u52a0\u8f7d\u4e0e\u9884\u5904\u7406 \u00b6 \u4e0b\u4e00\u6b65\uff0c\u5c06\u6211\u4eec\u4e4b\u524d\u4e0b\u8f7d\u7684\u6570\u636e\u91cd\u65b0\u683c\u5f0f\u5316\uff0c\u5e76\u628a\u6570\u636e\u52a0\u8f7d\u5230\u6211\u4eec\u6240\u8981\u4f7f\u7528\u7684\u7ed3\u6784\u4e2d\u3002 Cornell Movie-Dialogs Corpus \u662f\u4e00\u4e2a\u5185\u5bb9\u4e30\u5bcc\u7684\u7535\u5f71\u89d2\u8272\u5bf9\u8bdd\u6570\u636e\u96c6\uff1a \u6765\u81ea10,292\u5bf9\u7535\u5f71\u89d2\u8272\u7684220,579\u4e2a\u5bf9\u8bdd \u6765\u81ea617\u90e8\u7535\u5f71\u76849,035\u4e2a\u89d2\u8272 \u603b\u5171304,713\u53e5\u8bdd \u8fd9\u4e2a\u6570\u636e\u96c6\u5e9e\u5927\u4e14\u591a\u6837\uff0c\u5176\u8bed\u8a00\u5f62\u5f0f\u3001\u65f6\u95f4\u6bb5\u3001\u60c5\u611f\u90fd\u6709\u5f88\u5927\u53d8\u5316\u3002\u6211\u4eec\u5e0c\u671b\u8fd9\u79cd\u591a\u6837\u6027\u4f7f\u6211\u4eec\u7684\u6a21\u578b\u80fd\u591f\u9002\u5e94\u591a\u79cd\u5f62\u5f0f\u7684\u8f93\u5165\u548c\u8be2\u95ee\u3002 \u9996\u5148\uff0c\u6211\u4eec\u770b\u51e0\u884c\u6570\u636e\u6587\u4ef6\uff0c\u4e86\u89e3\u5176\u539f\u59cb\u683c\u5f0f\u3002 1 2 3 4 5 6 7 8 9 10 corpus_name = \"cornell movie-dialogs corpus\" corpus = os . path . join ( \"data\" , corpus_name ) def printLines ( file , n = 10 ): with open ( file , encoding = 'iso-8859-1' ) as datafile : for i , line in enumerate ( datafile ): if i < n : print ( line ) printLines ( os . path . join ( corpus , \"movie_lines.txt\" )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not! L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to! L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so. L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay? L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go. L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie. L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I'm kidding. You know how sometimes you just become this \"persona\"? And you don't know how to quit? L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels? \u521b\u5efa\u683c\u5f0f\u5316\u6570\u636e\u6587\u4ef6 \u00b6 \u4e3a\u65b9\u4fbf\u8d77\u89c1\uff0c\u6211\u4eec\u5c06\u521b\u5efa\u4e00\u4e2a\u683c\u5f0f\u826f\u597d\u7684\u6570\u636e\u6587\u4ef6\uff0c\u5176\u4e2d\u6bcf\u4e00\u884c\u5305\u542b\u4e00\u4e2a\u4ee5\u5236\u8868\u7b26\u5206\u9694\u7684 \u8be2\u95ee\u8bed\u53e5 \u548c\u4e00\u4e2a \u54cd\u5e94\u8bed\u53e5 \u5bf9\u3002 \u4ee5\u4e0b\u51fd\u6570\u7528\u4e8e\u5206\u6790 movie_lines.txt \u6570\u636e\u6587\u4ef6\u7684\u539f\u59cb\u884c\u6570\u636e loadLines \u5c06\u6bcf\u4e00\u884c\u5206\u5272\u6210\u5177\u6709\u591a\u4e2a\u5b57\u6bb5(lineID, characterID, movieID, character, text)\u7684\u5b57\u5178 loadConversations \u5c06\u6765\u81ea loadLines \u7684\uff0c\u5305\u542b\u591a\u4e2a\u5b57\u6bb5\u7684\u884c\uff0c\u6839\u636e movie_conversations.txt \u7ec4\u6210\u5bf9\u8bdd\u3002 extractSentencePairs \u4ece\u5bf9\u8bdd\u4e2d\u63d0\u53d6\u201c\u8be2\u95ee-\u56de\u7b54\u201d\u53e5\u5b50\u5bf9 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 # Splits each line of the file into a dictionary of fields def loadLines ( fileName , fields ): lines = {} with open ( fileName , encoding = 'iso-8859-1' ) as f : for line in f : values = line . split ( \" +++$+++ \" ) # Extract fields lineObj = {} for i , field in enumerate ( fields ): lineObj [ field ] = values [ i ] lines [ lineObj [ 'lineID' ]] = lineObj return lines # Groups fields of lines from `loadLines` into conversations based on *movie_conversations.txt* def loadConversations ( fileName , lines , fields ): conversations = [] with open ( fileName , encoding = 'iso-8859-1' ) as f : for line in f : values = line . split ( \" +++$+++ \" ) # Extract fields convObj = {} for i , field in enumerate ( fields ): convObj [ field ] = values [ i ] # Convert string to list (convObj[\"utteranceIDs\"] == \"['L598485', 'L598486', ...]\") lineIds = eval ( convObj [ \"utteranceIDs\" ]) # Reassemble lines convObj [ \"lines\" ] = [] for lineId in lineIds : convObj [ \"lines\" ] . append ( lines [ lineId ]) conversations . append ( convObj ) return conversations # Extracts pairs of sentences from conversations def extractSentencePairs ( conversations ): qa_pairs = [] for conversation in conversations : # Iterate over all the lines of the conversation for i in range ( len ( conversation [ \"lines\" ]) - 1 ): # We ignore the last line (no answer for it) inputLine = conversation [ \"lines\" ][ i ][ \"text\" ] . strip () targetLine = conversation [ \"lines\" ][ i + 1 ][ \"text\" ] . strip () # Filter wrong samples (if one of the lists is empty) if inputLine and targetLine : qa_pairs . append ([ inputLine , targetLine ]) return qa_pairs \u73b0\u5728\uff0c\u8c03\u7528\u8fd9\u4e9b\u51fd\u6570\uff0c\u5e76\u521b\u5efa\u6587\u4ef6\uff0c\u6587\u4ef6\u540d\u662f formatted_movie_lines.txt \u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # Define path to new file datafile = os . path . join ( corpus , \"formatted_movie_lines.txt\" ) delimiter = ' \\t ' # Unescape the delimiter delimiter = str ( codecs . decode ( delimiter , \"unicode_escape\" )) # Initialize lines dict, conversations list, and field ids lines = {} conversations = [] MOVIE_LINES_FIELDS = [ \"lineID\" , \"characterID\" , \"movieID\" , \"character\" , \"text\" ] MOVIE_CONVERSATIONS_FIELDS = [ \"character1ID\" , \"character2ID\" , \"movieID\" , \"utteranceIDs\" ] # Load lines and process conversations print ( \" \\n Processing corpus...\" ) lines = loadLines ( os . path . join ( corpus , \"movie_lines.txt\" ), MOVIE_LINES_FIELDS ) print ( \" \\n Loading conversations...\" ) conversations = loadConversations ( os . path . join ( corpus , \"movie_conversations.txt\" ), lines , MOVIE_CONVERSATIONS_FIELDS ) # Write new csv file print ( \" \\n Writing newly formatted file...\" ) with open ( datafile , 'w' , encoding = 'utf-8' ) as outputfile : writer = csv . writer ( outputfile , delimiter = delimiter , lineterminator = ' \\n ' ) for pair in extractSentencePairs ( conversations ): writer . writerow ( pair ) # Print a sample of lines print ( \" \\n Sample lines from file:\" ) printLines ( datafile ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 Processing corpus... Loading conversations... Writing newly formatted file... Sample lines from file: Can we make this quick? Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad. Again. Well, I thought we'd start with pronunciation, if that's okay with you. Well, I thought we'd start with pronunciation, if that's okay with you. Not the hacking and gagging and spitting part. Please. Not the hacking and gagging and spitting part. Please. Okay... then how 'bout we try out some French cuisine. Saturday? Night? You're asking me out. That's so cute. What's your name again? Forget it. No, no, it's my fault -- we didn't have a proper introduction --- Cameron. Cameron. The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser. My sister. I can't date until she does. The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser. My sister. I can't date until she does. Seems like she could get a date easy enough... Why? Unsolved mystery. She used to be really popular when she started high school, then it was just like she got sick of it or something. Unsolved mystery. She used to be really popular when she started high school, then it was just like she got sick of it or something. That's a shame. Gosh, if only we could find Kat a boyfriend... Let me see what I can do. \u6570\u636e\u52a0\u8f7d\u548c\u4fee\u526a \u00b6 \u6211\u4eec\u7684\u4e0b\u4e00\u4e2a\u4efb\u52a1\u662f\u521b\u5efa\u8bcd\u6c47\u8868\u5e76\u5c06\u8be2\u95ee/\u54cd\u5e94\u53e5\u5b50\u5bf9\u52a0\u8f7d\u5230\u5185\u5b58\u4e2d\u3002 \u6ce8\u610f\u6211\u4eec\u5904\u7406\u7684\u662f \u8bcd \u5e8f\u5217\uff0c\u5b83\u4eec\u6ca1\u6709\u9690\u5f0f\u6620\u5c04\u5230\u79bb\u6563\u6570\u503c\u7a7a\u95f4\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5fc5\u987b\u521b\u5efa\u4e00\u4e2a\u5c06\u6211\u4eec\u5728\u6570\u636e\u96c6\u4e2d\u9047\u5230\u7684\u6bcf\u4e2a\u552f\u4e00\u8bcd\u6620\u5c04\u5230\u7d22\u5f15\u503c\u7684\u6620\u5c04(Mapping)\u3002 \u4e3a\u6b64\uff0c\u6211\u4eec\u5b9a\u4e49\u4e00\u4e2a Voc \u7c7b\uff0c\u5b83\u4fdd\u5b58\u4ece\u8bcd\u5230\u7d22\u5f15\u503c\u7684\u6620\u5c04\uff0c\u4ee5\u53ca\u4ece\u7d22\u5f15\u503c\u5230\u8bcd\u7684\u53cd\u5411\u6620\u5c04\uff0c\u552f\u4e00\u8bcd\u7684\u6570\u91cf\uff0c\u603b\u8bcd\u6570\u3002 \u8fd9\u4e2a\u7c7b\u63d0\u4f9b\u4e86\u50cf\u8bcd\u8868\u6dfb\u52a0\u8bcd\u7684\u65b9\u6cd5( addWord )\uff0c\u6dfb\u52a0\u53e5\u5b50\u4e2d\u6240\u6709\u8bcd\u7684\u65b9\u6cd5( addSentence )\uff0c\u4ee5\u53ca\u524a\u51cf\u4e0d\u5e38\u89c1\u8bcd\u7684\u65b9\u6cd5( trim )\u3002 More on trimming later. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 # Default word tokens PAD_token = 0 # Used for padding short sentences SOS_token = 1 # Start-of-sentence token EOS_token = 2 # End-of-sentence token class Voc : def __init__ ( self , name ): self . name = name self . trimmed = False self . word2index = {} self . word2count = {} self . index2word = { PAD_token : \"PAD\" , SOS_token : \"SOS\" , EOS_token : \"EOS\" } self . num_words = 3 # Count SOS, EOS, PAD def addSentence ( self , sentence ): for word in sentence . split ( ' ' ): self . addWord ( word ) def addWord ( self , word ): if word not in self . word2index : self . word2index [ word ] = self . num_words self . word2count [ word ] = 1 self . index2word [ self . num_words ] = word self . num_words += 1 else : self . word2count [ word ] += 1 # Remove words below a certain count threshold def trim ( self , min_count ): if self . trimmed : return self . trimmed = True keep_words = [] for k , v in self . word2count . items (): if v >= min_count : keep_words . append ( k ) print ( 'keep_words {} / {} = {:.4f}' . format ( len ( keep_words ), len ( self . word2index ), len ( keep_words ) / len ( self . word2index ) )) # Reinitialize dictionaries self . word2index = {} self . word2count = {} self . index2word = { PAD_token : \"PAD\" , SOS_token : \"SOS\" , EOS_token : \"EOS\" } self . num_words = 3 # Count default tokens for word in keep_words : self . addWord ( word ) \u73b0\u5728\uff0c\u53ef\u4ee5\u5f00\u59cb\u88c5\u914d\u8bcd\u8868\u548c\u8be2\u95ee/\u56de\u590d\u53e5\u5b50\u5bf9\u3002\u5728\u51c6\u5907\u597d\u4f7f\u7528\u8fd9\u4e9b\u6570\u636e\u4e4b\u524d\uff0c\u6211\u4eec\u8fd8\u5f97\u6267\u884c\u4e00\u4e9b\u9884\u5904\u7406\u3002 \u9996\u5148\uff0c\u6211\u4eec\u5fc5\u987b\u4f7f\u7528 unicodeToAscii \u5c06\u5b57\u7b26\u4e32\u4ece Unicode \u8f6c\u4e3a ASCII \u3002 \u5176\u6b21\uff0c\u6211\u4eec\u5e94\u8be5\u628a\u6240\u6709\u5b57\u6bcd\u8f6c\u4e3a\u5c0f\u5199\uff0c\u5e76\u4e14\u88c1\u526a\u6389\u6240\u6709\u7684\u9664\u57fa\u672c\u6807\u70b9\u7b26\u53f7\u4e4b\u5916\u7684\u6240\u6709\u975e\u5b57\u6bcd\u5b57\u7b26( normalizeString )\u3002 \u6700\u540e\uff0c\u4e3a\u4e86\u8bad\u7ec3\u66f4\u5feb\u7684\u6536\u655b\uff0c\u6211\u4eec\u5c06\u8fc7\u6ee4\u6389( filterPairs )\u957f\u5ea6\u5927\u4e8e MAX_LENGTH \u9608\u503c\u7684\u53e5\u5b50\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 MAX_LENGTH = 10 # Maximum sentence length to consider # Turn a Unicode string to plain ASCII, thanks to # https://stackoverflow.com/a/518232/2809427 def unicodeToAscii ( s ): return '' . join ( c for c in unicodedata . normalize ( 'NFD' , s ) if unicodedata . category ( c ) != 'Mn' ) # Lowercase, trim, and remove non-letter characters def normalizeString ( s ): s = unicodeToAscii ( s . lower () . strip ()) s = re . sub ( r \"([.!?])\" , r \" \\1\" , s ) s = re . sub ( r \"[^a-zA-Z.!?]+\" , r \" \" , s ) s = re . sub ( r \"\\s+\" , r \" \" , s ) . strip () return s # Read query/response pairs and return a voc object def readVocs ( datafile , corpus_name ): print ( \"Reading lines...\" ) # Read the file and split into lines lines = open ( datafile , encoding = 'utf-8' ) . \\ read () . strip () . split ( ' \\n ' ) # Split every line into pairs and normalize pairs = [[ normalizeString ( s ) for s in l . split ( ' \\t ' )] for l in lines ] voc = Voc ( corpus_name ) return voc , pairs # Returns True if both sentences in a pair 'p' are under the MAX_LENGTH threshold def filterPair ( p ): # Input sequences need to preserve the last word for EOS token return all ( len ( s . split ( ' ' )) < MAX_LENGTH for s in p ) # Filter pairs using filterPair condition def filterPairs ( pairs ): return [ pair for pair in pairs if filterPair ( pair )] # Using the functions defined above, return a populated voc object and pairs list def loadPrepareData ( corpus , corpus_name , datafile , save_dir ): print ( \"Start preparing training data ...\" ) voc , pairs = readVocs ( datafile , corpus_name ) print ( \"Read {!s} sentence pairs\" . format ( len ( pairs ))) pairs = filterPairs ( pairs ) print ( \"Trimmed to {!s} sentence pairs\" . format ( len ( pairs ))) print ( \"Counting words...\" ) for pair in pairs : voc . addSentence ( pair [ 0 ]) voc . addSentence ( pair [ 1 ]) print ( \"Counted words:\" , voc . num_words ) return voc , pairs # Load/Assemble voc and pairs save_dir = os . path . join ( \"data\" , \"save\" ) voc , pairs = loadPrepareData ( corpus , corpus_name , datafile , save_dir ) # Print some pairs to validate print ( \" \\n pairs:\" ) for pair in pairs [: 10 ]: print ( pair ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 Start preparing training data ... Reading lines... Read 221282 sentence pairs Trimmed to 64217 sentence pairs Counting words... Counted words: 17996 pairs: ['there .', 'where ?'] ['you have my word . as a gentleman', 'you re sweet .'] ['hi .', 'looks like things worked out tonight huh ?'] ['you know chastity ?', 'i believe we share an art instructor'] ['have fun tonight ?', 'tons'] ['well no . . .', 'then that s all you had to say .'] ['then that s all you had to say .', 'but'] ['but', 'you always been this selfish ?'] ['do you listen to this crap ?', 'what crap ?'] ['what good stuff ?', 'the real you .'] \u53e6\u4e00\u79cd\u6709\u5229\u4e8e\u5728\u8bad\u7ec3\u671f\u95f4\u5b9e\u73b0\u66f4\u5feb\u6536\u655b\u7684\u7b56\u7565\u662f\u4fee\u526a\u6389\u6211\u4eec\u8bcd\u6c47\u8868\u4e2d\u5f88\u5c11\u4f7f\u7528\u7684\u5355\u8bcd\u3002\u51cf\u5c0f\u7279\u5f81\u7a7a\u95f4\u4e5f\u4f1a\u7f13\u548c\u6a21\u578b\u903c\u8fd1\u7684\u96be\u5ea6\u3002\u6211\u4eec\u5c06\u901a\u8fc7\u4e24\u4e2a\u6b65\u9aa4\u5b8c\u6210\u6b64\u64cd\u4f5c\uff1a \u7528 voc.trim \u4fee\u526a\u6389\u6570\u91cf\u5c0f\u4e8e MIN_COUNT \u9608\u503c\u7684\u8bcd\u3002 \u8fc7\u6ee4\u6389\u542b\u6709\u88ab\u4fee\u526a\u8bcd\u7684\u53e5\u5b50\u5bf9\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 MIN_COUNT = 3 # Minimum word count threshold for trimming def trimRareWords ( voc , pairs , MIN_COUNT ): # Trim words used under the MIN_COUNT from the voc voc . trim ( MIN_COUNT ) # Filter out pairs with trimmed words keep_pairs = [] for pair in pairs : input_sentence = pair [ 0 ] output_sentence = pair [ 1 ] keep_input = True keep_output = True # Check input sentence for word in input_sentence . split ( ' ' ): if word not in voc . word2index : keep_input = False break # Check output sentence for word in output_sentence . split ( ' ' ): if word not in voc . word2index : keep_output = False break # Only keep pairs that do not contain trimmed word(s) in their input or output sentence if keep_input and keep_output : keep_pairs . append ( pair ) print ( \"Trimmed from {} pairs to {}, {:.4f} of total\" . format ( len ( pairs ), len ( keep_pairs ), len ( keep_pairs ) / len ( pairs ))) return keep_pairs # Trim voc and pairs pairs = trimRareWords ( voc , pairs , MIN_COUNT ) 1 2 keep_words 7820 / 17993 = 0.4346 Trimmed from 64217 pairs to 53120, 0.8272 of total \u4e3a\u6a21\u578b\u51c6\u5907\u6570\u636e \u00b6 \u5c3d\u7ba1\u6211\u4eec\u5df2\u7ecf\u6295\u5165\u5f88\u5927\u7cbe\u529b\u6765\u51c6\u5907\u548c\u6574\u7406\u6570\u636e\uff0c\u628a\u5b83\u4eec\u653e\u5230\u4e86\u6f02\u4eae\u7684\u8bcd\u8868\u5bf9\u8c61\u548c\u53e5\u5b50\u5bf9\u5217\u8868\uff0c\u4f46\u6211\u4eec\u7684\u6a21\u578b\u6700\u7ec8\u9700\u8981\u7684\u7684\u8f93\u5165\u5374\u662fTensor\u7684\u5f20\u91cf\u6570\u5b57\u3002 \u53ef\u4ee5\u5728 Seq2Seq\u7ffb\u8bd1\u6559\u7a0b \u4e2d\u627e\u5230\u4e3a\u6a21\u578b\u51c6\u5907\u9884\u5904\u7406\u6570\u636e\u7684\u65b9\u6cd5\u3002 \u5728\u90a3\u4e2a\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528\u7684\u6279\u6b21\u503c\u4e3a1\uff0c\u8fd9\u610f\u5473\u7740\u6211\u4eec\u6240\u8981\u505a\u7684\u5c31\u662f\u5c06\u53e5\u5b50\u5bf9\u4e2d\u7684\u5355\u8bcd\u8f6c\u6362\u4e3a\u8bcd\u6c47\u8868\u4e2d\u5bf9\u5e94\u7684\u7d22\u5f15\u503c\uff0c\u5e76\u5c06\u5176\u63d0\u4f9b\u7ed9\u6a21\u578b\u3002 \u53e6\u5916\uff0c\u5982\u679c\u4f60\u5bf9\u52a0\u901f\u57f9\u8bad\u548c/\u6216\u60f3\u8981\u5229\u7528GPU\u5e76\u884c\u5316\u529f\u80fd\u611f\u5174\u8da3\uff0c\u5219\u9700\u8981\u4f7f\u7528\u5c0f\u6279\u91cf\u57f9\u8bad\u3002 \u4f7f\u7528\u5c0f\u6279\u91cf\u4e5f\u610f\u5473\u7740\u6211\u4eec\u5fc5\u987b\u6ce8\u610f\u6279\u91cf\u4e2d\u53e5\u5b50\u957f\u5ea6\u7684\u53d8\u5316\u3002 \u4e3a\u4e86\u5bb9\u7eb3\u540c\u4e00\u6279\u6b21\u4e2d\u4e0d\u540c\u5927\u5c0f\u7684\u53e5\u5b50\uff0c\u6211\u4eec\u8981\u8ba9\u6279\u91cf\u8f93\u5165\u5f20\u91cf\u7684\u5f62\u72b6 (max_length\uff0cbatch_size) \u4e2d\u7684\u77ed\u4e8e max_length \u7684\u53e5\u5b50\u5728 EOS_token \u4e4b\u540e\u7528\u96f6\u586b\u5145\u3002 \u5982\u679c\u6211\u4eec\u53ea\u662f\u7b80\u5355\u5730\u901a\u8fc7\u5c06\u5355\u8bcd\u8f6c\u6362\u4e3a\u5176\u7d22\u5f15\u503c( indexesFromSentence )\u548c\u96f6\u586b\u5145\u7684\u65b9\u6cd5\u5c06\u82f1\u8bed\u53e5\u5b50\u8f6c\u6362\u4e3a\u5f20\u91cf\uff0c\u5f20\u91cf\u7684\u5f62\u72b6\u5c06\u662f (batch_size\uff0cmax_length) \uff0c\u5e76\u4e14\u5728\u7b2c\u4e00\u7ef4\u4e0a\u7684\u7d22\u5f15\u5c06\u5728\u6240\u6709\u65f6\u95f4\u6b65\u9aa4\u4e2d\u8fd4\u56de\u5b8c\u6574\u5e8f\u5217\u3002 \u4f46\u662f\uff0c\u6211\u4eec\u9700\u8981\u80fd\u591f\u6cbf\u7740\u65f6\u95f4\u3001\u8de8\u6279\u6b21\u3001\u5728\u6240\u6709\u5e8f\u5217\u4e0a\u8fdb\u884c\u7d22\u5f15\u3002 \u56e0\u6b64\uff0c\u6211\u4eec\u5c06\u8f93\u5165\u6279\u5904\u7406\u7684\u5f62\u72b6\u8f6c\u7f6e\u4e3a (max_length\uff0cbatch_size) \uff0c\u4ee5\u4fbf\u8de8\u7b2c\u4e00\u7ef4\u7684\u7d22\u5f15\u8fd4\u56de\u6279\u4e2d\u6240\u6709\u53e5\u5b50\u7684\u65f6\u95f4\u6b65\u957f\u3002 \u6211\u4eec\u5728 zeroPadding \u51fd\u6570\u4e2d\u9690\u5f0f\u5904\u7406\u8fd9\u4e2a\u8f6c\u7f6e\u3002 \u51fd\u6570 inputVar \u5904\u7406\u53e5\u5b50\u5230\u5f20\u91cf\u7684\u8f6c\u6362\u8fc7\u7a0b\uff0c\u6700\u7ec8\u521b\u5efa\u4e00\u4e2a\u5f62\u72b6\u6b63\u786e\u7684\u96f6\u586b\u5145\u5f20\u91cf\u3002 \u5b83\u8fd8\u8fd4\u56de\u6279\u6b21\u4e2d\u6bcf\u4e2a\u5e8f\u5217\u7684\u957f\u5ea6( lengths )\u7684\u5f20\u91cf\uff0c\u5b83\u7a0d\u540e\u4f1a\u88ab\u4f20\u7ed9\u7f16\u7801\u5668\u3002 \u51fd\u6570 outputVar \u7684\u6267\u884c\u8fc7\u7a0b\u4e0e inputVar \u7c7b\u4f3c\uff0c\u4f46\u662f\u4e0d\u8fd4\u56de\u957f\u5ea6\uff08 lenghts \uff09\u5f20\u91cf\uff0c\u800c\u662f\u8fd4\u56de\u4e8c\u8fdb\u5236\u7684\u63a9\u7801\u5f20\u91cf\u548c\u76ee\u6807\u53e5\u5b50\u7684\u6700\u5927\u957f\u5ea6\u3002 \u4e8c\u8fdb\u5236\u63a9\u7801\u4e0e\u8f93\u51fa\u76ee\u6807\u5f20\u91cf\u5177\u6709\u540c\u6837\u7684\u5f62\u72b6\uff0c\u4f46\u662f\u5176\u4e2d\u7684\u6bcf\u4e2a PAD_token \u5143\u7d20\u90fd\u4e3a0\uff0c\u5176\u5b83\u6240\u6709\u5143\u7d20\u90fd\u4e3a1\u3002 batch2TrainData \u7b80\u5355\u7684\u4f7f\u7528\u4e00\u5806\u53e5\u5b50\u5bf9\u5e76\u4f7f\u7528\u4e0a\u8ff0\u51fd\u6570\u8fd4\u56de\u8f93\u5165\u548c\u76ee\u6807\u5f20\u91cf\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 def indexesFromSentence ( voc , sentence ): return [ voc . word2index [ word ] for word in sentence . split ( ' ' )] + [ EOS_token ] def zeroPadding ( l , fillvalue = PAD_token ): return list ( itertools . zip_longest ( * l , fillvalue = fillvalue )) def binaryMatrix ( l , value = PAD_token ): m = [] for i , seq in enumerate ( l ): m . append ([]) for token in seq : if token == PAD_token : m [ i ] . append ( 0 ) else : m [ i ] . append ( 1 ) return m # Returns padded input sequence tensor and lengths def inputVar ( l , voc ): indexes_batch = [ indexesFromSentence ( voc , sentence ) for sentence in l ] lengths = torch . tensor ([ len ( indexes ) for indexes in indexes_batch ]) padList = zeroPadding ( indexes_batch ) padVar = torch . LongTensor ( padList ) return padVar , lengths # Returns padded target sequence tensor, padding mask, and max target length def outputVar ( l , voc ): indexes_batch = [ indexesFromSentence ( voc , sentence ) for sentence in l ] max_target_len = max ([ len ( indexes ) for indexes in indexes_batch ]) padList = zeroPadding ( indexes_batch ) mask = binaryMatrix ( padList ) mask = torch . ByteTensor ( mask ) padVar = torch . LongTensor ( padList ) return padVar , mask , max_target_len # Returns all items for a given batch of pairs def batch2TrainData ( voc , pair_batch ): pair_batch . sort ( key = lambda x : len ( x [ 0 ] . split ( \" \" )), reverse = True ) input_batch , output_batch = [], [] for pair in pair_batch : input_batch . append ( pair [ 0 ]) output_batch . append ( pair [ 1 ]) inp , lengths = inputVar ( input_batch , voc ) output , mask , max_target_len = outputVar ( output_batch , voc ) return inp , lengths , output , mask , max_target_len # Example for validation small_batch_size = 5 batches = batch2TrainData ( voc , [ random . choice ( pairs ) for _ in range ( small_batch_size )]) input_variable , lengths , target_variable , mask , max_target_len = batches print ( \"input_variable:\" , input_variable ) print ( \"lengths:\" , lengths ) print ( \"target_variable:\" , target_variable ) print ( \"mask:\" , mask ) print ( \"max_target_len:\" , max_target_len ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 input_variable: tensor([[122, 443, 70, 16, 111], [ 7, 45, 519, 997, 66], [ 26, 571, 3, 4, 2], [ 36, 4, 2, 2, 0], [ 4, 2, 0, 0, 0], [ 2, 0, 0, 0, 0]]) lengths: tensor([6, 5, 4, 4, 3]) target_variable: tensor([[ 192, 785, 7319, 88, 95], [ 4, 12, 4, 2556, 4], [ 2, 786, 2, 4, 4], [ 0, 2, 0, 2, 4], [ 0, 0, 0, 0, 2]]) mask: tensor([[1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [0, 1, 0, 1, 1], [0, 0, 0, 0, 1]], dtype=torch.uint8) max_target_len: 5 \u5b9a\u4e49\u6a21\u578b \u00b6 Seq2Seq \u6a21\u578b \u00b6 \u6211\u4eec\u8fd9\u4e2a\u804a\u5929\u673a\u5668\u4eba\u7684\u5927\u8111\u662f\u5e8f\u5217\u5230\u5e8f\u5217\uff08seq2seq\uff09\u6a21\u578b\u3002seq2seq \u6a21\u578b\u7684\u76ee\u6807\u662f\u5c06\u53ef\u53d8\u957f\u5ea6\u5e8f\u5217\u4f5c\u4e3a\u8f93\u5165\uff0c\u5e76\u4f7f\u7528\u56fa\u5b9a\u5927\u5c0f\u7684\u6a21\u578b\u5c06\u53ef\u53d8\u957f\u5ea6\u5e8f\u5217\u4f5c\u4e3a\u8f93\u51fa\u8fd4\u56de\u3002 Sutskever \u7b49\u4eba \u53d1\u73b0\u901a\u8fc7\u4f7f\u7528\u4e24\u4e2a\u72ec\u7acb\u7684\u9012\u5f52\u795e\u7ecf\u7f51\u7edc(RNN)\uff0c\u6211\u4eec\u53ef\u4ee5\u5b8c\u6210\u8fd9\u9879\u4efb\u52a1\u3002 \u7b2c\u4e00\u4e2a RNN \u5145\u5f53 \u7f16\u7801\u5668 \uff0c\u5b83\u5c06\u53ef\u53d8\u957f\u5ea6\u8f93\u5165\u5e8f\u5217\u7f16\u7801\u6210\u56fa\u5b9a\u957f\u5ea6\u7684\u4e0a\u4e0b\u6587\u5411\u91cf\u3002 \u7406\u8bba\u4e0a\uff0c\u8fd9\u4e2a\u4e0a\u4e0b\u6587\u5411\u91cf\uff08RNN\u7684\u6700\u7ec8\u9690\u85cf\u5c42\uff09\u5c06\u5305\u542b\u5173\u4e8e\u8f93\u5165\u7ed9\u673a\u5668\u4eba\u7684\u8be2\u95ee\u53e5\u7684\u8bed\u4e49\u4fe1\u606f\u3002 \u7b2c\u4e8c\u4e2a RNN \u662f \u89e3\u7801\u5668 \uff0c\u5b83\u91c7\u7528\u8f93\u5165\u8bcd\u548c\u4e0a\u4e0b\u6587\u5411\u91cf\uff0c\u5e76\u8fd4\u56de\u5e8f\u5217\u4e2d\u540e\u7eed\u8bcd\u7684\u731c\u6d4b\u503c\uff0c\u4ee5\u53ca\u7528\u4e8e\u4e0b\u6b21\u8fed\u4ee3\u7684\u9690\u85cf\u5c42\u3002 \u56fe\u7247\u6765\u6e90\uff1a https://jeddy92.github.io/JEddy92.github.io/ts_seq2seq_intro/ \u7f16\u7801\u5668 \u00b6 \u7f16\u7801\u5668RNN\u4e00\u6b21\u8fed\u4ee3\u8f93\u5165\u53e5\u5b50\u7684\u4e00\u4e2a\u6807\u8bb0(token)\uff0c\u6bcf\u4e2a\u65f6\u95f4\u6b65\u9aa4\u8f93\u51fa\u4e00\u4e2a\u201c\u8f93\u51fa\u201d\u5411\u91cf\u548c\u4e00\u4e2a\u201c\u9690\u85cf\u72b6\u6001\u201d\u5411\u91cf\u3002 \u7136\u540e\u5c06\u9690\u85cf\u72b6\u6001\u5411\u91cf\u4f20\u9012\u7ed9\u4e0b\u4e00\u4e2a\u65f6\u95f4\u6b65\u9aa4\uff0c\u540c\u65f6\u8bb0\u5f55\u8f93\u51fa\u5411\u91cf\u3002 \u7f16\u7801\u5668\u5c06\u5176\u5728\u5e8f\u5217\u4e2d\u7684\u6bcf\u4e2a\u70b9\u770b\u5230\u7684\u4e0a\u4e0b\u6587\u8f6c\u6362\u4e3a\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u7684\u4e00\u7ec4\u70b9\uff0c\u89e3\u7801\u5668\u5c06\u4f7f\u7528\u8fd9\u7ec4\u70b9\u6765\u4e3a\u7ed9\u5b9a\u4efb\u52a1\u751f\u6210\u6709\u610f\u4e49\u7684\u8f93\u51fa\u3002 \u6211\u4eec\u8fd9\u4e2a\u7f16\u7801\u5668\u7684\u7684\u6838\u5fc3\u662f\u591a\u5c42\u95e8\u63a7\u5355\u5143(GRU)\uff0c\u7531 Cho \u7b49\u4eba \u4e8e2014\u5e74\u53d1\u660e\u3002 \u6211\u4eec\u5c06\u4f7f\u7528GRU\u7684\u4e00\u79cd\u53d8\u79cd\u2014\u2014\u53cc\u5411GRU\uff0c\u5b83\u4f7f\u7528\u4e24\u79cd\u72ec\u7acb\u7684RNN\uff1a\u4e00\u4e2a\u4ee5\u6b63\u5e38\u7684\u987a\u5e8f\u63a5\u6536\u8f93\u5165\u5e8f\u5217\uff0c\u53e6\u4e00\u4e2a\u4ee5\u53cd\u65b9\u5411\u63a5\u6536\u8f93\u5165\u5e8f\u5217\u3002 \u5728\u540c\u4e00\u65f6\u95f4\u6b65\u9aa4\u4e2d\u5bf9\u6bcf\u4e2a\u7f51\u7edc\u7684\u8f93\u51fa\u6c42\u548c\u3002 \u4f7f\u7528\u53cc\u5411GRU\u8bb2\u7ed9\u6211\u4eec\u5e26\u6765\u5bf9\u8fc7\u53bb\u548c\u672a\u6765\u4e0a\u4e0b\u6587\u8fdb\u884c\u7f16\u7801\u7684\u4f18\u52bf\u3002 \u53cc\u5411RNN: \u56fe\u7247\u6765\u6e90\uff1a https://colah.github.io/posts/2015-09-NN-Types-FP/ \u6ce8\u610f\uff0c\u4e00\u4e2a embedding \u5c42\u7528\u4e8e\u5728\u4efb\u610f\u5927\u5c0f\u7684\u7279\u5f81\u7a7a\u95f4\u4e2d\u7f16\u7801\u6211\u4eec\u7684\u5355\u8bcd\u7d22\u5f15\u3002 \u5bf9\u4e8e\u6211\u4eec\u7684\u6a21\u578b\uff0c\u8fd9\u4e2a\u5c42\u4f1a\u5c06\u6bcf\u4e2a\u8bcd\u6620\u5c04\u5230\u5927\u5c0f\u4e3a hidden_size \u7684\u7279\u5f81\u7a7a\u95f4\u3002 \u8bad\u7ec3\u540e\uff0c\u8fd9\u4e9b\u503c\u5e94\u8be5\u7f16\u7801\u4e86\u8fd1\u4e49\u8bcd\u4e4b\u95f4\u7684\u8bed\u4e49\u76f8\u4f3c\u5ea6\u3002 \u6700\u6709\uff0c\u5982\u679c\u8981\u586b\u5145\u540e\u7684\u4e00\u6279\u5e8f\u5217\u4f20\u5165RNN\u6a21\u5757\uff0c\u6211\u4eec\u5fc5\u987b\u56f4\u7ed5RNN\u8fdb\u884c\u6253\u5305\u548c\u89e3\u5305\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5206\u522b\u662f\uff1a - nn.utils.rnn.pack_padded_sequence - nn.utils.rnn.pad_packed_sequence \u8ba1\u7b97\u56fe\uff1a \u5c06\u8bcd\u7684\u7d22\u5f15\u503c\u8f6c\u4e3a\u5d4c\u5165 \u4e3aRNN\u6a21\u5757\u6253\u5305\u586b\u5145\u540e\u7684\u5e8f\u5217\u6279\u6b21 \u901a\u8fc7GRU\u524d\u5411\u4f20\u9012 \u89e3\u5305\u586b\u5145 \u53cc\u5411GRU\u8f93\u51fa\u6c42\u548c \u8fd4\u56de\u8f93\u51fa\u548c\u6700\u7ec8\u9690\u85cf\u72b6\u6001 \u8f93\u5165\uff1a input_seq : \u8f93\u5165\u53e5\u5b50\u6279\u6b21\uff1b\u5f62\u72b6= (max_length,batch_size) input_lengths : \u7531\u6279\u6b21\u4e2d\u6bcf\u4e2a\u53e5\u5b50\u7684\u957f\u5ea6\u7684\u6240\u6784\u6210\u7684\u5217\u8868\uff1b\u5f62\u72b6= (batch_size) hidden : \u9690\u85cf\u72b6\u6001\uff1b\u5f62\u72b6= (n_layers x num_directions, batch_size, hidden_size) \u8f93\u51fa\uff1a outputs : \u4eceGRN\u6700\u7ec8\u9690\u85cf\u5c42\u7684\u8f93\u51fa\u7279\u5f81\uff1b\u5f62\u72b6= (max_length, batch_size, hidden_size) hidden : \u4eceGRU\u66f4\u65b0\u7684\u9690\u85cf\u72b6\u6001\uff1b\u5f62\u72b6= (n_layers x num_directions, batch_size, hidden_size) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class EncoderRNN ( nn . Module ): def __init__ ( self , hidden_size , embedding , n_layers = 1 , dropout = 0 ): super ( EncoderRNN , self ) . __init__ () self . n_layers = n_layers self . hidden_size = hidden_size self . embedding = embedding # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size' # because our input size is a word embedding with number of features == hidden_size self . gru = nn . GRU ( hidden_size , hidden_size , n_layers , dropout = ( 0 if n_layers == 1 else dropout ), bidirectional = True ) def forward ( self , input_seq , input_lengths , hidden = None ): # Convert word indexes to embeddings embedded = self . embedding ( input_seq ) # Pack padded batch of sequences for RNN module packed = nn . utils . rnn . pack_padded_sequence ( embedded , input_lengths ) # Forward pass through GRU outputs , hidden = self . gru ( packed , hidden ) # Unpack padding outputs , _ = nn . utils . rnn . pad_packed_sequence ( outputs ) # Sum bidirectional GRU outputs outputs = outputs [:, :, : self . hidden_size ] + outputs [:, : , self . hidden_size :] # Return output and final hidden state return outputs , hidden \u89e3\u7801\u5668 \u00b6 \u89e3\u7801\u5668RNN\u4ee5\u201c\u4e00\u4e2a\u6807\u8bb0\u63a5\u4e00\u4e2a\u6807\u8bb0(token-by-token)\u201d\u7684\u5f62\u5f0f\u751f\u6210\u56de\u590d\u53e5\u5b50\u3002 \u5b83\u4f7f\u7528\u7f16\u7801\u5668\u7684\u4e0a\u4e0b\u6587\u5411\u91cf\u548c\u5185\u7f6e\u9690\u85cf\u72b6\u6001\u751f\u6210\u5e8f\u5217\u4e2d\u7684\u540e\u7eed\u8bcd\u3002 \u5b83\u6301\u7eed\u7684\u751f\u6210\u8bcd\uff0c\u76f4\u5230\u8f93\u51fa EOS_token \u2014\u2014\u8868\u793a\u53e5\u5b50\u7684\u7ed3\u675f\u3002 \u5bfb\u5e38\u7684Seq2Seq\u89e3\u7801\u5668\u5e38\u5e38\u9047\u5230\u7684\u4e00\u4e2a\u95ee\u9898\u5c31\u662f\uff0c\u5982\u679c\u6211\u4eec\u4f9d\u8d56\u4e8e\u4e0a\u4e0b\u6587\u5411\u91cf\u6765\u7f16\u7801\u6574\u4e2a\u8f93\u5165\u7684\u8bed\u4e49\uff0c\u90a3\u4e48\u6211\u4eec\u5f88\u53ef\u80fd\u4e22\u5931\u4fe1\u606f\u3002 \u5728\u5904\u7406\u957f\u8f93\u5165\u5e8f\u5217\u7684\u65f6\u5019\u5c24\u5176\u5982\u6b64\uff0c\u8fd9\u6781\u5927\u7684\u9650\u5236\u4e86\u6211\u4eec\u8fd9\u4e2a\u89e3\u7801\u5668\u7684\u80fd\u529b\u3002 \u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c Bahdanau \u7b49\u4eba \u521b\u5efa\u4e86\u201c\u6ce8\u610f\u529b\u673a\u5236\u201d\uff0c\u5141\u8bb8\u89e3\u7801\u5668\u53ea\u5173\u6ce8\u8f93\u5165\u5e8f\u5217\u7684\u67d0\u4e9b\u90e8\u5206\uff0c\u800c\u4e0d\u662f\u5728\u6bcf\u4e00\u6b65\u90fd\u4f7f\u7528\u6574\u4e2a\u56fa\u5b9a\u7684\u4e0a\u4e0b\u6587\u3002 \u5728\u4e0a\u5c42\uff0c\u7528\u89e3\u7801\u5668\u7684\u5f53\u524d\u9690\u85cf\u72b6\u6001\u548c\u7f16\u7801\u5668\u7684\u8f93\u51fa\u8ba1\u7b97\u6ce8\u610f\u529b\u3002 \u8f93\u51fa\u6ce8\u610f\u529b\u6743\u91cd\u548c\u8f93\u5165\u5e8f\u5217\u5177\u6709\u76f8\u540c\u7684\u5f62\u72b6\uff0c\u8fd9\u8ba9\u6211\u4eec\u53ef\u4ee5\u5c06\u5b83\u548c\u7f16\u7801\u5668\u8f93\u51fa\u76f8\u4e58\uff0c\u5f97\u5230\u7f16\u7801\u5668\u8f93\u51fa\u4e2d\u7684\u8981\u88ab\u52a0\u4ee5\u6ce8\u610f\u529b\u7684\u90e8\u5206\u7684\u52a0\u6743\u548c\u3002 Sean Robertson \u7684\u56fe\u793a\u5f88\u597d\u7684\u63cf\u8ff0\u4e86\u8fd9\u70b9\uff1a Luong \u7b49\u4eba \u521b\u5efa\u4e86\u201c\u5168\u5c40\u6ce8\u610f\u529b\u201d\u6765\u6539\u8fdb Bahdanau \u7b49\u4eba \u7684\u57fa\u7840\u5de5\u4f5c\u3002 \u201c\u5168\u5c40\u6ce8\u610f\u529b\u201d\u6700\u5173\u952e\u7684\u4e0d\u540c\u4e4b\u5904\u5728\u4e8e\uff1a\u5b83\u4f1a\u8003\u8651\u6240\u6709\u7684\u7f16\u7801\u5668\u9690\u85cf\u72b6\u6001\uff0c\u800c\u4e0d\u662f Bahdanau \u7b49\u4eba \u7684\u53ea\u8003\u8651\u5f53\u524d\u65f6\u95f4\u6b65\u9aa4\u4e2d\u7684\u7f16\u7801\u5668\u9690\u85cf\u72b6\u6001\u7684\u201c\u5c40\u90e8\u6ce8\u610f\u529b\u201d\u65b9\u5f0f\u3002 \u53e6\u4e00\u4e2a\u4e0d\u540c\u4e4b\u5904\u5728\u4e8e\uff0c\u4f7f\u7528\u201c\u5168\u5c40\u6ce8\u610f\u529b\u201d\uff0c\u6211\u4eec\u4ec5\u4ec5\u4f7f\u7528\u5f53\u524d\u65f6\u95f4\u6b65\u9aa4\u7684\u7f16\u7801\u5668\u7684\u9690\u85cf\u72b6\u6001\u6765\u8ba1\u7b97\u6ce8\u610f\u529b\u7684\u6743\u91cd\u6216\u80fd\u91cf\u503c\u3002 Bahdanau \u7b49\u4eba \u7684\u6ce8\u610f\u529b\u8ba1\u7b97\u9700\u8981\u4e86\u89e3\u4e0a\u4e00\u4e2a\u65f6\u95f4\u6b65\u9aa4\u4e2d\u7f16\u7801\u5668\u7684\u72b6\u6001\u3002 \u6b64\u5916\uff0c Luong \u7b49\u4eba \u63d0\u4f9b\u4e86\u7528\u4e8e\u8ba1\u7b97\u7f16\u7801\u5668\u8f93\u51fa\u548c\u89e3\u7801\u5668\u8f93\u51fa\u4e4b\u95f4\u7684\u6ce8\u610f\u529b\u7684\u7684\u591a\u79cd\u65b9\u6cd5\uff0c\u4ed6\u4eec\u88ab\u6210\u4e3a\u201c\u5f97\u5206\u51fd\u6570\u201d\uff08score functions\uff09\uff1a \u5176\u4e2d\uff0c - h_t h_t \u4e3a\u5f53\u524d\u76ee\u6807\u89e3\u7801\u5668\u72b6\u7684\u6001 - \\bar{h}_s \\bar{h}_s \u4e3a\u6240\u6709\u7f16\u7801\u5668\u7684\u72b6\u6001 \u603b\u4f53\u800c\u8a00\uff0c\u5168\u5c40\u6ce8\u610f\u529b\u673a\u5236\u53ef\u4ee5\u901a\u8fc7\u4e0b\u56fe\u6765\u603b\u7ed3\u3002 \u6ce8\u610f\u6211\u4eec\u5c06\u5728\u88ab\u79f0\u4f5c Attn \u7684\u5206\u79bb\u7684 nn.Module \u4e2d\u5b9e\u73b0\u201c\u6ce8\u610f\u529b\u5c42\u201d\u3002 \u8fd9\u4e2a\u6a21\u5757\u7684\u8f93\u51fa\u662f\u4e00\u4e2a softmax \u6807\u51c6\u5316\u6743\u91cd\u5f20\u91cf\uff0c\u5176\u5f62\u72b6\u662f (batch_size, 1, max_length) \u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 # Luong attention layer class Attn ( nn . Module ): def __init__ ( self , method , hidden_size ): super ( Attn , self ) . __init__ () self . method = method if self . method not in [ 'dot' , 'general' , 'concat' ]: raise ValueError ( self . method , \"is not an appropriate attention method.\" ) self . hidden_size = hidden_size if self . method == 'general' : self . attn = nn . Linear ( self . hidden_size , hidden_size ) elif self . method == 'concat' : self . attn = nn . Linear ( self . hidden_size * 2 , hidden_size ) self . v = nn . Parameter ( torch . FloatTensor ( hidden_size )) def dot_score ( self , hidden , encoder_output ): return torch . sum ( hidden * encoder_output , dim = 2 ) def general_score ( self , hidden , encoder_output ): energy = self . attn ( encoder_output ) return torch . sum ( hidden * energy , dim = 2 ) def concat_score ( self , hidden , encoder_output ): energy = self . attn ( torch . cat (( hidden . expand ( encoder_output . size ( 0 ), - 1 , - 1 ), encoder_output ), 2 )) . tanh () return torch . sum ( self . v * energy , dim = 2 ) def forward ( self , hidden , encoder_outputs ): # Calculate the attention weights (energies) based on the given method if self . method == 'general' : attn_energies = self . general_score ( hidden , encoder_outputs ) elif self . method == 'concat' : attn_energies = self . concat_score ( hidden , encoder_outputs ) elif self . method == 'dot' : attn_energies = self . dot_score ( hidden , encoder_outputs ) # Transpose max_length and batch_size dimensions attn_energies = attn_energies . t () # Return the softmax normalized probability scores (with added dimension) return F . softmax ( attn_energies , dim = 1 ) . unsqueeze ( 1 ) \u73b0\u5728\uff0c\u6211\u4eec\u5df2\u7ecf\u5b9a\u4e49\u4e86\u6ce8\u610f\u529b\u5b50\u6a21\u5757\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5b9e\u9645\u7684\u89e3\u7801\u5668\u6a21\u5757\u4e86\u3002 \u5bf9\u4e8e\u89e3\u7801\u5668\uff0c\u6211\u4eec\u5c06\u624b\u52a8\u7684\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u9aa4\u4e2d\u63d0\u4f9b\u6279\u6570\u636e\u3002 \u8fd9\u610f\u5473\u7740\u6211\u4eec\u7684\u5d4c\u5165\u8bcd\u5f20\u91cf\u548cGRU\u8f93\u51fa\u7684\u5f62\u72b6\u90fd\u662f (1, batch_size, hidden_size) \u3002 \u8ba1\u7b97\u56fe\uff1a \u83b7\u5f97\u5f53\u524d\u8f93\u5165\u8bcd\u7684\u5f3a\u5982\u5d4c\u5165\u3002 \u5355\u5411GRU\u524d\u5411\u3002 \u6709\u7b2c\u4e8c\u6b65\u7684GRU\u8f93\u51fa\u8ba1\u7b97\u6ce8\u610f\u529b\u6743\u91cd\u3002 \u6ce8\u610f\u529b\u6743\u91cd\u4e0e\u7f16\u7801\u5668\u8f93\u51fa\u76f8\u4e58\uff0c\u5f97\u5230\u201c\u52a0\u6743\u548c\u201d(weighted sum)\u4e0a\u4e0b\u6587\u5411\u91cf\u3002 \u4f7f\u7528 Luong \u7b49\u4eba\u7684\u65b9\u6cd5\u5c06\u52a0\u6743\u4e0a\u4e0b\u6587\u5411\u91cf\u548cGRU\u8f93\u51fa\u76f8\u52a0\u3002 \u4f7f\u7528 Luong \u7b49\u4eba\u7684\u65b9\u6cd5(\u4e0d\u7528 softmax)\u9884\u6d4b\u540e\u7eed\u8bcd\u3002 \u8fd4\u56de\u8f93\u51fa\u548c\u6700\u7ec8\u9690\u85cf\u5c42\u3002 \u8f93\u5165\uff1a input_step : \u8f93\u5165\u5e8f\u5217\u6279\u7684\u4e00\u4e2a\u65f6\u95f4\u6b65\u9aa4 (\u4e00\u4e2a\u8bcd)\uff1b\u5f62\u72b6= (1, batch_size) last_hidden : GRU\u7684\u6700\u7ec8\u9690\u85cf\u5c42\uff1b\u5f62\u72b6=(n_layers x num_directions, batch_size, hidden_size)* encoder_outputs : \u7f16\u7801\u5668\u7684\u6a21\u578b\u8f93\u51fa; \u6027\u8f6c= (max_length, batch_size, hidden_size) \u8f93\u51fa\uff1a \u8f93\u51fa : softmax \u6b63\u89c4\u5316\u5f20\u91cf\uff0c\u7ed9\u51fa\u4e86\u88ab\u89e3\u7801\u5e8f\u5217\u4e2d\u6bcf\u4e2a\u8bcd\u662f\u6b63\u786e\u7684\u540e\u7eed\u8bcd\u7684\u6982\u7387; \u5f62\u72b6= (batch_size, voc.num_words) hidden : GRU\u7684\u6700\u7ec8\u9690\u85cf\u72b6\u6001; \u5f62\u72b6= (n_layers x num_directions, batch_size, hidden_size) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 class LuongAttnDecoderRNN ( nn . Module ): def __init__ ( self , attn_model , embedding , hidden_size , output_size , n_layers = 1 , dropout = 0.1 ): super ( LuongAttnDecoderRNN , self ) . __init__ () # Keep for reference self . attn_model = attn_model self . hidden_size = hidden_size self . output_size = output_size self . n_layers = n_layers self . dropout = dropout # Define layers self . embedding = embedding self . embedding_dropout = nn . Dropout ( dropout ) self . gru = nn . GRU ( hidden_size , hidden_size , n_layers , dropout = ( 0 if n_layers == 1 else dropout )) self . concat = nn . Linear ( hidden_size * 2 , hidden_size ) self . out = nn . Linear ( hidden_size , output_size ) self . attn = Attn ( attn_model , hidden_size ) def forward ( self , input_step , last_hidden , encoder_outputs ): # Note: we run this one step (word) at a time # Get embedding of current input word embedded = self . embedding ( input_step ) embedded = self . embedding_dropout ( embedded ) # Forward through unidirectional GRU rnn_output , hidden = self . gru ( embedded , last_hidden ) # Calculate attention weights from the current GRU output attn_weights = self . attn ( rnn_output , encoder_outputs ) # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector context = attn_weights . bmm ( encoder_outputs . transpose ( 0 , 1 )) # Concatenate weighted context vector and GRU output using Luong eq. 5 rnn_output = rnn_output . squeeze ( 0 ) context = context . squeeze ( 1 ) concat_input = torch . cat (( rnn_output , context ), 1 ) concat_output = torch . tanh ( self . concat ( concat_input )) # Predict next word using Luong eq. 6 output = self . out ( concat_output ) output = F . softmax ( output , dim = 1 ) # Return output and final hidden state return output , hidden \u5b9a\u4e49\u8bad\u7ec3\u8fc7\u7a0b \u00b6 \u635f\u5931\u63a9\u7801 \u00b6 \u7531\u4e8e\u6211\u4eec\u5904\u7406\u7684\u662f\u6279\u91cf\u586b\u5145\u5e8f\u5217\uff0c\u56e0\u6b64\u5728\u8ba1\u7b97\u635f\u5931\u65f6\u6211\u4eec\u4e0d\u80fd\u7b80\u5355\u5730\u4ec5\u8003\u8651\u5f20\u91cf\u7684\u5168\u90e8\u5143\u7d20\u3002 \u800c\u662f\u901a\u8fc7\u5b9a\u4e49 maskNLLLoss \u635f\u5931\u51fd\u6570\uff0c\u57fa\u4e8e\u89e3\u7801\u5668\u8f93\u51fa\u5f20\u91cf\uff0c\u76ee\u6807\u5f20\u91cf\u548c\u63cf\u8ff0\u76ee\u6807\u5f20\u91cf\u586b\u5145\u7684\u4e8c\u8fdb\u5236\u63a9\u7801\u5f20\u91cf\uff0c\u6765\u8ba1\u7b97\u635f\u5931\u3002 \u8be5\u635f\u5931\u51fd\u6570\u8ba1\u7b97\u4e86\u5bf9\u5e94\u4e8e\u63a9\u7801\u5411\u91cf\u4e2d 1 \u7684\u5143\u7d20\u7684\u8d1f\u5bf9\u6570\u76f8\u4f3c\u5ea6\u3002 1 2 3 4 5 6 def maskNLLLoss ( inp , target , mask ): nTotal = mask . sum () crossEntropy = - torch . log ( torch . gather ( inp , 1 , target . view ( - 1 , 1 )) . squeeze ( 1 )) loss = crossEntropy . masked_select ( mask ) . mean () loss = loss . to ( device ) return loss , nTotal . item () \u5355\u6b21\u8bad\u7ec3\u7684\u8fed\u4ee3 \u00b6 \u51fd\u6570 train \u5305\u542b\u4e00\u4e2a\u5355\u6b21\u8bad\u7ec3\u8fed\u4ee3\uff08\u5355\u4e2a\u8f93\u5165\u6279\u6b21\uff09\u7b97\u6cd5\u3002 \u6211\u4eec\u7528\u4e00\u4e9b\u5de7\u5999\u7684\u6280\u5de7\u6765\u4fc3\u8fdb\u6536\u655b\uff1a \u7b2c\u4e00\u4e2a\u6280\u5de7\u662f\u4f7f\u7528 tacher forcing \u3002 \u8fd9\u610f\u5473\u7740\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u8bbe\u7f6e teacher_forcing_ratio \uff0c\u6211\u4eec\u4f7f\u7528\u5f53\u524d\u76ee\u6807\u8bcd\uff0c\u800c\u4e0d\u662f\u4f7f\u7528\u89e3\u7801\u5668\u7684\u5f53\u524d\u731c\u6d4b\u7ed3\u679c\uff0c\u4f5c\u4e3a\u89e3\u7801\u5668\u7684\u540e\u7eed\u8f93\u5165\u3002 \u8fd9\u9879\u6280\u672f\u662f\u8bad\u7ec3\u89e3\u7801\u5668\u7684\u8f6e\u5b50\uff0c\u6709\u52a9\u4e8e\u66f4\u6709\u6548\u7684\u8bad\u7ec3\u3002 \u201ctacher forcing\u201d \u4f1a\u5bfc\u81f4\u6a21\u578b\u5728\u63a8\u7406\u671f\u95f4\u4e0d\u7a33\u5b9a\uff0c\u8fd9\u662f\u56e0\u4e3a\u89e3\u7801\u5668\u5728\u8bad\u7ec3\u4e2d\u6ca1\u6709\u8db3\u591f\u7684\u673a\u4f1a\u771f\u6b63\u7684\u5236\u4f5c\u5b83\u81ea\u5df1\u7684\u8f93\u51fa\u5e8f\u5217\u3002 \u6240\u6709\uff0c\u6211\u4eec\u5fc5\u987b\u6ce8\u610f\u5730\u5982\u4f55\u8bbe\u7f6e teacher_forcing_ratio \uff0c\u4e0d\u8981\u88ab\u5feb\u901f\u6536\u655b\u6b3a\u9a97\u3002 \u6211\u4eec\u8981\u5b9e\u73b0\u7684\u7b2c\u4e8c\u4e2a\u6280\u5de7\u662f \u68af\u5ea6\u526a\u5207 \u3002 \u8fd9\u662f\u4e00\u79cd\u7528\u4e8e\u5bf9\u6297\u201c\u68af\u5ea6\u7206\u70b8\u201d\u7684\u5e38\u7528\u6280\u672f\u3002 \u672c\u8d28\u4e0a\uff0c\u901a\u8fc7\u526a\u5207\u6216\u8005\u6700\u5927\u9608\u503c\uff0c\u6211\u4eec\u9632\u6b62\u68af\u5ea6\u7206\u70b8\u6027\u589e\u957f\u548c\u6ea2\u51fa(NaN)\uff0c\u6216\u8005\u5728\u6210\u672c\u51fd\u6570\u4e2d\u4ece\u60ac\u5d16\u8dcc\u843d\u3002 \u56fe\u50cf\u6765\u6e90: Goodfellow \u7b49\u4eba Deep Learning . 2016. \u8fd0\u7b97\u987a\u5e8f\uff1a \u901a\u8fc7\u7f16\u7801\u5668\u5411\u524d\u4f20\u9012\u6574\u4e2a\u8f93\u5165\u6279\u6b21\u3002 \u5c06\u89e3\u7801\u5668\u8f93\u5165\u521d\u59cb\u5316\u4e3a SOS_token \u548c\u7f16\u7801\u5668\u6700\u7ec8\u9690\u85cf\u5c42\u7684\u9690\u85cf\u72b6\u6001\u3002 \u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u9aa4\u4e2d\uff0c\u901a\u8fc7\u89e3\u7801\u5668\u5411\u524d\u4f20\u9012\u8f93\u5165\u6279\u7684\u5e8f\u5217\u3002 \u5982\u679c\u7528\u5230\u4e86 \u201ctacher forcing\u201d\uff1a\u628a\u4e0b\u4e00\u4e2a\u89e3\u7801\u5668\u8f93\u5165\u4f5c\u4e3a\u5f53\u524d\u76ee\u6807\uff1b\u5176\u5b83\uff1a\u7528\u4e0b\u4e00\u4e2a\u89e3\u7801\u5668\u8f93\u5165\u4f5c\u4e3a\u5f53\u524d\u89e3\u7801\u5668\u8f93\u51fa\u3002 \u8ba1\u7b97\u548c\u7d2f\u79ef\u635f\u5931\u3002 \u8fdb\u884c\u53cd\u5411\u4f20\u64ad \u526a\u5207\u68af\u5ea6\u3002 \u66f4\u65b0\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u6a21\u578b\u7684\u53c2\u6570\u3002 \u6ce8\u610f \u53ea\u9700\u5c06 PyTorch RNN \u6a21\u5757( RNN , LSTM , GRU )\u7684\u6574\u4e2a\u8f93\u5165\u5e8f\u5217\uff08\u6216\u6279\u6b21\u7684\u5e8f\u5217\uff09\u4f20\u5165\uff0c\u5b83\u4eec\u5c31\u53ef\u4ee5\u88ab\u7528\u4e8e\u4efb\u4f55\u5176\u5b83\u7c7b\u4f3c\u7684\u975e\u9012\u5f52\u5c42\u3002 \u6211\u4eec\u5728 encoder \u4e2d\u8fd9\u6837\u4f7f\u7528 GUR \u5c42\u3002 \u5b9e\u9645\u60c5\u51b5\u662f\uff0c\u5728\u5e95\u5c42\uff0c\u6bcf\u4e00\u4e2a\u65f6\u95f4\u6b65\u9aa4\u4e0a\u90fd\u6709\u4e00\u4e2a\u4ea4\u4e92\u8fc7\u7a0b\u8fed\u4ee3\u8ba1\u7b97\u9690\u85cf\u72b6\u6001\u3002 \u53e6\u5916\uff0c\u4e5f\u53ef\u4ee5\u6bcf\u4e2a\u65f6\u95f4\u6b65\u9aa4\u8fd0\u884c\u8fd9\u4e9b\u6a21\u578b\u3002 \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u5728\u8bad\u7ec3\u8fc7\u7a0b\u91cc\u624b\u52a8\u5faa\u73af\u904d\u5386\u5e8f\u5217\uff0c\u5c31\u50cf\u5bf9 decoder \u6a21\u578b\u505a\u7684\u90a3\u6837\u3002 \u53ea\u8981\u7ef4\u62a4\u597d\u8fd9\u4e9b\u6a21\u5757\u6b63\u786e\u7684\u6982\u5ff5\u6a21\u578b\uff0c\u5b9e\u73b0\u5e8f\u5217\u6a21\u578b\u5c31\u53ef\u4ee5\u975e\u5e38\u7b80\u5355\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def train ( input_variable , lengths , target_variable , mask , max_target_len , encoder , decoder , embedding , encoder_optimizer , decoder_optimizer , batch_size , clip , max_length = MAX_LENGTH ): # Zero gradients encoder_optimizer . zero_grad () decoder_optimizer . zero_grad () # Set device options input_variable = input_variable . to ( device ) lengths = lengths . to ( device ) target_variable = target_variable . to ( device ) mask = mask . to ( device ) # Initialize variables loss = 0 print_losses = [] n_totals = 0 # Forward pass through encoder encoder_outputs , encoder_hidden = encoder ( input_variable , lengths ) # Create initial decoder input (start with SOS tokens for each sentence) decoder_input = torch . LongTensor ([[ SOS_token for _ in range ( batch_size )]]) decoder_input = decoder_input . to ( device ) # Set initial decoder hidden state to the encoder's final hidden state decoder_hidden = encoder_hidden [: decoder . n_layers ] # Determine if we are using teacher forcing this iteration use_teacher_forcing = True if random . random () < teacher_forcing_ratio else False # Forward batch of sequences through decoder one time step at a time if use_teacher_forcing : for t in range ( max_target_len ): decoder_output , decoder_hidden = decoder ( decoder_input , decoder_hidden , encoder_outputs ) # Teacher forcing: next input is current target decoder_input = target_variable [ t ] . view ( 1 , - 1 ) # Calculate and accumulate loss mask_loss , nTotal = maskNLLLoss ( decoder_output , target_variable [ t ], mask [ t ]) loss += mask_loss print_losses . append ( mask_loss . item () * nTotal ) n_totals += nTotal else : for t in range ( max_target_len ): decoder_output , decoder_hidden = decoder ( decoder_input , decoder_hidden , encoder_outputs ) # No teacher forcing: next input is decoder's own current output _ , topi = decoder_output . topk ( 1 ) decoder_input = torch . LongTensor ([[ topi [ i ][ 0 ] for i in range ( batch_size )]]) decoder_input = decoder_input . to ( device ) # Calculate and accumulate loss mask_loss , nTotal = maskNLLLoss ( decoder_output , target_variable [ t ], mask [ t ]) loss += mask_loss print_losses . append ( mask_loss . item () * nTotal ) n_totals += nTotal # Perform backpropatation loss . backward () # Clip gradients: gradients are modified in place _ = nn . utils . clip_grad_norm_ ( encoder . parameters (), clip ) _ = nn . utils . clip_grad_norm_ ( decoder . parameters (), clip ) # Adjust model weights encoder_optimizer . step () decoder_optimizer . step () return sum ( print_losses ) / n_totals \u591a\u6b21\u8bad\u7ec3\u7684\u8fed\u4ee3\u5668 \u00b6 \u73b0\u5728\u662f\u65f6\u5019\u628a\u5b8c\u6574\u548c\u7684\u8bad\u7ec3\u8fc7\u7a0b\u548c\u6570\u636e\u7ed3\u5408\u5728\u4e00\u8d77\u4e86\u3002 \u51fd\u6570 trainIters \u8d1f\u8d23\u8fd0\u884c\u628a\u6a21\u578b\u3001\u4f18\u5316\u5668\u3001\u6570\u636e\u7b49\u4f20\u9012\u7ed9 n_iterations \u5e76\u8fd0\u884c\u3002 \u8fd9\u4e2a\u51fd\u6570\u5f88\u5bb9\u6613\u89e3\u91ca\u7684\uff0c\u56e0\u4e3a\u6211\u4eec\u5df2\u7ecf\u5bf9 train \u51fd\u6570\u505a\u4e86\u7e41\u591a\u7684\u5de5\u4f5c\u3002 \u6709\u4e00\u4e2a\u8981\u6ce8\u610f\u7684\u4e8b\u60c5\uff1a\u5f53\u6211\u4eec\u4fdd\u5b58\u6a21\u578b\u65f6\uff0c\u6240\u6709\u7684\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u72b6\u6001\u5b57\u5178(\u53c2\u6570)\u3001\u4f18\u5316\u5668\u7684\u72b6\u6001\u5b57\u5178\u3001\u635f\u5931\u3001\u8fed\u4ee3\u5668\u3001\u7b49\u7b49\uff0c\u90fd\u88ab\u4fdd\u5b58\u5728\u4e00\u4e2atarball\u4e2d\u3002 \u88ab\u8fd9\u6837\u4fdd\u5b58\u7684\u6a21\u578b\u53ef\u4ee5\u4e3a\u6211\u4eec\u63d0\u4f9b\u4e86\u5177\u6709\u6700\u5927\u7075\u6d3b\u6027\u7684\u68c0\u67e5\u70b9\u3002 \u4ece\u68c0\u67e5\u70b9\u52a0\u8f7d\u6a21\u578b\u4e4b\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u6a21\u578b\u53c2\u6570\u8fdb\u884c\u9884\u6d4b\uff0c\u6216\u8005\u4ece\u4e0a\u6b21\u5bf9\u51fa\u7684\u5730\u65b9\u7ee7\u7eed\u8bad\u7ec3\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def trainIters ( model_name , voc , pairs , encoder , decoder , encoder_optimizer , decoder_optimizer , embedding , encoder_n_layers , decoder_n_layers , save_dir , n_iteration , batch_size , print_every , save_every , clip , corpus_name , loadFilename ): # Load batches for each iteration training_batches = [ batch2TrainData ( voc , [ random . choice ( pairs ) for _ in range ( batch_size )]) for _ in range ( n_iteration )] # Initializations print ( 'Initializing ...' ) start_iteration = 1 print_loss = 0 if loadFilename : start_iteration = checkpoint [ 'iteration' ] + 1 # Training loop print ( \"Training...\" ) for iteration in range ( start_iteration , n_iteration + 1 ): training_batch = training_batches [ iteration - 1 ] # Extract fields from batch input_variable , lengths , target_variable , mask , max_target_len = training_batch # Run a training iteration with batch loss = train ( input_variable , lengths , target_variable , mask , max_target_len , encoder , decoder , embedding , encoder_optimizer , decoder_optimizer , batch_size , clip ) print_loss += loss # Print progress if iteration % print_every == 0 : print_loss_avg = print_loss / print_every print ( \"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\" . format ( iteration , iteration / n_iteration * 100 , print_loss_avg )) print_loss = 0 # Save checkpoint if ( iteration % save_every == 0 ): directory = os . path . join ( save_dir , model_name , corpus_name , '{}-{}_{}' . format ( encoder_n_layers , decoder_n_layers , hidden_size )) if not os . path . exists ( directory ): os . makedirs ( directory ) torch . save ({ 'iteration' : iteration , 'en' : encoder . state_dict (), 'de' : decoder . state_dict (), 'en_opt' : encoder_optimizer . state_dict (), 'de_opt' : decoder_optimizer . state_dict (), 'loss' : loss , 'voc_dict' : voc . __dict__ , 'embedding' : embedding . state_dict () }, os . path . join ( directory , '{}_{}.tar' . format ( iteration , 'checkpoint' ))) \u5b9a\u4e49\u8bc4\u4f30\u8fc7\u7a0b \u00b6 \u5728\u8bad\u7ec3\u6a21\u578b\u540e\uff0c\u6211\u4eec\u8981\u4e0e\u673a\u5668\u4eba\u4ea4\u8c08\u3002 \u9996\u5148\uff0c\u6211\u4eec\u5fc5\u987b\u5b9a\u4e49\u6a21\u578b\u5982\u4f55\u5bf9\u8f93\u5165\u8fdb\u884c\u7f16\u89e3\u7801\u3002 Greedy decoding \u00b6 \u201cGreedy decoding\u201d \u662f\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d \u4e0d \u4f7f\u7528 \u201ctacher forcing\u201d \u7684\u60c5\u51b5\u4e0b\u4f7f\u7528\u7684\u4e00\u79cd\u89e3\u7801\u65b9\u5f0f\u3002 \u6362\u800c\u8a00\u4e4b\uff0c\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u9aa4\uff0c\u6211\u4eec\u53ea\u9700\u4ece decoder_output \u9009\u5b9asoftmax\u503c\u6700\u9ad8\u7684\u8bcd\u3002 \u8fd9\u4e2a\u89e3\u7801\u65b9\u5f0f\u5728\u5355\u65f6\u95f4\u6b65\u9aa4\u7ea7\u522b\u4e0a\u662f\u6700\u4f73\u7684\u3002 \u4e3a\u4e86\u65b9\u4fbf\u5730\u4f7f\u7528 \u201cGreedy decoding\u201d\uff0c\u6211\u4eec\u5b9a\u4e49\u4e00\u4e2a GreedySearchDecoder \u7c7b\u3002 \u5728\u8fd0\u884c\u65f6\uff0c\u8fd9\u4e2a\u7c7b\u7684\u5bf9\u8c61\u91c7\u7528\u7684\u53c2\u6570\u662f\uff1a \u5f62\u72b6\u4e3a (input_seq length, 1) \u7684\u8f93\u5165\u5e8f\u5217( input_seq ) \u6807\u91cf\u8f93\u5165\u957f\u5ea6( input_length ) tensor \u7b54\u590d\u53e5\u5b50\u957f\u5ea6\u9650\u5236\u503c max_length \u4f7f\u7528\u4ee5\u4e0b\u8ba1\u7b97\u56fe\u5bf9\u8f93\u5165\u53e5\u5b50\u8fdb\u884c\u8bc4\u4f30\u8f93\u5165\uff1a \u8ba1\u7b97\u56fe\uff1a \u901a\u8fc7\u7f16\u7801\u5668\u6a21\u578b\u8f6c\u53d1\u8f93\u5165\u3002 \u5c06\u7f16\u7801\u5668\u7684\u6700\u7ec8\u9690\u85cf\u5c42\u7528\u4f5c\u89e3\u7801\u5668\u7684\u7b2c\u4e00\u4e2a\u9690\u85cf\u8f93\u5165\u3002 \u5c06\u89e3\u7801\u5668\u7684\u7b2c\u4e00\u4e2a\u8f93\u5165\u521d\u59cb\u5316\u4e3a SOS_token \u521d\u59cb\u5316\u5f20\u91cf\uff0c\u5728\u5176\u672b\u5c3e\u9644\u52a0\u9644\u52a0\u89e3\u7801\u540e\u7684\u8bcd \u4e00\u6b21\u8fed\u4ee3\u89e3\u7801\u4e00\u4e2a\u8bcd\u7684\u6807\u8bb0(token)\uff1a \u901a\u8fc7\u89e3\u7801\u5668\u524d\u5411\u4f20\u64ad\u3002 \u83b7\u5f97\u53ef\u80fd\u6027\u6700\u5927\u8bcd\u7684token\u548csoftmax\u5206\u503c(score)\u3002 \u8bb0\u5f55token\u548cscore \u5c06\u5f53\u524dtoken\u7528\u4f5c\u4e0b\u4e00\u4e2a\u89e3\u7801\u5668\u7684\u8f93\u5165 \u8fd4\u56de\u6536\u96c6\u5230\u7684\u8bcd\u7684\u6807\u8bb0(token)\u548csoftmax\u5206\u503c(score)\u5217\u8868 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class GreedySearchDecoder ( nn . Module ): def __init__ ( self , encoder , decoder ): super ( GreedySearchDecoder , self ) . __init__ () self . encoder = encoder self . decoder = decoder def forward ( self , input_seq , input_length , max_length ): # Forward input through encoder model encoder_outputs , encoder_hidden = self . encoder ( input_seq , input_length ) # Prepare encoder's final hidden layer to be first hidden input to the decoder decoder_hidden = encoder_hidden [: decoder . n_layers ] # Initialize decoder input with SOS_token decoder_input = torch . ones ( 1 , 1 , device = device , dtype = torch . long ) * SOS_token # Initialize tensors to append decoded words to all_tokens = torch . zeros ([ 0 ], device = device , dtype = torch . long ) all_scores = torch . zeros ([ 0 ], device = device ) # Iteratively decode one word token at a time for _ in range ( max_length ): # Forward pass through decoder decoder_output , decoder_hidden = self . decoder ( decoder_input , decoder_hidden , encoder_outputs ) # Obtain most likely word token and its softmax score decoder_scores , decoder_input = torch . max ( decoder_output , dim = 1 ) # Record token and score all_tokens = torch . cat (( all_tokens , decoder_input ), dim = 0 ) all_scores = torch . cat (( all_scores , decoder_scores ), dim = 0 ) # Prepare current token to be next decoder input (add a dimension) decoder_input = torch . unsqueeze ( decoder_input , 0 ) # Return collections of word tokens and scores return all_tokens , all_scores \u8bc4\u4f30\u7528\u6237\u8f93\u5165\u6587\u672c \u00b6 \u73b0\u5728\u6211\u4eec\u5df2\u7ecf\u5b9a\u4e49\u4e86\u89e3\u7801\u65b9\u6cd5\uff0c\u53ef\u4ee5\u7f16\u5199\u5bf9\u8f93\u5165\u53e5\u5b50\u6587\u672c\u8fdb\u884c\u8bc4\u4f30\u7684\u51fd\u6570\u3002 \u51fd\u6570 evaluate \u7528\u4e8e\u7ba1\u7406\u5904\u7406\u8f93\u5165\u53e5\u5b50\u7684\u5e95\u5c42\u8fc7\u7a0b\u3002 \u9996\u5148\u5c06\u53e5\u5b50\u683c\u5f0f\u5316\u4e3a batch_size==1 \u7684\u8bcd\u7d22\u5f15\u503c\u8f93\u5165\u6279\u3002 \u8981\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff0c\u9700\u8981\u5c06\u53e5\u5b50\u4e2d\u7684\u8bcd\u8f6c\u4e3a\u4ed6\u4eec\u5bf9\u5e94\u7684\u7d22\u5f15\u503c\uff0c\u5e76\u8c03\u6362\u5750\u6807\u8f74\u4ee5\u751f\u6210\u6211\u4eec\u6a21\u578b\u6240\u9700\u7684\u5f20\u91cf\u3002 \u8fd8\u8981\u521b\u5efa\u4e00\u4e2a lengths \u5f20\u91cf\uff0c\u7528\u4e8e\u8bb0\u5f55\u8f93\u5165\u53e5\u5b50\u7684\u957f\u5ea6\u3002 \u6b64\u65f6\uff0c lengths \u662f\u4e00\u4e2a\u6807\u91cf\uff0c\u56e0\u4e3a\u6211\u4eec\u4e00\u6b21\u53ea\u8bc4\u4f30\u4e00\u4e2a\u53e5\u5b50( batch_size==1 )\u3002 \u7136\u540e\uff0c\u901a\u8fc7 GreedySearchDecoder \u5bf9\u8c61( searcher )\uff0c\u6211\u4eec\u53ef\u4ee5\u83b7\u5f97\u89e3\u7801\u540e\u7684\u56de\u590d\u53e5\u7684\u5f20\u91cf\u3002 \u6700\u540e\uff0c\u5c06\u56de\u590d\u7684\u7d22\u5f15\u503c\u8f6c\u4e3a\u8bcd\uff0c\u5e76\u8fd4\u56de\u89e3\u7801\u51fa\u6765\u7684\u8bcd\u5217\u8868\u3002 evaluateInput acts as the user interface for our chatbot. evaluateInput \u5145\u5f53\u804a\u5929\u673a\u5668\u4eba\u7528\u6237\u754c\u9762\u3002 \u8c03\u7528\u5b83\u7684\u65f6\u5019\uff0c\u63d0\u4f9b\u4e00\u4e2a\u6587\u672c\u8f93\u5165\u754c\u9762\uff0c\u6211\u4eec\u5728\u8fd9\u91cc\u8f93\u5165\u81ea\u5df1\u7684\u8be2\u95ee\u53e5\u3002 \u6309 \u56de\u8f66 \u5b8c\u6210\u540e\uff0c\u7528\u548c\u5904\u7406\u8bad\u7ec3\u6570\u636e\u4e00\u81f4\u7684\u65b9\u6cd5\u5bf9\u8f93\u5165\u6587\u672c\u8fdb\u884c\u6b63\u89c4\u5316\u5904\u7406\uff0c\u4f20\u9012\u7ed9 evaluate \u51fd\u6570\uff0c\u83b7\u5f97\u89e3\u7801\u540e\u7684\u8f93\u51fa\u53e5\u5b50\u3002 \u5faa\u73af\u6267\u884c\u8fd9\u4e2a\u8fc7\u7a0b\uff0c\u8fd9\u6837\u5c31\u53ef\u4ee5\u548c\u673a\u5668\u4eba\u4e0d\u505c\u5730\u804a\u5929\u3002\u8f93\u5165\u201cq\u201d\u6216\u8005\u201dquit\u201d\u9000\u51fa\u3002 \u6700\u540e\uff0c\u5982\u679c\u53e5\u5b50\u4e2d\u542b\u6709\u8bcd\u8868\u4e2d\u4e0d\u5b58\u5728\u7684\u8bcd\uff0c\u5e94\u8be5\u8fdb\u884c\u4f18\u96c5\u7684\u9519\u8bef\u5904\u7406\uff1a\u8f93\u51fa\u9519\u8bef\u63d0\u793a\u4fe1\u606f\uff0c\u63d0\u9192\u7528\u6237\u8f93\u5165\u5176\u5b83\u8bcd\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def evaluate ( encoder , decoder , searcher , voc , sentence , max_length = MAX_LENGTH ): ### Format input sentence as a batch # words -> indexes indexes_batch = [ indexesFromSentence ( voc , sentence )] # Create lengths tensor lengths = torch . tensor ([ len ( indexes ) for indexes in indexes_batch ]) # Transpose dimensions of batch to match models' expectations input_batch = torch . LongTensor ( indexes_batch ) . transpose ( 0 , 1 ) # Use appropriate device input_batch = input_batch . to ( device ) lengths = lengths . to ( device ) # Decode sentence with searcher tokens , scores = searcher ( input_batch , lengths , max_length ) # indexes -> words decoded_words = [ voc . index2word [ token . item ()] for token in tokens ] return decoded_words def evaluateInput ( encoder , decoder , searcher , voc ): input_sentence = '' while ( 1 ): try : # Get input sentence input_sentence = input ( '> ' ) # Check if it is quit case if input_sentence == 'q' or input_sentence == 'quit' : break # Normalize sentence input_sentence = normalizeString ( input_sentence ) # Evaluate sentence output_words = evaluate ( encoder , decoder , searcher , voc , input_sentence ) # Format and print response sentence output_words [:] = [ x for x in output_words if not ( x == 'EOS' or x == 'PAD' )] print ( 'Bot:' , ' ' . join ( output_words )) except KeyError : print ( \"Error: Encountered unknown word.\" ) \u8fd0\u884c\u6a21\u578b \u00b6 \u6700\u540e\uff0c\u662f\u65f6\u5019\u8fd0\u884c\u6211\u4eec\u7684\u6a21\u578b\u4e86\uff01 \u4e0d\u7ba1\u8bad\u7ec3\u8fd8\u662f\u6d4b\u8bd5\uff0c\u6211\u4eec\u90fd\u5f97\u521d\u59cb\u5316\u72ec\u7acb\u7684\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u6a21\u578b\u3002 \u5728\u4e0b\u9762\u7684\u4ee3\u7801\u5757\u4e2d\uff0c\u6211\u4eec\u8bbe\u7f6e\u4e86\u6240\u9700\u914d\u7f6e\uff0c\u9009\u62e9\u4ece\u5934\u5f00\u59cb\u6216\u8005\u4ece\u68c0\u67e5\u70b9\u52a0\u8f7d\u5e76\u6784\u5efa\u548c\u521d\u59cb\u5316\u6a21\u578b\u3002 \u53ef\u4ee5\u81ea\u7531\u5730\u4f7f\u7528\u4e0d\u540c\u7684\u6a21\u578b\u8bbe\u7f6e\u6765\u4f18\u5316\u6027\u80fd\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 # Configure models model_name = 'cb_model' attn_model = 'dot' #attn_model = 'general' #attn_model = 'concat' hidden_size = 500 encoder_n_layers = 2 decoder_n_layers = 2 dropout = 0.1 batch_size = 64 # Set checkpoint to load from; set to None if starting from scratch loadFilename = None checkpoint_iter = 4000 #loadFilename = os.path.join(save_dir, model_name, corpus_name, # '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size), # '{}_checkpoint.tar'.format(checkpoint_iter)) # Load model if a loadFilename is provided if loadFilename : # If loading on same machine the model was trained on checkpoint = torch . load ( loadFilename ) # If loading a model trained on GPU to CPU #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu')) encoder_sd = checkpoint [ 'en' ] decoder_sd = checkpoint [ 'de' ] encoder_optimizer_sd = checkpoint [ 'en_opt' ] decoder_optimizer_sd = checkpoint [ 'de_opt' ] embedding_sd = checkpoint [ 'embedding' ] voc . __dict__ = checkpoint [ 'voc_dict' ] print ( 'Building encoder and decoder ...' ) # Initialize word embeddings embedding = nn . Embedding ( voc . num_words , hidden_size ) if loadFilename : embedding . load_state_dict ( embedding_sd ) # Initialize encoder & decoder models encoder = EncoderRNN ( hidden_size , embedding , encoder_n_layers , dropout ) decoder = LuongAttnDecoderRNN ( attn_model , embedding , hidden_size , voc . num_words , decoder_n_layers , dropout ) if loadFilename : encoder . load_state_dict ( encoder_sd ) decoder . load_state_dict ( decoder_sd ) # Use appropriate device encoder = encoder . to ( device ) decoder = decoder . to ( device ) print ( 'Models built and ready to go!' ) 1 2 Building encoder and decoder ... Models built and ready to go! \u8fd0\u884c\u8bad\u7ec3 \u00b6 \u8fd0\u884c\u4ee5\u4e0b\u4ee3\u7801\u5757\u8bad\u7ec3\u6a21\u578b\u3002 \u9996\u5148\u662f\u8bbe\u7f6e\u8bad\u7ec3\u53c2\u6570 \u7136\u540e\u521d\u59cb\u5316\u4f18\u5316\u5668(optimizer) \u6700\u540e\u8c03\u7528 trainIters \u51fd\u6570\u8fd0\u884c\u8bad\u7ec3\u8fed\u4ee3\u5668\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Configure training/optimization clip = 50.0 teacher_forcing_ratio = 1.0 learning_rate = 0.0001 decoder_learning_ratio = 5.0 n_iteration = 4000 print_every = 1 save_every = 500 # Ensure dropout layers are in train mode encoder . train () decoder . train () # Initialize optimizers print ( 'Building optimizers ...' ) encoder_optimizer = optim . Adam ( encoder . parameters (), lr = learning_rate ) decoder_optimizer = optim . Adam ( decoder . parameters (), lr = learning_rate * decoder_learning_ratio ) if loadFilename : encoder_optimizer . load_state_dict ( encoder_optimizer_sd ) decoder_optimizer . load_state_dict ( decoder_optimizer_sd ) # Run training iterations print ( \"Starting Training!\" ) trainIters ( model_name , voc , pairs , encoder , decoder , encoder_optimizer , decoder_optimizer , embedding , encoder_n_layers , decoder_n_layers , save_dir , n_iteration , batch_size , print_every , save_every , clip , corpus_name , loadFilename ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026 2027 2028 2029 2030 2031 2032 2033 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067 2068 2069 2070 2071 2072 2073 2074 2075 2076 2077 2078 2079 2080 2081 2082 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 2097 2098 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 2222 2223 2224 2225 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236 2237 2238 2239 2240 2241 2242 2243 2244 2245 2246 2247 2248 2249 2250 2251 2252 2253 2254 2255 2256 2257 2258 2259 2260 2261 2262 2263 2264 2265 2266 2267 2268 2269 2270 2271 2272 2273 2274 2275 2276 2277 2278 2279 2280 2281 2282 2283 2284 2285 2286 2287 2288 2289 2290 2291 2292 2293 2294 2295 2296 2297 2298 2299 2300 2301 2302 2303 2304 2305 2306 2307 2308 2309 2310 2311 2312 2313 2314 2315 2316 2317 2318 2319 2320 2321 2322 2323 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333 2334 2335 2336 2337 2338 2339 2340 2341 2342 2343 2344 2345 2346 2347 2348 2349 2350 2351 2352 2353 2354 2355 2356 2357 2358 2359 2360 2361 2362 2363 2364 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 2375 2376 2377 2378 2379 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390 2391 2392 2393 2394 2395 2396 2397 2398 2399 2400 2401 2402 2403 2404 2405 2406 2407 2408 2409 2410 2411 2412 2413 2414 2415 2416 2417 2418 2419 2420 2421 2422 2423 2424 2425 2426 2427 2428 2429 2430 2431 2432 2433 2434 2435 2436 2437 2438 2439 2440 2441 2442 2443 2444 2445 2446 2447 2448 2449 2450 2451 2452 2453 2454 2455 2456 2457 2458 2459 2460 2461 2462 2463 2464 2465 2466 2467 2468 2469 2470 2471 2472 2473 2474 2475 2476 2477 2478 2479 2480 2481 2482 2483 2484 2485 2486 2487 2488 2489 2490 2491 2492 2493 2494 2495 2496 2497 2498 2499 2500 2501 2502 2503 2504 2505 2506 2507 2508 2509 2510 2511 2512 2513 2514 2515 2516 2517 2518 2519 2520 2521 2522 2523 2524 2525 2526 2527 2528 2529 2530 2531 2532 2533 2534 2535 2536 2537 2538 2539 2540 2541 2542 2543 2544 2545 2546 2547 2548 2549 2550 2551 2552 2553 2554 2555 2556 2557 2558 2559 2560 2561 2562 2563 2564 2565 2566 2567 2568 2569 2570 2571 2572 2573 2574 2575 2576 2577 2578 2579 2580 2581 2582 2583 2584 2585 2586 2587 2588 2589 2590 2591 2592 2593 2594 2595 2596 2597 2598 2599 2600 2601 2602 2603 2604 2605 2606 2607 2608 2609 2610 2611 2612 2613 2614 2615 2616 2617 2618 2619 2620 2621 2622 2623 2624 2625 2626 2627 2628 2629 2630 2631 2632 2633 2634 2635 2636 2637 2638 2639 2640 2641 2642 2643 2644 2645 2646 2647 2648 2649 2650 2651 2652 2653 2654 2655 2656 2657 2658 2659 2660 2661 2662 2663 2664 2665 2666 2667 2668 2669 2670 2671 2672 2673 2674 2675 2676 2677 2678 2679 2680 2681 2682 2683 2684 2685 2686 2687 2688 2689 2690 2691 2692 2693 2694 2695 2696 2697 2698 2699 2700 2701 2702 2703 2704 2705 2706 2707 2708 2709 2710 2711 2712 2713 2714 2715 2716 2717 2718 2719 2720 2721 2722 2723 2724 2725 2726 2727 2728 2729 2730 2731 2732 2733 2734 2735 2736 2737 2738 2739 2740 2741 2742 2743 2744 2745 2746 2747 2748 2749 2750 2751 2752 2753 2754 2755 2756 2757 2758 2759 2760 2761 2762 2763 2764 2765 2766 2767 2768 2769 2770 2771 2772 2773 2774 2775 2776 2777 2778 2779 2780 2781 2782 2783 2784 2785 2786 2787 2788 2789 2790 2791 2792 2793 2794 2795 2796 2797 2798 2799 2800 2801 2802 2803 2804 2805 2806 2807 2808 2809 2810 2811 2812 2813 2814 2815 2816 2817 2818 2819 2820 2821 2822 2823 2824 2825 2826 2827 2828 2829 2830 2831 2832 2833 2834 2835 2836 2837 2838 2839 2840 2841 2842 2843 2844 2845 2846 2847 2848 2849 2850 2851 2852 2853 2854 2855 2856 2857 2858 2859 2860 2861 2862 2863 2864 2865 2866 2867 2868 2869 2870 2871 2872 2873 2874 2875 2876 2877 2878 2879 2880 2881 2882 2883 2884 2885 2886 2887 2888 2889 2890 2891 2892 2893 2894 2895 2896 2897 2898 2899 2900 2901 2902 2903 2904 2905 2906 2907 2908 2909 2910 2911 2912 2913 2914 2915 2916 2917 2918 2919 2920 2921 2922 2923 2924 2925 2926 2927 2928 2929 2930 2931 2932 2933 2934 2935 2936 2937 2938 2939 2940 2941 2942 2943 2944 2945 2946 2947 2948 2949 2950 2951 2952 2953 2954 2955 2956 2957 2958 2959 2960 2961 2962 2963 2964 2965 2966 2967 2968 2969 2970 2971 2972 2973 2974 2975 2976 2977 2978 2979 2980 2981 2982 2983 2984 2985 2986 2987 2988 2989 2990 2991 2992 2993 2994 2995 2996 2997 2998 2999 3000 3001 3002 3003 3004 3005 3006 3007 3008 3009 3010 3011 3012 3013 3014 3015 3016 3017 3018 3019 3020 3021 3022 3023 3024 3025 3026 3027 3028 3029 3030 3031 3032 3033 3034 3035 3036 3037 3038 3039 3040 3041 3042 3043 3044 3045 3046 3047 3048 3049 3050 3051 3052 3053 3054 3055 3056 3057 3058 3059 3060 3061 3062 3063 3064 3065 3066 3067 3068 3069 3070 3071 3072 3073 3074 3075 3076 3077 3078 3079 3080 3081 3082 3083 3084 3085 3086 3087 3088 3089 3090 3091 3092 3093 3094 3095 3096 3097 3098 3099 3100 3101 3102 3103 3104 3105 3106 3107 3108 3109 3110 3111 3112 3113 3114 3115 3116 3117 3118 3119 3120 3121 3122 3123 3124 3125 3126 3127 3128 3129 3130 3131 3132 3133 3134 3135 3136 3137 3138 3139 3140 3141 3142 3143 3144 3145 3146 3147 3148 3149 3150 3151 3152 3153 3154 3155 3156 3157 3158 3159 3160 3161 3162 3163 3164 3165 3166 3167 3168 3169 3170 3171 3172 3173 3174 3175 3176 3177 3178 3179 3180 3181 3182 3183 3184 3185 3186 3187 3188 3189 3190 3191 3192 3193 3194 3195 3196 3197 3198 3199 3200 3201 3202 3203 3204 3205 3206 3207 3208 3209 3210 3211 3212 3213 3214 3215 3216 3217 3218 3219 3220 3221 3222 3223 3224 3225 3226 3227 3228 3229 3230 3231 3232 3233 3234 3235 3236 3237 3238 3239 3240 3241 3242 3243 3244 3245 3246 3247 3248 3249 3250 3251 3252 3253 3254 3255 3256 3257 3258 3259 3260 3261 3262 3263 3264 3265 3266 3267 3268 3269 3270 3271 3272 3273 3274 3275 3276 3277 3278 3279 3280 3281 3282 3283 3284 3285 3286 3287 3288 3289 3290 3291 3292 3293 3294 3295 3296 3297 3298 3299 3300 3301 3302 3303 3304 3305 3306 3307 3308 3309 3310 3311 3312 3313 3314 3315 3316 3317 3318 3319 3320 3321 3322 3323 3324 3325 3326 3327 3328 3329 3330 3331 3332 3333 3334 3335 3336 3337 3338 3339 3340 3341 3342 3343 3344 3345 3346 3347 3348 3349 3350 3351 3352 3353 3354 3355 3356 3357 3358 3359 3360 3361 3362 3363 3364 3365 3366 3367 3368 3369 3370 3371 3372 3373 3374 3375 3376 3377 3378 3379 3380 3381 3382 3383 3384 3385 3386 3387 3388 3389 3390 3391 3392 3393 3394 3395 3396 3397 3398 3399 3400 3401 3402 3403 3404 3405 3406 3407 3408 3409 3410 3411 3412 3413 3414 3415 3416 3417 3418 3419 3420 3421 3422 3423 3424 3425 3426 3427 3428 3429 3430 3431 3432 3433 3434 3435 3436 3437 3438 3439 3440 3441 3442 3443 3444 3445 3446 3447 3448 3449 3450 3451 3452 3453 3454 3455 3456 3457 3458 3459 3460 3461 3462 3463 3464 3465 3466 3467 3468 3469 3470 3471 3472 3473 3474 3475 3476 3477 3478 3479 3480 3481 3482 3483 3484 3485 3486 3487 3488 3489 3490 3491 3492 3493 3494 3495 3496 3497 3498 3499 3500 3501 3502 3503 3504 3505 3506 3507 3508 3509 3510 3511 3512 3513 3514 3515 3516 3517 3518 3519 3520 3521 3522 3523 3524 3525 3526 3527 3528 3529 3530 3531 3532 3533 3534 3535 3536 3537 3538 3539 3540 3541 3542 3543 3544 3545 3546 3547 3548 3549 3550 3551 3552 3553 3554 3555 3556 3557 3558 3559 3560 3561 3562 3563 3564 3565 3566 3567 3568 3569 3570 3571 3572 3573 3574 3575 3576 3577 3578 3579 3580 3581 3582 3583 3584 3585 3586 3587 3588 3589 3590 3591 3592 3593 3594 3595 3596 3597 3598 3599 3600 3601 3602 3603 3604 3605 3606 3607 3608 3609 3610 3611 3612 3613 3614 3615 3616 3617 3618 3619 3620 3621 3622 3623 3624 3625 3626 3627 3628 3629 3630 3631 3632 3633 3634 3635 3636 3637 3638 3639 3640 3641 3642 3643 3644 3645 3646 3647 3648 3649 3650 3651 3652 3653 3654 3655 3656 3657 3658 3659 3660 3661 3662 3663 3664 3665 3666 3667 3668 3669 3670 3671 3672 3673 3674 3675 3676 3677 3678 3679 3680 3681 3682 3683 3684 3685 3686 3687 3688 3689 3690 3691 3692 3693 3694 3695 3696 3697 3698 3699 3700 3701 3702 3703 3704 3705 3706 3707 3708 3709 3710 3711 3712 3713 3714 3715 3716 3717 3718 3719 3720 3721 3722 3723 3724 3725 3726 3727 3728 3729 3730 3731 3732 3733 3734 3735 3736 3737 3738 3739 3740 3741 3742 3743 3744 3745 3746 3747 3748 3749 3750 3751 3752 3753 3754 3755 3756 3757 3758 3759 3760 3761 3762 3763 3764 3765 3766 3767 3768 3769 3770 3771 3772 3773 3774 3775 3776 3777 3778 3779 3780 3781 3782 3783 3784 3785 3786 3787 3788 3789 3790 3791 3792 3793 3794 3795 3796 3797 3798 3799 3800 3801 3802 3803 3804 3805 3806 3807 3808 3809 3810 3811 3812 3813 3814 3815 3816 3817 3818 3819 3820 3821 3822 3823 3824 3825 3826 3827 3828 3829 3830 3831 3832 3833 3834 3835 3836 3837 3838 3839 3840 3841 3842 3843 3844 3845 3846 3847 3848 3849 3850 3851 3852 3853 3854 3855 3856 3857 3858 3859 3860 3861 3862 3863 3864 3865 3866 3867 3868 3869 3870 3871 3872 3873 3874 3875 3876 3877 3878 3879 3880 3881 3882 3883 3884 3885 3886 3887 3888 3889 3890 3891 3892 3893 3894 3895 3896 3897 3898 3899 3900 3901 3902 3903 3904 3905 3906 3907 3908 3909 3910 3911 3912 3913 3914 3915 3916 3917 3918 3919 3920 3921 3922 3923 3924 3925 3926 3927 3928 3929 3930 3931 3932 3933 3934 3935 3936 3937 3938 3939 3940 3941 3942 3943 3944 3945 3946 3947 3948 3949 3950 3951 3952 3953 3954 3955 3956 3957 3958 3959 3960 3961 3962 3963 3964 3965 3966 3967 3968 3969 3970 3971 3972 3973 3974 3975 3976 3977 3978 3979 3980 3981 3982 3983 3984 3985 3986 3987 3988 3989 3990 3991 3992 3993 3994 3995 3996 3997 3998 3999 4000 4001 4002 4003 4004 Building optimizers ... Starting Training! Initializing ... Training... Iteration: 1; Percent complete: 0.0%; Average loss: 8.9628 Iteration: 2; Percent complete: 0.1%; Average loss: 8.8343 Iteration: 3; Percent complete: 0.1%; Average loss: 8.6592 Iteration: 4; Percent complete: 0.1%; Average loss: 8.3297 Iteration: 5; Percent complete: 0.1%; Average loss: 7.9770 Iteration: 6; Percent complete: 0.1%; Average loss: 7.3291 Iteration: 7; Percent complete: 0.2%; Average loss: 6.7586 Iteration: 8; Percent complete: 0.2%; Average loss: 6.9260 Iteration: 9; Percent complete: 0.2%; Average loss: 6.8659 Iteration: 10; Percent complete: 0.2%; Average loss: 6.6029 Iteration: 11; Percent complete: 0.3%; Average loss: 6.2245 Iteration: 12; Percent complete: 0.3%; Average loss: 6.1180 Iteration: 13; Percent complete: 0.3%; Average loss: 5.6463 Iteration: 14; Percent complete: 0.4%; Average loss: 5.6409 Iteration: 15; Percent complete: 0.4%; Average loss: 5.4941 Iteration: 16; Percent complete: 0.4%; Average loss: 5.6748 Iteration: 17; Percent complete: 0.4%; Average loss: 5.3386 Iteration: 18; Percent complete: 0.4%; Average loss: 5.3936 Iteration: 19; Percent complete: 0.5%; Average loss: 4.8958 Iteration: 20; Percent complete: 0.5%; Average loss: 4.9486 Iteration: 21; Percent complete: 0.5%; Average loss: 4.8804 Iteration: 22; Percent complete: 0.5%; Average loss: 4.8959 Iteration: 23; Percent complete: 0.6%; Average loss: 4.8363 Iteration: 24; Percent complete: 0.6%; Average loss: 4.9209 Iteration: 25; Percent complete: 0.6%; Average loss: 4.9964 Iteration: 26; Percent complete: 0.7%; Average loss: 4.8157 Iteration: 27; Percent complete: 0.7%; Average loss: 4.8140 Iteration: 28; Percent complete: 0.7%; Average loss: 4.9719 Iteration: 29; Percent complete: 0.7%; Average loss: 4.6794 Iteration: 30; Percent complete: 0.8%; Average loss: 4.7190 Iteration: 31; Percent complete: 0.8%; Average loss: 4.6346 Iteration: 32; Percent complete: 0.8%; Average loss: 4.9546 Iteration: 33; Percent complete: 0.8%; Average loss: 4.7239 Iteration: 34; Percent complete: 0.9%; Average loss: 4.8934 Iteration: 35; Percent complete: 0.9%; Average loss: 4.9053 Iteration: 36; Percent complete: 0.9%; Average loss: 4.8707 Iteration: 37; Percent complete: 0.9%; Average loss: 4.7875 Iteration: 38; Percent complete: 0.9%; Average loss: 4.6638 Iteration: 39; Percent complete: 1.0%; Average loss: 4.8682 Iteration: 40; Percent complete: 1.0%; Average loss: 4.5905 Iteration: 41; Percent complete: 1.0%; Average loss: 4.5374 Iteration: 42; Percent complete: 1.1%; Average loss: 4.6670 Iteration: 43; Percent complete: 1.1%; Average loss: 4.8148 Iteration: 44; Percent complete: 1.1%; Average loss: 4.6093 Iteration: 45; Percent complete: 1.1%; Average loss: 4.6322 Iteration: 46; Percent complete: 1.1%; Average loss: 4.5139 Iteration: 47; Percent complete: 1.2%; Average loss: 4.6834 Iteration: 48; Percent complete: 1.2%; Average loss: 4.5035 Iteration: 49; Percent complete: 1.2%; Average loss: 4.8119 Iteration: 50; Percent complete: 1.2%; Average loss: 4.5661 Iteration: 51; Percent complete: 1.3%; Average loss: 4.8288 Iteration: 52; Percent complete: 1.3%; Average loss: 4.5483 Iteration: 53; Percent complete: 1.3%; Average loss: 4.6328 Iteration: 54; Percent complete: 1.4%; Average loss: 4.5040 Iteration: 55; Percent complete: 1.4%; Average loss: 4.6982 Iteration: 56; Percent complete: 1.4%; Average loss: 4.5518 Iteration: 57; Percent complete: 1.4%; Average loss: 4.5344 Iteration: 58; Percent complete: 1.5%; Average loss: 4.5677 Iteration: 59; Percent complete: 1.5%; Average loss: 4.7938 Iteration: 60; Percent complete: 1.5%; Average loss: 4.5126 Iteration: 61; Percent complete: 1.5%; Average loss: 4.4967 Iteration: 62; Percent complete: 1.6%; Average loss: 4.6333 Iteration: 63; Percent complete: 1.6%; Average loss: 4.5888 Iteration: 64; Percent complete: 1.6%; Average loss: 4.3075 Iteration: 65; Percent complete: 1.6%; Average loss: 4.5133 Iteration: 66; Percent complete: 1.7%; Average loss: 4.5084 Iteration: 67; Percent complete: 1.7%; Average loss: 4.8161 Iteration: 68; Percent complete: 1.7%; Average loss: 4.5948 Iteration: 69; Percent complete: 1.7%; Average loss: 4.6278 Iteration: 70; Percent complete: 1.8%; Average loss: 4.4582 Iteration: 71; Percent complete: 1.8%; Average loss: 4.4484 Iteration: 72; Percent complete: 1.8%; Average loss: 4.4757 Iteration: 73; Percent complete: 1.8%; Average loss: 4.5937 Iteration: 74; Percent complete: 1.8%; Average loss: 4.3474 Iteration: 75; Percent complete: 1.9%; Average loss: 4.4921 Iteration: 76; Percent complete: 1.9%; Average loss: 4.6886 Iteration: 77; Percent complete: 1.9%; Average loss: 4.3638 Iteration: 78; Percent complete: 1.9%; Average loss: 4.6839 Iteration: 79; Percent complete: 2.0%; Average loss: 4.4069 Iteration: 80; Percent complete: 2.0%; Average loss: 4.4496 Iteration: 81; Percent complete: 2.0%; Average loss: 4.5592 Iteration: 82; Percent complete: 2.1%; Average loss: 4.7000 Iteration: 83; Percent complete: 2.1%; Average loss: 4.3022 Iteration: 84; Percent complete: 2.1%; Average loss: 4.5013 Iteration: 85; Percent complete: 2.1%; Average loss: 4.3784 Iteration: 86; Percent complete: 2.1%; Average loss: 4.5025 Iteration: 87; Percent complete: 2.2%; Average loss: 4.5840 Iteration: 88; Percent complete: 2.2%; Average loss: 4.4873 Iteration: 89; Percent complete: 2.2%; Average loss: 4.2982 Iteration: 90; Percent complete: 2.2%; Average loss: 4.3686 Iteration: 91; Percent complete: 2.3%; Average loss: 4.3155 Iteration: 92; Percent complete: 2.3%; Average loss: 4.2115 Iteration: 93; Percent complete: 2.3%; Average loss: 4.2979 Iteration: 94; Percent complete: 2.4%; Average loss: 4.3565 Iteration: 95; Percent complete: 2.4%; Average loss: 4.3283 Iteration: 96; Percent complete: 2.4%; Average loss: 4.4177 Iteration: 97; Percent complete: 2.4%; Average loss: 4.3689 Iteration: 98; Percent complete: 2.5%; Average loss: 4.4567 Iteration: 99; Percent complete: 2.5%; Average loss: 4.3242 Iteration: 100; Percent complete: 2.5%; Average loss: 4.4249 Iteration: 101; Percent complete: 2.5%; Average loss: 4.1899 Iteration: 102; Percent complete: 2.5%; Average loss: 4.5148 Iteration: 103; Percent complete: 2.6%; Average loss: 4.2772 Iteration: 104; Percent complete: 2.6%; Average loss: 4.2775 Iteration: 105; Percent complete: 2.6%; Average loss: 4.5088 Iteration: 106; Percent complete: 2.6%; Average loss: 4.5121 Iteration: 107; Percent complete: 2.7%; Average loss: 4.3327 Iteration: 108; Percent complete: 2.7%; Average loss: 4.5252 Iteration: 109; Percent complete: 2.7%; Average loss: 4.1831 Iteration: 110; Percent complete: 2.8%; Average loss: 4.1833 Iteration: 111; Percent complete: 2.8%; Average loss: 4.3274 Iteration: 112; Percent complete: 2.8%; Average loss: 4.5191 Iteration: 113; Percent complete: 2.8%; Average loss: 4.3452 Iteration: 114; Percent complete: 2.9%; Average loss: 4.4353 Iteration: 115; Percent complete: 2.9%; Average loss: 4.3186 Iteration: 116; Percent complete: 2.9%; Average loss: 4.3743 Iteration: 117; Percent complete: 2.9%; Average loss: 4.3036 Iteration: 118; Percent complete: 2.9%; Average loss: 4.4618 Iteration: 119; Percent complete: 3.0%; Average loss: 4.4117 Iteration: 120; Percent complete: 3.0%; Average loss: 4.1575 Iteration: 121; Percent complete: 3.0%; Average loss: 4.3188 Iteration: 122; Percent complete: 3.0%; Average loss: 4.4365 Iteration: 123; Percent complete: 3.1%; Average loss: 4.1238 Iteration: 124; Percent complete: 3.1%; Average loss: 4.3645 Iteration: 125; Percent complete: 3.1%; Average loss: 4.2155 Iteration: 126; Percent complete: 3.1%; Average loss: 4.7236 Iteration: 127; Percent complete: 3.2%; Average loss: 4.1829 Iteration: 128; Percent complete: 3.2%; Average loss: 4.2557 Iteration: 129; Percent complete: 3.2%; Average loss: 4.4819 Iteration: 130; Percent complete: 3.2%; Average loss: 4.4005 Iteration: 131; Percent complete: 3.3%; Average loss: 4.2772 Iteration: 132; Percent complete: 3.3%; Average loss: 4.2948 Iteration: 133; Percent complete: 3.3%; Average loss: 4.2153 Iteration: 134; Percent complete: 3.4%; Average loss: 4.3869 Iteration: 135; Percent complete: 3.4%; Average loss: 4.3561 Iteration: 136; Percent complete: 3.4%; Average loss: 4.2666 Iteration: 137; Percent complete: 3.4%; Average loss: 4.2705 Iteration: 138; Percent complete: 3.5%; Average loss: 4.3425 Iteration: 139; Percent complete: 3.5%; Average loss: 4.3308 Iteration: 140; Percent complete: 3.5%; Average loss: 4.2482 Iteration: 141; Percent complete: 3.5%; Average loss: 4.2002 Iteration: 142; Percent complete: 3.5%; Average loss: 4.0395 Iteration: 143; Percent complete: 3.6%; Average loss: 4.4818 Iteration: 144; Percent complete: 3.6%; Average loss: 4.0831 Iteration: 145; Percent complete: 3.6%; Average loss: 4.1008 Iteration: 146; Percent complete: 3.6%; Average loss: 4.2408 Iteration: 147; Percent complete: 3.7%; Average loss: 4.1107 Iteration: 148; Percent complete: 3.7%; Average loss: 4.3374 Iteration: 149; Percent complete: 3.7%; Average loss: 4.0637 Iteration: 150; Percent complete: 3.8%; Average loss: 4.2662 Iteration: 151; Percent complete: 3.8%; Average loss: 4.2423 Iteration: 152; Percent complete: 3.8%; Average loss: 4.1396 Iteration: 153; Percent complete: 3.8%; Average loss: 4.2338 Iteration: 154; Percent complete: 3.9%; Average loss: 4.1130 Iteration: 155; Percent complete: 3.9%; Average loss: 4.2585 Iteration: 156; Percent complete: 3.9%; Average loss: 4.2221 Iteration: 157; Percent complete: 3.9%; Average loss: 4.1251 Iteration: 158; Percent complete: 4.0%; Average loss: 4.2255 Iteration: 159; Percent complete: 4.0%; Average loss: 4.3224 Iteration: 160; Percent complete: 4.0%; Average loss: 4.0658 Iteration: 161; Percent complete: 4.0%; Average loss: 4.2790 Iteration: 162; Percent complete: 4.0%; Average loss: 4.1332 Iteration: 163; Percent complete: 4.1%; Average loss: 3.9183 Iteration: 164; Percent complete: 4.1%; Average loss: 4.0841 Iteration: 165; Percent complete: 4.1%; Average loss: 3.7811 Iteration: 166; Percent complete: 4.2%; Average loss: 4.2144 Iteration: 167; Percent complete: 4.2%; Average loss: 3.9774 Iteration: 168; Percent complete: 4.2%; Average loss: 4.1725 Iteration: 169; Percent complete: 4.2%; Average loss: 4.2217 Iteration: 170; Percent complete: 4.2%; Average loss: 4.2590 Iteration: 171; Percent complete: 4.3%; Average loss: 4.2249 Iteration: 172; Percent complete: 4.3%; Average loss: 4.1298 Iteration: 173; Percent complete: 4.3%; Average loss: 4.2693 Iteration: 174; Percent complete: 4.3%; Average loss: 4.0924 Iteration: 175; Percent complete: 4.4%; Average loss: 3.8366 Iteration: 176; Percent complete: 4.4%; Average loss: 4.0131 Iteration: 177; Percent complete: 4.4%; Average loss: 4.0707 Iteration: 178; Percent complete: 4.5%; Average loss: 4.3873 Iteration: 179; Percent complete: 4.5%; Average loss: 3.9353 Iteration: 180; Percent complete: 4.5%; Average loss: 4.1841 Iteration: 181; Percent complete: 4.5%; Average loss: 3.9691 Iteration: 182; Percent complete: 4.5%; Average loss: 4.2445 Iteration: 183; Percent complete: 4.6%; Average loss: 3.9909 Iteration: 184; Percent complete: 4.6%; Average loss: 4.2100 Iteration: 185; Percent complete: 4.6%; Average loss: 4.0717 Iteration: 186; Percent complete: 4.7%; Average loss: 4.1351 Iteration: 187; Percent complete: 4.7%; Average loss: 3.9354 Iteration: 188; Percent complete: 4.7%; Average loss: 3.8134 Iteration: 189; Percent complete: 4.7%; Average loss: 4.3966 Iteration: 190; Percent complete: 4.8%; Average loss: 4.0509 Iteration: 191; Percent complete: 4.8%; Average loss: 3.9912 Iteration: 192; Percent complete: 4.8%; Average loss: 3.9772 Iteration: 193; Percent complete: 4.8%; Average loss: 3.9212 Iteration: 194; Percent complete: 4.9%; Average loss: 4.0303 Iteration: 195; Percent complete: 4.9%; Average loss: 4.1305 Iteration: 196; Percent complete: 4.9%; Average loss: 4.3996 Iteration: 197; Percent complete: 4.9%; Average loss: 4.3042 Iteration: 198; Percent complete: 5.0%; Average loss: 4.3489 Iteration: 199; Percent complete: 5.0%; Average loss: 4.2896 Iteration: 200; Percent complete: 5.0%; Average loss: 3.8091 Iteration: 201; Percent complete: 5.0%; Average loss: 4.0971 Iteration: 202; Percent complete: 5.1%; Average loss: 3.9714 Iteration: 203; Percent complete: 5.1%; Average loss: 4.0337 Iteration: 204; Percent complete: 5.1%; Average loss: 4.1092 Iteration: 205; Percent complete: 5.1%; Average loss: 4.0293 Iteration: 206; Percent complete: 5.1%; Average loss: 4.2051 Iteration: 207; Percent complete: 5.2%; Average loss: 4.1471 Iteration: 208; Percent complete: 5.2%; Average loss: 4.2239 Iteration: 209; Percent complete: 5.2%; Average loss: 3.8455 Iteration: 210; Percent complete: 5.2%; Average loss: 3.9024 Iteration: 211; Percent complete: 5.3%; Average loss: 4.2336 Iteration: 212; Percent complete: 5.3%; Average loss: 4.0724 Iteration: 213; Percent complete: 5.3%; Average loss: 4.0936 Iteration: 214; Percent complete: 5.3%; Average loss: 3.9975 Iteration: 215; Percent complete: 5.4%; Average loss: 3.9113 Iteration: 216; Percent complete: 5.4%; Average loss: 4.0042 Iteration: 217; Percent complete: 5.4%; Average loss: 4.0473 Iteration: 218; Percent complete: 5.5%; Average loss: 3.9720 Iteration: 219; Percent complete: 5.5%; Average loss: 3.8636 Iteration: 220; Percent complete: 5.5%; Average loss: 3.7938 Iteration: 221; Percent complete: 5.5%; Average loss: 3.9970 Iteration: 222; Percent complete: 5.5%; Average loss: 3.9214 Iteration: 223; Percent complete: 5.6%; Average loss: 3.9747 Iteration: 224; Percent complete: 5.6%; Average loss: 4.4310 Iteration: 225; Percent complete: 5.6%; Average loss: 4.2264 Iteration: 226; Percent complete: 5.7%; Average loss: 4.1461 Iteration: 227; Percent complete: 5.7%; Average loss: 3.8592 Iteration: 228; Percent complete: 5.7%; Average loss: 3.8012 Iteration: 229; Percent complete: 5.7%; Average loss: 4.0243 Iteration: 230; Percent complete: 5.8%; Average loss: 4.1253 Iteration: 231; Percent complete: 5.8%; Average loss: 4.0399 Iteration: 232; Percent complete: 5.8%; Average loss: 3.8528 Iteration: 233; Percent complete: 5.8%; Average loss: 4.2015 Iteration: 234; Percent complete: 5.9%; Average loss: 3.8354 Iteration: 235; Percent complete: 5.9%; Average loss: 4.0100 Iteration: 236; Percent complete: 5.9%; Average loss: 4.1361 Iteration: 237; Percent complete: 5.9%; Average loss: 4.0682 Iteration: 238; Percent complete: 5.9%; Average loss: 3.8058 Iteration: 239; Percent complete: 6.0%; Average loss: 4.0169 Iteration: 240; Percent complete: 6.0%; Average loss: 3.8472 Iteration: 241; Percent complete: 6.0%; Average loss: 4.0268 Iteration: 242; Percent complete: 6.0%; Average loss: 3.8856 Iteration: 243; Percent complete: 6.1%; Average loss: 3.9712 Iteration: 244; Percent complete: 6.1%; Average loss: 4.0298 Iteration: 245; Percent complete: 6.1%; Average loss: 3.9250 Iteration: 246; Percent complete: 6.2%; Average loss: 4.0529 Iteration: 247; Percent complete: 6.2%; Average loss: 4.0036 Iteration: 248; Percent complete: 6.2%; Average loss: 4.2044 Iteration: 249; Percent complete: 6.2%; Average loss: 4.1692 Iteration: 250; Percent complete: 6.2%; Average loss: 3.9902 Iteration: 251; Percent complete: 6.3%; Average loss: 3.9847 Iteration: 252; Percent complete: 6.3%; Average loss: 3.9312 Iteration: 253; Percent complete: 6.3%; Average loss: 4.0968 Iteration: 254; Percent complete: 6.3%; Average loss: 3.7440 Iteration: 255; Percent complete: 6.4%; Average loss: 3.7814 Iteration: 256; Percent complete: 6.4%; Average loss: 3.8778 Iteration: 257; Percent complete: 6.4%; Average loss: 3.9462 Iteration: 258; Percent complete: 6.5%; Average loss: 3.8670 Iteration: 259; Percent complete: 6.5%; Average loss: 3.9583 Iteration: 260; Percent complete: 6.5%; Average loss: 3.8852 Iteration: 261; Percent complete: 6.5%; Average loss: 3.9413 Iteration: 262; Percent complete: 6.6%; Average loss: 4.1769 Iteration: 263; Percent complete: 6.6%; Average loss: 3.9672 Iteration: 264; Percent complete: 6.6%; Average loss: 4.0755 Iteration: 265; Percent complete: 6.6%; Average loss: 3.6813 Iteration: 266; Percent complete: 6.7%; Average loss: 3.6722 Iteration: 267; Percent complete: 6.7%; Average loss: 4.1432 Iteration: 268; Percent complete: 6.7%; Average loss: 4.3056 Iteration: 269; Percent complete: 6.7%; Average loss: 4.2333 Iteration: 270; Percent complete: 6.8%; Average loss: 3.9264 Iteration: 271; Percent complete: 6.8%; Average loss: 4.1835 Iteration: 272; Percent complete: 6.8%; Average loss: 4.2209 Iteration: 273; Percent complete: 6.8%; Average loss: 3.8886 Iteration: 274; Percent complete: 6.9%; Average loss: 3.9627 Iteration: 275; Percent complete: 6.9%; Average loss: 4.0668 Iteration: 276; Percent complete: 6.9%; Average loss: 4.1140 Iteration: 277; Percent complete: 6.9%; Average loss: 3.9334 Iteration: 278; Percent complete: 7.0%; Average loss: 3.9210 Iteration: 279; Percent complete: 7.0%; Average loss: 3.9074 Iteration: 280; Percent complete: 7.0%; Average loss: 3.6459 Iteration: 281; Percent complete: 7.0%; Average loss: 3.4367 Iteration: 282; Percent complete: 7.0%; Average loss: 4.0782 Iteration: 283; Percent complete: 7.1%; Average loss: 3.8470 Iteration: 284; Percent complete: 7.1%; Average loss: 3.8382 Iteration: 285; Percent complete: 7.1%; Average loss: 4.2122 Iteration: 286; Percent complete: 7.1%; Average loss: 4.0994 Iteration: 287; Percent complete: 7.2%; Average loss: 3.8104 Iteration: 288; Percent complete: 7.2%; Average loss: 4.0726 Iteration: 289; Percent complete: 7.2%; Average loss: 3.9776 Iteration: 290; Percent complete: 7.2%; Average loss: 3.9142 Iteration: 291; Percent complete: 7.3%; Average loss: 3.9161 Iteration: 292; Percent complete: 7.3%; Average loss: 4.1642 Iteration: 293; Percent complete: 7.3%; Average loss: 4.1852 Iteration: 294; Percent complete: 7.3%; Average loss: 3.9292 Iteration: 295; Percent complete: 7.4%; Average loss: 4.2290 Iteration: 296; Percent complete: 7.4%; Average loss: 3.6722 Iteration: 297; Percent complete: 7.4%; Average loss: 3.9260 Iteration: 298; Percent complete: 7.4%; Average loss: 3.8653 Iteration: 299; Percent complete: 7.5%; Average loss: 3.9611 Iteration: 300; Percent complete: 7.5%; Average loss: 4.0588 Iteration: 301; Percent complete: 7.5%; Average loss: 3.8498 Iteration: 302; Percent complete: 7.5%; Average loss: 3.8037 Iteration: 303; Percent complete: 7.6%; Average loss: 4.0109 Iteration: 304; Percent complete: 7.6%; Average loss: 3.7357 Iteration: 305; Percent complete: 7.6%; Average loss: 4.0324 Iteration: 306; Percent complete: 7.6%; Average loss: 3.7437 Iteration: 307; Percent complete: 7.7%; Average loss: 3.9824 Iteration: 308; Percent complete: 7.7%; Average loss: 4.0195 Iteration: 309; Percent complete: 7.7%; Average loss: 3.6357 Iteration: 310; Percent complete: 7.8%; Average loss: 4.2062 Iteration: 311; Percent complete: 7.8%; Average loss: 3.9330 Iteration: 312; Percent complete: 7.8%; Average loss: 4.0195 Iteration: 313; Percent complete: 7.8%; Average loss: 3.8258 Iteration: 314; Percent complete: 7.8%; Average loss: 3.6191 Iteration: 315; Percent complete: 7.9%; Average loss: 3.7524 Iteration: 316; Percent complete: 7.9%; Average loss: 3.7817 Iteration: 317; Percent complete: 7.9%; Average loss: 3.7227 Iteration: 318; Percent complete: 8.0%; Average loss: 3.6559 Iteration: 319; Percent complete: 8.0%; Average loss: 3.9009 Iteration: 320; Percent complete: 8.0%; Average loss: 4.0995 Iteration: 321; Percent complete: 8.0%; Average loss: 3.9927 Iteration: 322; Percent complete: 8.1%; Average loss: 4.1229 Iteration: 323; Percent complete: 8.1%; Average loss: 3.8154 Iteration: 324; Percent complete: 8.1%; Average loss: 4.0058 Iteration: 325; Percent complete: 8.1%; Average loss: 3.5169 Iteration: 326; Percent complete: 8.2%; Average loss: 3.8350 Iteration: 327; Percent complete: 8.2%; Average loss: 3.8259 Iteration: 328; Percent complete: 8.2%; Average loss: 3.9748 Iteration: 329; Percent complete: 8.2%; Average loss: 3.7931 Iteration: 330; Percent complete: 8.2%; Average loss: 3.8386 Iteration: 331; Percent complete: 8.3%; Average loss: 3.8571 Iteration: 332; Percent complete: 8.3%; Average loss: 3.8165 Iteration: 333; Percent complete: 8.3%; Average loss: 3.8858 Iteration: 334; Percent complete: 8.3%; Average loss: 3.6886 Iteration: 335; Percent complete: 8.4%; Average loss: 3.6719 Iteration: 336; Percent complete: 8.4%; Average loss: 3.7104 Iteration: 337; Percent complete: 8.4%; Average loss: 3.5886 Iteration: 338; Percent complete: 8.5%; Average loss: 3.7231 Iteration: 339; Percent complete: 8.5%; Average loss: 3.8111 Iteration: 340; Percent complete: 8.5%; Average loss: 4.0626 Iteration: 341; Percent complete: 8.5%; Average loss: 3.8089 Iteration: 342; Percent complete: 8.6%; Average loss: 3.9092 Iteration: 343; Percent complete: 8.6%; Average loss: 3.8564 Iteration: 344; Percent complete: 8.6%; Average loss: 3.7870 Iteration: 345; Percent complete: 8.6%; Average loss: 4.0120 Iteration: 346; Percent complete: 8.6%; Average loss: 3.7148 Iteration: 347; Percent complete: 8.7%; Average loss: 3.8811 Iteration: 348; Percent complete: 8.7%; Average loss: 3.8372 Iteration: 349; Percent complete: 8.7%; Average loss: 3.7754 Iteration: 350; Percent complete: 8.8%; Average loss: 4.1021 Iteration: 351; Percent complete: 8.8%; Average loss: 3.9062 Iteration: 352; Percent complete: 8.8%; Average loss: 3.7697 Iteration: 353; Percent complete: 8.8%; Average loss: 3.6772 Iteration: 354; Percent complete: 8.8%; Average loss: 4.0363 Iteration: 355; Percent complete: 8.9%; Average loss: 3.9751 Iteration: 356; Percent complete: 8.9%; Average loss: 3.7483 Iteration: 357; Percent complete: 8.9%; Average loss: 3.9562 Iteration: 358; Percent complete: 8.9%; Average loss: 3.8029 Iteration: 359; Percent complete: 9.0%; Average loss: 3.9083 Iteration: 360; Percent complete: 9.0%; Average loss: 3.8005 Iteration: 361; Percent complete: 9.0%; Average loss: 3.8683 Iteration: 362; Percent complete: 9.0%; Average loss: 3.8801 Iteration: 363; Percent complete: 9.1%; Average loss: 3.8870 Iteration: 364; Percent complete: 9.1%; Average loss: 3.8691 Iteration: 365; Percent complete: 9.1%; Average loss: 3.6718 Iteration: 366; Percent complete: 9.2%; Average loss: 4.0133 Iteration: 367; Percent complete: 9.2%; Average loss: 3.6297 Iteration: 368; Percent complete: 9.2%; Average loss: 3.8958 Iteration: 369; Percent complete: 9.2%; Average loss: 3.7986 Iteration: 370; Percent complete: 9.2%; Average loss: 3.7870 Iteration: 371; Percent complete: 9.3%; Average loss: 3.9622 Iteration: 372; Percent complete: 9.3%; Average loss: 3.8877 Iteration: 373; Percent complete: 9.3%; Average loss: 3.7196 Iteration: 374; Percent complete: 9.3%; Average loss: 3.7833 Iteration: 375; Percent complete: 9.4%; Average loss: 3.9449 Iteration: 376; Percent complete: 9.4%; Average loss: 3.9530 Iteration: 377; Percent complete: 9.4%; Average loss: 3.4805 Iteration: 378; Percent complete: 9.4%; Average loss: 3.6414 Iteration: 379; Percent complete: 9.5%; Average loss: 3.6594 Iteration: 380; Percent complete: 9.5%; Average loss: 3.7621 Iteration: 381; Percent complete: 9.5%; Average loss: 3.9670 Iteration: 382; Percent complete: 9.6%; Average loss: 3.5196 Iteration: 383; Percent complete: 9.6%; Average loss: 3.5022 Iteration: 384; Percent complete: 9.6%; Average loss: 3.9580 Iteration: 385; Percent complete: 9.6%; Average loss: 3.7315 Iteration: 386; Percent complete: 9.7%; Average loss: 4.0758 Iteration: 387; Percent complete: 9.7%; Average loss: 3.8051 Iteration: 388; Percent complete: 9.7%; Average loss: 3.8562 Iteration: 389; Percent complete: 9.7%; Average loss: 3.7532 Iteration: 390; Percent complete: 9.8%; Average loss: 4.1445 Iteration: 391; Percent complete: 9.8%; Average loss: 3.9031 Iteration: 392; Percent complete: 9.8%; Average loss: 3.6054 Iteration: 393; Percent complete: 9.8%; Average loss: 4.0023 Iteration: 394; Percent complete: 9.8%; Average loss: 3.8720 Iteration: 395; Percent complete: 9.9%; Average loss: 3.8136 Iteration: 396; Percent complete: 9.9%; Average loss: 3.7782 Iteration: 397; Percent complete: 9.9%; Average loss: 3.8818 Iteration: 398; Percent complete: 10.0%; Average loss: 3.6830 Iteration: 399; Percent complete: 10.0%; Average loss: 3.6042 Iteration: 400; Percent complete: 10.0%; Average loss: 3.6033 Iteration: 401; Percent complete: 10.0%; Average loss: 4.0543 Iteration: 402; Percent complete: 10.1%; Average loss: 3.8603 Iteration: 403; Percent complete: 10.1%; Average loss: 3.7808 Iteration: 404; Percent complete: 10.1%; Average loss: 4.1854 Iteration: 405; Percent complete: 10.1%; Average loss: 3.7163 Iteration: 406; Percent complete: 10.2%; Average loss: 3.7211 Iteration: 407; Percent complete: 10.2%; Average loss: 4.0517 Iteration: 408; Percent complete: 10.2%; Average loss: 3.9546 Iteration: 409; Percent complete: 10.2%; Average loss: 3.7168 Iteration: 410; Percent complete: 10.2%; Average loss: 3.8187 Iteration: 411; Percent complete: 10.3%; Average loss: 3.7692 Iteration: 412; Percent complete: 10.3%; Average loss: 3.7195 Iteration: 413; Percent complete: 10.3%; Average loss: 3.8198 Iteration: 414; Percent complete: 10.3%; Average loss: 3.9124 Iteration: 415; Percent complete: 10.4%; Average loss: 4.0625 Iteration: 416; Percent complete: 10.4%; Average loss: 3.7785 Iteration: 417; Percent complete: 10.4%; Average loss: 3.5422 Iteration: 418; Percent complete: 10.4%; Average loss: 3.5326 Iteration: 419; Percent complete: 10.5%; Average loss: 3.7042 Iteration: 420; Percent complete: 10.5%; Average loss: 4.1719 Iteration: 421; Percent complete: 10.5%; Average loss: 3.8295 Iteration: 422; Percent complete: 10.5%; Average loss: 3.8512 Iteration: 423; Percent complete: 10.6%; Average loss: 3.9442 Iteration: 424; Percent complete: 10.6%; Average loss: 3.8014 Iteration: 425; Percent complete: 10.6%; Average loss: 3.6830 Iteration: 426; Percent complete: 10.7%; Average loss: 3.8851 Iteration: 427; Percent complete: 10.7%; Average loss: 3.4497 Iteration: 428; Percent complete: 10.7%; Average loss: 3.8967 Iteration: 429; Percent complete: 10.7%; Average loss: 3.8292 Iteration: 430; Percent complete: 10.8%; Average loss: 4.0449 Iteration: 431; Percent complete: 10.8%; Average loss: 3.7376 Iteration: 432; Percent complete: 10.8%; Average loss: 3.8036 Iteration: 433; Percent complete: 10.8%; Average loss: 4.0134 Iteration: 434; Percent complete: 10.8%; Average loss: 3.7928 Iteration: 435; Percent complete: 10.9%; Average loss: 3.7534 Iteration: 436; Percent complete: 10.9%; Average loss: 3.6854 Iteration: 437; Percent complete: 10.9%; Average loss: 3.6860 Iteration: 438; Percent complete: 10.9%; Average loss: 3.6491 Iteration: 439; Percent complete: 11.0%; Average loss: 3.6968 Iteration: 440; Percent complete: 11.0%; Average loss: 3.7408 Iteration: 441; Percent complete: 11.0%; Average loss: 4.0733 Iteration: 442; Percent complete: 11.1%; Average loss: 4.0408 Iteration: 443; Percent complete: 11.1%; Average loss: 3.7522 Iteration: 444; Percent complete: 11.1%; Average loss: 3.7933 Iteration: 445; Percent complete: 11.1%; Average loss: 3.8746 Iteration: 446; Percent complete: 11.2%; Average loss: 3.7394 Iteration: 447; Percent complete: 11.2%; Average loss: 3.6027 Iteration: 448; Percent complete: 11.2%; Average loss: 3.6644 Iteration: 449; Percent complete: 11.2%; Average loss: 3.7040 Iteration: 450; Percent complete: 11.2%; Average loss: 3.6673 Iteration: 451; Percent complete: 11.3%; Average loss: 3.8281 Iteration: 452; Percent complete: 11.3%; Average loss: 3.7541 Iteration: 453; Percent complete: 11.3%; Average loss: 3.6738 Iteration: 454; Percent complete: 11.3%; Average loss: 3.6599 Iteration: 455; Percent complete: 11.4%; Average loss: 3.7282 Iteration: 456; Percent complete: 11.4%; Average loss: 3.5735 Iteration: 457; Percent complete: 11.4%; Average loss: 3.7592 Iteration: 458; Percent complete: 11.5%; Average loss: 3.7065 Iteration: 459; Percent complete: 11.5%; Average loss: 3.8106 Iteration: 460; Percent complete: 11.5%; Average loss: 3.5145 Iteration: 461; Percent complete: 11.5%; Average loss: 3.5765 Iteration: 462; Percent complete: 11.6%; Average loss: 3.8841 Iteration: 463; Percent complete: 11.6%; Average loss: 3.8804 Iteration: 464; Percent complete: 11.6%; Average loss: 3.7238 Iteration: 465; Percent complete: 11.6%; Average loss: 4.0917 Iteration: 466; Percent complete: 11.7%; Average loss: 3.6467 Iteration: 467; Percent complete: 11.7%; Average loss: 3.7112 Iteration: 468; Percent complete: 11.7%; Average loss: 3.9512 Iteration: 469; Percent complete: 11.7%; Average loss: 3.6835 Iteration: 470; Percent complete: 11.8%; Average loss: 3.4151 Iteration: 471; Percent complete: 11.8%; Average loss: 3.7216 Iteration: 472; Percent complete: 11.8%; Average loss: 3.7019 Iteration: 473; Percent complete: 11.8%; Average loss: 3.6703 Iteration: 474; Percent complete: 11.8%; Average loss: 3.9414 Iteration: 475; Percent complete: 11.9%; Average loss: 3.7452 Iteration: 476; Percent complete: 11.9%; Average loss: 3.9532 Iteration: 477; Percent complete: 11.9%; Average loss: 3.8105 Iteration: 478; Percent complete: 11.9%; Average loss: 3.8009 Iteration: 479; Percent complete: 12.0%; Average loss: 3.7122 Iteration: 480; Percent complete: 12.0%; Average loss: 3.7430 Iteration: 481; Percent complete: 12.0%; Average loss: 3.8451 Iteration: 482; Percent complete: 12.0%; Average loss: 3.7796 Iteration: 483; Percent complete: 12.1%; Average loss: 3.6644 Iteration: 484; Percent complete: 12.1%; Average loss: 3.7532 Iteration: 485; Percent complete: 12.1%; Average loss: 3.8663 Iteration: 486; Percent complete: 12.2%; Average loss: 3.5834 Iteration: 487; Percent complete: 12.2%; Average loss: 3.9960 Iteration: 488; Percent complete: 12.2%; Average loss: 3.6028 Iteration: 489; Percent complete: 12.2%; Average loss: 3.9242 Iteration: 490; Percent complete: 12.2%; Average loss: 3.5886 Iteration: 491; Percent complete: 12.3%; Average loss: 3.7678 Iteration: 492; Percent complete: 12.3%; Average loss: 3.7378 Iteration: 493; Percent complete: 12.3%; Average loss: 3.6492 Iteration: 494; Percent complete: 12.3%; Average loss: 3.7200 Iteration: 495; Percent complete: 12.4%; Average loss: 3.7285 Iteration: 496; Percent complete: 12.4%; Average loss: 3.6411 Iteration: 497; Percent complete: 12.4%; Average loss: 3.7766 Iteration: 498; Percent complete: 12.4%; Average loss: 3.5712 Iteration: 499; Percent complete: 12.5%; Average loss: 3.6934 Iteration: 500; Percent complete: 12.5%; Average loss: 3.4050 Iteration: 501; Percent complete: 12.5%; Average loss: 3.7637 Iteration: 502; Percent complete: 12.6%; Average loss: 3.6943 Iteration: 503; Percent complete: 12.6%; Average loss: 4.0403 Iteration: 504; Percent complete: 12.6%; Average loss: 3.8772 Iteration: 505; Percent complete: 12.6%; Average loss: 3.6806 Iteration: 506; Percent complete: 12.7%; Average loss: 3.9077 Iteration: 507; Percent complete: 12.7%; Average loss: 3.8560 Iteration: 508; Percent complete: 12.7%; Average loss: 3.7142 Iteration: 509; Percent complete: 12.7%; Average loss: 3.7543 Iteration: 510; Percent complete: 12.8%; Average loss: 3.8389 Iteration: 511; Percent complete: 12.8%; Average loss: 3.7334 Iteration: 512; Percent complete: 12.8%; Average loss: 3.7529 Iteration: 513; Percent complete: 12.8%; Average loss: 3.7681 Iteration: 514; Percent complete: 12.8%; Average loss: 3.9264 Iteration: 515; Percent complete: 12.9%; Average loss: 3.9280 Iteration: 516; Percent complete: 12.9%; Average loss: 3.5613 Iteration: 517; Percent complete: 12.9%; Average loss: 3.7961 Iteration: 518; Percent complete: 13.0%; Average loss: 4.2622 Iteration: 519; Percent complete: 13.0%; Average loss: 3.7063 Iteration: 520; Percent complete: 13.0%; Average loss: 3.7242 Iteration: 521; Percent complete: 13.0%; Average loss: 3.6466 Iteration: 522; Percent complete: 13.1%; Average loss: 3.6605 Iteration: 523; Percent complete: 13.1%; Average loss: 3.6951 Iteration: 524; Percent complete: 13.1%; Average loss: 3.8780 Iteration: 525; Percent complete: 13.1%; Average loss: 3.6828 Iteration: 526; Percent complete: 13.2%; Average loss: 3.6776 Iteration: 527; Percent complete: 13.2%; Average loss: 3.6464 Iteration: 528; Percent complete: 13.2%; Average loss: 3.8845 Iteration: 529; Percent complete: 13.2%; Average loss: 3.5866 Iteration: 530; Percent complete: 13.2%; Average loss: 3.6426 Iteration: 531; Percent complete: 13.3%; Average loss: 3.5789 Iteration: 532; Percent complete: 13.3%; Average loss: 3.9215 Iteration: 533; Percent complete: 13.3%; Average loss: 3.7895 Iteration: 534; Percent complete: 13.4%; Average loss: 3.7361 Iteration: 535; Percent complete: 13.4%; Average loss: 3.5429 Iteration: 536; Percent complete: 13.4%; Average loss: 3.6919 Iteration: 537; Percent complete: 13.4%; Average loss: 3.6720 Iteration: 538; Percent complete: 13.5%; Average loss: 3.4389 Iteration: 539; Percent complete: 13.5%; Average loss: 3.6855 Iteration: 540; Percent complete: 13.5%; Average loss: 3.5914 Iteration: 541; Percent complete: 13.5%; Average loss: 3.6102 Iteration: 542; Percent complete: 13.6%; Average loss: 3.5880 Iteration: 543; Percent complete: 13.6%; Average loss: 3.6183 Iteration: 544; Percent complete: 13.6%; Average loss: 3.6859 Iteration: 545; Percent complete: 13.6%; Average loss: 3.7343 Iteration: 546; Percent complete: 13.7%; Average loss: 3.8276 Iteration: 547; Percent complete: 13.7%; Average loss: 3.8533 Iteration: 548; Percent complete: 13.7%; Average loss: 3.6370 Iteration: 549; Percent complete: 13.7%; Average loss: 3.7400 Iteration: 550; Percent complete: 13.8%; Average loss: 3.4673 Iteration: 551; Percent complete: 13.8%; Average loss: 3.6513 Iteration: 552; Percent complete: 13.8%; Average loss: 3.8294 Iteration: 553; Percent complete: 13.8%; Average loss: 3.8554 Iteration: 554; Percent complete: 13.9%; Average loss: 3.7860 Iteration: 555; Percent complete: 13.9%; Average loss: 3.7085 Iteration: 556; Percent complete: 13.9%; Average loss: 3.6445 Iteration: 557; Percent complete: 13.9%; Average loss: 3.5229 Iteration: 558; Percent complete: 14.0%; Average loss: 3.9394 Iteration: 559; Percent complete: 14.0%; Average loss: 3.8552 Iteration: 560; Percent complete: 14.0%; Average loss: 3.8065 Iteration: 561; Percent complete: 14.0%; Average loss: 3.7205 Iteration: 562; Percent complete: 14.1%; Average loss: 3.7204 Iteration: 563; Percent complete: 14.1%; Average loss: 3.6498 Iteration: 564; Percent complete: 14.1%; Average loss: 3.6531 Iteration: 565; Percent complete: 14.1%; Average loss: 3.6938 Iteration: 566; Percent complete: 14.1%; Average loss: 3.4877 Iteration: 567; Percent complete: 14.2%; Average loss: 3.7694 Iteration: 568; Percent complete: 14.2%; Average loss: 3.6035 Iteration: 569; Percent complete: 14.2%; Average loss: 3.6045 Iteration: 570; Percent complete: 14.2%; Average loss: 3.7176 Iteration: 571; Percent complete: 14.3%; Average loss: 3.6273 Iteration: 572; Percent complete: 14.3%; Average loss: 3.6440 Iteration: 573; Percent complete: 14.3%; Average loss: 3.5800 Iteration: 574; Percent complete: 14.3%; Average loss: 3.7222 Iteration: 575; Percent complete: 14.4%; Average loss: 3.4439 Iteration: 576; Percent complete: 14.4%; Average loss: 3.4557 Iteration: 577; Percent complete: 14.4%; Average loss: 3.8653 Iteration: 578; Percent complete: 14.4%; Average loss: 3.4434 Iteration: 579; Percent complete: 14.5%; Average loss: 4.0301 Iteration: 580; Percent complete: 14.5%; Average loss: 3.7929 Iteration: 581; Percent complete: 14.5%; Average loss: 3.6134 Iteration: 582; Percent complete: 14.5%; Average loss: 3.7934 Iteration: 583; Percent complete: 14.6%; Average loss: 3.5687 Iteration: 584; Percent complete: 14.6%; Average loss: 3.3886 Iteration: 585; Percent complete: 14.6%; Average loss: 3.8155 Iteration: 586; Percent complete: 14.6%; Average loss: 3.8307 Iteration: 587; Percent complete: 14.7%; Average loss: 3.5291 Iteration: 588; Percent complete: 14.7%; Average loss: 3.3265 Iteration: 589; Percent complete: 14.7%; Average loss: 3.4991 Iteration: 590; Percent complete: 14.8%; Average loss: 3.7146 Iteration: 591; Percent complete: 14.8%; Average loss: 3.8169 Iteration: 592; Percent complete: 14.8%; Average loss: 3.4924 Iteration: 593; Percent complete: 14.8%; Average loss: 3.4134 Iteration: 594; Percent complete: 14.8%; Average loss: 3.5661 Iteration: 595; Percent complete: 14.9%; Average loss: 3.6817 Iteration: 596; Percent complete: 14.9%; Average loss: 3.6530 Iteration: 597; Percent complete: 14.9%; Average loss: 3.3046 Iteration: 598; Percent complete: 14.9%; Average loss: 3.8228 Iteration: 599; Percent complete: 15.0%; Average loss: 3.9314 Iteration: 600; Percent complete: 15.0%; Average loss: 3.7030 Iteration: 601; Percent complete: 15.0%; Average loss: 3.5892 Iteration: 602; Percent complete: 15.0%; Average loss: 3.5931 Iteration: 603; Percent complete: 15.1%; Average loss: 3.7933 Iteration: 604; Percent complete: 15.1%; Average loss: 3.5734 Iteration: 605; Percent complete: 15.1%; Average loss: 3.5999 Iteration: 606; Percent complete: 15.2%; Average loss: 3.4313 Iteration: 607; Percent complete: 15.2%; Average loss: 3.6277 Iteration: 608; Percent complete: 15.2%; Average loss: 3.7649 Iteration: 609; Percent complete: 15.2%; Average loss: 3.6757 Iteration: 610; Percent complete: 15.2%; Average loss: 3.9065 Iteration: 611; Percent complete: 15.3%; Average loss: 3.5413 Iteration: 612; Percent complete: 15.3%; Average loss: 3.5871 Iteration: 613; Percent complete: 15.3%; Average loss: 3.8822 Iteration: 614; Percent complete: 15.3%; Average loss: 3.6300 Iteration: 615; Percent complete: 15.4%; Average loss: 3.8569 Iteration: 616; Percent complete: 15.4%; Average loss: 3.7766 Iteration: 617; Percent complete: 15.4%; Average loss: 3.7841 Iteration: 618; Percent complete: 15.4%; Average loss: 3.8145 Iteration: 619; Percent complete: 15.5%; Average loss: 3.8911 Iteration: 620; Percent complete: 15.5%; Average loss: 3.4458 Iteration: 621; Percent complete: 15.5%; Average loss: 3.3701 Iteration: 622; Percent complete: 15.6%; Average loss: 3.7175 Iteration: 623; Percent complete: 15.6%; Average loss: 3.5337 Iteration: 624; Percent complete: 15.6%; Average loss: 4.0521 Iteration: 625; Percent complete: 15.6%; Average loss: 3.8515 Iteration: 626; Percent complete: 15.7%; Average loss: 3.7054 Iteration: 627; Percent complete: 15.7%; Average loss: 3.5507 Iteration: 628; Percent complete: 15.7%; Average loss: 3.6639 Iteration: 629; Percent complete: 15.7%; Average loss: 3.5412 Iteration: 630; Percent complete: 15.8%; Average loss: 3.6610 Iteration: 631; Percent complete: 15.8%; Average loss: 3.6869 Iteration: 632; Percent complete: 15.8%; Average loss: 3.9102 Iteration: 633; Percent complete: 15.8%; Average loss: 3.4345 Iteration: 634; Percent complete: 15.8%; Average loss: 3.7326 Iteration: 635; Percent complete: 15.9%; Average loss: 3.8203 Iteration: 636; Percent complete: 15.9%; Average loss: 3.3605 Iteration: 637; Percent complete: 15.9%; Average loss: 3.8944 Iteration: 638; Percent complete: 16.0%; Average loss: 3.5699 Iteration: 639; Percent complete: 16.0%; Average loss: 3.5902 Iteration: 640; Percent complete: 16.0%; Average loss: 3.8047 Iteration: 641; Percent complete: 16.0%; Average loss: 3.4144 Iteration: 642; Percent complete: 16.1%; Average loss: 3.6899 Iteration: 643; Percent complete: 16.1%; Average loss: 3.8405 Iteration: 644; Percent complete: 16.1%; Average loss: 3.5504 Iteration: 645; Percent complete: 16.1%; Average loss: 3.4787 Iteration: 646; Percent complete: 16.2%; Average loss: 3.5888 Iteration: 647; Percent complete: 16.2%; Average loss: 3.5492 Iteration: 648; Percent complete: 16.2%; Average loss: 3.5877 Iteration: 649; Percent complete: 16.2%; Average loss: 3.8612 Iteration: 650; Percent complete: 16.2%; Average loss: 3.4469 Iteration: 651; Percent complete: 16.3%; Average loss: 3.5281 Iteration: 652; Percent complete: 16.3%; Average loss: 3.4904 Iteration: 653; Percent complete: 16.3%; Average loss: 3.5642 Iteration: 654; Percent complete: 16.4%; Average loss: 3.3360 Iteration: 655; Percent complete: 16.4%; Average loss: 4.0082 Iteration: 656; Percent complete: 16.4%; Average loss: 3.6184 Iteration: 657; Percent complete: 16.4%; Average loss: 3.6745 Iteration: 658; Percent complete: 16.4%; Average loss: 3.6890 Iteration: 659; Percent complete: 16.5%; Average loss: 3.6936 Iteration: 660; Percent complete: 16.5%; Average loss: 3.4461 Iteration: 661; Percent complete: 16.5%; Average loss: 3.4395 Iteration: 662; Percent complete: 16.6%; Average loss: 3.3765 Iteration: 663; Percent complete: 16.6%; Average loss: 3.6121 Iteration: 664; Percent complete: 16.6%; Average loss: 3.5725 Iteration: 665; Percent complete: 16.6%; Average loss: 3.5572 Iteration: 666; Percent complete: 16.7%; Average loss: 3.8594 Iteration: 667; Percent complete: 16.7%; Average loss: 3.4797 Iteration: 668; Percent complete: 16.7%; Average loss: 3.6888 Iteration: 669; Percent complete: 16.7%; Average loss: 3.6844 Iteration: 670; Percent complete: 16.8%; Average loss: 3.4451 Iteration: 671; Percent complete: 16.8%; Average loss: 3.7818 Iteration: 672; Percent complete: 16.8%; Average loss: 3.4658 Iteration: 673; Percent complete: 16.8%; Average loss: 3.2167 Iteration: 674; Percent complete: 16.9%; Average loss: 3.3791 Iteration: 675; Percent complete: 16.9%; Average loss: 3.5105 Iteration: 676; Percent complete: 16.9%; Average loss: 3.5353 Iteration: 677; Percent complete: 16.9%; Average loss: 3.4139 Iteration: 678; Percent complete: 17.0%; Average loss: 3.7559 Iteration: 679; Percent complete: 17.0%; Average loss: 3.9090 Iteration: 680; Percent complete: 17.0%; Average loss: 3.7739 Iteration: 681; Percent complete: 17.0%; Average loss: 3.5693 Iteration: 682; Percent complete: 17.1%; Average loss: 3.8804 Iteration: 683; Percent complete: 17.1%; Average loss: 3.6567 Iteration: 684; Percent complete: 17.1%; Average loss: 3.5583 Iteration: 685; Percent complete: 17.1%; Average loss: 3.5636 Iteration: 686; Percent complete: 17.2%; Average loss: 3.3426 Iteration: 687; Percent complete: 17.2%; Average loss: 3.6325 Iteration: 688; Percent complete: 17.2%; Average loss: 3.5755 Iteration: 689; Percent complete: 17.2%; Average loss: 3.9421 Iteration: 690; Percent complete: 17.2%; Average loss: 3.7110 Iteration: 691; Percent complete: 17.3%; Average loss: 3.6895 Iteration: 692; Percent complete: 17.3%; Average loss: 3.6401 Iteration: 693; Percent complete: 17.3%; Average loss: 3.9377 Iteration: 694; Percent complete: 17.3%; Average loss: 3.5453 Iteration: 695; Percent complete: 17.4%; Average loss: 3.7622 Iteration: 696; Percent complete: 17.4%; Average loss: 3.5246 Iteration: 697; Percent complete: 17.4%; Average loss: 3.6749 Iteration: 698; Percent complete: 17.4%; Average loss: 3.5956 Iteration: 699; Percent complete: 17.5%; Average loss: 3.6051 Iteration: 700; Percent complete: 17.5%; Average loss: 3.9431 Iteration: 701; Percent complete: 17.5%; Average loss: 3.5296 Iteration: 702; Percent complete: 17.5%; Average loss: 3.8589 Iteration: 703; Percent complete: 17.6%; Average loss: 3.3820 Iteration: 704; Percent complete: 17.6%; Average loss: 3.4485 Iteration: 705; Percent complete: 17.6%; Average loss: 3.3530 Iteration: 706; Percent complete: 17.6%; Average loss: 3.6531 Iteration: 707; Percent complete: 17.7%; Average loss: 3.4037 Iteration: 708; Percent complete: 17.7%; Average loss: 3.5056 Iteration: 709; Percent complete: 17.7%; Average loss: 3.7576 Iteration: 710; Percent complete: 17.8%; Average loss: 3.3733 Iteration: 711; Percent complete: 17.8%; Average loss: 3.5497 Iteration: 712; Percent complete: 17.8%; Average loss: 3.4299 Iteration: 713; Percent complete: 17.8%; Average loss: 3.6077 Iteration: 714; Percent complete: 17.8%; Average loss: 3.6160 Iteration: 715; Percent complete: 17.9%; Average loss: 3.6875 Iteration: 716; Percent complete: 17.9%; Average loss: 3.8424 Iteration: 717; Percent complete: 17.9%; Average loss: 3.5597 Iteration: 718; Percent complete: 17.9%; Average loss: 3.3719 Iteration: 719; Percent complete: 18.0%; Average loss: 3.7144 Iteration: 720; Percent complete: 18.0%; Average loss: 3.7321 Iteration: 721; Percent complete: 18.0%; Average loss: 3.7388 Iteration: 722; Percent complete: 18.1%; Average loss: 3.4843 Iteration: 723; Percent complete: 18.1%; Average loss: 3.4233 Iteration: 724; Percent complete: 18.1%; Average loss: 3.5898 Iteration: 725; Percent complete: 18.1%; Average loss: 3.8557 Iteration: 726; Percent complete: 18.1%; Average loss: 3.8385 Iteration: 727; Percent complete: 18.2%; Average loss: 3.7131 Iteration: 728; Percent complete: 18.2%; Average loss: 3.7178 Iteration: 729; Percent complete: 18.2%; Average loss: 4.1074 Iteration: 730; Percent complete: 18.2%; Average loss: 3.5278 Iteration: 731; Percent complete: 18.3%; Average loss: 3.7509 Iteration: 732; Percent complete: 18.3%; Average loss: 3.6628 Iteration: 733; Percent complete: 18.3%; Average loss: 3.5518 Iteration: 734; Percent complete: 18.4%; Average loss: 3.6094 Iteration: 735; Percent complete: 18.4%; Average loss: 3.5243 Iteration: 736; Percent complete: 18.4%; Average loss: 3.7603 Iteration: 737; Percent complete: 18.4%; Average loss: 3.3238 Iteration: 738; Percent complete: 18.4%; Average loss: 3.5140 Iteration: 739; Percent complete: 18.5%; Average loss: 3.5051 Iteration: 740; Percent complete: 18.5%; Average loss: 3.6614 Iteration: 741; Percent complete: 18.5%; Average loss: 3.7263 Iteration: 742; Percent complete: 18.6%; Average loss: 3.7878 Iteration: 743; Percent complete: 18.6%; Average loss: 3.7736 Iteration: 744; Percent complete: 18.6%; Average loss: 3.3851 Iteration: 745; Percent complete: 18.6%; Average loss: 3.6624 Iteration: 746; Percent complete: 18.6%; Average loss: 3.5097 Iteration: 747; Percent complete: 18.7%; Average loss: 3.8095 Iteration: 748; Percent complete: 18.7%; Average loss: 3.3399 Iteration: 749; Percent complete: 18.7%; Average loss: 3.7161 Iteration: 750; Percent complete: 18.8%; Average loss: 3.6133 Iteration: 751; Percent complete: 18.8%; Average loss: 3.7416 Iteration: 752; Percent complete: 18.8%; Average loss: 3.7949 Iteration: 753; Percent complete: 18.8%; Average loss: 3.5560 Iteration: 754; Percent complete: 18.9%; Average loss: 3.7997 Iteration: 755; Percent complete: 18.9%; Average loss: 3.4895 Iteration: 756; Percent complete: 18.9%; Average loss: 3.7303 Iteration: 757; Percent complete: 18.9%; Average loss: 3.5593 Iteration: 758; Percent complete: 18.9%; Average loss: 3.5854 Iteration: 759; Percent complete: 19.0%; Average loss: 3.3611 Iteration: 760; Percent complete: 19.0%; Average loss: 3.5116 Iteration: 761; Percent complete: 19.0%; Average loss: 3.6401 Iteration: 762; Percent complete: 19.1%; Average loss: 3.6278 Iteration: 763; Percent complete: 19.1%; Average loss: 3.6157 Iteration: 764; Percent complete: 19.1%; Average loss: 3.4133 Iteration: 765; Percent complete: 19.1%; Average loss: 3.6686 Iteration: 766; Percent complete: 19.1%; Average loss: 3.5565 Iteration: 767; Percent complete: 19.2%; Average loss: 3.5312 Iteration: 768; Percent complete: 19.2%; Average loss: 3.6221 Iteration: 769; Percent complete: 19.2%; Average loss: 3.6621 Iteration: 770; Percent complete: 19.2%; Average loss: 3.6298 Iteration: 771; Percent complete: 19.3%; Average loss: 3.4978 Iteration: 772; Percent complete: 19.3%; Average loss: 3.5311 Iteration: 773; Percent complete: 19.3%; Average loss: 3.4143 Iteration: 774; Percent complete: 19.4%; Average loss: 3.1761 Iteration: 775; Percent complete: 19.4%; Average loss: 3.5972 Iteration: 776; Percent complete: 19.4%; Average loss: 3.5982 Iteration: 777; Percent complete: 19.4%; Average loss: 3.5905 Iteration: 778; Percent complete: 19.4%; Average loss: 3.4382 Iteration: 779; Percent complete: 19.5%; Average loss: 3.4797 Iteration: 780; Percent complete: 19.5%; Average loss: 3.4883 Iteration: 781; Percent complete: 19.5%; Average loss: 3.6410 Iteration: 782; Percent complete: 19.6%; Average loss: 3.7022 Iteration: 783; Percent complete: 19.6%; Average loss: 3.6563 Iteration: 784; Percent complete: 19.6%; Average loss: 3.5219 Iteration: 785; Percent complete: 19.6%; Average loss: 3.3917 Iteration: 786; Percent complete: 19.7%; Average loss: 3.5456 Iteration: 787; Percent complete: 19.7%; Average loss: 3.8979 Iteration: 788; Percent complete: 19.7%; Average loss: 3.5261 Iteration: 789; Percent complete: 19.7%; Average loss: 3.6247 Iteration: 790; Percent complete: 19.8%; Average loss: 3.6166 Iteration: 791; Percent complete: 19.8%; Average loss: 3.3911 Iteration: 792; Percent complete: 19.8%; Average loss: 3.5538 Iteration: 793; Percent complete: 19.8%; Average loss: 3.6301 Iteration: 794; Percent complete: 19.9%; Average loss: 3.5063 Iteration: 795; Percent complete: 19.9%; Average loss: 3.6609 Iteration: 796; Percent complete: 19.9%; Average loss: 3.5223 Iteration: 797; Percent complete: 19.9%; Average loss: 3.7565 Iteration: 798; Percent complete: 20.0%; Average loss: 3.5414 Iteration: 799; Percent complete: 20.0%; Average loss: 3.4539 Iteration: 800; Percent complete: 20.0%; Average loss: 3.5645 Iteration: 801; Percent complete: 20.0%; Average loss: 3.4538 Iteration: 802; Percent complete: 20.1%; Average loss: 3.6022 Iteration: 803; Percent complete: 20.1%; Average loss: 3.5889 Iteration: 804; Percent complete: 20.1%; Average loss: 3.4370 Iteration: 805; Percent complete: 20.1%; Average loss: 3.4511 Iteration: 806; Percent complete: 20.2%; Average loss: 3.5771 Iteration: 807; Percent complete: 20.2%; Average loss: 3.5379 Iteration: 808; Percent complete: 20.2%; Average loss: 3.3270 Iteration: 809; Percent complete: 20.2%; Average loss: 3.1621 Iteration: 810; Percent complete: 20.2%; Average loss: 3.6326 Iteration: 811; Percent complete: 20.3%; Average loss: 3.7955 Iteration: 812; Percent complete: 20.3%; Average loss: 3.5261 Iteration: 813; Percent complete: 20.3%; Average loss: 3.5820 Iteration: 814; Percent complete: 20.3%; Average loss: 3.3628 Iteration: 815; Percent complete: 20.4%; Average loss: 3.6605 Iteration: 816; Percent complete: 20.4%; Average loss: 3.5128 Iteration: 817; Percent complete: 20.4%; Average loss: 3.7156 Iteration: 818; Percent complete: 20.4%; Average loss: 3.9293 Iteration: 819; Percent complete: 20.5%; Average loss: 3.4814 Iteration: 820; Percent complete: 20.5%; Average loss: 3.4905 Iteration: 821; Percent complete: 20.5%; Average loss: 3.3458 Iteration: 822; Percent complete: 20.5%; Average loss: 3.4010 Iteration: 823; Percent complete: 20.6%; Average loss: 3.7499 Iteration: 824; Percent complete: 20.6%; Average loss: 3.4270 Iteration: 825; Percent complete: 20.6%; Average loss: 3.5985 Iteration: 826; Percent complete: 20.6%; Average loss: 3.3891 Iteration: 827; Percent complete: 20.7%; Average loss: 3.5869 Iteration: 828; Percent complete: 20.7%; Average loss: 3.4493 Iteration: 829; Percent complete: 20.7%; Average loss: 3.8053 Iteration: 830; Percent complete: 20.8%; Average loss: 3.5465 Iteration: 831; Percent complete: 20.8%; Average loss: 3.3203 Iteration: 832; Percent complete: 20.8%; Average loss: 3.7204 Iteration: 833; Percent complete: 20.8%; Average loss: 3.2725 Iteration: 834; Percent complete: 20.8%; Average loss: 3.5377 Iteration: 835; Percent complete: 20.9%; Average loss: 3.5848 Iteration: 836; Percent complete: 20.9%; Average loss: 3.5857 Iteration: 837; Percent complete: 20.9%; Average loss: 3.5773 Iteration: 838; Percent complete: 20.9%; Average loss: 3.3663 Iteration: 839; Percent complete: 21.0%; Average loss: 3.6197 Iteration: 840; Percent complete: 21.0%; Average loss: 3.6037 Iteration: 841; Percent complete: 21.0%; Average loss: 3.5231 Iteration: 842; Percent complete: 21.1%; Average loss: 3.5001 Iteration: 843; Percent complete: 21.1%; Average loss: 3.5841 Iteration: 844; Percent complete: 21.1%; Average loss: 3.6276 Iteration: 845; Percent complete: 21.1%; Average loss: 3.7463 Iteration: 846; Percent complete: 21.1%; Average loss: 3.4006 Iteration: 847; Percent complete: 21.2%; Average loss: 3.3359 Iteration: 848; Percent complete: 21.2%; Average loss: 3.3949 Iteration: 849; Percent complete: 21.2%; Average loss: 3.5315 Iteration: 850; Percent complete: 21.2%; Average loss: 3.6480 Iteration: 851; Percent complete: 21.3%; Average loss: 3.5539 Iteration: 852; Percent complete: 21.3%; Average loss: 3.6276 Iteration: 853; Percent complete: 21.3%; Average loss: 3.4616 Iteration: 854; Percent complete: 21.3%; Average loss: 3.3897 Iteration: 855; Percent complete: 21.4%; Average loss: 3.5216 Iteration: 856; Percent complete: 21.4%; Average loss: 3.3250 Iteration: 857; Percent complete: 21.4%; Average loss: 3.5170 Iteration: 858; Percent complete: 21.4%; Average loss: 3.6663 Iteration: 859; Percent complete: 21.5%; Average loss: 3.6312 Iteration: 860; Percent complete: 21.5%; Average loss: 3.4810 Iteration: 861; Percent complete: 21.5%; Average loss: 3.4135 Iteration: 862; Percent complete: 21.6%; Average loss: 3.5074 Iteration: 863; Percent complete: 21.6%; Average loss: 3.4013 Iteration: 864; Percent complete: 21.6%; Average loss: 3.4131 Iteration: 865; Percent complete: 21.6%; Average loss: 3.4332 Iteration: 866; Percent complete: 21.6%; Average loss: 3.6537 Iteration: 867; Percent complete: 21.7%; Average loss: 3.5194 Iteration: 868; Percent complete: 21.7%; Average loss: 3.5412 Iteration: 869; Percent complete: 21.7%; Average loss: 3.8455 Iteration: 870; Percent complete: 21.8%; Average loss: 3.5537 Iteration: 871; Percent complete: 21.8%; Average loss: 3.4311 Iteration: 872; Percent complete: 21.8%; Average loss: 3.5531 Iteration: 873; Percent complete: 21.8%; Average loss: 3.6795 Iteration: 874; Percent complete: 21.9%; Average loss: 3.9202 Iteration: 875; Percent complete: 21.9%; Average loss: 3.4337 Iteration: 876; Percent complete: 21.9%; Average loss: 3.6008 Iteration: 877; Percent complete: 21.9%; Average loss: 3.6333 Iteration: 878; Percent complete: 21.9%; Average loss: 3.5520 Iteration: 879; Percent complete: 22.0%; Average loss: 3.7014 Iteration: 880; Percent complete: 22.0%; Average loss: 3.4167 Iteration: 881; Percent complete: 22.0%; Average loss: 3.4480 Iteration: 882; Percent complete: 22.1%; Average loss: 3.3562 Iteration: 883; Percent complete: 22.1%; Average loss: 3.7513 Iteration: 884; Percent complete: 22.1%; Average loss: 3.6109 Iteration: 885; Percent complete: 22.1%; Average loss: 3.4533 Iteration: 886; Percent complete: 22.1%; Average loss: 3.5784 Iteration: 887; Percent complete: 22.2%; Average loss: 3.8337 Iteration: 888; Percent complete: 22.2%; Average loss: 3.6128 Iteration: 889; Percent complete: 22.2%; Average loss: 3.5425 Iteration: 890; Percent complete: 22.2%; Average loss: 3.5607 Iteration: 891; Percent complete: 22.3%; Average loss: 3.3575 Iteration: 892; Percent complete: 22.3%; Average loss: 3.4576 Iteration: 893; Percent complete: 22.3%; Average loss: 3.3373 Iteration: 894; Percent complete: 22.4%; Average loss: 3.5568 Iteration: 895; Percent complete: 22.4%; Average loss: 3.6575 Iteration: 896; Percent complete: 22.4%; Average loss: 3.7700 Iteration: 897; Percent complete: 22.4%; Average loss: 3.3262 Iteration: 898; Percent complete: 22.4%; Average loss: 3.6849 Iteration: 899; Percent complete: 22.5%; Average loss: 3.3986 Iteration: 900; Percent complete: 22.5%; Average loss: 3.7253 Iteration: 901; Percent complete: 22.5%; Average loss: 3.3032 Iteration: 902; Percent complete: 22.6%; Average loss: 3.6491 Iteration: 903; Percent complete: 22.6%; Average loss: 3.6549 Iteration: 904; Percent complete: 22.6%; Average loss: 3.4655 Iteration: 905; Percent complete: 22.6%; Average loss: 3.4510 Iteration: 906; Percent complete: 22.7%; Average loss: 3.4310 Iteration: 907; Percent complete: 22.7%; Average loss: 3.4675 Iteration: 908; Percent complete: 22.7%; Average loss: 3.4511 Iteration: 909; Percent complete: 22.7%; Average loss: 3.4042 Iteration: 910; Percent complete: 22.8%; Average loss: 3.3496 Iteration: 911; Percent complete: 22.8%; Average loss: 3.5439 Iteration: 912; Percent complete: 22.8%; Average loss: 3.4406 Iteration: 913; Percent complete: 22.8%; Average loss: 3.3593 Iteration: 914; Percent complete: 22.9%; Average loss: 3.5348 Iteration: 915; Percent complete: 22.9%; Average loss: 3.7245 Iteration: 916; Percent complete: 22.9%; Average loss: 3.6471 Iteration: 917; Percent complete: 22.9%; Average loss: 3.6803 Iteration: 918; Percent complete: 22.9%; Average loss: 3.4830 Iteration: 919; Percent complete: 23.0%; Average loss: 3.2568 Iteration: 920; Percent complete: 23.0%; Average loss: 3.7267 Iteration: 921; Percent complete: 23.0%; Average loss: 3.6653 Iteration: 922; Percent complete: 23.1%; Average loss: 3.4491 Iteration: 923; Percent complete: 23.1%; Average loss: 3.6209 Iteration: 924; Percent complete: 23.1%; Average loss: 3.6114 Iteration: 925; Percent complete: 23.1%; Average loss: 3.1260 Iteration: 926; Percent complete: 23.2%; Average loss: 3.7398 Iteration: 927; Percent complete: 23.2%; Average loss: 3.3786 Iteration: 928; Percent complete: 23.2%; Average loss: 3.5939 Iteration: 929; Percent complete: 23.2%; Average loss: 3.2974 Iteration: 930; Percent complete: 23.2%; Average loss: 3.5081 Iteration: 931; Percent complete: 23.3%; Average loss: 3.6164 Iteration: 932; Percent complete: 23.3%; Average loss: 3.4276 Iteration: 933; Percent complete: 23.3%; Average loss: 3.4698 Iteration: 934; Percent complete: 23.4%; Average loss: 3.4674 Iteration: 935; Percent complete: 23.4%; Average loss: 3.4487 Iteration: 936; Percent complete: 23.4%; Average loss: 3.8329 Iteration: 937; Percent complete: 23.4%; Average loss: 3.4054 Iteration: 938; Percent complete: 23.4%; Average loss: 3.6054 Iteration: 939; Percent complete: 23.5%; Average loss: 3.4963 Iteration: 940; Percent complete: 23.5%; Average loss: 3.6354 Iteration: 941; Percent complete: 23.5%; Average loss: 3.6042 Iteration: 942; Percent complete: 23.5%; Average loss: 3.3475 Iteration: 943; Percent complete: 23.6%; Average loss: 3.3167 Iteration: 944; Percent complete: 23.6%; Average loss: 3.3889 Iteration: 945; Percent complete: 23.6%; Average loss: 3.2977 Iteration: 946; Percent complete: 23.6%; Average loss: 3.4713 Iteration: 947; Percent complete: 23.7%; Average loss: 3.4775 Iteration: 948; Percent complete: 23.7%; Average loss: 3.5051 Iteration: 949; Percent complete: 23.7%; Average loss: 3.1252 Iteration: 950; Percent complete: 23.8%; Average loss: 3.6533 Iteration: 951; Percent complete: 23.8%; Average loss: 3.5571 Iteration: 952; Percent complete: 23.8%; Average loss: 3.5177 Iteration: 953; Percent complete: 23.8%; Average loss: 3.3169 Iteration: 954; Percent complete: 23.8%; Average loss: 3.7467 Iteration: 955; Percent complete: 23.9%; Average loss: 3.4274 Iteration: 956; Percent complete: 23.9%; Average loss: 3.5838 Iteration: 957; Percent complete: 23.9%; Average loss: 3.3769 Iteration: 958; Percent complete: 23.9%; Average loss: 3.4067 Iteration: 959; Percent complete: 24.0%; Average loss: 3.4789 Iteration: 960; Percent complete: 24.0%; Average loss: 3.4014 Iteration: 961; Percent complete: 24.0%; Average loss: 3.5607 Iteration: 962; Percent complete: 24.1%; Average loss: 3.3497 Iteration: 963; Percent complete: 24.1%; Average loss: 3.3740 Iteration: 964; Percent complete: 24.1%; Average loss: 3.3644 Iteration: 965; Percent complete: 24.1%; Average loss: 3.6642 Iteration: 966; Percent complete: 24.1%; Average loss: 3.5393 Iteration: 967; Percent complete: 24.2%; Average loss: 3.5352 Iteration: 968; Percent complete: 24.2%; Average loss: 3.4962 Iteration: 969; Percent complete: 24.2%; Average loss: 3.4899 Iteration: 970; Percent complete: 24.2%; Average loss: 3.5454 Iteration: 971; Percent complete: 24.3%; Average loss: 3.4335 Iteration: 972; Percent complete: 24.3%; Average loss: 3.4028 Iteration: 973; Percent complete: 24.3%; Average loss: 3.4868 Iteration: 974; Percent complete: 24.3%; Average loss: 3.3918 Iteration: 975; Percent complete: 24.4%; Average loss: 3.4541 Iteration: 976; Percent complete: 24.4%; Average loss: 3.2493 Iteration: 977; Percent complete: 24.4%; Average loss: 3.4464 Iteration: 978; Percent complete: 24.4%; Average loss: 3.4639 Iteration: 979; Percent complete: 24.5%; Average loss: 3.3465 Iteration: 980; Percent complete: 24.5%; Average loss: 3.2723 Iteration: 981; Percent complete: 24.5%; Average loss: 3.2729 Iteration: 982; Percent complete: 24.6%; Average loss: 3.6451 Iteration: 983; Percent complete: 24.6%; Average loss: 3.4533 Iteration: 984; Percent complete: 24.6%; Average loss: 3.5463 Iteration: 985; Percent complete: 24.6%; Average loss: 3.4242 Iteration: 986; Percent complete: 24.6%; Average loss: 3.5540 Iteration: 987; Percent complete: 24.7%; Average loss: 3.4739 Iteration: 988; Percent complete: 24.7%; Average loss: 3.4252 Iteration: 989; Percent complete: 24.7%; Average loss: 3.3033 Iteration: 990; Percent complete: 24.8%; Average loss: 3.4102 Iteration: 991; Percent complete: 24.8%; Average loss: 3.5793 Iteration: 992; Percent complete: 24.8%; Average loss: 3.4535 Iteration: 993; Percent complete: 24.8%; Average loss: 3.4009 Iteration: 994; Percent complete: 24.9%; Average loss: 3.5699 Iteration: 995; Percent complete: 24.9%; Average loss: 3.4716 Iteration: 996; Percent complete: 24.9%; Average loss: 3.5791 Iteration: 997; Percent complete: 24.9%; Average loss: 3.2969 Iteration: 998; Percent complete: 24.9%; Average loss: 3.1974 Iteration: 999; Percent complete: 25.0%; Average loss: 3.2873 Iteration: 1000; Percent complete: 25.0%; Average loss: 3.6855 Iteration: 1001; Percent complete: 25.0%; Average loss: 3.2325 Iteration: 1002; Percent complete: 25.1%; Average loss: 3.5274 Iteration: 1003; Percent complete: 25.1%; Average loss: 3.7042 Iteration: 1004; Percent complete: 25.1%; Average loss: 3.6182 Iteration: 1005; Percent complete: 25.1%; Average loss: 3.2686 Iteration: 1006; Percent complete: 25.1%; Average loss: 3.2909 Iteration: 1007; Percent complete: 25.2%; Average loss: 3.4840 Iteration: 1008; Percent complete: 25.2%; Average loss: 3.5833 Iteration: 1009; Percent complete: 25.2%; Average loss: 3.4722 Iteration: 1010; Percent complete: 25.2%; Average loss: 3.4814 Iteration: 1011; Percent complete: 25.3%; Average loss: 3.6635 Iteration: 1012; Percent complete: 25.3%; Average loss: 3.6570 Iteration: 1013; Percent complete: 25.3%; Average loss: 3.3822 Iteration: 1014; Percent complete: 25.4%; Average loss: 3.4184 Iteration: 1015; Percent complete: 25.4%; Average loss: 3.5297 Iteration: 1016; Percent complete: 25.4%; Average loss: 3.0870 Iteration: 1017; Percent complete: 25.4%; Average loss: 3.4151 Iteration: 1018; Percent complete: 25.4%; Average loss: 3.1984 Iteration: 1019; Percent complete: 25.5%; Average loss: 3.5597 Iteration: 1020; Percent complete: 25.5%; Average loss: 3.5073 Iteration: 1021; Percent complete: 25.5%; Average loss: 3.4539 Iteration: 1022; Percent complete: 25.6%; Average loss: 3.4809 Iteration: 1023; Percent complete: 25.6%; Average loss: 3.3777 Iteration: 1024; Percent complete: 25.6%; Average loss: 3.7249 Iteration: 1025; Percent complete: 25.6%; Average loss: 3.4981 Iteration: 1026; Percent complete: 25.7%; Average loss: 3.4003 Iteration: 1027; Percent complete: 25.7%; Average loss: 3.2648 Iteration: 1028; Percent complete: 25.7%; Average loss: 3.5892 Iteration: 1029; Percent complete: 25.7%; Average loss: 3.6561 Iteration: 1030; Percent complete: 25.8%; Average loss: 3.2470 Iteration: 1031; Percent complete: 25.8%; Average loss: 3.3200 Iteration: 1032; Percent complete: 25.8%; Average loss: 3.5689 Iteration: 1033; Percent complete: 25.8%; Average loss: 3.3992 Iteration: 1034; Percent complete: 25.9%; Average loss: 3.5077 Iteration: 1035; Percent complete: 25.9%; Average loss: 3.3738 Iteration: 1036; Percent complete: 25.9%; Average loss: 3.5188 Iteration: 1037; Percent complete: 25.9%; Average loss: 3.3653 Iteration: 1038; Percent complete: 25.9%; Average loss: 3.3464 Iteration: 1039; Percent complete: 26.0%; Average loss: 3.4738 Iteration: 1040; Percent complete: 26.0%; Average loss: 3.3141 Iteration: 1041; Percent complete: 26.0%; Average loss: 3.3878 Iteration: 1042; Percent complete: 26.1%; Average loss: 3.4279 Iteration: 1043; Percent complete: 26.1%; Average loss: 3.4310 Iteration: 1044; Percent complete: 26.1%; Average loss: 3.3725 Iteration: 1045; Percent complete: 26.1%; Average loss: 3.8419 Iteration: 1046; Percent complete: 26.2%; Average loss: 3.3358 Iteration: 1047; Percent complete: 26.2%; Average loss: 3.6724 Iteration: 1048; Percent complete: 26.2%; Average loss: 3.3505 Iteration: 1049; Percent complete: 26.2%; Average loss: 3.2956 Iteration: 1050; Percent complete: 26.2%; Average loss: 3.2860 Iteration: 1051; Percent complete: 26.3%; Average loss: 3.4088 Iteration: 1052; Percent complete: 26.3%; Average loss: 3.6164 Iteration: 1053; Percent complete: 26.3%; Average loss: 3.7105 Iteration: 1054; Percent complete: 26.4%; Average loss: 3.6051 Iteration: 1055; Percent complete: 26.4%; Average loss: 3.3170 Iteration: 1056; Percent complete: 26.4%; Average loss: 3.4801 Iteration: 1057; Percent complete: 26.4%; Average loss: 3.4772 Iteration: 1058; Percent complete: 26.5%; Average loss: 3.3950 Iteration: 1059; Percent complete: 26.5%; Average loss: 3.1410 Iteration: 1060; Percent complete: 26.5%; Average loss: 3.4419 Iteration: 1061; Percent complete: 26.5%; Average loss: 3.6071 Iteration: 1062; Percent complete: 26.6%; Average loss: 3.4191 Iteration: 1063; Percent complete: 26.6%; Average loss: 3.4889 Iteration: 1064; Percent complete: 26.6%; Average loss: 3.7739 Iteration: 1065; Percent complete: 26.6%; Average loss: 3.5487 Iteration: 1066; Percent complete: 26.7%; Average loss: 3.4202 Iteration: 1067; Percent complete: 26.7%; Average loss: 3.6634 Iteration: 1068; Percent complete: 26.7%; Average loss: 3.2873 Iteration: 1069; Percent complete: 26.7%; Average loss: 3.5069 Iteration: 1070; Percent complete: 26.8%; Average loss: 3.2480 Iteration: 1071; Percent complete: 26.8%; Average loss: 3.4556 Iteration: 1072; Percent complete: 26.8%; Average loss: 3.5446 Iteration: 1073; Percent complete: 26.8%; Average loss: 3.5157 Iteration: 1074; Percent complete: 26.9%; Average loss: 3.4550 Iteration: 1075; Percent complete: 26.9%; Average loss: 3.5131 Iteration: 1076; Percent complete: 26.9%; Average loss: 3.4119 Iteration: 1077; Percent complete: 26.9%; Average loss: 3.4230 Iteration: 1078; Percent complete: 27.0%; Average loss: 3.5849 Iteration: 1079; Percent complete: 27.0%; Average loss: 3.1230 Iteration: 1080; Percent complete: 27.0%; Average loss: 3.1496 Iteration: 1081; Percent complete: 27.0%; Average loss: 3.3635 Iteration: 1082; Percent complete: 27.1%; Average loss: 3.7272 Iteration: 1083; Percent complete: 27.1%; Average loss: 3.2388 Iteration: 1084; Percent complete: 27.1%; Average loss: 3.5667 Iteration: 1085; Percent complete: 27.1%; Average loss: 3.3594 Iteration: 1086; Percent complete: 27.2%; Average loss: 3.3956 Iteration: 1087; Percent complete: 27.2%; Average loss: 3.1149 Iteration: 1088; Percent complete: 27.2%; Average loss: 3.3598 Iteration: 1089; Percent complete: 27.2%; Average loss: 3.8926 Iteration: 1090; Percent complete: 27.3%; Average loss: 3.7141 Iteration: 1091; Percent complete: 27.3%; Average loss: 3.6096 Iteration: 1092; Percent complete: 27.3%; Average loss: 3.4429 Iteration: 1093; Percent complete: 27.3%; Average loss: 3.6357 Iteration: 1094; Percent complete: 27.4%; Average loss: 3.3907 Iteration: 1095; Percent complete: 27.4%; Average loss: 3.3197 Iteration: 1096; Percent complete: 27.4%; Average loss: 3.1119 Iteration: 1097; Percent complete: 27.4%; Average loss: 3.0901 Iteration: 1098; Percent complete: 27.5%; Average loss: 3.5611 Iteration: 1099; Percent complete: 27.5%; Average loss: 3.7190 Iteration: 1100; Percent complete: 27.5%; Average loss: 3.6275 Iteration: 1101; Percent complete: 27.5%; Average loss: 3.5717 Iteration: 1102; Percent complete: 27.6%; Average loss: 3.1786 Iteration: 1103; Percent complete: 27.6%; Average loss: 3.7171 Iteration: 1104; Percent complete: 27.6%; Average loss: 3.3975 Iteration: 1105; Percent complete: 27.6%; Average loss: 3.4284 Iteration: 1106; Percent complete: 27.7%; Average loss: 3.2208 Iteration: 1107; Percent complete: 27.7%; Average loss: 3.5802 Iteration: 1108; Percent complete: 27.7%; Average loss: 3.2912 Iteration: 1109; Percent complete: 27.7%; Average loss: 3.5566 Iteration: 1110; Percent complete: 27.8%; Average loss: 3.1842 Iteration: 1111; Percent complete: 27.8%; Average loss: 3.5535 Iteration: 1112; Percent complete: 27.8%; Average loss: 3.2853 Iteration: 1113; Percent complete: 27.8%; Average loss: 3.5890 Iteration: 1114; Percent complete: 27.9%; Average loss: 3.2193 Iteration: 1115; Percent complete: 27.9%; Average loss: 3.2593 Iteration: 1116; Percent complete: 27.9%; Average loss: 3.5051 Iteration: 1117; Percent complete: 27.9%; Average loss: 3.3173 Iteration: 1118; Percent complete: 28.0%; Average loss: 3.3634 Iteration: 1119; Percent complete: 28.0%; Average loss: 3.4383 Iteration: 1120; Percent complete: 28.0%; Average loss: 3.2621 Iteration: 1121; Percent complete: 28.0%; Average loss: 3.2845 Iteration: 1122; Percent complete: 28.1%; Average loss: 3.3336 Iteration: 1123; Percent complete: 28.1%; Average loss: 3.2216 Iteration: 1124; Percent complete: 28.1%; Average loss: 3.2762 Iteration: 1125; Percent complete: 28.1%; Average loss: 3.5245 Iteration: 1126; Percent complete: 28.1%; Average loss: 3.1087 Iteration: 1127; Percent complete: 28.2%; Average loss: 3.6523 Iteration: 1128; Percent complete: 28.2%; Average loss: 3.3426 Iteration: 1129; Percent complete: 28.2%; Average loss: 3.3690 Iteration: 1130; Percent complete: 28.2%; Average loss: 3.4334 Iteration: 1131; Percent complete: 28.3%; Average loss: 3.2746 Iteration: 1132; Percent complete: 28.3%; Average loss: 3.5091 Iteration: 1133; Percent complete: 28.3%; Average loss: 3.5834 Iteration: 1134; Percent complete: 28.3%; Average loss: 3.2084 Iteration: 1135; Percent complete: 28.4%; Average loss: 3.5185 Iteration: 1136; Percent complete: 28.4%; Average loss: 3.4850 Iteration: 1137; Percent complete: 28.4%; Average loss: 3.1962 Iteration: 1138; Percent complete: 28.4%; Average loss: 3.4021 Iteration: 1139; Percent complete: 28.5%; Average loss: 3.6018 Iteration: 1140; Percent complete: 28.5%; Average loss: 3.5646 Iteration: 1141; Percent complete: 28.5%; Average loss: 3.2766 Iteration: 1142; Percent complete: 28.5%; Average loss: 3.3545 Iteration: 1143; Percent complete: 28.6%; Average loss: 3.2054 Iteration: 1144; Percent complete: 28.6%; Average loss: 3.5684 Iteration: 1145; Percent complete: 28.6%; Average loss: 3.3730 Iteration: 1146; Percent complete: 28.6%; Average loss: 3.3580 Iteration: 1147; Percent complete: 28.7%; Average loss: 3.5411 Iteration: 1148; Percent complete: 28.7%; Average loss: 3.5103 Iteration: 1149; Percent complete: 28.7%; Average loss: 3.5753 Iteration: 1150; Percent complete: 28.7%; Average loss: 3.2917 Iteration: 1151; Percent complete: 28.8%; Average loss: 3.3203 Iteration: 1152; Percent complete: 28.8%; Average loss: 3.2972 Iteration: 1153; Percent complete: 28.8%; Average loss: 3.4172 Iteration: 1154; Percent complete: 28.8%; Average loss: 3.3854 Iteration: 1155; Percent complete: 28.9%; Average loss: 3.6790 Iteration: 1156; Percent complete: 28.9%; Average loss: 3.6520 Iteration: 1157; Percent complete: 28.9%; Average loss: 3.5157 Iteration: 1158; Percent complete: 28.9%; Average loss: 3.1978 Iteration: 1159; Percent complete: 29.0%; Average loss: 3.3821 Iteration: 1160; Percent complete: 29.0%; Average loss: 3.7369 Iteration: 1161; Percent complete: 29.0%; Average loss: 3.2006 Iteration: 1162; Percent complete: 29.0%; Average loss: 3.1708 Iteration: 1163; Percent complete: 29.1%; Average loss: 3.4499 Iteration: 1164; Percent complete: 29.1%; Average loss: 3.3072 Iteration: 1165; Percent complete: 29.1%; Average loss: 3.4391 Iteration: 1166; Percent complete: 29.1%; Average loss: 3.4102 Iteration: 1167; Percent complete: 29.2%; Average loss: 3.5171 Iteration: 1168; Percent complete: 29.2%; Average loss: 3.2941 Iteration: 1169; Percent complete: 29.2%; Average loss: 3.4647 Iteration: 1170; Percent complete: 29.2%; Average loss: 3.4245 Iteration: 1171; Percent complete: 29.3%; Average loss: 3.4860 Iteration: 1172; Percent complete: 29.3%; Average loss: 3.2974 Iteration: 1173; Percent complete: 29.3%; Average loss: 3.3110 Iteration: 1174; Percent complete: 29.3%; Average loss: 3.2935 Iteration: 1175; Percent complete: 29.4%; Average loss: 3.6118 Iteration: 1176; Percent complete: 29.4%; Average loss: 3.3962 Iteration: 1177; Percent complete: 29.4%; Average loss: 3.3924 Iteration: 1178; Percent complete: 29.4%; Average loss: 3.3507 Iteration: 1179; Percent complete: 29.5%; Average loss: 3.5898 Iteration: 1180; Percent complete: 29.5%; Average loss: 3.5854 Iteration: 1181; Percent complete: 29.5%; Average loss: 3.2520 Iteration: 1182; Percent complete: 29.5%; Average loss: 3.3434 Iteration: 1183; Percent complete: 29.6%; Average loss: 3.3307 Iteration: 1184; Percent complete: 29.6%; Average loss: 3.2823 Iteration: 1185; Percent complete: 29.6%; Average loss: 3.4077 Iteration: 1186; Percent complete: 29.6%; Average loss: 3.2617 Iteration: 1187; Percent complete: 29.7%; Average loss: 3.3286 Iteration: 1188; Percent complete: 29.7%; Average loss: 3.6794 Iteration: 1189; Percent complete: 29.7%; Average loss: 3.3547 Iteration: 1190; Percent complete: 29.8%; Average loss: 3.4249 Iteration: 1191; Percent complete: 29.8%; Average loss: 3.3854 Iteration: 1192; Percent complete: 29.8%; Average loss: 3.2782 Iteration: 1193; Percent complete: 29.8%; Average loss: 3.2950 Iteration: 1194; Percent complete: 29.8%; Average loss: 3.3090 Iteration: 1195; Percent complete: 29.9%; Average loss: 3.3662 Iteration: 1196; Percent complete: 29.9%; Average loss: 3.5265 Iteration: 1197; Percent complete: 29.9%; Average loss: 3.3203 Iteration: 1198; Percent complete: 29.9%; Average loss: 3.2653 Iteration: 1199; Percent complete: 30.0%; Average loss: 3.3040 Iteration: 1200; Percent complete: 30.0%; Average loss: 3.4843 Iteration: 1201; Percent complete: 30.0%; Average loss: 3.5059 Iteration: 1202; Percent complete: 30.0%; Average loss: 3.2906 Iteration: 1203; Percent complete: 30.1%; Average loss: 3.5463 Iteration: 1204; Percent complete: 30.1%; Average loss: 3.6588 Iteration: 1205; Percent complete: 30.1%; Average loss: 3.2958 Iteration: 1206; Percent complete: 30.1%; Average loss: 3.4777 Iteration: 1207; Percent complete: 30.2%; Average loss: 3.2783 Iteration: 1208; Percent complete: 30.2%; Average loss: 3.4041 Iteration: 1209; Percent complete: 30.2%; Average loss: 3.3784 Iteration: 1210; Percent complete: 30.2%; Average loss: 3.4538 Iteration: 1211; Percent complete: 30.3%; Average loss: 3.2769 Iteration: 1212; Percent complete: 30.3%; Average loss: 3.4682 Iteration: 1213; Percent complete: 30.3%; Average loss: 3.1071 Iteration: 1214; Percent complete: 30.3%; Average loss: 3.4931 Iteration: 1215; Percent complete: 30.4%; Average loss: 3.5234 Iteration: 1216; Percent complete: 30.4%; Average loss: 3.3680 Iteration: 1217; Percent complete: 30.4%; Average loss: 3.3327 Iteration: 1218; Percent complete: 30.4%; Average loss: 3.4088 Iteration: 1219; Percent complete: 30.5%; Average loss: 3.6000 Iteration: 1220; Percent complete: 30.5%; Average loss: 3.5628 Iteration: 1221; Percent complete: 30.5%; Average loss: 3.2329 Iteration: 1222; Percent complete: 30.6%; Average loss: 3.4396 Iteration: 1223; Percent complete: 30.6%; Average loss: 3.3859 Iteration: 1224; Percent complete: 30.6%; Average loss: 3.2597 Iteration: 1225; Percent complete: 30.6%; Average loss: 3.2235 Iteration: 1226; Percent complete: 30.6%; Average loss: 3.5944 Iteration: 1227; Percent complete: 30.7%; Average loss: 3.2327 Iteration: 1228; Percent complete: 30.7%; Average loss: 3.2696 Iteration: 1229; Percent complete: 30.7%; Average loss: 3.1657 Iteration: 1230; Percent complete: 30.8%; Average loss: 3.4409 Iteration: 1231; Percent complete: 30.8%; Average loss: 3.2590 Iteration: 1232; Percent complete: 30.8%; Average loss: 3.5405 Iteration: 1233; Percent complete: 30.8%; Average loss: 3.4164 Iteration: 1234; Percent complete: 30.9%; Average loss: 3.2695 Iteration: 1235; Percent complete: 30.9%; Average loss: 3.2286 Iteration: 1236; Percent complete: 30.9%; Average loss: 3.3534 Iteration: 1237; Percent complete: 30.9%; Average loss: 3.5711 Iteration: 1238; Percent complete: 30.9%; Average loss: 3.4164 Iteration: 1239; Percent complete: 31.0%; Average loss: 3.3308 Iteration: 1240; Percent complete: 31.0%; Average loss: 3.4876 Iteration: 1241; Percent complete: 31.0%; Average loss: 3.5128 Iteration: 1242; Percent complete: 31.1%; Average loss: 3.4155 Iteration: 1243; Percent complete: 31.1%; Average loss: 3.3458 Iteration: 1244; Percent complete: 31.1%; Average loss: 3.5423 Iteration: 1245; Percent complete: 31.1%; Average loss: 3.4959 Iteration: 1246; Percent complete: 31.1%; Average loss: 3.2350 Iteration: 1247; Percent complete: 31.2%; Average loss: 3.1694 Iteration: 1248; Percent complete: 31.2%; Average loss: 3.4860 Iteration: 1249; Percent complete: 31.2%; Average loss: 3.3868 Iteration: 1250; Percent complete: 31.2%; Average loss: 3.3078 Iteration: 1251; Percent complete: 31.3%; Average loss: 3.6386 Iteration: 1252; Percent complete: 31.3%; Average loss: 3.5785 Iteration: 1253; Percent complete: 31.3%; Average loss: 3.7612 Iteration: 1254; Percent complete: 31.4%; Average loss: 3.2868 Iteration: 1255; Percent complete: 31.4%; Average loss: 3.4709 Iteration: 1256; Percent complete: 31.4%; Average loss: 3.3390 Iteration: 1257; Percent complete: 31.4%; Average loss: 3.4660 Iteration: 1258; Percent complete: 31.4%; Average loss: 3.2997 Iteration: 1259; Percent complete: 31.5%; Average loss: 3.2849 Iteration: 1260; Percent complete: 31.5%; Average loss: 3.2408 Iteration: 1261; Percent complete: 31.5%; Average loss: 3.3716 Iteration: 1262; Percent complete: 31.6%; Average loss: 3.4106 Iteration: 1263; Percent complete: 31.6%; Average loss: 3.3841 Iteration: 1264; Percent complete: 31.6%; Average loss: 3.4994 Iteration: 1265; Percent complete: 31.6%; Average loss: 3.5058 Iteration: 1266; Percent complete: 31.6%; Average loss: 3.2695 Iteration: 1267; Percent complete: 31.7%; Average loss: 3.2332 Iteration: 1268; Percent complete: 31.7%; Average loss: 3.5514 Iteration: 1269; Percent complete: 31.7%; Average loss: 3.3619 Iteration: 1270; Percent complete: 31.8%; Average loss: 3.3013 Iteration: 1271; Percent complete: 31.8%; Average loss: 3.5951 Iteration: 1272; Percent complete: 31.8%; Average loss: 3.5497 Iteration: 1273; Percent complete: 31.8%; Average loss: 3.4859 Iteration: 1274; Percent complete: 31.9%; Average loss: 3.2828 Iteration: 1275; Percent complete: 31.9%; Average loss: 3.5645 Iteration: 1276; Percent complete: 31.9%; Average loss: 3.4103 Iteration: 1277; Percent complete: 31.9%; Average loss: 3.3707 Iteration: 1278; Percent complete: 31.9%; Average loss: 3.4423 Iteration: 1279; Percent complete: 32.0%; Average loss: 3.3864 Iteration: 1280; Percent complete: 32.0%; Average loss: 3.4208 Iteration: 1281; Percent complete: 32.0%; Average loss: 3.6646 Iteration: 1282; Percent complete: 32.0%; Average loss: 3.5360 Iteration: 1283; Percent complete: 32.1%; Average loss: 3.3565 Iteration: 1284; Percent complete: 32.1%; Average loss: 3.2624 Iteration: 1285; Percent complete: 32.1%; Average loss: 3.1663 Iteration: 1286; Percent complete: 32.1%; Average loss: 3.4615 Iteration: 1287; Percent complete: 32.2%; Average loss: 3.4985 Iteration: 1288; Percent complete: 32.2%; Average loss: 3.3606 Iteration: 1289; Percent complete: 32.2%; Average loss: 3.2504 Iteration: 1290; Percent complete: 32.2%; Average loss: 3.1189 Iteration: 1291; Percent complete: 32.3%; Average loss: 3.1394 Iteration: 1292; Percent complete: 32.3%; Average loss: 3.5708 Iteration: 1293; Percent complete: 32.3%; Average loss: 3.3905 Iteration: 1294; Percent complete: 32.4%; Average loss: 3.6477 Iteration: 1295; Percent complete: 32.4%; Average loss: 3.3037 Iteration: 1296; Percent complete: 32.4%; Average loss: 3.5348 Iteration: 1297; Percent complete: 32.4%; Average loss: 3.2369 Iteration: 1298; Percent complete: 32.5%; Average loss: 3.2639 Iteration: 1299; Percent complete: 32.5%; Average loss: 3.3452 Iteration: 1300; Percent complete: 32.5%; Average loss: 3.1631 Iteration: 1301; Percent complete: 32.5%; Average loss: 3.3955 Iteration: 1302; Percent complete: 32.6%; Average loss: 3.2996 Iteration: 1303; Percent complete: 32.6%; Average loss: 3.3941 Iteration: 1304; Percent complete: 32.6%; Average loss: 3.4682 Iteration: 1305; Percent complete: 32.6%; Average loss: 3.2827 Iteration: 1306; Percent complete: 32.6%; Average loss: 3.4269 Iteration: 1307; Percent complete: 32.7%; Average loss: 3.5330 Iteration: 1308; Percent complete: 32.7%; Average loss: 3.3512 Iteration: 1309; Percent complete: 32.7%; Average loss: 3.1152 Iteration: 1310; Percent complete: 32.8%; Average loss: 3.3459 Iteration: 1311; Percent complete: 32.8%; Average loss: 3.4356 Iteration: 1312; Percent complete: 32.8%; Average loss: 3.5147 Iteration: 1313; Percent complete: 32.8%; Average loss: 3.3596 Iteration: 1314; Percent complete: 32.9%; Average loss: 3.5494 Iteration: 1315; Percent complete: 32.9%; Average loss: 3.2821 Iteration: 1316; Percent complete: 32.9%; Average loss: 3.3712 Iteration: 1317; Percent complete: 32.9%; Average loss: 3.5842 Iteration: 1318; Percent complete: 33.0%; Average loss: 3.3862 Iteration: 1319; Percent complete: 33.0%; Average loss: 3.5419 Iteration: 1320; Percent complete: 33.0%; Average loss: 3.3121 Iteration: 1321; Percent complete: 33.0%; Average loss: 3.4419 Iteration: 1322; Percent complete: 33.1%; Average loss: 3.3700 Iteration: 1323; Percent complete: 33.1%; Average loss: 3.2580 Iteration: 1324; Percent complete: 33.1%; Average loss: 3.2560 Iteration: 1325; Percent complete: 33.1%; Average loss: 3.1759 Iteration: 1326; Percent complete: 33.1%; Average loss: 3.3677 Iteration: 1327; Percent complete: 33.2%; Average loss: 3.4352 Iteration: 1328; Percent complete: 33.2%; Average loss: 3.1544 Iteration: 1329; Percent complete: 33.2%; Average loss: 3.2133 Iteration: 1330; Percent complete: 33.2%; Average loss: 3.1968 Iteration: 1331; Percent complete: 33.3%; Average loss: 3.1073 Iteration: 1332; Percent complete: 33.3%; Average loss: 3.3342 Iteration: 1333; Percent complete: 33.3%; Average loss: 3.3237 Iteration: 1334; Percent complete: 33.4%; Average loss: 3.5931 Iteration: 1335; Percent complete: 33.4%; Average loss: 3.5538 Iteration: 1336; Percent complete: 33.4%; Average loss: 3.4758 Iteration: 1337; Percent complete: 33.4%; Average loss: 3.2387 Iteration: 1338; Percent complete: 33.5%; Average loss: 3.1353 Iteration: 1339; Percent complete: 33.5%; Average loss: 3.3341 Iteration: 1340; Percent complete: 33.5%; Average loss: 3.4654 Iteration: 1341; Percent complete: 33.5%; Average loss: 3.1349 Iteration: 1342; Percent complete: 33.6%; Average loss: 3.2993 Iteration: 1343; Percent complete: 33.6%; Average loss: 3.1209 Iteration: 1344; Percent complete: 33.6%; Average loss: 3.3624 Iteration: 1345; Percent complete: 33.6%; Average loss: 3.3508 Iteration: 1346; Percent complete: 33.7%; Average loss: 3.0245 Iteration: 1347; Percent complete: 33.7%; Average loss: 3.3446 Iteration: 1348; Percent complete: 33.7%; Average loss: 3.4064 Iteration: 1349; Percent complete: 33.7%; Average loss: 3.3198 Iteration: 1350; Percent complete: 33.8%; Average loss: 3.4660 Iteration: 1351; Percent complete: 33.8%; Average loss: 3.1197 Iteration: 1352; Percent complete: 33.8%; Average loss: 3.4816 Iteration: 1353; Percent complete: 33.8%; Average loss: 3.4092 Iteration: 1354; Percent complete: 33.9%; Average loss: 3.1850 Iteration: 1355; Percent complete: 33.9%; Average loss: 3.3011 Iteration: 1356; Percent complete: 33.9%; Average loss: 3.3445 Iteration: 1357; Percent complete: 33.9%; Average loss: 3.5676 Iteration: 1358; Percent complete: 34.0%; Average loss: 3.3175 Iteration: 1359; Percent complete: 34.0%; Average loss: 3.5758 Iteration: 1360; Percent complete: 34.0%; Average loss: 3.0686 Iteration: 1361; Percent complete: 34.0%; Average loss: 3.1018 Iteration: 1362; Percent complete: 34.1%; Average loss: 3.4027 Iteration: 1363; Percent complete: 34.1%; Average loss: 3.4235 Iteration: 1364; Percent complete: 34.1%; Average loss: 3.2070 Iteration: 1365; Percent complete: 34.1%; Average loss: 3.4107 Iteration: 1366; Percent complete: 34.2%; Average loss: 3.2742 Iteration: 1367; Percent complete: 34.2%; Average loss: 3.3266 Iteration: 1368; Percent complete: 34.2%; Average loss: 3.4072 Iteration: 1369; Percent complete: 34.2%; Average loss: 3.4958 Iteration: 1370; Percent complete: 34.2%; Average loss: 3.2853 Iteration: 1371; Percent complete: 34.3%; Average loss: 3.5436 Iteration: 1372; Percent complete: 34.3%; Average loss: 3.5354 Iteration: 1373; Percent complete: 34.3%; Average loss: 3.2952 Iteration: 1374; Percent complete: 34.4%; Average loss: 3.4385 Iteration: 1375; Percent complete: 34.4%; Average loss: 3.3569 Iteration: 1376; Percent complete: 34.4%; Average loss: 3.0008 Iteration: 1377; Percent complete: 34.4%; Average loss: 3.8009 Iteration: 1378; Percent complete: 34.4%; Average loss: 3.3266 Iteration: 1379; Percent complete: 34.5%; Average loss: 3.1768 Iteration: 1380; Percent complete: 34.5%; Average loss: 3.5706 Iteration: 1381; Percent complete: 34.5%; Average loss: 3.4232 Iteration: 1382; Percent complete: 34.5%; Average loss: 3.5156 Iteration: 1383; Percent complete: 34.6%; Average loss: 3.4124 Iteration: 1384; Percent complete: 34.6%; Average loss: 3.2435 Iteration: 1385; Percent complete: 34.6%; Average loss: 3.3121 Iteration: 1386; Percent complete: 34.6%; Average loss: 3.4889 Iteration: 1387; Percent complete: 34.7%; Average loss: 3.3456 Iteration: 1388; Percent complete: 34.7%; Average loss: 3.4371 Iteration: 1389; Percent complete: 34.7%; Average loss: 3.3818 Iteration: 1390; Percent complete: 34.8%; Average loss: 3.4394 Iteration: 1391; Percent complete: 34.8%; Average loss: 3.1665 Iteration: 1392; Percent complete: 34.8%; Average loss: 3.4033 Iteration: 1393; Percent complete: 34.8%; Average loss: 3.5298 Iteration: 1394; Percent complete: 34.8%; Average loss: 3.2085 Iteration: 1395; Percent complete: 34.9%; Average loss: 3.0194 Iteration: 1396; Percent complete: 34.9%; Average loss: 3.3512 Iteration: 1397; Percent complete: 34.9%; Average loss: 3.3296 Iteration: 1398; Percent complete: 34.9%; Average loss: 3.0238 Iteration: 1399; Percent complete: 35.0%; Average loss: 3.1455 Iteration: 1400; Percent complete: 35.0%; Average loss: 2.9905 Iteration: 1401; Percent complete: 35.0%; Average loss: 3.1701 Iteration: 1402; Percent complete: 35.0%; Average loss: 3.4527 Iteration: 1403; Percent complete: 35.1%; Average loss: 3.2137 Iteration: 1404; Percent complete: 35.1%; Average loss: 3.1841 Iteration: 1405; Percent complete: 35.1%; Average loss: 3.2687 Iteration: 1406; Percent complete: 35.1%; Average loss: 3.4170 Iteration: 1407; Percent complete: 35.2%; Average loss: 3.2435 Iteration: 1408; Percent complete: 35.2%; Average loss: 3.5382 Iteration: 1409; Percent complete: 35.2%; Average loss: 3.2707 Iteration: 1410; Percent complete: 35.2%; Average loss: 3.4647 Iteration: 1411; Percent complete: 35.3%; Average loss: 3.3420 Iteration: 1412; Percent complete: 35.3%; Average loss: 3.1993 Iteration: 1413; Percent complete: 35.3%; Average loss: 3.5202 Iteration: 1414; Percent complete: 35.4%; Average loss: 3.2845 Iteration: 1415; Percent complete: 35.4%; Average loss: 3.2461 Iteration: 1416; Percent complete: 35.4%; Average loss: 3.3519 Iteration: 1417; Percent complete: 35.4%; Average loss: 3.3718 Iteration: 1418; Percent complete: 35.4%; Average loss: 3.4330 Iteration: 1419; Percent complete: 35.5%; Average loss: 3.4659 Iteration: 1420; Percent complete: 35.5%; Average loss: 3.4026 Iteration: 1421; Percent complete: 35.5%; Average loss: 3.2468 Iteration: 1422; Percent complete: 35.5%; Average loss: 3.3979 Iteration: 1423; Percent complete: 35.6%; Average loss: 3.1711 Iteration: 1424; Percent complete: 35.6%; Average loss: 3.4085 Iteration: 1425; Percent complete: 35.6%; Average loss: 3.2346 Iteration: 1426; Percent complete: 35.6%; Average loss: 3.3699 Iteration: 1427; Percent complete: 35.7%; Average loss: 3.3563 Iteration: 1428; Percent complete: 35.7%; Average loss: 3.4010 Iteration: 1429; Percent complete: 35.7%; Average loss: 3.3687 Iteration: 1430; Percent complete: 35.8%; Average loss: 3.1894 Iteration: 1431; Percent complete: 35.8%; Average loss: 3.4224 Iteration: 1432; Percent complete: 35.8%; Average loss: 3.1035 Iteration: 1433; Percent complete: 35.8%; Average loss: 3.2205 Iteration: 1434; Percent complete: 35.9%; Average loss: 3.0578 Iteration: 1435; Percent complete: 35.9%; Average loss: 3.3864 Iteration: 1436; Percent complete: 35.9%; Average loss: 3.4729 Iteration: 1437; Percent complete: 35.9%; Average loss: 3.4461 Iteration: 1438; Percent complete: 35.9%; Average loss: 3.3293 Iteration: 1439; Percent complete: 36.0%; Average loss: 3.2806 Iteration: 1440; Percent complete: 36.0%; Average loss: 3.2169 Iteration: 1441; Percent complete: 36.0%; Average loss: 3.4162 Iteration: 1442; Percent complete: 36.0%; Average loss: 3.2754 Iteration: 1443; Percent complete: 36.1%; Average loss: 3.5577 Iteration: 1444; Percent complete: 36.1%; Average loss: 3.1890 Iteration: 1445; Percent complete: 36.1%; Average loss: 3.2804 Iteration: 1446; Percent complete: 36.1%; Average loss: 3.4320 Iteration: 1447; Percent complete: 36.2%; Average loss: 3.2852 Iteration: 1448; Percent complete: 36.2%; Average loss: 3.5093 Iteration: 1449; Percent complete: 36.2%; Average loss: 3.1936 Iteration: 1450; Percent complete: 36.2%; Average loss: 3.1954 Iteration: 1451; Percent complete: 36.3%; Average loss: 3.3873 Iteration: 1452; Percent complete: 36.3%; Average loss: 3.2636 Iteration: 1453; Percent complete: 36.3%; Average loss: 3.2706 Iteration: 1454; Percent complete: 36.4%; Average loss: 3.1513 Iteration: 1455; Percent complete: 36.4%; Average loss: 3.4187 Iteration: 1456; Percent complete: 36.4%; Average loss: 3.4063 Iteration: 1457; Percent complete: 36.4%; Average loss: 3.2307 Iteration: 1458; Percent complete: 36.4%; Average loss: 3.3445 Iteration: 1459; Percent complete: 36.5%; Average loss: 3.2706 Iteration: 1460; Percent complete: 36.5%; Average loss: 3.0638 Iteration: 1461; Percent complete: 36.5%; Average loss: 2.9783 Iteration: 1462; Percent complete: 36.5%; Average loss: 3.4609 Iteration: 1463; Percent complete: 36.6%; Average loss: 3.1469 Iteration: 1464; Percent complete: 36.6%; Average loss: 3.3021 Iteration: 1465; Percent complete: 36.6%; Average loss: 3.2708 Iteration: 1466; Percent complete: 36.6%; Average loss: 3.1350 Iteration: 1467; Percent complete: 36.7%; Average loss: 3.1991 Iteration: 1468; Percent complete: 36.7%; Average loss: 3.3467 Iteration: 1469; Percent complete: 36.7%; Average loss: 3.2377 Iteration: 1470; Percent complete: 36.8%; Average loss: 3.1275 Iteration: 1471; Percent complete: 36.8%; Average loss: 3.3203 Iteration: 1472; Percent complete: 36.8%; Average loss: 3.6607 Iteration: 1473; Percent complete: 36.8%; Average loss: 3.4064 Iteration: 1474; Percent complete: 36.9%; Average loss: 3.1828 Iteration: 1475; Percent complete: 36.9%; Average loss: 3.3238 Iteration: 1476; Percent complete: 36.9%; Average loss: 3.4740 Iteration: 1477; Percent complete: 36.9%; Average loss: 3.0380 Iteration: 1478; Percent complete: 37.0%; Average loss: 3.6326 Iteration: 1479; Percent complete: 37.0%; Average loss: 3.6307 Iteration: 1480; Percent complete: 37.0%; Average loss: 3.3536 Iteration: 1481; Percent complete: 37.0%; Average loss: 3.3328 Iteration: 1482; Percent complete: 37.0%; Average loss: 3.5123 Iteration: 1483; Percent complete: 37.1%; Average loss: 3.2374 Iteration: 1484; Percent complete: 37.1%; Average loss: 3.2034 Iteration: 1485; Percent complete: 37.1%; Average loss: 3.4141 Iteration: 1486; Percent complete: 37.1%; Average loss: 3.3675 Iteration: 1487; Percent complete: 37.2%; Average loss: 3.1559 Iteration: 1488; Percent complete: 37.2%; Average loss: 3.1924 Iteration: 1489; Percent complete: 37.2%; Average loss: 3.3427 Iteration: 1490; Percent complete: 37.2%; Average loss: 3.5639 Iteration: 1491; Percent complete: 37.3%; Average loss: 3.3479 Iteration: 1492; Percent complete: 37.3%; Average loss: 3.2384 Iteration: 1493; Percent complete: 37.3%; Average loss: 3.1886 Iteration: 1494; Percent complete: 37.4%; Average loss: 3.6953 Iteration: 1495; Percent complete: 37.4%; Average loss: 3.5848 Iteration: 1496; Percent complete: 37.4%; Average loss: 3.2389 Iteration: 1497; Percent complete: 37.4%; Average loss: 3.3284 Iteration: 1498; Percent complete: 37.5%; Average loss: 3.1658 Iteration: 1499; Percent complete: 37.5%; Average loss: 3.3764 Iteration: 1500; Percent complete: 37.5%; Average loss: 3.1852 Iteration: 1501; Percent complete: 37.5%; Average loss: 3.2811 Iteration: 1502; Percent complete: 37.5%; Average loss: 3.3556 Iteration: 1503; Percent complete: 37.6%; Average loss: 3.0454 Iteration: 1504; Percent complete: 37.6%; Average loss: 3.2825 Iteration: 1505; Percent complete: 37.6%; Average loss: 3.1059 Iteration: 1506; Percent complete: 37.6%; Average loss: 3.2047 Iteration: 1507; Percent complete: 37.7%; Average loss: 3.1982 Iteration: 1508; Percent complete: 37.7%; Average loss: 2.9349 Iteration: 1509; Percent complete: 37.7%; Average loss: 3.3957 Iteration: 1510; Percent complete: 37.8%; Average loss: 2.9322 Iteration: 1511; Percent complete: 37.8%; Average loss: 3.1681 Iteration: 1512; Percent complete: 37.8%; Average loss: 3.5575 Iteration: 1513; Percent complete: 37.8%; Average loss: 2.9218 Iteration: 1514; Percent complete: 37.9%; Average loss: 3.2164 Iteration: 1515; Percent complete: 37.9%; Average loss: 3.3670 Iteration: 1516; Percent complete: 37.9%; Average loss: 3.2559 Iteration: 1517; Percent complete: 37.9%; Average loss: 3.2558 Iteration: 1518; Percent complete: 38.0%; Average loss: 3.3094 Iteration: 1519; Percent complete: 38.0%; Average loss: 3.5517 Iteration: 1520; Percent complete: 38.0%; Average loss: 3.2588 Iteration: 1521; Percent complete: 38.0%; Average loss: 3.1360 Iteration: 1522; Percent complete: 38.0%; Average loss: 3.2725 Iteration: 1523; Percent complete: 38.1%; Average loss: 3.3827 Iteration: 1524; Percent complete: 38.1%; Average loss: 3.2261 Iteration: 1525; Percent complete: 38.1%; Average loss: 3.2958 Iteration: 1526; Percent complete: 38.1%; Average loss: 3.1877 Iteration: 1527; Percent complete: 38.2%; Average loss: 3.1610 Iteration: 1528; Percent complete: 38.2%; Average loss: 3.1740 Iteration: 1529; Percent complete: 38.2%; Average loss: 3.4668 Iteration: 1530; Percent complete: 38.2%; Average loss: 3.3595 Iteration: 1531; Percent complete: 38.3%; Average loss: 3.3976 Iteration: 1532; Percent complete: 38.3%; Average loss: 3.3456 Iteration: 1533; Percent complete: 38.3%; Average loss: 3.2906 Iteration: 1534; Percent complete: 38.4%; Average loss: 3.3467 Iteration: 1535; Percent complete: 38.4%; Average loss: 3.2514 Iteration: 1536; Percent complete: 38.4%; Average loss: 3.0888 Iteration: 1537; Percent complete: 38.4%; Average loss: 3.0713 Iteration: 1538; Percent complete: 38.5%; Average loss: 3.2358 Iteration: 1539; Percent complete: 38.5%; Average loss: 3.2046 Iteration: 1540; Percent complete: 38.5%; Average loss: 3.6050 Iteration: 1541; Percent complete: 38.5%; Average loss: 3.4687 Iteration: 1542; Percent complete: 38.6%; Average loss: 3.2880 Iteration: 1543; Percent complete: 38.6%; Average loss: 3.3498 Iteration: 1544; Percent complete: 38.6%; Average loss: 3.2711 Iteration: 1545; Percent complete: 38.6%; Average loss: 3.0882 Iteration: 1546; Percent complete: 38.6%; Average loss: 3.4675 Iteration: 1547; Percent complete: 38.7%; Average loss: 3.3136 Iteration: 1548; Percent complete: 38.7%; Average loss: 3.4352 Iteration: 1549; Percent complete: 38.7%; Average loss: 3.2541 Iteration: 1550; Percent complete: 38.8%; Average loss: 3.2738 Iteration: 1551; Percent complete: 38.8%; Average loss: 3.2710 Iteration: 1552; Percent complete: 38.8%; Average loss: 3.2654 Iteration: 1553; Percent complete: 38.8%; Average loss: 3.4501 Iteration: 1554; Percent complete: 38.9%; Average loss: 3.2156 Iteration: 1555; Percent complete: 38.9%; Average loss: 3.3635 Iteration: 1556; Percent complete: 38.9%; Average loss: 3.4191 Iteration: 1557; Percent complete: 38.9%; Average loss: 3.1605 Iteration: 1558; Percent complete: 39.0%; Average loss: 3.2360 Iteration: 1559; Percent complete: 39.0%; Average loss: 3.4577 Iteration: 1560; Percent complete: 39.0%; Average loss: 3.1584 Iteration: 1561; Percent complete: 39.0%; Average loss: 3.4698 Iteration: 1562; Percent complete: 39.1%; Average loss: 3.4334 Iteration: 1563; Percent complete: 39.1%; Average loss: 3.2543 Iteration: 1564; Percent complete: 39.1%; Average loss: 3.3854 Iteration: 1565; Percent complete: 39.1%; Average loss: 3.2546 Iteration: 1566; Percent complete: 39.1%; Average loss: 3.3022 Iteration: 1567; Percent complete: 39.2%; Average loss: 3.3012 Iteration: 1568; Percent complete: 39.2%; Average loss: 3.5721 Iteration: 1569; Percent complete: 39.2%; Average loss: 3.0039 Iteration: 1570; Percent complete: 39.2%; Average loss: 3.2307 Iteration: 1571; Percent complete: 39.3%; Average loss: 3.2429 Iteration: 1572; Percent complete: 39.3%; Average loss: 3.2137 Iteration: 1573; Percent complete: 39.3%; Average loss: 3.4119 Iteration: 1574; Percent complete: 39.4%; Average loss: 3.3878 Iteration: 1575; Percent complete: 39.4%; Average loss: 3.1604 Iteration: 1576; Percent complete: 39.4%; Average loss: 3.1626 Iteration: 1577; Percent complete: 39.4%; Average loss: 3.3007 Iteration: 1578; Percent complete: 39.5%; Average loss: 3.2592 Iteration: 1579; Percent complete: 39.5%; Average loss: 3.2758 Iteration: 1580; Percent complete: 39.5%; Average loss: 3.6267 Iteration: 1581; Percent complete: 39.5%; Average loss: 3.4473 Iteration: 1582; Percent complete: 39.6%; Average loss: 3.4671 Iteration: 1583; Percent complete: 39.6%; Average loss: 3.4252 Iteration: 1584; Percent complete: 39.6%; Average loss: 3.3068 Iteration: 1585; Percent complete: 39.6%; Average loss: 3.5446 Iteration: 1586; Percent complete: 39.6%; Average loss: 3.0792 Iteration: 1587; Percent complete: 39.7%; Average loss: 3.2319 Iteration: 1588; Percent complete: 39.7%; Average loss: 3.1976 Iteration: 1589; Percent complete: 39.7%; Average loss: 3.4802 Iteration: 1590; Percent complete: 39.8%; Average loss: 3.2631 Iteration: 1591; Percent complete: 39.8%; Average loss: 3.0641 Iteration: 1592; Percent complete: 39.8%; Average loss: 3.4544 Iteration: 1593; Percent complete: 39.8%; Average loss: 3.3098 Iteration: 1594; Percent complete: 39.9%; Average loss: 3.2107 Iteration: 1595; Percent complete: 39.9%; Average loss: 3.4340 Iteration: 1596; Percent complete: 39.9%; Average loss: 3.1531 Iteration: 1597; Percent complete: 39.9%; Average loss: 3.3466 Iteration: 1598; Percent complete: 40.0%; Average loss: 3.1351 Iteration: 1599; Percent complete: 40.0%; Average loss: 3.4859 Iteration: 1600; Percent complete: 40.0%; Average loss: 3.3846 Iteration: 1601; Percent complete: 40.0%; Average loss: 3.2511 Iteration: 1602; Percent complete: 40.1%; Average loss: 3.2000 Iteration: 1603; Percent complete: 40.1%; Average loss: 3.3334 Iteration: 1604; Percent complete: 40.1%; Average loss: 3.1224 Iteration: 1605; Percent complete: 40.1%; Average loss: 3.2327 Iteration: 1606; Percent complete: 40.2%; Average loss: 3.3431 Iteration: 1607; Percent complete: 40.2%; Average loss: 3.2837 Iteration: 1608; Percent complete: 40.2%; Average loss: 3.3838 Iteration: 1609; Percent complete: 40.2%; Average loss: 3.3573 Iteration: 1610; Percent complete: 40.2%; Average loss: 3.1297 Iteration: 1611; Percent complete: 40.3%; Average loss: 3.4124 Iteration: 1612; Percent complete: 40.3%; Average loss: 3.1478 Iteration: 1613; Percent complete: 40.3%; Average loss: 3.0738 Iteration: 1614; Percent complete: 40.4%; Average loss: 3.2790 Iteration: 1615; Percent complete: 40.4%; Average loss: 3.2837 Iteration: 1616; Percent complete: 40.4%; Average loss: 3.1461 Iteration: 1617; Percent complete: 40.4%; Average loss: 3.3423 Iteration: 1618; Percent complete: 40.5%; Average loss: 3.3120 Iteration: 1619; Percent complete: 40.5%; Average loss: 3.3059 Iteration: 1620; Percent complete: 40.5%; Average loss: 3.5898 Iteration: 1621; Percent complete: 40.5%; Average loss: 3.0933 Iteration: 1622; Percent complete: 40.6%; Average loss: 2.9213 Iteration: 1623; Percent complete: 40.6%; Average loss: 3.4153 Iteration: 1624; Percent complete: 40.6%; Average loss: 3.3667 Iteration: 1625; Percent complete: 40.6%; Average loss: 3.3382 Iteration: 1626; Percent complete: 40.6%; Average loss: 3.3901 Iteration: 1627; Percent complete: 40.7%; Average loss: 3.3473 Iteration: 1628; Percent complete: 40.7%; Average loss: 3.3875 Iteration: 1629; Percent complete: 40.7%; Average loss: 3.1542 Iteration: 1630; Percent complete: 40.8%; Average loss: 3.3152 Iteration: 1631; Percent complete: 40.8%; Average loss: 3.1604 Iteration: 1632; Percent complete: 40.8%; Average loss: 3.0765 Iteration: 1633; Percent complete: 40.8%; Average loss: 3.2234 Iteration: 1634; Percent complete: 40.8%; Average loss: 3.1725 Iteration: 1635; Percent complete: 40.9%; Average loss: 3.2468 Iteration: 1636; Percent complete: 40.9%; Average loss: 3.1424 Iteration: 1637; Percent complete: 40.9%; Average loss: 3.0094 Iteration: 1638; Percent complete: 40.9%; Average loss: 3.1595 Iteration: 1639; Percent complete: 41.0%; Average loss: 3.4891 Iteration: 1640; Percent complete: 41.0%; Average loss: 3.1030 Iteration: 1641; Percent complete: 41.0%; Average loss: 3.1865 Iteration: 1642; Percent complete: 41.0%; Average loss: 3.2970 Iteration: 1643; Percent complete: 41.1%; Average loss: 3.2997 Iteration: 1644; Percent complete: 41.1%; Average loss: 3.4412 Iteration: 1645; Percent complete: 41.1%; Average loss: 3.1460 Iteration: 1646; Percent complete: 41.1%; Average loss: 3.3175 Iteration: 1647; Percent complete: 41.2%; Average loss: 3.2048 Iteration: 1648; Percent complete: 41.2%; Average loss: 3.3547 Iteration: 1649; Percent complete: 41.2%; Average loss: 3.2310 Iteration: 1650; Percent complete: 41.2%; Average loss: 3.0978 Iteration: 1651; Percent complete: 41.3%; Average loss: 3.3480 Iteration: 1652; Percent complete: 41.3%; Average loss: 3.2720 Iteration: 1653; Percent complete: 41.3%; Average loss: 3.0122 Iteration: 1654; Percent complete: 41.3%; Average loss: 3.1171 Iteration: 1655; Percent complete: 41.4%; Average loss: 3.1559 Iteration: 1656; Percent complete: 41.4%; Average loss: 3.3425 Iteration: 1657; Percent complete: 41.4%; Average loss: 3.1542 Iteration: 1658; Percent complete: 41.4%; Average loss: 3.2723 Iteration: 1659; Percent complete: 41.5%; Average loss: 3.0934 Iteration: 1660; Percent complete: 41.5%; Average loss: 3.2247 Iteration: 1661; Percent complete: 41.5%; Average loss: 2.9709 Iteration: 1662; Percent complete: 41.5%; Average loss: 3.2690 Iteration: 1663; Percent complete: 41.6%; Average loss: 3.1796 Iteration: 1664; Percent complete: 41.6%; Average loss: 3.3143 Iteration: 1665; Percent complete: 41.6%; Average loss: 3.2776 Iteration: 1666; Percent complete: 41.6%; Average loss: 3.2233 Iteration: 1667; Percent complete: 41.7%; Average loss: 3.2503 Iteration: 1668; Percent complete: 41.7%; Average loss: 3.1394 Iteration: 1669; Percent complete: 41.7%; Average loss: 3.2403 Iteration: 1670; Percent complete: 41.8%; Average loss: 3.3145 Iteration: 1671; Percent complete: 41.8%; Average loss: 3.3339 Iteration: 1672; Percent complete: 41.8%; Average loss: 3.1796 Iteration: 1673; Percent complete: 41.8%; Average loss: 3.1970 Iteration: 1674; Percent complete: 41.9%; Average loss: 3.0952 Iteration: 1675; Percent complete: 41.9%; Average loss: 3.4530 Iteration: 1676; Percent complete: 41.9%; Average loss: 3.1250 Iteration: 1677; Percent complete: 41.9%; Average loss: 3.3322 Iteration: 1678; Percent complete: 41.9%; Average loss: 3.3537 Iteration: 1679; Percent complete: 42.0%; Average loss: 3.3503 Iteration: 1680; Percent complete: 42.0%; Average loss: 3.3016 Iteration: 1681; Percent complete: 42.0%; Average loss: 3.0670 Iteration: 1682; Percent complete: 42.0%; Average loss: 3.2754 Iteration: 1683; Percent complete: 42.1%; Average loss: 3.4398 Iteration: 1684; Percent complete: 42.1%; Average loss: 3.1970 Iteration: 1685; Percent complete: 42.1%; Average loss: 3.2584 Iteration: 1686; Percent complete: 42.1%; Average loss: 3.2808 Iteration: 1687; Percent complete: 42.2%; Average loss: 3.3765 Iteration: 1688; Percent complete: 42.2%; Average loss: 3.2109 Iteration: 1689; Percent complete: 42.2%; Average loss: 3.1530 Iteration: 1690; Percent complete: 42.2%; Average loss: 3.3300 Iteration: 1691; Percent complete: 42.3%; Average loss: 3.3281 Iteration: 1692; Percent complete: 42.3%; Average loss: 3.2467 Iteration: 1693; Percent complete: 42.3%; Average loss: 3.3082 Iteration: 1694; Percent complete: 42.4%; Average loss: 3.3633 Iteration: 1695; Percent complete: 42.4%; Average loss: 3.2194 Iteration: 1696; Percent complete: 42.4%; Average loss: 3.2342 Iteration: 1697; Percent complete: 42.4%; Average loss: 3.1391 Iteration: 1698; Percent complete: 42.4%; Average loss: 3.3145 Iteration: 1699; Percent complete: 42.5%; Average loss: 3.2582 Iteration: 1700; Percent complete: 42.5%; Average loss: 3.1322 Iteration: 1701; Percent complete: 42.5%; Average loss: 2.9353 Iteration: 1702; Percent complete: 42.5%; Average loss: 2.9088 Iteration: 1703; Percent complete: 42.6%; Average loss: 3.4067 Iteration: 1704; Percent complete: 42.6%; Average loss: 3.0896 Iteration: 1705; Percent complete: 42.6%; Average loss: 3.1764 Iteration: 1706; Percent complete: 42.6%; Average loss: 3.2341 Iteration: 1707; Percent complete: 42.7%; Average loss: 3.4767 Iteration: 1708; Percent complete: 42.7%; Average loss: 3.1424 Iteration: 1709; Percent complete: 42.7%; Average loss: 3.1852 Iteration: 1710; Percent complete: 42.8%; Average loss: 3.2949 Iteration: 1711; Percent complete: 42.8%; Average loss: 3.6801 Iteration: 1712; Percent complete: 42.8%; Average loss: 3.4333 Iteration: 1713; Percent complete: 42.8%; Average loss: 3.2099 Iteration: 1714; Percent complete: 42.9%; Average loss: 3.2764 Iteration: 1715; Percent complete: 42.9%; Average loss: 3.3786 Iteration: 1716; Percent complete: 42.9%; Average loss: 3.3092 Iteration: 1717; Percent complete: 42.9%; Average loss: 3.1970 Iteration: 1718; Percent complete: 43.0%; Average loss: 3.2539 Iteration: 1719; Percent complete: 43.0%; Average loss: 3.2503 Iteration: 1720; Percent complete: 43.0%; Average loss: 3.4296 Iteration: 1721; Percent complete: 43.0%; Average loss: 3.3935 Iteration: 1722; Percent complete: 43.0%; Average loss: 3.3546 Iteration: 1723; Percent complete: 43.1%; Average loss: 3.1472 Iteration: 1724; Percent complete: 43.1%; Average loss: 3.3800 Iteration: 1725; Percent complete: 43.1%; Average loss: 3.2387 Iteration: 1726; Percent complete: 43.1%; Average loss: 3.2898 Iteration: 1727; Percent complete: 43.2%; Average loss: 3.3427 Iteration: 1728; Percent complete: 43.2%; Average loss: 3.4329 Iteration: 1729; Percent complete: 43.2%; Average loss: 3.0428 Iteration: 1730; Percent complete: 43.2%; Average loss: 3.3042 Iteration: 1731; Percent complete: 43.3%; Average loss: 3.2811 Iteration: 1732; Percent complete: 43.3%; Average loss: 3.2720 Iteration: 1733; Percent complete: 43.3%; Average loss: 2.9724 Iteration: 1734; Percent complete: 43.4%; Average loss: 3.0596 Iteration: 1735; Percent complete: 43.4%; Average loss: 3.2289 Iteration: 1736; Percent complete: 43.4%; Average loss: 3.2371 Iteration: 1737; Percent complete: 43.4%; Average loss: 3.1310 Iteration: 1738; Percent complete: 43.5%; Average loss: 3.3377 Iteration: 1739; Percent complete: 43.5%; Average loss: 3.0105 Iteration: 1740; Percent complete: 43.5%; Average loss: 3.2333 Iteration: 1741; Percent complete: 43.5%; Average loss: 3.0533 Iteration: 1742; Percent complete: 43.5%; Average loss: 3.3368 Iteration: 1743; Percent complete: 43.6%; Average loss: 3.2564 Iteration: 1744; Percent complete: 43.6%; Average loss: 3.1726 Iteration: 1745; Percent complete: 43.6%; Average loss: 3.3298 Iteration: 1746; Percent complete: 43.6%; Average loss: 3.3548 Iteration: 1747; Percent complete: 43.7%; Average loss: 3.2832 Iteration: 1748; Percent complete: 43.7%; Average loss: 3.2664 Iteration: 1749; Percent complete: 43.7%; Average loss: 3.3260 Iteration: 1750; Percent complete: 43.8%; Average loss: 3.2545 Iteration: 1751; Percent complete: 43.8%; Average loss: 3.3269 Iteration: 1752; Percent complete: 43.8%; Average loss: 3.1759 Iteration: 1753; Percent complete: 43.8%; Average loss: 3.2739 Iteration: 1754; Percent complete: 43.9%; Average loss: 3.3581 Iteration: 1755; Percent complete: 43.9%; Average loss: 3.1249 Iteration: 1756; Percent complete: 43.9%; Average loss: 3.0306 Iteration: 1757; Percent complete: 43.9%; Average loss: 3.2474 Iteration: 1758; Percent complete: 44.0%; Average loss: 3.2258 Iteration: 1759; Percent complete: 44.0%; Average loss: 3.2227 Iteration: 1760; Percent complete: 44.0%; Average loss: 3.0180 Iteration: 1761; Percent complete: 44.0%; Average loss: 3.1512 Iteration: 1762; Percent complete: 44.0%; Average loss: 3.3239 Iteration: 1763; Percent complete: 44.1%; Average loss: 3.1343 Iteration: 1764; Percent complete: 44.1%; Average loss: 3.2860 Iteration: 1765; Percent complete: 44.1%; Average loss: 3.0836 Iteration: 1766; Percent complete: 44.1%; Average loss: 3.1725 Iteration: 1767; Percent complete: 44.2%; Average loss: 3.1051 Iteration: 1768; Percent complete: 44.2%; Average loss: 3.2529 Iteration: 1769; Percent complete: 44.2%; Average loss: 3.1671 Iteration: 1770; Percent complete: 44.2%; Average loss: 3.2439 Iteration: 1771; Percent complete: 44.3%; Average loss: 3.2535 Iteration: 1772; Percent complete: 44.3%; Average loss: 3.3010 Iteration: 1773; Percent complete: 44.3%; Average loss: 3.2438 Iteration: 1774; Percent complete: 44.4%; Average loss: 3.3068 Iteration: 1775; Percent complete: 44.4%; Average loss: 3.1590 Iteration: 1776; Percent complete: 44.4%; Average loss: 3.2097 Iteration: 1777; Percent complete: 44.4%; Average loss: 2.9020 Iteration: 1778; Percent complete: 44.5%; Average loss: 3.2168 Iteration: 1779; Percent complete: 44.5%; Average loss: 3.1102 Iteration: 1780; Percent complete: 44.5%; Average loss: 3.4255 Iteration: 1781; Percent complete: 44.5%; Average loss: 3.3038 Iteration: 1782; Percent complete: 44.5%; Average loss: 3.1064 Iteration: 1783; Percent complete: 44.6%; Average loss: 3.1988 Iteration: 1784; Percent complete: 44.6%; Average loss: 3.3512 Iteration: 1785; Percent complete: 44.6%; Average loss: 3.0708 Iteration: 1786; Percent complete: 44.6%; Average loss: 3.2355 Iteration: 1787; Percent complete: 44.7%; Average loss: 3.0953 Iteration: 1788; Percent complete: 44.7%; Average loss: 2.9998 Iteration: 1789; Percent complete: 44.7%; Average loss: 3.4002 Iteration: 1790; Percent complete: 44.8%; Average loss: 3.0080 Iteration: 1791; Percent complete: 44.8%; Average loss: 3.2352 Iteration: 1792; Percent complete: 44.8%; Average loss: 3.2270 Iteration: 1793; Percent complete: 44.8%; Average loss: 3.2408 Iteration: 1794; Percent complete: 44.9%; Average loss: 3.3257 Iteration: 1795; Percent complete: 44.9%; Average loss: 3.1964 Iteration: 1796; Percent complete: 44.9%; Average loss: 3.2535 Iteration: 1797; Percent complete: 44.9%; Average loss: 3.1854 Iteration: 1798; Percent complete: 45.0%; Average loss: 3.1728 Iteration: 1799; Percent complete: 45.0%; Average loss: 3.1533 Iteration: 1800; Percent complete: 45.0%; Average loss: 3.4538 Iteration: 1801; Percent complete: 45.0%; Average loss: 3.1524 Iteration: 1802; Percent complete: 45.1%; Average loss: 3.1420 Iteration: 1803; Percent complete: 45.1%; Average loss: 3.2081 Iteration: 1804; Percent complete: 45.1%; Average loss: 3.3607 Iteration: 1805; Percent complete: 45.1%; Average loss: 3.3200 Iteration: 1806; Percent complete: 45.1%; Average loss: 3.2826 Iteration: 1807; Percent complete: 45.2%; Average loss: 3.4802 Iteration: 1808; Percent complete: 45.2%; Average loss: 3.4306 Iteration: 1809; Percent complete: 45.2%; Average loss: 3.2518 Iteration: 1810; Percent complete: 45.2%; Average loss: 3.3997 Iteration: 1811; Percent complete: 45.3%; Average loss: 3.3589 Iteration: 1812; Percent complete: 45.3%; Average loss: 3.3485 Iteration: 1813; Percent complete: 45.3%; Average loss: 3.0921 Iteration: 1814; Percent complete: 45.4%; Average loss: 3.3622 Iteration: 1815; Percent complete: 45.4%; Average loss: 3.3516 Iteration: 1816; Percent complete: 45.4%; Average loss: 2.9022 Iteration: 1817; Percent complete: 45.4%; Average loss: 3.2554 Iteration: 1818; Percent complete: 45.5%; Average loss: 3.3143 Iteration: 1819; Percent complete: 45.5%; Average loss: 3.1725 Iteration: 1820; Percent complete: 45.5%; Average loss: 2.9438 Iteration: 1821; Percent complete: 45.5%; Average loss: 3.0712 Iteration: 1822; Percent complete: 45.6%; Average loss: 3.0540 Iteration: 1823; Percent complete: 45.6%; Average loss: 3.5431 Iteration: 1824; Percent complete: 45.6%; Average loss: 3.1571 Iteration: 1825; Percent complete: 45.6%; Average loss: 3.4339 Iteration: 1826; Percent complete: 45.6%; Average loss: 2.8519 Iteration: 1827; Percent complete: 45.7%; Average loss: 3.0372 Iteration: 1828; Percent complete: 45.7%; Average loss: 3.1696 Iteration: 1829; Percent complete: 45.7%; Average loss: 3.4680 Iteration: 1830; Percent complete: 45.8%; Average loss: 3.2369 Iteration: 1831; Percent complete: 45.8%; Average loss: 2.9861 Iteration: 1832; Percent complete: 45.8%; Average loss: 3.1067 Iteration: 1833; Percent complete: 45.8%; Average loss: 3.0964 Iteration: 1834; Percent complete: 45.9%; Average loss: 3.1945 Iteration: 1835; Percent complete: 45.9%; Average loss: 3.5193 Iteration: 1836; Percent complete: 45.9%; Average loss: 3.0755 Iteration: 1837; Percent complete: 45.9%; Average loss: 3.1492 Iteration: 1838; Percent complete: 46.0%; Average loss: 3.0795 Iteration: 1839; Percent complete: 46.0%; Average loss: 3.2730 Iteration: 1840; Percent complete: 46.0%; Average loss: 3.1958 Iteration: 1841; Percent complete: 46.0%; Average loss: 3.2035 Iteration: 1842; Percent complete: 46.1%; Average loss: 3.3586 Iteration: 1843; Percent complete: 46.1%; Average loss: 3.0348 Iteration: 1844; Percent complete: 46.1%; Average loss: 3.2771 Iteration: 1845; Percent complete: 46.1%; Average loss: 3.0922 Iteration: 1846; Percent complete: 46.2%; Average loss: 2.9237 Iteration: 1847; Percent complete: 46.2%; Average loss: 3.4554 Iteration: 1848; Percent complete: 46.2%; Average loss: 2.8235 Iteration: 1849; Percent complete: 46.2%; Average loss: 3.3057 Iteration: 1850; Percent complete: 46.2%; Average loss: 3.2573 Iteration: 1851; Percent complete: 46.3%; Average loss: 3.1695 Iteration: 1852; Percent complete: 46.3%; Average loss: 2.8142 Iteration: 1853; Percent complete: 46.3%; Average loss: 3.2474 Iteration: 1854; Percent complete: 46.4%; Average loss: 3.1892 Iteration: 1855; Percent complete: 46.4%; Average loss: 3.1886 Iteration: 1856; Percent complete: 46.4%; Average loss: 3.1296 Iteration: 1857; Percent complete: 46.4%; Average loss: 3.0317 Iteration: 1858; Percent complete: 46.5%; Average loss: 3.4448 Iteration: 1859; Percent complete: 46.5%; Average loss: 3.3902 Iteration: 1860; Percent complete: 46.5%; Average loss: 3.3883 Iteration: 1861; Percent complete: 46.5%; Average loss: 3.4157 Iteration: 1862; Percent complete: 46.6%; Average loss: 3.4026 Iteration: 1863; Percent complete: 46.6%; Average loss: 3.2992 Iteration: 1864; Percent complete: 46.6%; Average loss: 3.6761 Iteration: 1865; Percent complete: 46.6%; Average loss: 3.0394 Iteration: 1866; Percent complete: 46.7%; Average loss: 3.1771 Iteration: 1867; Percent complete: 46.7%; Average loss: 3.0232 Iteration: 1868; Percent complete: 46.7%; Average loss: 3.3837 Iteration: 1869; Percent complete: 46.7%; Average loss: 2.9927 Iteration: 1870; Percent complete: 46.8%; Average loss: 3.0303 Iteration: 1871; Percent complete: 46.8%; Average loss: 3.1939 Iteration: 1872; Percent complete: 46.8%; Average loss: 3.2114 Iteration: 1873; Percent complete: 46.8%; Average loss: 3.2382 Iteration: 1874; Percent complete: 46.9%; Average loss: 3.0336 Iteration: 1875; Percent complete: 46.9%; Average loss: 3.2703 Iteration: 1876; Percent complete: 46.9%; Average loss: 3.0808 Iteration: 1877; Percent complete: 46.9%; Average loss: 3.0507 Iteration: 1878; Percent complete: 46.9%; Average loss: 3.0155 Iteration: 1879; Percent complete: 47.0%; Average loss: 2.9668 Iteration: 1880; Percent complete: 47.0%; Average loss: 3.0084 Iteration: 1881; Percent complete: 47.0%; Average loss: 3.3885 Iteration: 1882; Percent complete: 47.0%; Average loss: 3.2488 Iteration: 1883; Percent complete: 47.1%; Average loss: 3.2768 Iteration: 1884; Percent complete: 47.1%; Average loss: 3.0737 Iteration: 1885; Percent complete: 47.1%; Average loss: 3.1842 Iteration: 1886; Percent complete: 47.1%; Average loss: 3.3481 Iteration: 1887; Percent complete: 47.2%; Average loss: 3.1089 Iteration: 1888; Percent complete: 47.2%; Average loss: 3.4178 Iteration: 1889; Percent complete: 47.2%; Average loss: 3.0599 Iteration: 1890; Percent complete: 47.2%; Average loss: 3.3511 Iteration: 1891; Percent complete: 47.3%; Average loss: 3.1818 Iteration: 1892; Percent complete: 47.3%; Average loss: 3.0544 Iteration: 1893; Percent complete: 47.3%; Average loss: 3.0559 Iteration: 1894; Percent complete: 47.3%; Average loss: 3.4408 Iteration: 1895; Percent complete: 47.4%; Average loss: 3.3146 Iteration: 1896; Percent complete: 47.4%; Average loss: 3.3139 Iteration: 1897; Percent complete: 47.4%; Average loss: 3.0016 Iteration: 1898; Percent complete: 47.4%; Average loss: 3.0670 Iteration: 1899; Percent complete: 47.5%; Average loss: 3.0942 Iteration: 1900; Percent complete: 47.5%; Average loss: 3.2458 Iteration: 1901; Percent complete: 47.5%; Average loss: 3.1989 Iteration: 1902; Percent complete: 47.5%; Average loss: 3.1257 Iteration: 1903; Percent complete: 47.6%; Average loss: 3.5761 Iteration: 1904; Percent complete: 47.6%; Average loss: 3.2674 Iteration: 1905; Percent complete: 47.6%; Average loss: 3.1304 Iteration: 1906; Percent complete: 47.6%; Average loss: 3.0014 Iteration: 1907; Percent complete: 47.7%; Average loss: 3.2440 Iteration: 1908; Percent complete: 47.7%; Average loss: 3.1727 Iteration: 1909; Percent complete: 47.7%; Average loss: 3.0841 Iteration: 1910; Percent complete: 47.8%; Average loss: 3.2016 Iteration: 1911; Percent complete: 47.8%; Average loss: 3.2067 Iteration: 1912; Percent complete: 47.8%; Average loss: 3.0726 Iteration: 1913; Percent complete: 47.8%; Average loss: 3.2902 Iteration: 1914; Percent complete: 47.9%; Average loss: 3.1222 Iteration: 1915; Percent complete: 47.9%; Average loss: 3.5211 Iteration: 1916; Percent complete: 47.9%; Average loss: 3.2679 Iteration: 1917; Percent complete: 47.9%; Average loss: 2.8842 Iteration: 1918; Percent complete: 47.9%; Average loss: 3.1873 Iteration: 1919; Percent complete: 48.0%; Average loss: 3.3119 Iteration: 1920; Percent complete: 48.0%; Average loss: 3.3168 Iteration: 1921; Percent complete: 48.0%; Average loss: 2.9828 Iteration: 1922; Percent complete: 48.0%; Average loss: 3.2156 Iteration: 1923; Percent complete: 48.1%; Average loss: 3.1331 Iteration: 1924; Percent complete: 48.1%; Average loss: 3.0838 Iteration: 1925; Percent complete: 48.1%; Average loss: 3.2389 Iteration: 1926; Percent complete: 48.1%; Average loss: 3.3909 Iteration: 1927; Percent complete: 48.2%; Average loss: 3.4071 Iteration: 1928; Percent complete: 48.2%; Average loss: 3.0535 Iteration: 1929; Percent complete: 48.2%; Average loss: 3.1975 Iteration: 1930; Percent complete: 48.2%; Average loss: 3.2160 Iteration: 1931; Percent complete: 48.3%; Average loss: 3.1258 Iteration: 1932; Percent complete: 48.3%; Average loss: 3.2427 Iteration: 1933; Percent complete: 48.3%; Average loss: 3.0206 Iteration: 1934; Percent complete: 48.4%; Average loss: 3.0848 Iteration: 1935; Percent complete: 48.4%; Average loss: 3.2541 Iteration: 1936; Percent complete: 48.4%; Average loss: 3.0758 Iteration: 1937; Percent complete: 48.4%; Average loss: 3.3375 Iteration: 1938; Percent complete: 48.4%; Average loss: 3.1487 Iteration: 1939; Percent complete: 48.5%; Average loss: 3.1254 Iteration: 1940; Percent complete: 48.5%; Average loss: 3.1642 Iteration: 1941; Percent complete: 48.5%; Average loss: 3.2941 Iteration: 1942; Percent complete: 48.5%; Average loss: 3.2600 Iteration: 1943; Percent complete: 48.6%; Average loss: 3.2646 Iteration: 1944; Percent complete: 48.6%; Average loss: 3.0745 Iteration: 1945; Percent complete: 48.6%; Average loss: 3.0826 Iteration: 1946; Percent complete: 48.6%; Average loss: 3.2309 Iteration: 1947; Percent complete: 48.7%; Average loss: 3.0008 Iteration: 1948; Percent complete: 48.7%; Average loss: 3.1870 Iteration: 1949; Percent complete: 48.7%; Average loss: 3.0244 Iteration: 1950; Percent complete: 48.8%; Average loss: 3.0549 Iteration: 1951; Percent complete: 48.8%; Average loss: 3.2872 Iteration: 1952; Percent complete: 48.8%; Average loss: 3.3491 Iteration: 1953; Percent complete: 48.8%; Average loss: 3.1662 Iteration: 1954; Percent complete: 48.9%; Average loss: 3.3495 Iteration: 1955; Percent complete: 48.9%; Average loss: 3.1466 Iteration: 1956; Percent complete: 48.9%; Average loss: 3.2604 Iteration: 1957; Percent complete: 48.9%; Average loss: 3.0930 Iteration: 1958; Percent complete: 48.9%; Average loss: 3.0189 Iteration: 1959; Percent complete: 49.0%; Average loss: 3.4434 Iteration: 1960; Percent complete: 49.0%; Average loss: 3.2460 Iteration: 1961; Percent complete: 49.0%; Average loss: 3.2129 Iteration: 1962; Percent complete: 49.0%; Average loss: 3.0597 Iteration: 1963; Percent complete: 49.1%; Average loss: 3.2764 Iteration: 1964; Percent complete: 49.1%; Average loss: 3.3456 Iteration: 1965; Percent complete: 49.1%; Average loss: 3.1536 Iteration: 1966; Percent complete: 49.1%; Average loss: 3.0357 Iteration: 1967; Percent complete: 49.2%; Average loss: 3.2141 Iteration: 1968; Percent complete: 49.2%; Average loss: 3.4683 Iteration: 1969; Percent complete: 49.2%; Average loss: 3.1661 Iteration: 1970; Percent complete: 49.2%; Average loss: 3.4083 Iteration: 1971; Percent complete: 49.3%; Average loss: 3.3786 Iteration: 1972; Percent complete: 49.3%; Average loss: 2.9513 Iteration: 1973; Percent complete: 49.3%; Average loss: 3.2350 Iteration: 1974; Percent complete: 49.4%; Average loss: 3.2921 Iteration: 1975; Percent complete: 49.4%; Average loss: 3.4953 Iteration: 1976; Percent complete: 49.4%; Average loss: 3.3263 Iteration: 1977; Percent complete: 49.4%; Average loss: 3.2543 Iteration: 1978; Percent complete: 49.5%; Average loss: 3.1496 Iteration: 1979; Percent complete: 49.5%; Average loss: 3.1834 Iteration: 1980; Percent complete: 49.5%; Average loss: 3.1350 Iteration: 1981; Percent complete: 49.5%; Average loss: 3.2269 Iteration: 1982; Percent complete: 49.5%; Average loss: 2.9852 Iteration: 1983; Percent complete: 49.6%; Average loss: 3.1887 Iteration: 1984; Percent complete: 49.6%; Average loss: 3.0983 Iteration: 1985; Percent complete: 49.6%; Average loss: 3.0130 Iteration: 1986; Percent complete: 49.6%; Average loss: 3.1037 Iteration: 1987; Percent complete: 49.7%; Average loss: 3.1281 Iteration: 1988; Percent complete: 49.7%; Average loss: 3.2399 Iteration: 1989; Percent complete: 49.7%; Average loss: 3.1808 Iteration: 1990; Percent complete: 49.8%; Average loss: 3.1546 Iteration: 1991; Percent complete: 49.8%; Average loss: 3.0688 Iteration: 1992; Percent complete: 49.8%; Average loss: 3.0411 Iteration: 1993; Percent complete: 49.8%; Average loss: 3.2910 Iteration: 1994; Percent complete: 49.9%; Average loss: 3.0836 Iteration: 1995; Percent complete: 49.9%; Average loss: 3.3743 Iteration: 1996; Percent complete: 49.9%; Average loss: 3.2843 Iteration: 1997; Percent complete: 49.9%; Average loss: 3.1963 Iteration: 1998; Percent complete: 50.0%; Average loss: 3.2841 Iteration: 1999; Percent complete: 50.0%; Average loss: 3.1529 Iteration: 2000; Percent complete: 50.0%; Average loss: 3.2082 Iteration: 2001; Percent complete: 50.0%; Average loss: 3.1149 Iteration: 2002; Percent complete: 50.0%; Average loss: 3.4692 Iteration: 2003; Percent complete: 50.1%; Average loss: 3.1024 Iteration: 2004; Percent complete: 50.1%; Average loss: 3.1624 Iteration: 2005; Percent complete: 50.1%; Average loss: 3.1737 Iteration: 2006; Percent complete: 50.1%; Average loss: 3.1565 Iteration: 2007; Percent complete: 50.2%; Average loss: 3.3474 Iteration: 2008; Percent complete: 50.2%; Average loss: 3.0540 Iteration: 2009; Percent complete: 50.2%; Average loss: 2.9797 Iteration: 2010; Percent complete: 50.2%; Average loss: 3.2565 Iteration: 2011; Percent complete: 50.3%; Average loss: 3.3492 Iteration: 2012; Percent complete: 50.3%; Average loss: 3.1628 Iteration: 2013; Percent complete: 50.3%; Average loss: 3.1572 Iteration: 2014; Percent complete: 50.3%; Average loss: 3.2238 Iteration: 2015; Percent complete: 50.4%; Average loss: 3.2448 Iteration: 2016; Percent complete: 50.4%; Average loss: 3.2495 Iteration: 2017; Percent complete: 50.4%; Average loss: 2.9318 Iteration: 2018; Percent complete: 50.4%; Average loss: 3.2012 Iteration: 2019; Percent complete: 50.5%; Average loss: 3.2043 Iteration: 2020; Percent complete: 50.5%; Average loss: 3.4169 Iteration: 2021; Percent complete: 50.5%; Average loss: 3.2700 Iteration: 2022; Percent complete: 50.5%; Average loss: 3.0297 Iteration: 2023; Percent complete: 50.6%; Average loss: 2.9735 Iteration: 2024; Percent complete: 50.6%; Average loss: 2.7525 Iteration: 2025; Percent complete: 50.6%; Average loss: 3.3517 Iteration: 2026; Percent complete: 50.6%; Average loss: 3.2983 Iteration: 2027; Percent complete: 50.7%; Average loss: 2.9989 Iteration: 2028; Percent complete: 50.7%; Average loss: 2.8659 Iteration: 2029; Percent complete: 50.7%; Average loss: 3.2319 Iteration: 2030; Percent complete: 50.7%; Average loss: 3.1767 Iteration: 2031; Percent complete: 50.8%; Average loss: 3.0906 Iteration: 2032; Percent complete: 50.8%; Average loss: 2.9274 Iteration: 2033; Percent complete: 50.8%; Average loss: 3.3168 Iteration: 2034; Percent complete: 50.8%; Average loss: 3.0443 Iteration: 2035; Percent complete: 50.9%; Average loss: 3.3384 Iteration: 2036; Percent complete: 50.9%; Average loss: 3.1877 Iteration: 2037; Percent complete: 50.9%; Average loss: 3.2322 Iteration: 2038; Percent complete: 50.9%; Average loss: 3.0349 Iteration: 2039; Percent complete: 51.0%; Average loss: 3.1963 Iteration: 2040; Percent complete: 51.0%; Average loss: 2.9312 Iteration: 2041; Percent complete: 51.0%; Average loss: 3.0710 Iteration: 2042; Percent complete: 51.0%; Average loss: 3.2978 Iteration: 2043; Percent complete: 51.1%; Average loss: 3.0428 Iteration: 2044; Percent complete: 51.1%; Average loss: 3.1851 Iteration: 2045; Percent complete: 51.1%; Average loss: 3.4392 Iteration: 2046; Percent complete: 51.1%; Average loss: 3.0813 Iteration: 2047; Percent complete: 51.2%; Average loss: 3.0319 Iteration: 2048; Percent complete: 51.2%; Average loss: 3.1984 Iteration: 2049; Percent complete: 51.2%; Average loss: 3.1841 Iteration: 2050; Percent complete: 51.2%; Average loss: 3.0969 Iteration: 2051; Percent complete: 51.3%; Average loss: 3.2731 Iteration: 2052; Percent complete: 51.3%; Average loss: 2.9168 Iteration: 2053; Percent complete: 51.3%; Average loss: 3.1649 Iteration: 2054; Percent complete: 51.3%; Average loss: 3.1549 Iteration: 2055; Percent complete: 51.4%; Average loss: 3.2137 Iteration: 2056; Percent complete: 51.4%; Average loss: 3.0599 Iteration: 2057; Percent complete: 51.4%; Average loss: 3.0577 Iteration: 2058; Percent complete: 51.4%; Average loss: 3.0192 Iteration: 2059; Percent complete: 51.5%; Average loss: 3.0032 Iteration: 2060; Percent complete: 51.5%; Average loss: 3.3567 Iteration: 2061; Percent complete: 51.5%; Average loss: 3.0327 Iteration: 2062; Percent complete: 51.5%; Average loss: 3.0040 Iteration: 2063; Percent complete: 51.6%; Average loss: 3.1066 Iteration: 2064; Percent complete: 51.6%; Average loss: 3.3245 Iteration: 2065; Percent complete: 51.6%; Average loss: 3.1876 Iteration: 2066; Percent complete: 51.6%; Average loss: 3.1613 Iteration: 2067; Percent complete: 51.7%; Average loss: 2.9272 Iteration: 2068; Percent complete: 51.7%; Average loss: 3.3433 Iteration: 2069; Percent complete: 51.7%; Average loss: 3.1538 Iteration: 2070; Percent complete: 51.7%; Average loss: 2.9813 Iteration: 2071; Percent complete: 51.8%; Average loss: 3.0278 Iteration: 2072; Percent complete: 51.8%; Average loss: 3.4067 Iteration: 2073; Percent complete: 51.8%; Average loss: 3.1327 Iteration: 2074; Percent complete: 51.8%; Average loss: 3.1426 Iteration: 2075; Percent complete: 51.9%; Average loss: 3.0571 Iteration: 2076; Percent complete: 51.9%; Average loss: 3.0072 Iteration: 2077; Percent complete: 51.9%; Average loss: 3.0279 Iteration: 2078; Percent complete: 51.9%; Average loss: 3.1783 Iteration: 2079; Percent complete: 52.0%; Average loss: 3.2371 Iteration: 2080; Percent complete: 52.0%; Average loss: 3.1959 Iteration: 2081; Percent complete: 52.0%; Average loss: 3.2585 Iteration: 2082; Percent complete: 52.0%; Average loss: 3.0316 Iteration: 2083; Percent complete: 52.1%; Average loss: 3.2417 Iteration: 2084; Percent complete: 52.1%; Average loss: 3.1226 Iteration: 2085; Percent complete: 52.1%; Average loss: 3.0579 Iteration: 2086; Percent complete: 52.1%; Average loss: 3.0619 Iteration: 2087; Percent complete: 52.2%; Average loss: 3.0890 Iteration: 2088; Percent complete: 52.2%; Average loss: 3.1590 Iteration: 2089; Percent complete: 52.2%; Average loss: 3.2042 Iteration: 2090; Percent complete: 52.2%; Average loss: 3.0238 Iteration: 2091; Percent complete: 52.3%; Average loss: 3.2154 Iteration: 2092; Percent complete: 52.3%; Average loss: 3.0813 Iteration: 2093; Percent complete: 52.3%; Average loss: 3.0748 Iteration: 2094; Percent complete: 52.3%; Average loss: 3.0433 Iteration: 2095; Percent complete: 52.4%; Average loss: 3.1053 Iteration: 2096; Percent complete: 52.4%; Average loss: 3.1631 Iteration: 2097; Percent complete: 52.4%; Average loss: 3.2154 Iteration: 2098; Percent complete: 52.4%; Average loss: 3.1838 Iteration: 2099; Percent complete: 52.5%; Average loss: 3.3070 Iteration: 2100; Percent complete: 52.5%; Average loss: 3.1872 Iteration: 2101; Percent complete: 52.5%; Average loss: 3.2999 Iteration: 2102; Percent complete: 52.5%; Average loss: 3.4601 Iteration: 2103; Percent complete: 52.6%; Average loss: 3.0459 Iteration: 2104; Percent complete: 52.6%; Average loss: 3.0534 Iteration: 2105; Percent complete: 52.6%; Average loss: 2.9912 Iteration: 2106; Percent complete: 52.6%; Average loss: 3.0896 Iteration: 2107; Percent complete: 52.7%; Average loss: 3.1473 Iteration: 2108; Percent complete: 52.7%; Average loss: 3.0212 Iteration: 2109; Percent complete: 52.7%; Average loss: 3.4153 Iteration: 2110; Percent complete: 52.8%; Average loss: 3.0631 Iteration: 2111; Percent complete: 52.8%; Average loss: 3.3058 Iteration: 2112; Percent complete: 52.8%; Average loss: 3.0522 Iteration: 2113; Percent complete: 52.8%; Average loss: 3.1656 Iteration: 2114; Percent complete: 52.8%; Average loss: 3.1904 Iteration: 2115; Percent complete: 52.9%; Average loss: 3.2335 Iteration: 2116; Percent complete: 52.9%; Average loss: 3.4354 Iteration: 2117; Percent complete: 52.9%; Average loss: 3.0054 Iteration: 2118; Percent complete: 52.9%; Average loss: 3.0576 Iteration: 2119; Percent complete: 53.0%; Average loss: 3.1077 Iteration: 2120; Percent complete: 53.0%; Average loss: 3.0456 Iteration: 2121; Percent complete: 53.0%; Average loss: 3.2399 Iteration: 2122; Percent complete: 53.0%; Average loss: 3.1187 Iteration: 2123; Percent complete: 53.1%; Average loss: 3.2444 Iteration: 2124; Percent complete: 53.1%; Average loss: 3.1705 Iteration: 2125; Percent complete: 53.1%; Average loss: 2.8593 Iteration: 2126; Percent complete: 53.1%; Average loss: 3.0219 Iteration: 2127; Percent complete: 53.2%; Average loss: 3.2337 Iteration: 2128; Percent complete: 53.2%; Average loss: 3.2209 Iteration: 2129; Percent complete: 53.2%; Average loss: 3.1871 Iteration: 2130; Percent complete: 53.2%; Average loss: 2.9840 Iteration: 2131; Percent complete: 53.3%; Average loss: 3.0416 Iteration: 2132; Percent complete: 53.3%; Average loss: 3.1449 Iteration: 2133; Percent complete: 53.3%; Average loss: 3.0035 Iteration: 2134; Percent complete: 53.3%; Average loss: 3.1888 Iteration: 2135; Percent complete: 53.4%; Average loss: 3.3041 Iteration: 2136; Percent complete: 53.4%; Average loss: 3.1467 Iteration: 2137; Percent complete: 53.4%; Average loss: 3.2681 Iteration: 2138; Percent complete: 53.4%; Average loss: 3.0517 Iteration: 2139; Percent complete: 53.5%; Average loss: 2.9784 Iteration: 2140; Percent complete: 53.5%; Average loss: 3.2595 Iteration: 2141; Percent complete: 53.5%; Average loss: 3.0933 Iteration: 2142; Percent complete: 53.5%; Average loss: 3.0125 Iteration: 2143; Percent complete: 53.6%; Average loss: 3.0328 Iteration: 2144; Percent complete: 53.6%; Average loss: 3.0783 Iteration: 2145; Percent complete: 53.6%; Average loss: 3.0843 Iteration: 2146; Percent complete: 53.6%; Average loss: 3.1724 Iteration: 2147; Percent complete: 53.7%; Average loss: 3.2402 Iteration: 2148; Percent complete: 53.7%; Average loss: 2.9745 Iteration: 2149; Percent complete: 53.7%; Average loss: 3.1805 Iteration: 2150; Percent complete: 53.8%; Average loss: 3.0659 Iteration: 2151; Percent complete: 53.8%; Average loss: 3.1741 Iteration: 2152; Percent complete: 53.8%; Average loss: 3.2166 Iteration: 2153; Percent complete: 53.8%; Average loss: 3.0210 Iteration: 2154; Percent complete: 53.8%; Average loss: 3.1465 Iteration: 2155; Percent complete: 53.9%; Average loss: 3.1798 Iteration: 2156; Percent complete: 53.9%; Average loss: 2.9233 Iteration: 2157; Percent complete: 53.9%; Average loss: 3.2107 Iteration: 2158; Percent complete: 53.9%; Average loss: 3.1495 Iteration: 2159; Percent complete: 54.0%; Average loss: 3.0062 Iteration: 2160; Percent complete: 54.0%; Average loss: 3.3022 Iteration: 2161; Percent complete: 54.0%; Average loss: 3.0096 Iteration: 2162; Percent complete: 54.0%; Average loss: 3.1913 Iteration: 2163; Percent complete: 54.1%; Average loss: 3.1533 Iteration: 2164; Percent complete: 54.1%; Average loss: 3.0350 Iteration: 2165; Percent complete: 54.1%; Average loss: 3.1658 Iteration: 2166; Percent complete: 54.1%; Average loss: 3.2373 Iteration: 2167; Percent complete: 54.2%; Average loss: 3.2373 Iteration: 2168; Percent complete: 54.2%; Average loss: 3.0903 Iteration: 2169; Percent complete: 54.2%; Average loss: 3.0564 Iteration: 2170; Percent complete: 54.2%; Average loss: 2.9251 Iteration: 2171; Percent complete: 54.3%; Average loss: 3.2464 Iteration: 2172; Percent complete: 54.3%; Average loss: 3.3825 Iteration: 2173; Percent complete: 54.3%; Average loss: 3.3807 Iteration: 2174; Percent complete: 54.4%; Average loss: 3.2350 Iteration: 2175; Percent complete: 54.4%; Average loss: 2.9695 Iteration: 2176; Percent complete: 54.4%; Average loss: 2.9056 Iteration: 2177; Percent complete: 54.4%; Average loss: 3.0504 Iteration: 2178; Percent complete: 54.4%; Average loss: 3.0704 Iteration: 2179; Percent complete: 54.5%; Average loss: 3.2341 Iteration: 2180; Percent complete: 54.5%; Average loss: 2.9216 Iteration: 2181; Percent complete: 54.5%; Average loss: 2.9119 Iteration: 2182; Percent complete: 54.5%; Average loss: 3.1728 Iteration: 2183; Percent complete: 54.6%; Average loss: 3.2105 Iteration: 2184; Percent complete: 54.6%; Average loss: 2.9327 Iteration: 2185; Percent complete: 54.6%; Average loss: 3.1283 Iteration: 2186; Percent complete: 54.6%; Average loss: 3.2375 Iteration: 2187; Percent complete: 54.7%; Average loss: 3.1406 Iteration: 2188; Percent complete: 54.7%; Average loss: 3.2877 Iteration: 2189; Percent complete: 54.7%; Average loss: 3.0811 Iteration: 2190; Percent complete: 54.8%; Average loss: 2.9333 Iteration: 2191; Percent complete: 54.8%; Average loss: 3.0446 Iteration: 2192; Percent complete: 54.8%; Average loss: 3.1889 Iteration: 2193; Percent complete: 54.8%; Average loss: 3.2479 Iteration: 2194; Percent complete: 54.9%; Average loss: 3.2153 Iteration: 2195; Percent complete: 54.9%; Average loss: 3.3670 Iteration: 2196; Percent complete: 54.9%; Average loss: 2.9339 Iteration: 2197; Percent complete: 54.9%; Average loss: 3.0430 Iteration: 2198; Percent complete: 54.9%; Average loss: 3.0997 Iteration: 2199; Percent complete: 55.0%; Average loss: 3.2640 Iteration: 2200; Percent complete: 55.0%; Average loss: 3.1570 Iteration: 2201; Percent complete: 55.0%; Average loss: 3.0195 Iteration: 2202; Percent complete: 55.0%; Average loss: 3.0562 Iteration: 2203; Percent complete: 55.1%; Average loss: 2.9020 Iteration: 2204; Percent complete: 55.1%; Average loss: 3.1450 Iteration: 2205; Percent complete: 55.1%; Average loss: 3.1889 Iteration: 2206; Percent complete: 55.1%; Average loss: 3.1361 Iteration: 2207; Percent complete: 55.2%; Average loss: 3.2197 Iteration: 2208; Percent complete: 55.2%; Average loss: 3.1695 Iteration: 2209; Percent complete: 55.2%; Average loss: 3.1391 Iteration: 2210; Percent complete: 55.2%; Average loss: 3.0901 Iteration: 2211; Percent complete: 55.3%; Average loss: 3.1009 Iteration: 2212; Percent complete: 55.3%; Average loss: 3.0961 Iteration: 2213; Percent complete: 55.3%; Average loss: 3.1071 Iteration: 2214; Percent complete: 55.4%; Average loss: 3.3075 Iteration: 2215; Percent complete: 55.4%; Average loss: 3.1559 Iteration: 2216; Percent complete: 55.4%; Average loss: 3.0978 Iteration: 2217; Percent complete: 55.4%; Average loss: 3.2209 Iteration: 2218; Percent complete: 55.5%; Average loss: 2.8812 Iteration: 2219; Percent complete: 55.5%; Average loss: 3.0528 Iteration: 2220; Percent complete: 55.5%; Average loss: 2.9478 Iteration: 2221; Percent complete: 55.5%; Average loss: 3.2022 Iteration: 2222; Percent complete: 55.5%; Average loss: 3.0690 Iteration: 2223; Percent complete: 55.6%; Average loss: 3.0582 Iteration: 2224; Percent complete: 55.6%; Average loss: 2.9638 Iteration: 2225; Percent complete: 55.6%; Average loss: 3.2415 Iteration: 2226; Percent complete: 55.6%; Average loss: 3.5418 Iteration: 2227; Percent complete: 55.7%; Average loss: 2.9684 Iteration: 2228; Percent complete: 55.7%; Average loss: 2.9168 Iteration: 2229; Percent complete: 55.7%; Average loss: 3.0042 Iteration: 2230; Percent complete: 55.8%; Average loss: 3.3420 Iteration: 2231; Percent complete: 55.8%; Average loss: 2.7673 Iteration: 2232; Percent complete: 55.8%; Average loss: 3.3168 Iteration: 2233; Percent complete: 55.8%; Average loss: 3.0216 Iteration: 2234; Percent complete: 55.9%; Average loss: 3.1888 Iteration: 2235; Percent complete: 55.9%; Average loss: 2.9440 Iteration: 2236; Percent complete: 55.9%; Average loss: 3.0129 Iteration: 2237; Percent complete: 55.9%; Average loss: 3.2862 Iteration: 2238; Percent complete: 56.0%; Average loss: 3.3401 Iteration: 2239; Percent complete: 56.0%; Average loss: 3.0064 Iteration: 2240; Percent complete: 56.0%; Average loss: 2.9815 Iteration: 2241; Percent complete: 56.0%; Average loss: 3.1758 Iteration: 2242; Percent complete: 56.0%; Average loss: 3.1954 Iteration: 2243; Percent complete: 56.1%; Average loss: 3.0948 Iteration: 2244; Percent complete: 56.1%; Average loss: 3.2322 Iteration: 2245; Percent complete: 56.1%; Average loss: 2.8999 Iteration: 2246; Percent complete: 56.1%; Average loss: 3.1323 Iteration: 2247; Percent complete: 56.2%; Average loss: 3.2062 Iteration: 2248; Percent complete: 56.2%; Average loss: 3.2596 Iteration: 2249; Percent complete: 56.2%; Average loss: 3.0669 Iteration: 2250; Percent complete: 56.2%; Average loss: 3.1418 Iteration: 2251; Percent complete: 56.3%; Average loss: 3.0625 Iteration: 2252; Percent complete: 56.3%; Average loss: 2.8354 Iteration: 2253; Percent complete: 56.3%; Average loss: 3.0152 Iteration: 2254; Percent complete: 56.4%; Average loss: 3.3499 Iteration: 2255; Percent complete: 56.4%; Average loss: 3.1123 Iteration: 2256; Percent complete: 56.4%; Average loss: 3.2161 Iteration: 2257; Percent complete: 56.4%; Average loss: 2.9346 Iteration: 2258; Percent complete: 56.5%; Average loss: 3.4500 Iteration: 2259; Percent complete: 56.5%; Average loss: 2.9546 Iteration: 2260; Percent complete: 56.5%; Average loss: 3.2969 Iteration: 2261; Percent complete: 56.5%; Average loss: 3.0232 Iteration: 2262; Percent complete: 56.5%; Average loss: 3.0781 Iteration: 2263; Percent complete: 56.6%; Average loss: 2.6908 Iteration: 2264; Percent complete: 56.6%; Average loss: 3.0890 Iteration: 2265; Percent complete: 56.6%; Average loss: 3.0787 Iteration: 2266; Percent complete: 56.6%; Average loss: 2.8231 Iteration: 2267; Percent complete: 56.7%; Average loss: 2.8522 Iteration: 2268; Percent complete: 56.7%; Average loss: 2.8848 Iteration: 2269; Percent complete: 56.7%; Average loss: 3.2179 Iteration: 2270; Percent complete: 56.8%; Average loss: 2.9336 Iteration: 2271; Percent complete: 56.8%; Average loss: 3.1807 Iteration: 2272; Percent complete: 56.8%; Average loss: 3.2438 Iteration: 2273; Percent complete: 56.8%; Average loss: 3.1505 Iteration: 2274; Percent complete: 56.9%; Average loss: 2.8398 Iteration: 2275; Percent complete: 56.9%; Average loss: 3.0749 Iteration: 2276; Percent complete: 56.9%; Average loss: 3.2126 Iteration: 2277; Percent complete: 56.9%; Average loss: 3.1575 Iteration: 2278; Percent complete: 57.0%; Average loss: 3.0908 Iteration: 2279; Percent complete: 57.0%; Average loss: 3.2163 Iteration: 2280; Percent complete: 57.0%; Average loss: 3.0343 Iteration: 2281; Percent complete: 57.0%; Average loss: 3.1416 Iteration: 2282; Percent complete: 57.0%; Average loss: 3.2054 Iteration: 2283; Percent complete: 57.1%; Average loss: 2.8907 Iteration: 2284; Percent complete: 57.1%; Average loss: 3.1890 Iteration: 2285; Percent complete: 57.1%; Average loss: 3.0720 Iteration: 2286; Percent complete: 57.1%; Average loss: 2.8727 Iteration: 2287; Percent complete: 57.2%; Average loss: 3.2101 Iteration: 2288; Percent complete: 57.2%; Average loss: 2.7598 Iteration: 2289; Percent complete: 57.2%; Average loss: 3.0717 Iteration: 2290; Percent complete: 57.2%; Average loss: 2.9435 Iteration: 2291; Percent complete: 57.3%; Average loss: 2.9831 Iteration: 2292; Percent complete: 57.3%; Average loss: 3.0723 Iteration: 2293; Percent complete: 57.3%; Average loss: 3.0705 Iteration: 2294; Percent complete: 57.4%; Average loss: 2.9697 Iteration: 2295; Percent complete: 57.4%; Average loss: 2.8842 Iteration: 2296; Percent complete: 57.4%; Average loss: 3.1654 Iteration: 2297; Percent complete: 57.4%; Average loss: 3.0200 Iteration: 2298; Percent complete: 57.5%; Average loss: 3.0309 Iteration: 2299; Percent complete: 57.5%; Average loss: 3.1875 Iteration: 2300; Percent complete: 57.5%; Average loss: 3.1743 Iteration: 2301; Percent complete: 57.5%; Average loss: 3.1811 Iteration: 2302; Percent complete: 57.6%; Average loss: 3.0401 Iteration: 2303; Percent complete: 57.6%; Average loss: 2.9408 Iteration: 2304; Percent complete: 57.6%; Average loss: 3.2218 Iteration: 2305; Percent complete: 57.6%; Average loss: 3.1464 Iteration: 2306; Percent complete: 57.6%; Average loss: 2.9642 Iteration: 2307; Percent complete: 57.7%; Average loss: 2.8079 Iteration: 2308; Percent complete: 57.7%; Average loss: 2.9996 Iteration: 2309; Percent complete: 57.7%; Average loss: 3.0726 Iteration: 2310; Percent complete: 57.8%; Average loss: 3.3397 Iteration: 2311; Percent complete: 57.8%; Average loss: 3.1430 Iteration: 2312; Percent complete: 57.8%; Average loss: 3.3286 Iteration: 2313; Percent complete: 57.8%; Average loss: 2.9164 Iteration: 2314; Percent complete: 57.9%; Average loss: 3.2109 Iteration: 2315; Percent complete: 57.9%; Average loss: 3.3330 Iteration: 2316; Percent complete: 57.9%; Average loss: 3.1248 Iteration: 2317; Percent complete: 57.9%; Average loss: 3.0707 Iteration: 2318; Percent complete: 58.0%; Average loss: 3.0612 Iteration: 2319; Percent complete: 58.0%; Average loss: 2.8473 Iteration: 2320; Percent complete: 58.0%; Average loss: 3.2078 Iteration: 2321; Percent complete: 58.0%; Average loss: 3.0709 Iteration: 2322; Percent complete: 58.1%; Average loss: 3.0883 Iteration: 2323; Percent complete: 58.1%; Average loss: 3.0236 Iteration: 2324; Percent complete: 58.1%; Average loss: 2.9528 Iteration: 2325; Percent complete: 58.1%; Average loss: 3.2924 Iteration: 2326; Percent complete: 58.1%; Average loss: 2.7107 Iteration: 2327; Percent complete: 58.2%; Average loss: 2.9690 Iteration: 2328; Percent complete: 58.2%; Average loss: 3.0400 Iteration: 2329; Percent complete: 58.2%; Average loss: 2.9971 Iteration: 2330; Percent complete: 58.2%; Average loss: 3.0223 Iteration: 2331; Percent complete: 58.3%; Average loss: 3.2198 Iteration: 2332; Percent complete: 58.3%; Average loss: 3.0670 Iteration: 2333; Percent complete: 58.3%; Average loss: 2.8575 Iteration: 2334; Percent complete: 58.4%; Average loss: 3.1535 Iteration: 2335; Percent complete: 58.4%; Average loss: 2.8534 Iteration: 2336; Percent complete: 58.4%; Average loss: 3.1788 Iteration: 2337; Percent complete: 58.4%; Average loss: 3.3014 Iteration: 2338; Percent complete: 58.5%; Average loss: 3.1429 Iteration: 2339; Percent complete: 58.5%; Average loss: 3.0781 Iteration: 2340; Percent complete: 58.5%; Average loss: 2.9397 Iteration: 2341; Percent complete: 58.5%; Average loss: 2.9744 Iteration: 2342; Percent complete: 58.6%; Average loss: 3.0321 Iteration: 2343; Percent complete: 58.6%; Average loss: 3.1011 Iteration: 2344; Percent complete: 58.6%; Average loss: 3.0047 Iteration: 2345; Percent complete: 58.6%; Average loss: 2.8201 Iteration: 2346; Percent complete: 58.7%; Average loss: 3.3459 Iteration: 2347; Percent complete: 58.7%; Average loss: 3.0558 Iteration: 2348; Percent complete: 58.7%; Average loss: 2.8595 Iteration: 2349; Percent complete: 58.7%; Average loss: 2.9791 Iteration: 2350; Percent complete: 58.8%; Average loss: 2.9259 Iteration: 2351; Percent complete: 58.8%; Average loss: 2.9089 Iteration: 2352; Percent complete: 58.8%; Average loss: 2.8353 Iteration: 2353; Percent complete: 58.8%; Average loss: 3.2003 Iteration: 2354; Percent complete: 58.9%; Average loss: 3.3361 Iteration: 2355; Percent complete: 58.9%; Average loss: 3.0339 Iteration: 2356; Percent complete: 58.9%; Average loss: 3.0604 Iteration: 2357; Percent complete: 58.9%; Average loss: 3.0730 Iteration: 2358; Percent complete: 59.0%; Average loss: 3.0103 Iteration: 2359; Percent complete: 59.0%; Average loss: 3.1640 Iteration: 2360; Percent complete: 59.0%; Average loss: 2.8391 Iteration: 2361; Percent complete: 59.0%; Average loss: 3.0450 Iteration: 2362; Percent complete: 59.1%; Average loss: 3.2066 Iteration: 2363; Percent complete: 59.1%; Average loss: 3.0557 Iteration: 2364; Percent complete: 59.1%; Average loss: 3.1378 Iteration: 2365; Percent complete: 59.1%; Average loss: 3.0553 Iteration: 2366; Percent complete: 59.2%; Average loss: 2.9069 Iteration: 2367; Percent complete: 59.2%; Average loss: 2.8837 Iteration: 2368; Percent complete: 59.2%; Average loss: 3.0619 Iteration: 2369; Percent complete: 59.2%; Average loss: 3.2510 Iteration: 2370; Percent complete: 59.2%; Average loss: 3.3935 Iteration: 2371; Percent complete: 59.3%; Average loss: 2.9976 Iteration: 2372; Percent complete: 59.3%; Average loss: 3.2120 Iteration: 2373; Percent complete: 59.3%; Average loss: 2.9935 Iteration: 2374; Percent complete: 59.4%; Average loss: 2.9533 Iteration: 2375; Percent complete: 59.4%; Average loss: 2.8794 Iteration: 2376; Percent complete: 59.4%; Average loss: 3.0426 Iteration: 2377; Percent complete: 59.4%; Average loss: 2.9115 Iteration: 2378; Percent complete: 59.5%; Average loss: 2.9513 Iteration: 2379; Percent complete: 59.5%; Average loss: 3.1140 Iteration: 2380; Percent complete: 59.5%; Average loss: 2.8890 Iteration: 2381; Percent complete: 59.5%; Average loss: 3.1780 Iteration: 2382; Percent complete: 59.6%; Average loss: 2.7383 Iteration: 2383; Percent complete: 59.6%; Average loss: 2.9650 Iteration: 2384; Percent complete: 59.6%; Average loss: 2.8571 Iteration: 2385; Percent complete: 59.6%; Average loss: 3.1086 Iteration: 2386; Percent complete: 59.7%; Average loss: 3.0087 Iteration: 2387; Percent complete: 59.7%; Average loss: 3.0994 Iteration: 2388; Percent complete: 59.7%; Average loss: 3.2480 Iteration: 2389; Percent complete: 59.7%; Average loss: 3.1875 Iteration: 2390; Percent complete: 59.8%; Average loss: 2.8676 Iteration: 2391; Percent complete: 59.8%; Average loss: 2.9218 Iteration: 2392; Percent complete: 59.8%; Average loss: 3.1167 Iteration: 2393; Percent complete: 59.8%; Average loss: 2.9529 Iteration: 2394; Percent complete: 59.9%; Average loss: 2.8647 Iteration: 2395; Percent complete: 59.9%; Average loss: 3.0451 Iteration: 2396; Percent complete: 59.9%; Average loss: 3.0807 Iteration: 2397; Percent complete: 59.9%; Average loss: 3.1120 Iteration: 2398; Percent complete: 60.0%; Average loss: 2.9271 Iteration: 2399; Percent complete: 60.0%; Average loss: 3.1743 Iteration: 2400; Percent complete: 60.0%; Average loss: 3.0094 Iteration: 2401; Percent complete: 60.0%; Average loss: 2.8377 Iteration: 2402; Percent complete: 60.1%; Average loss: 3.2161 Iteration: 2403; Percent complete: 60.1%; Average loss: 2.9270 Iteration: 2404; Percent complete: 60.1%; Average loss: 2.8011 Iteration: 2405; Percent complete: 60.1%; Average loss: 3.1273 Iteration: 2406; Percent complete: 60.2%; Average loss: 2.8971 Iteration: 2407; Percent complete: 60.2%; Average loss: 2.8312 Iteration: 2408; Percent complete: 60.2%; Average loss: 3.0171 Iteration: 2409; Percent complete: 60.2%; Average loss: 3.0027 Iteration: 2410; Percent complete: 60.2%; Average loss: 2.6430 Iteration: 2411; Percent complete: 60.3%; Average loss: 2.8715 Iteration: 2412; Percent complete: 60.3%; Average loss: 2.9399 Iteration: 2413; Percent complete: 60.3%; Average loss: 3.1935 Iteration: 2414; Percent complete: 60.4%; Average loss: 2.9675 Iteration: 2415; Percent complete: 60.4%; Average loss: 3.0074 Iteration: 2416; Percent complete: 60.4%; Average loss: 2.9939 Iteration: 2417; Percent complete: 60.4%; Average loss: 3.3969 Iteration: 2418; Percent complete: 60.5%; Average loss: 2.9387 Iteration: 2419; Percent complete: 60.5%; Average loss: 2.7750 Iteration: 2420; Percent complete: 60.5%; Average loss: 3.0736 Iteration: 2421; Percent complete: 60.5%; Average loss: 2.9006 Iteration: 2422; Percent complete: 60.6%; Average loss: 3.1029 Iteration: 2423; Percent complete: 60.6%; Average loss: 3.0998 Iteration: 2424; Percent complete: 60.6%; Average loss: 3.1131 Iteration: 2425; Percent complete: 60.6%; Average loss: 3.0232 Iteration: 2426; Percent complete: 60.7%; Average loss: 3.0732 Iteration: 2427; Percent complete: 60.7%; Average loss: 3.3019 Iteration: 2428; Percent complete: 60.7%; Average loss: 3.2356 Iteration: 2429; Percent complete: 60.7%; Average loss: 3.0353 Iteration: 2430; Percent complete: 60.8%; Average loss: 3.2755 Iteration: 2431; Percent complete: 60.8%; Average loss: 3.0375 Iteration: 2432; Percent complete: 60.8%; Average loss: 3.1990 Iteration: 2433; Percent complete: 60.8%; Average loss: 3.0002 Iteration: 2434; Percent complete: 60.9%; Average loss: 3.0500 Iteration: 2435; Percent complete: 60.9%; Average loss: 3.2308 Iteration: 2436; Percent complete: 60.9%; Average loss: 2.9852 Iteration: 2437; Percent complete: 60.9%; Average loss: 3.0617 Iteration: 2438; Percent complete: 61.0%; Average loss: 2.8609 Iteration: 2439; Percent complete: 61.0%; Average loss: 2.9806 Iteration: 2440; Percent complete: 61.0%; Average loss: 3.3641 Iteration: 2441; Percent complete: 61.0%; Average loss: 2.9916 Iteration: 2442; Percent complete: 61.1%; Average loss: 3.3711 Iteration: 2443; Percent complete: 61.1%; Average loss: 3.1155 Iteration: 2444; Percent complete: 61.1%; Average loss: 3.0574 Iteration: 2445; Percent complete: 61.1%; Average loss: 3.1826 Iteration: 2446; Percent complete: 61.2%; Average loss: 2.9566 Iteration: 2447; Percent complete: 61.2%; Average loss: 2.7333 Iteration: 2448; Percent complete: 61.2%; Average loss: 3.1451 Iteration: 2449; Percent complete: 61.2%; Average loss: 3.0976 Iteration: 2450; Percent complete: 61.3%; Average loss: 3.0232 Iteration: 2451; Percent complete: 61.3%; Average loss: 3.0026 Iteration: 2452; Percent complete: 61.3%; Average loss: 3.1373 Iteration: 2453; Percent complete: 61.3%; Average loss: 2.9024 Iteration: 2454; Percent complete: 61.4%; Average loss: 3.0756 Iteration: 2455; Percent complete: 61.4%; Average loss: 3.1137 Iteration: 2456; Percent complete: 61.4%; Average loss: 2.9910 Iteration: 2457; Percent complete: 61.4%; Average loss: 3.0233 Iteration: 2458; Percent complete: 61.5%; Average loss: 3.1661 Iteration: 2459; Percent complete: 61.5%; Average loss: 2.8733 Iteration: 2460; Percent complete: 61.5%; Average loss: 3.1447 Iteration: 2461; Percent complete: 61.5%; Average loss: 3.0538 Iteration: 2462; Percent complete: 61.6%; Average loss: 2.9841 Iteration: 2463; Percent complete: 61.6%; Average loss: 2.9982 Iteration: 2464; Percent complete: 61.6%; Average loss: 2.8630 Iteration: 2465; Percent complete: 61.6%; Average loss: 2.9709 Iteration: 2466; Percent complete: 61.7%; Average loss: 3.0314 Iteration: 2467; Percent complete: 61.7%; Average loss: 3.1028 Iteration: 2468; Percent complete: 61.7%; Average loss: 3.0246 Iteration: 2469; Percent complete: 61.7%; Average loss: 3.0360 Iteration: 2470; Percent complete: 61.8%; Average loss: 2.8862 Iteration: 2471; Percent complete: 61.8%; Average loss: 2.9073 Iteration: 2472; Percent complete: 61.8%; Average loss: 3.1232 Iteration: 2473; Percent complete: 61.8%; Average loss: 2.7387 Iteration: 2474; Percent complete: 61.9%; Average loss: 3.0861 Iteration: 2475; Percent complete: 61.9%; Average loss: 2.9135 Iteration: 2476; Percent complete: 61.9%; Average loss: 3.0335 Iteration: 2477; Percent complete: 61.9%; Average loss: 3.3159 Iteration: 2478; Percent complete: 62.0%; Average loss: 2.8861 Iteration: 2479; Percent complete: 62.0%; Average loss: 3.0872 Iteration: 2480; Percent complete: 62.0%; Average loss: 3.0781 Iteration: 2481; Percent complete: 62.0%; Average loss: 2.9064 Iteration: 2482; Percent complete: 62.1%; Average loss: 3.0143 Iteration: 2483; Percent complete: 62.1%; Average loss: 3.2912 Iteration: 2484; Percent complete: 62.1%; Average loss: 3.0475 Iteration: 2485; Percent complete: 62.1%; Average loss: 3.0903 Iteration: 2486; Percent complete: 62.2%; Average loss: 2.8582 Iteration: 2487; Percent complete: 62.2%; Average loss: 2.8165 Iteration: 2488; Percent complete: 62.2%; Average loss: 2.8773 Iteration: 2489; Percent complete: 62.2%; Average loss: 3.0432 Iteration: 2490; Percent complete: 62.3%; Average loss: 3.1007 Iteration: 2491; Percent complete: 62.3%; Average loss: 2.7417 Iteration: 2492; Percent complete: 62.3%; Average loss: 3.0895 Iteration: 2493; Percent complete: 62.3%; Average loss: 3.1848 Iteration: 2494; Percent complete: 62.4%; Average loss: 3.1985 Iteration: 2495; Percent complete: 62.4%; Average loss: 2.9238 Iteration: 2496; Percent complete: 62.4%; Average loss: 2.7739 Iteration: 2497; Percent complete: 62.4%; Average loss: 2.9055 Iteration: 2498; Percent complete: 62.5%; Average loss: 3.3160 Iteration: 2499; Percent complete: 62.5%; Average loss: 3.3404 Iteration: 2500; Percent complete: 62.5%; Average loss: 2.6977 Iteration: 2501; Percent complete: 62.5%; Average loss: 2.9520 Iteration: 2502; Percent complete: 62.5%; Average loss: 3.0517 Iteration: 2503; Percent complete: 62.6%; Average loss: 2.8219 Iteration: 2504; Percent complete: 62.6%; Average loss: 2.8717 Iteration: 2505; Percent complete: 62.6%; Average loss: 3.0007 Iteration: 2506; Percent complete: 62.6%; Average loss: 3.0205 Iteration: 2507; Percent complete: 62.7%; Average loss: 2.9983 Iteration: 2508; Percent complete: 62.7%; Average loss: 3.1072 Iteration: 2509; Percent complete: 62.7%; Average loss: 3.0707 Iteration: 2510; Percent complete: 62.7%; Average loss: 3.0299 Iteration: 2511; Percent complete: 62.8%; Average loss: 3.0130 Iteration: 2512; Percent complete: 62.8%; Average loss: 3.1158 Iteration: 2513; Percent complete: 62.8%; Average loss: 2.9568 Iteration: 2514; Percent complete: 62.8%; Average loss: 2.8753 Iteration: 2515; Percent complete: 62.9%; Average loss: 2.9292 Iteration: 2516; Percent complete: 62.9%; Average loss: 3.1571 Iteration: 2517; Percent complete: 62.9%; Average loss: 3.0246 Iteration: 2518; Percent complete: 62.9%; Average loss: 2.8949 Iteration: 2519; Percent complete: 63.0%; Average loss: 2.8011 Iteration: 2520; Percent complete: 63.0%; Average loss: 2.9427 Iteration: 2521; Percent complete: 63.0%; Average loss: 2.9891 Iteration: 2522; Percent complete: 63.0%; Average loss: 3.0095 Iteration: 2523; Percent complete: 63.1%; Average loss: 3.0353 Iteration: 2524; Percent complete: 63.1%; Average loss: 3.2749 Iteration: 2525; Percent complete: 63.1%; Average loss: 2.9561 Iteration: 2526; Percent complete: 63.1%; Average loss: 3.2806 Iteration: 2527; Percent complete: 63.2%; Average loss: 3.0784 Iteration: 2528; Percent complete: 63.2%; Average loss: 3.1953 Iteration: 2529; Percent complete: 63.2%; Average loss: 2.8902 Iteration: 2530; Percent complete: 63.2%; Average loss: 2.9831 Iteration: 2531; Percent complete: 63.3%; Average loss: 3.0368 Iteration: 2532; Percent complete: 63.3%; Average loss: 2.9397 Iteration: 2533; Percent complete: 63.3%; Average loss: 3.1506 Iteration: 2534; Percent complete: 63.3%; Average loss: 2.8618 Iteration: 2535; Percent complete: 63.4%; Average loss: 3.0549 Iteration: 2536; Percent complete: 63.4%; Average loss: 3.1664 Iteration: 2537; Percent complete: 63.4%; Average loss: 2.7106 Iteration: 2538; Percent complete: 63.4%; Average loss: 3.1740 Iteration: 2539; Percent complete: 63.5%; Average loss: 3.0385 Iteration: 2540; Percent complete: 63.5%; Average loss: 3.2137 Iteration: 2541; Percent complete: 63.5%; Average loss: 3.0837 Iteration: 2542; Percent complete: 63.5%; Average loss: 3.0074 Iteration: 2543; Percent complete: 63.6%; Average loss: 2.6155 Iteration: 2544; Percent complete: 63.6%; Average loss: 3.2696 Iteration: 2545; Percent complete: 63.6%; Average loss: 2.8813 Iteration: 2546; Percent complete: 63.6%; Average loss: 3.0037 Iteration: 2547; Percent complete: 63.7%; Average loss: 2.9897 Iteration: 2548; Percent complete: 63.7%; Average loss: 3.0735 Iteration: 2549; Percent complete: 63.7%; Average loss: 3.0650 Iteration: 2550; Percent complete: 63.7%; Average loss: 3.1080 Iteration: 2551; Percent complete: 63.8%; Average loss: 2.8200 Iteration: 2552; Percent complete: 63.8%; Average loss: 2.9047 Iteration: 2553; Percent complete: 63.8%; Average loss: 3.2542 Iteration: 2554; Percent complete: 63.8%; Average loss: 2.9342 Iteration: 2555; Percent complete: 63.9%; Average loss: 3.0346 Iteration: 2556; Percent complete: 63.9%; Average loss: 2.8772 Iteration: 2557; Percent complete: 63.9%; Average loss: 3.1414 Iteration: 2558; Percent complete: 63.9%; Average loss: 3.0918 Iteration: 2559; Percent complete: 64.0%; Average loss: 3.0061 Iteration: 2560; Percent complete: 64.0%; Average loss: 2.8662 Iteration: 2561; Percent complete: 64.0%; Average loss: 3.2474 Iteration: 2562; Percent complete: 64.0%; Average loss: 3.0393 Iteration: 2563; Percent complete: 64.1%; Average loss: 3.0809 Iteration: 2564; Percent complete: 64.1%; Average loss: 2.8227 Iteration: 2565; Percent complete: 64.1%; Average loss: 3.2218 Iteration: 2566; Percent complete: 64.1%; Average loss: 2.9595 Iteration: 2567; Percent complete: 64.2%; Average loss: 2.9379 Iteration: 2568; Percent complete: 64.2%; Average loss: 2.9421 Iteration: 2569; Percent complete: 64.2%; Average loss: 3.0058 Iteration: 2570; Percent complete: 64.2%; Average loss: 3.0435 Iteration: 2571; Percent complete: 64.3%; Average loss: 3.0110 Iteration: 2572; Percent complete: 64.3%; Average loss: 2.8702 Iteration: 2573; Percent complete: 64.3%; Average loss: 2.9370 Iteration: 2574; Percent complete: 64.3%; Average loss: 3.0840 Iteration: 2575; Percent complete: 64.4%; Average loss: 2.9967 Iteration: 2576; Percent complete: 64.4%; Average loss: 3.0738 Iteration: 2577; Percent complete: 64.4%; Average loss: 2.9509 Iteration: 2578; Percent complete: 64.5%; Average loss: 2.8010 Iteration: 2579; Percent complete: 64.5%; Average loss: 2.8671 Iteration: 2580; Percent complete: 64.5%; Average loss: 3.0494 Iteration: 2581; Percent complete: 64.5%; Average loss: 3.2313 Iteration: 2582; Percent complete: 64.5%; Average loss: 2.8002 Iteration: 2583; Percent complete: 64.6%; Average loss: 2.8280 Iteration: 2584; Percent complete: 64.6%; Average loss: 2.7542 Iteration: 2585; Percent complete: 64.6%; Average loss: 3.0977 Iteration: 2586; Percent complete: 64.6%; Average loss: 3.0219 Iteration: 2587; Percent complete: 64.7%; Average loss: 3.0494 Iteration: 2588; Percent complete: 64.7%; Average loss: 2.8396 Iteration: 2589; Percent complete: 64.7%; Average loss: 3.1587 Iteration: 2590; Percent complete: 64.8%; Average loss: 3.2286 Iteration: 2591; Percent complete: 64.8%; Average loss: 3.1454 Iteration: 2592; Percent complete: 64.8%; Average loss: 2.9944 Iteration: 2593; Percent complete: 64.8%; Average loss: 3.0932 Iteration: 2594; Percent complete: 64.8%; Average loss: 2.8769 Iteration: 2595; Percent complete: 64.9%; Average loss: 2.9790 Iteration: 2596; Percent complete: 64.9%; Average loss: 3.0198 Iteration: 2597; Percent complete: 64.9%; Average loss: 2.8238 Iteration: 2598; Percent complete: 65.0%; Average loss: 3.0501 Iteration: 2599; Percent complete: 65.0%; Average loss: 3.1038 Iteration: 2600; Percent complete: 65.0%; Average loss: 2.9094 Iteration: 2601; Percent complete: 65.0%; Average loss: 2.9104 Iteration: 2602; Percent complete: 65.0%; Average loss: 3.0460 Iteration: 2603; Percent complete: 65.1%; Average loss: 3.0343 Iteration: 2604; Percent complete: 65.1%; Average loss: 3.2776 Iteration: 2605; Percent complete: 65.1%; Average loss: 3.1113 Iteration: 2606; Percent complete: 65.1%; Average loss: 3.2273 Iteration: 2607; Percent complete: 65.2%; Average loss: 3.2279 Iteration: 2608; Percent complete: 65.2%; Average loss: 3.0195 Iteration: 2609; Percent complete: 65.2%; Average loss: 3.2257 Iteration: 2610; Percent complete: 65.2%; Average loss: 3.0352 Iteration: 2611; Percent complete: 65.3%; Average loss: 3.0399 Iteration: 2612; Percent complete: 65.3%; Average loss: 2.9857 Iteration: 2613; Percent complete: 65.3%; Average loss: 2.9917 Iteration: 2614; Percent complete: 65.3%; Average loss: 3.0639 Iteration: 2615; Percent complete: 65.4%; Average loss: 2.6918 Iteration: 2616; Percent complete: 65.4%; Average loss: 3.0844 Iteration: 2617; Percent complete: 65.4%; Average loss: 2.8985 Iteration: 2618; Percent complete: 65.5%; Average loss: 2.9986 Iteration: 2619; Percent complete: 65.5%; Average loss: 2.8861 Iteration: 2620; Percent complete: 65.5%; Average loss: 3.1542 Iteration: 2621; Percent complete: 65.5%; Average loss: 2.9965 Iteration: 2622; Percent complete: 65.5%; Average loss: 2.7434 Iteration: 2623; Percent complete: 65.6%; Average loss: 3.2364 Iteration: 2624; Percent complete: 65.6%; Average loss: 3.0255 Iteration: 2625; Percent complete: 65.6%; Average loss: 2.9321 Iteration: 2626; Percent complete: 65.6%; Average loss: 3.0955 Iteration: 2627; Percent complete: 65.7%; Average loss: 2.8944 Iteration: 2628; Percent complete: 65.7%; Average loss: 3.0704 Iteration: 2629; Percent complete: 65.7%; Average loss: 2.8900 Iteration: 2630; Percent complete: 65.8%; Average loss: 2.8992 Iteration: 2631; Percent complete: 65.8%; Average loss: 2.8165 Iteration: 2632; Percent complete: 65.8%; Average loss: 2.9634 Iteration: 2633; Percent complete: 65.8%; Average loss: 3.3636 Iteration: 2634; Percent complete: 65.8%; Average loss: 3.0438 Iteration: 2635; Percent complete: 65.9%; Average loss: 2.8746 Iteration: 2636; Percent complete: 65.9%; Average loss: 3.1292 Iteration: 2637; Percent complete: 65.9%; Average loss: 2.8750 Iteration: 2638; Percent complete: 66.0%; Average loss: 2.7047 Iteration: 2639; Percent complete: 66.0%; Average loss: 2.9193 Iteration: 2640; Percent complete: 66.0%; Average loss: 2.9350 Iteration: 2641; Percent complete: 66.0%; Average loss: 2.8696 Iteration: 2642; Percent complete: 66.0%; Average loss: 3.2226 Iteration: 2643; Percent complete: 66.1%; Average loss: 2.8267 Iteration: 2644; Percent complete: 66.1%; Average loss: 2.9209 Iteration: 2645; Percent complete: 66.1%; Average loss: 3.1970 Iteration: 2646; Percent complete: 66.1%; Average loss: 2.9289 Iteration: 2647; Percent complete: 66.2%; Average loss: 2.9753 Iteration: 2648; Percent complete: 66.2%; Average loss: 3.0245 Iteration: 2649; Percent complete: 66.2%; Average loss: 2.7824 Iteration: 2650; Percent complete: 66.2%; Average loss: 2.8589 Iteration: 2651; Percent complete: 66.3%; Average loss: 3.2010 Iteration: 2652; Percent complete: 66.3%; Average loss: 3.1796 Iteration: 2653; Percent complete: 66.3%; Average loss: 3.1595 Iteration: 2654; Percent complete: 66.3%; Average loss: 2.9745 Iteration: 2655; Percent complete: 66.4%; Average loss: 3.2552 Iteration: 2656; Percent complete: 66.4%; Average loss: 3.0038 Iteration: 2657; Percent complete: 66.4%; Average loss: 2.9362 Iteration: 2658; Percent complete: 66.5%; Average loss: 2.8249 Iteration: 2659; Percent complete: 66.5%; Average loss: 3.0315 Iteration: 2660; Percent complete: 66.5%; Average loss: 3.2433 Iteration: 2661; Percent complete: 66.5%; Average loss: 2.8465 Iteration: 2662; Percent complete: 66.5%; Average loss: 3.0026 Iteration: 2663; Percent complete: 66.6%; Average loss: 2.7188 Iteration: 2664; Percent complete: 66.6%; Average loss: 2.9460 Iteration: 2665; Percent complete: 66.6%; Average loss: 3.0766 Iteration: 2666; Percent complete: 66.6%; Average loss: 2.9762 Iteration: 2667; Percent complete: 66.7%; Average loss: 2.9885 Iteration: 2668; Percent complete: 66.7%; Average loss: 2.9136 Iteration: 2669; Percent complete: 66.7%; Average loss: 2.9530 Iteration: 2670; Percent complete: 66.8%; Average loss: 3.1029 Iteration: 2671; Percent complete: 66.8%; Average loss: 2.9581 Iteration: 2672; Percent complete: 66.8%; Average loss: 2.8751 Iteration: 2673; Percent complete: 66.8%; Average loss: 3.0228 Iteration: 2674; Percent complete: 66.8%; Average loss: 2.9980 Iteration: 2675; Percent complete: 66.9%; Average loss: 3.1778 Iteration: 2676; Percent complete: 66.9%; Average loss: 3.0110 Iteration: 2677; Percent complete: 66.9%; Average loss: 2.9999 Iteration: 2678; Percent complete: 67.0%; Average loss: 2.8637 Iteration: 2679; Percent complete: 67.0%; Average loss: 2.7602 Iteration: 2680; Percent complete: 67.0%; Average loss: 2.9435 Iteration: 2681; Percent complete: 67.0%; Average loss: 2.9638 Iteration: 2682; Percent complete: 67.0%; Average loss: 2.9956 Iteration: 2683; Percent complete: 67.1%; Average loss: 2.6761 Iteration: 2684; Percent complete: 67.1%; Average loss: 2.9263 Iteration: 2685; Percent complete: 67.1%; Average loss: 2.8377 Iteration: 2686; Percent complete: 67.2%; Average loss: 2.8527 Iteration: 2687; Percent complete: 67.2%; Average loss: 2.9922 Iteration: 2688; Percent complete: 67.2%; Average loss: 2.9717 Iteration: 2689; Percent complete: 67.2%; Average loss: 3.0173 Iteration: 2690; Percent complete: 67.2%; Average loss: 2.9750 Iteration: 2691; Percent complete: 67.3%; Average loss: 3.0568 Iteration: 2692; Percent complete: 67.3%; Average loss: 2.9489 Iteration: 2693; Percent complete: 67.3%; Average loss: 3.0926 Iteration: 2694; Percent complete: 67.3%; Average loss: 2.7249 Iteration: 2695; Percent complete: 67.4%; Average loss: 3.0322 Iteration: 2696; Percent complete: 67.4%; Average loss: 2.9278 Iteration: 2697; Percent complete: 67.4%; Average loss: 2.8933 Iteration: 2698; Percent complete: 67.5%; Average loss: 3.0013 Iteration: 2699; Percent complete: 67.5%; Average loss: 2.9842 Iteration: 2700; Percent complete: 67.5%; Average loss: 2.8574 Iteration: 2701; Percent complete: 67.5%; Average loss: 2.9005 Iteration: 2702; Percent complete: 67.5%; Average loss: 2.7546 Iteration: 2703; Percent complete: 67.6%; Average loss: 2.9167 Iteration: 2704; Percent complete: 67.6%; Average loss: 2.9049 Iteration: 2705; Percent complete: 67.6%; Average loss: 3.0145 Iteration: 2706; Percent complete: 67.7%; Average loss: 2.8532 Iteration: 2707; Percent complete: 67.7%; Average loss: 3.0567 Iteration: 2708; Percent complete: 67.7%; Average loss: 3.2285 Iteration: 2709; Percent complete: 67.7%; Average loss: 2.8267 Iteration: 2710; Percent complete: 67.8%; Average loss: 3.0982 Iteration: 2711; Percent complete: 67.8%; Average loss: 2.8877 Iteration: 2712; Percent complete: 67.8%; Average loss: 2.9180 Iteration: 2713; Percent complete: 67.8%; Average loss: 3.1380 Iteration: 2714; Percent complete: 67.8%; Average loss: 2.7569 Iteration: 2715; Percent complete: 67.9%; Average loss: 3.1548 Iteration: 2716; Percent complete: 67.9%; Average loss: 2.8605 Iteration: 2717; Percent complete: 67.9%; Average loss: 3.0388 Iteration: 2718; Percent complete: 68.0%; Average loss: 2.8688 Iteration: 2719; Percent complete: 68.0%; Average loss: 2.9155 Iteration: 2720; Percent complete: 68.0%; Average loss: 2.9601 Iteration: 2721; Percent complete: 68.0%; Average loss: 3.1063 Iteration: 2722; Percent complete: 68.0%; Average loss: 2.9606 Iteration: 2723; Percent complete: 68.1%; Average loss: 3.0425 Iteration: 2724; Percent complete: 68.1%; Average loss: 3.0224 Iteration: 2725; Percent complete: 68.1%; Average loss: 2.9436 Iteration: 2726; Percent complete: 68.2%; Average loss: 2.9152 Iteration: 2727; Percent complete: 68.2%; Average loss: 2.7811 Iteration: 2728; Percent complete: 68.2%; Average loss: 2.9955 Iteration: 2729; Percent complete: 68.2%; Average loss: 3.0642 Iteration: 2730; Percent complete: 68.2%; Average loss: 2.7513 Iteration: 2731; Percent complete: 68.3%; Average loss: 2.9346 Iteration: 2732; Percent complete: 68.3%; Average loss: 2.8338 Iteration: 2733; Percent complete: 68.3%; Average loss: 2.9069 Iteration: 2734; Percent complete: 68.3%; Average loss: 3.0149 Iteration: 2735; Percent complete: 68.4%; Average loss: 3.1079 Iteration: 2736; Percent complete: 68.4%; Average loss: 2.9963 Iteration: 2737; Percent complete: 68.4%; Average loss: 2.9656 Iteration: 2738; Percent complete: 68.5%; Average loss: 3.0681 Iteration: 2739; Percent complete: 68.5%; Average loss: 2.9722 Iteration: 2740; Percent complete: 68.5%; Average loss: 2.9638 Iteration: 2741; Percent complete: 68.5%; Average loss: 3.0068 Iteration: 2742; Percent complete: 68.5%; Average loss: 2.8337 Iteration: 2743; Percent complete: 68.6%; Average loss: 2.9058 Iteration: 2744; Percent complete: 68.6%; Average loss: 3.0459 Iteration: 2745; Percent complete: 68.6%; Average loss: 2.5502 Iteration: 2746; Percent complete: 68.7%; Average loss: 3.0915 Iteration: 2747; Percent complete: 68.7%; Average loss: 2.9855 Iteration: 2748; Percent complete: 68.7%; Average loss: 2.9987 Iteration: 2749; Percent complete: 68.7%; Average loss: 2.8893 Iteration: 2750; Percent complete: 68.8%; Average loss: 2.7861 Iteration: 2751; Percent complete: 68.8%; Average loss: 3.0766 Iteration: 2752; Percent complete: 68.8%; Average loss: 2.8763 Iteration: 2753; Percent complete: 68.8%; Average loss: 3.0608 Iteration: 2754; Percent complete: 68.8%; Average loss: 2.8179 Iteration: 2755; Percent complete: 68.9%; Average loss: 2.5586 Iteration: 2756; Percent complete: 68.9%; Average loss: 2.8236 Iteration: 2757; Percent complete: 68.9%; Average loss: 3.0911 Iteration: 2758; Percent complete: 69.0%; Average loss: 3.0338 Iteration: 2759; Percent complete: 69.0%; Average loss: 2.9071 Iteration: 2760; Percent complete: 69.0%; Average loss: 3.0498 Iteration: 2761; Percent complete: 69.0%; Average loss: 2.9858 Iteration: 2762; Percent complete: 69.0%; Average loss: 2.8432 Iteration: 2763; Percent complete: 69.1%; Average loss: 2.9627 Iteration: 2764; Percent complete: 69.1%; Average loss: 3.0823 Iteration: 2765; Percent complete: 69.1%; Average loss: 2.8110 Iteration: 2766; Percent complete: 69.2%; Average loss: 3.2241 Iteration: 2767; Percent complete: 69.2%; Average loss: 2.8854 Iteration: 2768; Percent complete: 69.2%; Average loss: 2.9809 Iteration: 2769; Percent complete: 69.2%; Average loss: 2.6752 Iteration: 2770; Percent complete: 69.2%; Average loss: 2.7684 Iteration: 2771; Percent complete: 69.3%; Average loss: 2.6598 Iteration: 2772; Percent complete: 69.3%; Average loss: 3.0240 Iteration: 2773; Percent complete: 69.3%; Average loss: 2.9068 Iteration: 2774; Percent complete: 69.3%; Average loss: 2.9570 Iteration: 2775; Percent complete: 69.4%; Average loss: 2.9165 Iteration: 2776; Percent complete: 69.4%; Average loss: 2.9637 Iteration: 2777; Percent complete: 69.4%; Average loss: 3.2319 Iteration: 2778; Percent complete: 69.5%; Average loss: 3.2035 Iteration: 2779; Percent complete: 69.5%; Average loss: 2.9817 Iteration: 2780; Percent complete: 69.5%; Average loss: 2.9455 Iteration: 2781; Percent complete: 69.5%; Average loss: 2.9081 Iteration: 2782; Percent complete: 69.5%; Average loss: 2.9686 Iteration: 2783; Percent complete: 69.6%; Average loss: 2.8813 Iteration: 2784; Percent complete: 69.6%; Average loss: 3.0033 Iteration: 2785; Percent complete: 69.6%; Average loss: 2.8987 Iteration: 2786; Percent complete: 69.7%; Average loss: 3.1428 Iteration: 2787; Percent complete: 69.7%; Average loss: 2.8328 Iteration: 2788; Percent complete: 69.7%; Average loss: 2.8144 Iteration: 2789; Percent complete: 69.7%; Average loss: 3.1540 Iteration: 2790; Percent complete: 69.8%; Average loss: 3.1196 Iteration: 2791; Percent complete: 69.8%; Average loss: 3.0040 Iteration: 2792; Percent complete: 69.8%; Average loss: 3.0600 Iteration: 2793; Percent complete: 69.8%; Average loss: 2.9361 Iteration: 2794; Percent complete: 69.8%; Average loss: 3.0799 Iteration: 2795; Percent complete: 69.9%; Average loss: 3.0402 Iteration: 2796; Percent complete: 69.9%; Average loss: 2.7618 Iteration: 2797; Percent complete: 69.9%; Average loss: 2.7964 Iteration: 2798; Percent complete: 70.0%; Average loss: 2.8617 Iteration: 2799; Percent complete: 70.0%; Average loss: 2.9878 Iteration: 2800; Percent complete: 70.0%; Average loss: 2.7504 Iteration: 2801; Percent complete: 70.0%; Average loss: 2.8580 Iteration: 2802; Percent complete: 70.0%; Average loss: 2.8685 Iteration: 2803; Percent complete: 70.1%; Average loss: 2.7516 Iteration: 2804; Percent complete: 70.1%; Average loss: 2.7257 Iteration: 2805; Percent complete: 70.1%; Average loss: 2.9471 Iteration: 2806; Percent complete: 70.2%; Average loss: 2.8982 Iteration: 2807; Percent complete: 70.2%; Average loss: 2.9259 Iteration: 2808; Percent complete: 70.2%; Average loss: 3.0059 Iteration: 2809; Percent complete: 70.2%; Average loss: 2.7959 Iteration: 2810; Percent complete: 70.2%; Average loss: 2.6522 Iteration: 2811; Percent complete: 70.3%; Average loss: 2.9103 Iteration: 2812; Percent complete: 70.3%; Average loss: 3.0946 Iteration: 2813; Percent complete: 70.3%; Average loss: 2.8603 Iteration: 2814; Percent complete: 70.3%; Average loss: 2.8077 Iteration: 2815; Percent complete: 70.4%; Average loss: 2.9464 Iteration: 2816; Percent complete: 70.4%; Average loss: 2.8726 Iteration: 2817; Percent complete: 70.4%; Average loss: 3.2415 Iteration: 2818; Percent complete: 70.5%; Average loss: 2.9972 Iteration: 2819; Percent complete: 70.5%; Average loss: 2.8944 Iteration: 2820; Percent complete: 70.5%; Average loss: 3.0370 Iteration: 2821; Percent complete: 70.5%; Average loss: 2.8722 Iteration: 2822; Percent complete: 70.5%; Average loss: 3.0710 Iteration: 2823; Percent complete: 70.6%; Average loss: 2.7812 Iteration: 2824; Percent complete: 70.6%; Average loss: 2.9932 Iteration: 2825; Percent complete: 70.6%; Average loss: 3.0553 Iteration: 2826; Percent complete: 70.7%; Average loss: 3.1295 Iteration: 2827; Percent complete: 70.7%; Average loss: 3.0668 Iteration: 2828; Percent complete: 70.7%; Average loss: 3.0899 Iteration: 2829; Percent complete: 70.7%; Average loss: 2.7914 Iteration: 2830; Percent complete: 70.8%; Average loss: 2.8199 Iteration: 2831; Percent complete: 70.8%; Average loss: 2.9094 Iteration: 2832; Percent complete: 70.8%; Average loss: 2.9319 Iteration: 2833; Percent complete: 70.8%; Average loss: 3.0194 Iteration: 2834; Percent complete: 70.9%; Average loss: 2.6769 Iteration: 2835; Percent complete: 70.9%; Average loss: 2.8394 Iteration: 2836; Percent complete: 70.9%; Average loss: 3.0220 Iteration: 2837; Percent complete: 70.9%; Average loss: 2.8327 Iteration: 2838; Percent complete: 71.0%; Average loss: 2.7978 Iteration: 2839; Percent complete: 71.0%; Average loss: 2.8368 Iteration: 2840; Percent complete: 71.0%; Average loss: 2.9183 Iteration: 2841; Percent complete: 71.0%; Average loss: 3.0764 Iteration: 2842; Percent complete: 71.0%; Average loss: 2.9923 Iteration: 2843; Percent complete: 71.1%; Average loss: 2.9126 Iteration: 2844; Percent complete: 71.1%; Average loss: 2.7952 Iteration: 2845; Percent complete: 71.1%; Average loss: 3.1327 Iteration: 2846; Percent complete: 71.2%; Average loss: 2.9010 Iteration: 2847; Percent complete: 71.2%; Average loss: 2.7796 Iteration: 2848; Percent complete: 71.2%; Average loss: 3.0164 Iteration: 2849; Percent complete: 71.2%; Average loss: 2.9841 Iteration: 2850; Percent complete: 71.2%; Average loss: 2.9307 Iteration: 2851; Percent complete: 71.3%; Average loss: 2.9832 Iteration: 2852; Percent complete: 71.3%; Average loss: 2.9146 Iteration: 2853; Percent complete: 71.3%; Average loss: 2.8312 Iteration: 2854; Percent complete: 71.4%; Average loss: 2.9593 Iteration: 2855; Percent complete: 71.4%; Average loss: 2.7897 Iteration: 2856; Percent complete: 71.4%; Average loss: 2.8131 Iteration: 2857; Percent complete: 71.4%; Average loss: 2.8403 Iteration: 2858; Percent complete: 71.5%; Average loss: 2.7888 Iteration: 2859; Percent complete: 71.5%; Average loss: 2.8594 Iteration: 2860; Percent complete: 71.5%; Average loss: 2.8666 Iteration: 2861; Percent complete: 71.5%; Average loss: 2.9241 Iteration: 2862; Percent complete: 71.5%; Average loss: 2.7550 Iteration: 2863; Percent complete: 71.6%; Average loss: 2.7998 Iteration: 2864; Percent complete: 71.6%; Average loss: 3.0297 Iteration: 2865; Percent complete: 71.6%; Average loss: 2.8772 Iteration: 2866; Percent complete: 71.7%; Average loss: 3.0335 Iteration: 2867; Percent complete: 71.7%; Average loss: 2.9476 Iteration: 2868; Percent complete: 71.7%; Average loss: 3.0405 Iteration: 2869; Percent complete: 71.7%; Average loss: 2.7103 Iteration: 2870; Percent complete: 71.8%; Average loss: 2.8975 Iteration: 2871; Percent complete: 71.8%; Average loss: 3.2626 Iteration: 2872; Percent complete: 71.8%; Average loss: 2.6274 Iteration: 2873; Percent complete: 71.8%; Average loss: 3.0785 Iteration: 2874; Percent complete: 71.9%; Average loss: 3.1895 Iteration: 2875; Percent complete: 71.9%; Average loss: 2.9454 Iteration: 2876; Percent complete: 71.9%; Average loss: 2.9936 Iteration: 2877; Percent complete: 71.9%; Average loss: 2.6867 Iteration: 2878; Percent complete: 72.0%; Average loss: 2.8551 Iteration: 2879; Percent complete: 72.0%; Average loss: 3.0061 Iteration: 2880; Percent complete: 72.0%; Average loss: 2.8279 Iteration: 2881; Percent complete: 72.0%; Average loss: 2.9551 Iteration: 2882; Percent complete: 72.0%; Average loss: 3.1905 Iteration: 2883; Percent complete: 72.1%; Average loss: 2.7876 Iteration: 2884; Percent complete: 72.1%; Average loss: 2.7943 Iteration: 2885; Percent complete: 72.1%; Average loss: 3.0628 Iteration: 2886; Percent complete: 72.2%; Average loss: 2.9779 Iteration: 2887; Percent complete: 72.2%; Average loss: 3.0812 Iteration: 2888; Percent complete: 72.2%; Average loss: 2.9815 Iteration: 2889; Percent complete: 72.2%; Average loss: 2.8360 Iteration: 2890; Percent complete: 72.2%; Average loss: 2.7097 Iteration: 2891; Percent complete: 72.3%; Average loss: 2.9955 Iteration: 2892; Percent complete: 72.3%; Average loss: 2.9710 Iteration: 2893; Percent complete: 72.3%; Average loss: 2.9034 Iteration: 2894; Percent complete: 72.4%; Average loss: 2.8781 Iteration: 2895; Percent complete: 72.4%; Average loss: 2.9648 Iteration: 2896; Percent complete: 72.4%; Average loss: 2.8218 Iteration: 2897; Percent complete: 72.4%; Average loss: 2.9542 Iteration: 2898; Percent complete: 72.5%; Average loss: 2.8892 Iteration: 2899; Percent complete: 72.5%; Average loss: 3.1090 Iteration: 2900; Percent complete: 72.5%; Average loss: 2.9539 Iteration: 2901; Percent complete: 72.5%; Average loss: 2.9935 Iteration: 2902; Percent complete: 72.5%; Average loss: 2.6629 Iteration: 2903; Percent complete: 72.6%; Average loss: 2.9627 Iteration: 2904; Percent complete: 72.6%; Average loss: 2.8354 Iteration: 2905; Percent complete: 72.6%; Average loss: 2.9876 Iteration: 2906; Percent complete: 72.7%; Average loss: 2.8022 Iteration: 2907; Percent complete: 72.7%; Average loss: 3.1498 Iteration: 2908; Percent complete: 72.7%; Average loss: 2.9000 Iteration: 2909; Percent complete: 72.7%; Average loss: 3.0950 Iteration: 2910; Percent complete: 72.8%; Average loss: 2.7017 Iteration: 2911; Percent complete: 72.8%; Average loss: 2.8750 Iteration: 2912; Percent complete: 72.8%; Average loss: 2.9354 Iteration: 2913; Percent complete: 72.8%; Average loss: 3.0945 Iteration: 2914; Percent complete: 72.9%; Average loss: 3.0731 Iteration: 2915; Percent complete: 72.9%; Average loss: 2.9518 Iteration: 2916; Percent complete: 72.9%; Average loss: 2.8993 Iteration: 2917; Percent complete: 72.9%; Average loss: 2.7700 Iteration: 2918; Percent complete: 73.0%; Average loss: 2.9766 Iteration: 2919; Percent complete: 73.0%; Average loss: 2.7121 Iteration: 2920; Percent complete: 73.0%; Average loss: 2.9228 Iteration: 2921; Percent complete: 73.0%; Average loss: 3.0062 Iteration: 2922; Percent complete: 73.0%; Average loss: 2.9907 Iteration: 2923; Percent complete: 73.1%; Average loss: 2.7383 Iteration: 2924; Percent complete: 73.1%; Average loss: 2.8742 Iteration: 2925; Percent complete: 73.1%; Average loss: 2.7704 Iteration: 2926; Percent complete: 73.2%; Average loss: 3.0878 Iteration: 2927; Percent complete: 73.2%; Average loss: 3.0383 Iteration: 2928; Percent complete: 73.2%; Average loss: 3.0491 Iteration: 2929; Percent complete: 73.2%; Average loss: 2.8969 Iteration: 2930; Percent complete: 73.2%; Average loss: 2.8859 Iteration: 2931; Percent complete: 73.3%; Average loss: 2.8969 Iteration: 2932; Percent complete: 73.3%; Average loss: 3.0829 Iteration: 2933; Percent complete: 73.3%; Average loss: 2.8648 Iteration: 2934; Percent complete: 73.4%; Average loss: 2.7475 Iteration: 2935; Percent complete: 73.4%; Average loss: 3.4020 Iteration: 2936; Percent complete: 73.4%; Average loss: 2.7819 Iteration: 2937; Percent complete: 73.4%; Average loss: 2.9354 Iteration: 2938; Percent complete: 73.5%; Average loss: 2.8160 Iteration: 2939; Percent complete: 73.5%; Average loss: 2.7817 Iteration: 2940; Percent complete: 73.5%; Average loss: 2.6799 Iteration: 2941; Percent complete: 73.5%; Average loss: 2.9063 Iteration: 2942; Percent complete: 73.6%; Average loss: 3.0292 Iteration: 2943; Percent complete: 73.6%; Average loss: 2.6448 Iteration: 2944; Percent complete: 73.6%; Average loss: 3.1482 Iteration: 2945; Percent complete: 73.6%; Average loss: 2.8997 Iteration: 2946; Percent complete: 73.7%; Average loss: 2.9586 Iteration: 2947; Percent complete: 73.7%; Average loss: 2.8941 Iteration: 2948; Percent complete: 73.7%; Average loss: 2.8444 Iteration: 2949; Percent complete: 73.7%; Average loss: 3.0576 Iteration: 2950; Percent complete: 73.8%; Average loss: 3.0507 Iteration: 2951; Percent complete: 73.8%; Average loss: 2.7605 Iteration: 2952; Percent complete: 73.8%; Average loss: 3.0545 Iteration: 2953; Percent complete: 73.8%; Average loss: 2.6876 Iteration: 2954; Percent complete: 73.9%; Average loss: 2.8494 Iteration: 2955; Percent complete: 73.9%; Average loss: 3.0959 Iteration: 2956; Percent complete: 73.9%; Average loss: 2.7438 Iteration: 2957; Percent complete: 73.9%; Average loss: 2.7802 Iteration: 2958; Percent complete: 74.0%; Average loss: 2.5535 Iteration: 2959; Percent complete: 74.0%; Average loss: 3.0118 Iteration: 2960; Percent complete: 74.0%; Average loss: 2.7613 Iteration: 2961; Percent complete: 74.0%; Average loss: 2.8696 Iteration: 2962; Percent complete: 74.1%; Average loss: 2.9938 Iteration: 2963; Percent complete: 74.1%; Average loss: 2.9215 Iteration: 2964; Percent complete: 74.1%; Average loss: 2.9053 Iteration: 2965; Percent complete: 74.1%; Average loss: 3.0314 Iteration: 2966; Percent complete: 74.2%; Average loss: 2.9922 Iteration: 2967; Percent complete: 74.2%; Average loss: 3.0080 Iteration: 2968; Percent complete: 74.2%; Average loss: 2.8914 Iteration: 2969; Percent complete: 74.2%; Average loss: 2.7259 Iteration: 2970; Percent complete: 74.2%; Average loss: 2.7464 Iteration: 2971; Percent complete: 74.3%; Average loss: 3.0716 Iteration: 2972; Percent complete: 74.3%; Average loss: 3.0906 Iteration: 2973; Percent complete: 74.3%; Average loss: 3.0441 Iteration: 2974; Percent complete: 74.4%; Average loss: 2.9178 Iteration: 2975; Percent complete: 74.4%; Average loss: 2.8915 Iteration: 2976; Percent complete: 74.4%; Average loss: 2.6867 Iteration: 2977; Percent complete: 74.4%; Average loss: 2.9638 Iteration: 2978; Percent complete: 74.5%; Average loss: 2.7096 Iteration: 2979; Percent complete: 74.5%; Average loss: 2.7930 Iteration: 2980; Percent complete: 74.5%; Average loss: 3.0344 Iteration: 2981; Percent complete: 74.5%; Average loss: 2.9932 Iteration: 2982; Percent complete: 74.6%; Average loss: 2.9410 Iteration: 2983; Percent complete: 74.6%; Average loss: 2.6728 Iteration: 2984; Percent complete: 74.6%; Average loss: 2.6244 Iteration: 2985; Percent complete: 74.6%; Average loss: 2.9220 Iteration: 2986; Percent complete: 74.7%; Average loss: 2.8158 Iteration: 2987; Percent complete: 74.7%; Average loss: 3.0462 Iteration: 2988; Percent complete: 74.7%; Average loss: 2.8750 Iteration: 2989; Percent complete: 74.7%; Average loss: 2.6633 Iteration: 2990; Percent complete: 74.8%; Average loss: 3.1500 Iteration: 2991; Percent complete: 74.8%; Average loss: 2.8159 Iteration: 2992; Percent complete: 74.8%; Average loss: 2.7437 Iteration: 2993; Percent complete: 74.8%; Average loss: 2.9685 Iteration: 2994; Percent complete: 74.9%; Average loss: 2.9506 Iteration: 2995; Percent complete: 74.9%; Average loss: 3.0977 Iteration: 2996; Percent complete: 74.9%; Average loss: 3.0489 Iteration: 2997; Percent complete: 74.9%; Average loss: 3.0138 Iteration: 2998; Percent complete: 75.0%; Average loss: 2.9891 Iteration: 2999; Percent complete: 75.0%; Average loss: 2.9733 Iteration: 3000; Percent complete: 75.0%; Average loss: 2.9357 Iteration: 3001; Percent complete: 75.0%; Average loss: 2.8092 Iteration: 3002; Percent complete: 75.0%; Average loss: 2.8705 Iteration: 3003; Percent complete: 75.1%; Average loss: 2.9608 Iteration: 3004; Percent complete: 75.1%; Average loss: 2.8782 Iteration: 3005; Percent complete: 75.1%; Average loss: 2.9570 Iteration: 3006; Percent complete: 75.1%; Average loss: 2.8634 Iteration: 3007; Percent complete: 75.2%; Average loss: 2.8878 Iteration: 3008; Percent complete: 75.2%; Average loss: 2.8199 Iteration: 3009; Percent complete: 75.2%; Average loss: 3.0053 Iteration: 3010; Percent complete: 75.2%; Average loss: 2.9324 Iteration: 3011; Percent complete: 75.3%; Average loss: 2.9002 Iteration: 3012; Percent complete: 75.3%; Average loss: 3.0516 Iteration: 3013; Percent complete: 75.3%; Average loss: 2.9159 Iteration: 3014; Percent complete: 75.3%; Average loss: 2.9177 Iteration: 3015; Percent complete: 75.4%; Average loss: 2.9223 Iteration: 3016; Percent complete: 75.4%; Average loss: 2.9078 Iteration: 3017; Percent complete: 75.4%; Average loss: 3.0318 Iteration: 3018; Percent complete: 75.4%; Average loss: 2.9476 Iteration: 3019; Percent complete: 75.5%; Average loss: 2.8563 Iteration: 3020; Percent complete: 75.5%; Average loss: 2.9087 Iteration: 3021; Percent complete: 75.5%; Average loss: 3.1355 Iteration: 3022; Percent complete: 75.5%; Average loss: 2.9919 Iteration: 3023; Percent complete: 75.6%; Average loss: 2.8395 Iteration: 3024; Percent complete: 75.6%; Average loss: 2.9893 Iteration: 3025; Percent complete: 75.6%; Average loss: 3.2082 Iteration: 3026; Percent complete: 75.6%; Average loss: 2.8194 Iteration: 3027; Percent complete: 75.7%; Average loss: 2.6423 Iteration: 3028; Percent complete: 75.7%; Average loss: 2.9622 Iteration: 3029; Percent complete: 75.7%; Average loss: 2.7897 Iteration: 3030; Percent complete: 75.8%; Average loss: 2.7169 Iteration: 3031; Percent complete: 75.8%; Average loss: 2.7786 Iteration: 3032; Percent complete: 75.8%; Average loss: 2.8090 Iteration: 3033; Percent complete: 75.8%; Average loss: 3.0403 Iteration: 3034; Percent complete: 75.8%; Average loss: 2.6833 Iteration: 3035; Percent complete: 75.9%; Average loss: 2.7725 Iteration: 3036; Percent complete: 75.9%; Average loss: 2.7909 Iteration: 3037; Percent complete: 75.9%; Average loss: 2.9215 Iteration: 3038; Percent complete: 75.9%; Average loss: 2.8880 Iteration: 3039; Percent complete: 76.0%; Average loss: 3.1363 Iteration: 3040; Percent complete: 76.0%; Average loss: 2.9210 Iteration: 3041; Percent complete: 76.0%; Average loss: 3.1818 Iteration: 3042; Percent complete: 76.0%; Average loss: 2.8843 Iteration: 3043; Percent complete: 76.1%; Average loss: 2.6322 Iteration: 3044; Percent complete: 76.1%; Average loss: 2.8226 Iteration: 3045; Percent complete: 76.1%; Average loss: 2.7612 Iteration: 3046; Percent complete: 76.1%; Average loss: 2.8491 Iteration: 3047; Percent complete: 76.2%; Average loss: 2.8252 Iteration: 3048; Percent complete: 76.2%; Average loss: 2.8672 Iteration: 3049; Percent complete: 76.2%; Average loss: 3.0725 Iteration: 3050; Percent complete: 76.2%; Average loss: 2.6355 Iteration: 3051; Percent complete: 76.3%; Average loss: 3.0267 Iteration: 3052; Percent complete: 76.3%; Average loss: 2.7756 Iteration: 3053; Percent complete: 76.3%; Average loss: 2.8999 Iteration: 3054; Percent complete: 76.3%; Average loss: 2.7297 Iteration: 3055; Percent complete: 76.4%; Average loss: 2.9017 Iteration: 3056; Percent complete: 76.4%; Average loss: 2.8614 Iteration: 3057; Percent complete: 76.4%; Average loss: 2.9103 Iteration: 3058; Percent complete: 76.4%; Average loss: 2.9228 Iteration: 3059; Percent complete: 76.5%; Average loss: 2.9057 Iteration: 3060; Percent complete: 76.5%; Average loss: 2.8180 Iteration: 3061; Percent complete: 76.5%; Average loss: 2.8708 Iteration: 3062; Percent complete: 76.5%; Average loss: 2.8121 Iteration: 3063; Percent complete: 76.6%; Average loss: 2.8796 Iteration: 3064; Percent complete: 76.6%; Average loss: 2.8903 Iteration: 3065; Percent complete: 76.6%; Average loss: 2.9282 Iteration: 3066; Percent complete: 76.6%; Average loss: 2.7941 Iteration: 3067; Percent complete: 76.7%; Average loss: 2.8311 Iteration: 3068; Percent complete: 76.7%; Average loss: 2.7265 Iteration: 3069; Percent complete: 76.7%; Average loss: 2.8328 Iteration: 3070; Percent complete: 76.8%; Average loss: 2.6190 Iteration: 3071; Percent complete: 76.8%; Average loss: 2.8061 Iteration: 3072; Percent complete: 76.8%; Average loss: 2.8321 Iteration: 3073; Percent complete: 76.8%; Average loss: 3.2295 Iteration: 3074; Percent complete: 76.8%; Average loss: 2.9620 Iteration: 3075; Percent complete: 76.9%; Average loss: 2.6073 Iteration: 3076; Percent complete: 76.9%; Average loss: 2.6937 Iteration: 3077; Percent complete: 76.9%; Average loss: 2.7879 Iteration: 3078; Percent complete: 77.0%; Average loss: 2.7540 Iteration: 3079; Percent complete: 77.0%; Average loss: 2.8467 Iteration: 3080; Percent complete: 77.0%; Average loss: 2.9061 Iteration: 3081; Percent complete: 77.0%; Average loss: 2.8316 Iteration: 3082; Percent complete: 77.0%; Average loss: 2.6672 Iteration: 3083; Percent complete: 77.1%; Average loss: 2.8990 Iteration: 3084; Percent complete: 77.1%; Average loss: 2.7778 Iteration: 3085; Percent complete: 77.1%; Average loss: 2.9400 Iteration: 3086; Percent complete: 77.1%; Average loss: 2.6451 Iteration: 3087; Percent complete: 77.2%; Average loss: 2.6923 Iteration: 3088; Percent complete: 77.2%; Average loss: 2.8971 Iteration: 3089; Percent complete: 77.2%; Average loss: 2.7108 Iteration: 3090; Percent complete: 77.2%; Average loss: 2.9312 Iteration: 3091; Percent complete: 77.3%; Average loss: 2.8065 Iteration: 3092; Percent complete: 77.3%; Average loss: 2.6729 Iteration: 3093; Percent complete: 77.3%; Average loss: 2.7770 Iteration: 3094; Percent complete: 77.3%; Average loss: 2.9211 Iteration: 3095; Percent complete: 77.4%; Average loss: 2.8852 Iteration: 3096; Percent complete: 77.4%; Average loss: 2.7850 Iteration: 3097; Percent complete: 77.4%; Average loss: 2.7957 Iteration: 3098; Percent complete: 77.5%; Average loss: 2.7621 Iteration: 3099; Percent complete: 77.5%; Average loss: 2.8050 Iteration: 3100; Percent complete: 77.5%; Average loss: 2.7617 Iteration: 3101; Percent complete: 77.5%; Average loss: 2.9096 Iteration: 3102; Percent complete: 77.5%; Average loss: 2.7998 Iteration: 3103; Percent complete: 77.6%; Average loss: 2.7245 Iteration: 3104; Percent complete: 77.6%; Average loss: 2.9060 Iteration: 3105; Percent complete: 77.6%; Average loss: 2.7620 Iteration: 3106; Percent complete: 77.6%; Average loss: 3.0188 Iteration: 3107; Percent complete: 77.7%; Average loss: 2.6290 Iteration: 3108; Percent complete: 77.7%; Average loss: 2.7612 Iteration: 3109; Percent complete: 77.7%; Average loss: 2.8992 Iteration: 3110; Percent complete: 77.8%; Average loss: 3.1011 Iteration: 3111; Percent complete: 77.8%; Average loss: 2.8471 Iteration: 3112; Percent complete: 77.8%; Average loss: 2.8335 Iteration: 3113; Percent complete: 77.8%; Average loss: 2.9999 Iteration: 3114; Percent complete: 77.8%; Average loss: 3.1837 Iteration: 3115; Percent complete: 77.9%; Average loss: 2.7911 Iteration: 3116; Percent complete: 77.9%; Average loss: 2.8297 Iteration: 3117; Percent complete: 77.9%; Average loss: 2.8399 Iteration: 3118; Percent complete: 78.0%; Average loss: 2.7940 Iteration: 3119; Percent complete: 78.0%; Average loss: 2.6527 Iteration: 3120; Percent complete: 78.0%; Average loss: 2.9140 Iteration: 3121; Percent complete: 78.0%; Average loss: 2.6164 Iteration: 3122; Percent complete: 78.0%; Average loss: 3.0743 Iteration: 3123; Percent complete: 78.1%; Average loss: 2.6136 Iteration: 3124; Percent complete: 78.1%; Average loss: 2.7134 Iteration: 3125; Percent complete: 78.1%; Average loss: 2.7049 Iteration: 3126; Percent complete: 78.1%; Average loss: 2.8565 Iteration: 3127; Percent complete: 78.2%; Average loss: 2.9278 Iteration: 3128; Percent complete: 78.2%; Average loss: 2.7772 Iteration: 3129; Percent complete: 78.2%; Average loss: 2.8107 Iteration: 3130; Percent complete: 78.2%; Average loss: 2.8486 Iteration: 3131; Percent complete: 78.3%; Average loss: 2.7065 Iteration: 3132; Percent complete: 78.3%; Average loss: 2.6814 Iteration: 3133; Percent complete: 78.3%; Average loss: 2.8686 Iteration: 3134; Percent complete: 78.3%; Average loss: 3.1442 Iteration: 3135; Percent complete: 78.4%; Average loss: 2.8334 Iteration: 3136; Percent complete: 78.4%; Average loss: 2.7880 Iteration: 3137; Percent complete: 78.4%; Average loss: 2.9166 Iteration: 3138; Percent complete: 78.5%; Average loss: 2.7405 Iteration: 3139; Percent complete: 78.5%; Average loss: 3.0819 Iteration: 3140; Percent complete: 78.5%; Average loss: 2.9483 Iteration: 3141; Percent complete: 78.5%; Average loss: 2.7527 Iteration: 3142; Percent complete: 78.5%; Average loss: 2.8566 Iteration: 3143; Percent complete: 78.6%; Average loss: 2.8262 Iteration: 3144; Percent complete: 78.6%; Average loss: 2.8635 Iteration: 3145; Percent complete: 78.6%; Average loss: 2.7892 Iteration: 3146; Percent complete: 78.6%; Average loss: 2.7291 Iteration: 3147; Percent complete: 78.7%; Average loss: 2.9425 Iteration: 3148; Percent complete: 78.7%; Average loss: 2.6925 Iteration: 3149; Percent complete: 78.7%; Average loss: 2.6886 Iteration: 3150; Percent complete: 78.8%; Average loss: 2.8279 Iteration: 3151; Percent complete: 78.8%; Average loss: 2.7638 Iteration: 3152; Percent complete: 78.8%; Average loss: 2.7155 Iteration: 3153; Percent complete: 78.8%; Average loss: 2.6350 Iteration: 3154; Percent complete: 78.8%; Average loss: 3.0614 Iteration: 3155; Percent complete: 78.9%; Average loss: 2.8030 Iteration: 3156; Percent complete: 78.9%; Average loss: 2.8196 Iteration: 3157; Percent complete: 78.9%; Average loss: 2.8922 Iteration: 3158; Percent complete: 79.0%; Average loss: 2.8866 Iteration: 3159; Percent complete: 79.0%; Average loss: 2.8563 Iteration: 3160; Percent complete: 79.0%; Average loss: 2.7527 Iteration: 3161; Percent complete: 79.0%; Average loss: 2.8517 Iteration: 3162; Percent complete: 79.0%; Average loss: 2.5093 Iteration: 3163; Percent complete: 79.1%; Average loss: 3.0855 Iteration: 3164; Percent complete: 79.1%; Average loss: 2.8036 Iteration: 3165; Percent complete: 79.1%; Average loss: 3.0055 Iteration: 3166; Percent complete: 79.1%; Average loss: 2.7585 Iteration: 3167; Percent complete: 79.2%; Average loss: 2.9728 Iteration: 3168; Percent complete: 79.2%; Average loss: 2.8109 Iteration: 3169; Percent complete: 79.2%; Average loss: 2.6927 Iteration: 3170; Percent complete: 79.2%; Average loss: 2.9480 Iteration: 3171; Percent complete: 79.3%; Average loss: 2.8216 Iteration: 3172; Percent complete: 79.3%; Average loss: 2.7929 Iteration: 3173; Percent complete: 79.3%; Average loss: 2.9728 Iteration: 3174; Percent complete: 79.3%; Average loss: 2.7576 Iteration: 3175; Percent complete: 79.4%; Average loss: 2.8780 Iteration: 3176; Percent complete: 79.4%; Average loss: 2.8704 Iteration: 3177; Percent complete: 79.4%; Average loss: 2.7198 Iteration: 3178; Percent complete: 79.5%; Average loss: 3.0021 Iteration: 3179; Percent complete: 79.5%; Average loss: 2.7362 Iteration: 3180; Percent complete: 79.5%; Average loss: 2.8147 Iteration: 3181; Percent complete: 79.5%; Average loss: 2.6615 Iteration: 3182; Percent complete: 79.5%; Average loss: 2.7305 Iteration: 3183; Percent complete: 79.6%; Average loss: 2.7453 Iteration: 3184; Percent complete: 79.6%; Average loss: 2.8089 Iteration: 3185; Percent complete: 79.6%; Average loss: 2.9210 Iteration: 3186; Percent complete: 79.7%; Average loss: 2.5632 Iteration: 3187; Percent complete: 79.7%; Average loss: 2.9106 Iteration: 3188; Percent complete: 79.7%; Average loss: 3.0693 Iteration: 3189; Percent complete: 79.7%; Average loss: 2.7385 Iteration: 3190; Percent complete: 79.8%; Average loss: 3.0175 Iteration: 3191; Percent complete: 79.8%; Average loss: 2.9093 Iteration: 3192; Percent complete: 79.8%; Average loss: 2.8568 Iteration: 3193; Percent complete: 79.8%; Average loss: 2.7054 Iteration: 3194; Percent complete: 79.8%; Average loss: 2.9797 Iteration: 3195; Percent complete: 79.9%; Average loss: 2.9680 Iteration: 3196; Percent complete: 79.9%; Average loss: 2.8626 Iteration: 3197; Percent complete: 79.9%; Average loss: 2.7779 Iteration: 3198; Percent complete: 80.0%; Average loss: 3.0058 Iteration: 3199; Percent complete: 80.0%; Average loss: 2.7425 Iteration: 3200; Percent complete: 80.0%; Average loss: 2.8558 Iteration: 3201; Percent complete: 80.0%; Average loss: 3.0021 Iteration: 3202; Percent complete: 80.0%; Average loss: 2.8705 Iteration: 3203; Percent complete: 80.1%; Average loss: 2.8362 Iteration: 3204; Percent complete: 80.1%; Average loss: 2.8670 Iteration: 3205; Percent complete: 80.1%; Average loss: 2.6027 Iteration: 3206; Percent complete: 80.2%; Average loss: 2.7443 Iteration: 3207; Percent complete: 80.2%; Average loss: 2.8722 Iteration: 3208; Percent complete: 80.2%; Average loss: 3.0095 Iteration: 3209; Percent complete: 80.2%; Average loss: 3.0660 Iteration: 3210; Percent complete: 80.2%; Average loss: 2.9273 Iteration: 3211; Percent complete: 80.3%; Average loss: 2.8697 Iteration: 3212; Percent complete: 80.3%; Average loss: 2.9546 Iteration: 3213; Percent complete: 80.3%; Average loss: 2.8880 Iteration: 3214; Percent complete: 80.3%; Average loss: 2.8072 Iteration: 3215; Percent complete: 80.4%; Average loss: 2.9747 Iteration: 3216; Percent complete: 80.4%; Average loss: 2.7713 Iteration: 3217; Percent complete: 80.4%; Average loss: 2.8272 Iteration: 3218; Percent complete: 80.5%; Average loss: 2.8666 Iteration: 3219; Percent complete: 80.5%; Average loss: 2.6254 Iteration: 3220; Percent complete: 80.5%; Average loss: 2.8812 Iteration: 3221; Percent complete: 80.5%; Average loss: 2.9393 Iteration: 3222; Percent complete: 80.5%; Average loss: 2.8712 Iteration: 3223; Percent complete: 80.6%; Average loss: 2.7416 Iteration: 3224; Percent complete: 80.6%; Average loss: 2.6847 Iteration: 3225; Percent complete: 80.6%; Average loss: 2.8571 Iteration: 3226; Percent complete: 80.7%; Average loss: 2.6676 Iteration: 3227; Percent complete: 80.7%; Average loss: 2.7207 Iteration: 3228; Percent complete: 80.7%; Average loss: 2.8412 Iteration: 3229; Percent complete: 80.7%; Average loss: 2.7326 Iteration: 3230; Percent complete: 80.8%; Average loss: 2.7722 Iteration: 3231; Percent complete: 80.8%; Average loss: 2.6594 Iteration: 3232; Percent complete: 80.8%; Average loss: 2.7645 Iteration: 3233; Percent complete: 80.8%; Average loss: 2.6013 Iteration: 3234; Percent complete: 80.8%; Average loss: 2.7871 Iteration: 3235; Percent complete: 80.9%; Average loss: 2.9532 Iteration: 3236; Percent complete: 80.9%; Average loss: 2.7464 Iteration: 3237; Percent complete: 80.9%; Average loss: 2.7431 Iteration: 3238; Percent complete: 81.0%; Average loss: 2.6413 Iteration: 3239; Percent complete: 81.0%; Average loss: 2.8418 Iteration: 3240; Percent complete: 81.0%; Average loss: 2.6745 Iteration: 3241; Percent complete: 81.0%; Average loss: 2.7505 Iteration: 3242; Percent complete: 81.0%; Average loss: 2.7462 Iteration: 3243; Percent complete: 81.1%; Average loss: 2.7887 Iteration: 3244; Percent complete: 81.1%; Average loss: 2.6753 Iteration: 3245; Percent complete: 81.1%; Average loss: 2.7981 Iteration: 3246; Percent complete: 81.2%; Average loss: 2.9032 Iteration: 3247; Percent complete: 81.2%; Average loss: 2.7923 Iteration: 3248; Percent complete: 81.2%; Average loss: 2.5653 Iteration: 3249; Percent complete: 81.2%; Average loss: 2.5290 Iteration: 3250; Percent complete: 81.2%; Average loss: 2.6837 Iteration: 3251; Percent complete: 81.3%; Average loss: 2.8940 Iteration: 3252; Percent complete: 81.3%; Average loss: 2.6113 Iteration: 3253; Percent complete: 81.3%; Average loss: 2.9735 Iteration: 3254; Percent complete: 81.3%; Average loss: 2.8283 Iteration: 3255; Percent complete: 81.4%; Average loss: 2.9087 Iteration: 3256; Percent complete: 81.4%; Average loss: 2.8201 Iteration: 3257; Percent complete: 81.4%; Average loss: 2.8443 Iteration: 3258; Percent complete: 81.5%; Average loss: 2.8271 Iteration: 3259; Percent complete: 81.5%; Average loss: 2.8334 Iteration: 3260; Percent complete: 81.5%; Average loss: 2.8674 Iteration: 3261; Percent complete: 81.5%; Average loss: 2.7372 Iteration: 3262; Percent complete: 81.5%; Average loss: 2.9765 Iteration: 3263; Percent complete: 81.6%; Average loss: 2.9380 Iteration: 3264; Percent complete: 81.6%; Average loss: 2.5100 Iteration: 3265; Percent complete: 81.6%; Average loss: 2.6849 Iteration: 3266; Percent complete: 81.7%; Average loss: 2.6896 Iteration: 3267; Percent complete: 81.7%; Average loss: 2.8358 Iteration: 3268; Percent complete: 81.7%; Average loss: 2.6404 Iteration: 3269; Percent complete: 81.7%; Average loss: 2.6790 Iteration: 3270; Percent complete: 81.8%; Average loss: 2.8264 Iteration: 3271; Percent complete: 81.8%; Average loss: 2.7410 Iteration: 3272; Percent complete: 81.8%; Average loss: 2.5339 Iteration: 3273; Percent complete: 81.8%; Average loss: 2.7553 Iteration: 3274; Percent complete: 81.8%; Average loss: 3.1105 Iteration: 3275; Percent complete: 81.9%; Average loss: 2.7710 Iteration: 3276; Percent complete: 81.9%; Average loss: 2.7339 Iteration: 3277; Percent complete: 81.9%; Average loss: 2.6729 Iteration: 3278; Percent complete: 82.0%; Average loss: 2.6078 Iteration: 3279; Percent complete: 82.0%; Average loss: 2.9693 Iteration: 3280; Percent complete: 82.0%; Average loss: 3.0510 Iteration: 3281; Percent complete: 82.0%; Average loss: 3.0771 Iteration: 3282; Percent complete: 82.0%; Average loss: 2.5780 Iteration: 3283; Percent complete: 82.1%; Average loss: 2.8725 Iteration: 3284; Percent complete: 82.1%; Average loss: 2.6944 Iteration: 3285; Percent complete: 82.1%; Average loss: 2.6317 Iteration: 3286; Percent complete: 82.2%; Average loss: 2.7456 Iteration: 3287; Percent complete: 82.2%; Average loss: 2.7917 Iteration: 3288; Percent complete: 82.2%; Average loss: 2.6939 Iteration: 3289; Percent complete: 82.2%; Average loss: 2.7856 Iteration: 3290; Percent complete: 82.2%; Average loss: 2.9281 Iteration: 3291; Percent complete: 82.3%; Average loss: 2.8005 Iteration: 3292; Percent complete: 82.3%; Average loss: 2.7644 Iteration: 3293; Percent complete: 82.3%; Average loss: 2.7684 Iteration: 3294; Percent complete: 82.3%; Average loss: 2.8228 Iteration: 3295; Percent complete: 82.4%; Average loss: 2.8956 Iteration: 3296; Percent complete: 82.4%; Average loss: 2.7739 Iteration: 3297; Percent complete: 82.4%; Average loss: 2.7526 Iteration: 3298; Percent complete: 82.5%; Average loss: 2.6945 Iteration: 3299; Percent complete: 82.5%; Average loss: 2.8824 Iteration: 3300; Percent complete: 82.5%; Average loss: 2.8571 Iteration: 3301; Percent complete: 82.5%; Average loss: 2.6536 Iteration: 3302; Percent complete: 82.5%; Average loss: 2.9658 Iteration: 3303; Percent complete: 82.6%; Average loss: 2.9487 Iteration: 3304; Percent complete: 82.6%; Average loss: 2.9744 Iteration: 3305; Percent complete: 82.6%; Average loss: 2.7595 Iteration: 3306; Percent complete: 82.7%; Average loss: 2.8810 Iteration: 3307; Percent complete: 82.7%; Average loss: 2.6865 Iteration: 3308; Percent complete: 82.7%; Average loss: 2.7194 Iteration: 3309; Percent complete: 82.7%; Average loss: 2.7930 Iteration: 3310; Percent complete: 82.8%; Average loss: 2.7958 Iteration: 3311; Percent complete: 82.8%; Average loss: 2.8537 Iteration: 3312; Percent complete: 82.8%; Average loss: 2.7709 Iteration: 3313; Percent complete: 82.8%; Average loss: 2.8710 Iteration: 3314; Percent complete: 82.8%; Average loss: 2.5551 Iteration: 3315; Percent complete: 82.9%; Average loss: 2.7790 Iteration: 3316; Percent complete: 82.9%; Average loss: 2.6726 Iteration: 3317; Percent complete: 82.9%; Average loss: 2.7081 Iteration: 3318; Percent complete: 83.0%; Average loss: 2.9245 Iteration: 3319; Percent complete: 83.0%; Average loss: 2.7310 Iteration: 3320; Percent complete: 83.0%; Average loss: 2.6148 Iteration: 3321; Percent complete: 83.0%; Average loss: 2.7530 Iteration: 3322; Percent complete: 83.0%; Average loss: 2.9128 Iteration: 3323; Percent complete: 83.1%; Average loss: 2.7109 Iteration: 3324; Percent complete: 83.1%; Average loss: 2.9249 Iteration: 3325; Percent complete: 83.1%; Average loss: 2.7082 Iteration: 3326; Percent complete: 83.2%; Average loss: 2.8828 Iteration: 3327; Percent complete: 83.2%; Average loss: 2.6030 Iteration: 3328; Percent complete: 83.2%; Average loss: 2.6886 Iteration: 3329; Percent complete: 83.2%; Average loss: 2.6496 Iteration: 3330; Percent complete: 83.2%; Average loss: 2.7618 Iteration: 3331; Percent complete: 83.3%; Average loss: 2.5974 Iteration: 3332; Percent complete: 83.3%; Average loss: 2.6716 Iteration: 3333; Percent complete: 83.3%; Average loss: 2.7390 Iteration: 3334; Percent complete: 83.4%; Average loss: 2.9533 Iteration: 3335; Percent complete: 83.4%; Average loss: 2.9526 Iteration: 3336; Percent complete: 83.4%; Average loss: 2.9923 Iteration: 3337; Percent complete: 83.4%; Average loss: 2.5338 Iteration: 3338; Percent complete: 83.5%; Average loss: 2.8745 Iteration: 3339; Percent complete: 83.5%; Average loss: 2.9527 Iteration: 3340; Percent complete: 83.5%; Average loss: 2.5320 Iteration: 3341; Percent complete: 83.5%; Average loss: 2.9352 Iteration: 3342; Percent complete: 83.5%; Average loss: 2.9315 Iteration: 3343; Percent complete: 83.6%; Average loss: 2.6279 Iteration: 3344; Percent complete: 83.6%; Average loss: 2.7975 Iteration: 3345; Percent complete: 83.6%; Average loss: 2.8075 Iteration: 3346; Percent complete: 83.7%; Average loss: 2.8766 Iteration: 3347; Percent complete: 83.7%; Average loss: 2.7700 Iteration: 3348; Percent complete: 83.7%; Average loss: 2.6275 Iteration: 3349; Percent complete: 83.7%; Average loss: 2.8007 Iteration: 3350; Percent complete: 83.8%; Average loss: 2.7294 Iteration: 3351; Percent complete: 83.8%; Average loss: 2.8822 Iteration: 3352; Percent complete: 83.8%; Average loss: 2.6167 Iteration: 3353; Percent complete: 83.8%; Average loss: 2.6327 Iteration: 3354; Percent complete: 83.9%; Average loss: 2.8003 Iteration: 3355; Percent complete: 83.9%; Average loss: 2.7561 Iteration: 3356; Percent complete: 83.9%; Average loss: 2.7685 Iteration: 3357; Percent complete: 83.9%; Average loss: 2.5949 Iteration: 3358; Percent complete: 84.0%; Average loss: 2.7598 Iteration: 3359; Percent complete: 84.0%; Average loss: 2.8619 Iteration: 3360; Percent complete: 84.0%; Average loss: 2.6904 Iteration: 3361; Percent complete: 84.0%; Average loss: 2.4538 Iteration: 3362; Percent complete: 84.0%; Average loss: 2.9059 Iteration: 3363; Percent complete: 84.1%; Average loss: 2.6084 Iteration: 3364; Percent complete: 84.1%; Average loss: 2.8299 Iteration: 3365; Percent complete: 84.1%; Average loss: 2.7287 Iteration: 3366; Percent complete: 84.2%; Average loss: 2.9264 Iteration: 3367; Percent complete: 84.2%; Average loss: 2.7792 Iteration: 3368; Percent complete: 84.2%; Average loss: 2.9716 Iteration: 3369; Percent complete: 84.2%; Average loss: 2.7664 Iteration: 3370; Percent complete: 84.2%; Average loss: 2.6142 Iteration: 3371; Percent complete: 84.3%; Average loss: 2.9256 Iteration: 3372; Percent complete: 84.3%; Average loss: 2.7091 Iteration: 3373; Percent complete: 84.3%; Average loss: 2.8418 Iteration: 3374; Percent complete: 84.4%; Average loss: 2.5683 Iteration: 3375; Percent complete: 84.4%; Average loss: 2.8068 Iteration: 3376; Percent complete: 84.4%; Average loss: 2.4184 Iteration: 3377; Percent complete: 84.4%; Average loss: 2.6956 Iteration: 3378; Percent complete: 84.5%; Average loss: 2.7040 Iteration: 3379; Percent complete: 84.5%; Average loss: 2.7223 Iteration: 3380; Percent complete: 84.5%; Average loss: 2.8612 Iteration: 3381; Percent complete: 84.5%; Average loss: 2.6737 Iteration: 3382; Percent complete: 84.5%; Average loss: 2.9890 Iteration: 3383; Percent complete: 84.6%; Average loss: 2.9207 Iteration: 3384; Percent complete: 84.6%; Average loss: 2.9666 Iteration: 3385; Percent complete: 84.6%; Average loss: 2.5636 Iteration: 3386; Percent complete: 84.7%; Average loss: 2.8154 Iteration: 3387; Percent complete: 84.7%; Average loss: 2.7641 Iteration: 3388; Percent complete: 84.7%; Average loss: 2.7416 Iteration: 3389; Percent complete: 84.7%; Average loss: 2.6718 Iteration: 3390; Percent complete: 84.8%; Average loss: 2.9008 Iteration: 3391; Percent complete: 84.8%; Average loss: 2.7515 Iteration: 3392; Percent complete: 84.8%; Average loss: 2.6745 Iteration: 3393; Percent complete: 84.8%; Average loss: 3.0190 Iteration: 3394; Percent complete: 84.9%; Average loss: 2.7931 Iteration: 3395; Percent complete: 84.9%; Average loss: 2.8146 Iteration: 3396; Percent complete: 84.9%; Average loss: 2.6754 Iteration: 3397; Percent complete: 84.9%; Average loss: 2.6769 Iteration: 3398; Percent complete: 85.0%; Average loss: 2.6979 Iteration: 3399; Percent complete: 85.0%; Average loss: 2.8452 Iteration: 3400; Percent complete: 85.0%; Average loss: 2.7310 Iteration: 3401; Percent complete: 85.0%; Average loss: 2.8965 Iteration: 3402; Percent complete: 85.0%; Average loss: 3.0748 Iteration: 3403; Percent complete: 85.1%; Average loss: 2.8856 Iteration: 3404; Percent complete: 85.1%; Average loss: 2.5991 Iteration: 3405; Percent complete: 85.1%; Average loss: 2.6787 Iteration: 3406; Percent complete: 85.2%; Average loss: 2.6551 Iteration: 3407; Percent complete: 85.2%; Average loss: 2.7821 Iteration: 3408; Percent complete: 85.2%; Average loss: 2.8974 Iteration: 3409; Percent complete: 85.2%; Average loss: 2.9564 Iteration: 3410; Percent complete: 85.2%; Average loss: 2.6635 Iteration: 3411; Percent complete: 85.3%; Average loss: 2.7594 Iteration: 3412; Percent complete: 85.3%; Average loss: 2.6535 Iteration: 3413; Percent complete: 85.3%; Average loss: 2.8085 Iteration: 3414; Percent complete: 85.4%; Average loss: 2.9373 Iteration: 3415; Percent complete: 85.4%; Average loss: 2.6269 Iteration: 3416; Percent complete: 85.4%; Average loss: 2.8624 Iteration: 3417; Percent complete: 85.4%; Average loss: 2.9595 Iteration: 3418; Percent complete: 85.5%; Average loss: 2.5837 Iteration: 3419; Percent complete: 85.5%; Average loss: 2.7707 Iteration: 3420; Percent complete: 85.5%; Average loss: 2.8404 Iteration: 3421; Percent complete: 85.5%; Average loss: 2.7980 Iteration: 3422; Percent complete: 85.5%; Average loss: 3.0416 Iteration: 3423; Percent complete: 85.6%; Average loss: 2.7625 Iteration: 3424; Percent complete: 85.6%; Average loss: 2.8074 Iteration: 3425; Percent complete: 85.6%; Average loss: 2.7947 Iteration: 3426; Percent complete: 85.7%; Average loss: 2.7064 Iteration: 3427; Percent complete: 85.7%; Average loss: 3.0650 Iteration: 3428; Percent complete: 85.7%; Average loss: 2.8685 Iteration: 3429; Percent complete: 85.7%; Average loss: 2.8462 Iteration: 3430; Percent complete: 85.8%; Average loss: 2.7492 Iteration: 3431; Percent complete: 85.8%; Average loss: 2.7039 Iteration: 3432; Percent complete: 85.8%; Average loss: 2.8781 Iteration: 3433; Percent complete: 85.8%; Average loss: 2.6406 Iteration: 3434; Percent complete: 85.9%; Average loss: 2.8070 Iteration: 3435; Percent complete: 85.9%; Average loss: 2.5837 Iteration: 3436; Percent complete: 85.9%; Average loss: 2.9474 Iteration: 3437; Percent complete: 85.9%; Average loss: 2.7634 Iteration: 3438; Percent complete: 86.0%; Average loss: 2.8880 Iteration: 3439; Percent complete: 86.0%; Average loss: 2.8609 Iteration: 3440; Percent complete: 86.0%; Average loss: 2.7405 Iteration: 3441; Percent complete: 86.0%; Average loss: 2.6488 Iteration: 3442; Percent complete: 86.1%; Average loss: 2.8511 Iteration: 3443; Percent complete: 86.1%; Average loss: 2.6294 Iteration: 3444; Percent complete: 86.1%; Average loss: 2.6564 Iteration: 3445; Percent complete: 86.1%; Average loss: 2.6365 Iteration: 3446; Percent complete: 86.2%; Average loss: 2.5888 Iteration: 3447; Percent complete: 86.2%; Average loss: 2.7230 Iteration: 3448; Percent complete: 86.2%; Average loss: 2.4821 Iteration: 3449; Percent complete: 86.2%; Average loss: 3.0519 Iteration: 3450; Percent complete: 86.2%; Average loss: 2.8094 Iteration: 3451; Percent complete: 86.3%; Average loss: 2.8395 Iteration: 3452; Percent complete: 86.3%; Average loss: 2.8834 Iteration: 3453; Percent complete: 86.3%; Average loss: 2.8109 Iteration: 3454; Percent complete: 86.4%; Average loss: 2.8134 Iteration: 3455; Percent complete: 86.4%; Average loss: 2.7765 Iteration: 3456; Percent complete: 86.4%; Average loss: 2.5413 Iteration: 3457; Percent complete: 86.4%; Average loss: 2.8004 Iteration: 3458; Percent complete: 86.5%; Average loss: 2.6987 Iteration: 3459; Percent complete: 86.5%; Average loss: 3.0946 Iteration: 3460; Percent complete: 86.5%; Average loss: 2.8857 Iteration: 3461; Percent complete: 86.5%; Average loss: 2.5598 Iteration: 3462; Percent complete: 86.6%; Average loss: 2.8451 Iteration: 3463; Percent complete: 86.6%; Average loss: 2.8108 Iteration: 3464; Percent complete: 86.6%; Average loss: 2.6672 Iteration: 3465; Percent complete: 86.6%; Average loss: 2.6177 Iteration: 3466; Percent complete: 86.7%; Average loss: 2.6013 Iteration: 3467; Percent complete: 86.7%; Average loss: 2.7564 Iteration: 3468; Percent complete: 86.7%; Average loss: 2.6710 Iteration: 3469; Percent complete: 86.7%; Average loss: 2.7328 Iteration: 3470; Percent complete: 86.8%; Average loss: 2.7745 Iteration: 3471; Percent complete: 86.8%; Average loss: 2.5268 Iteration: 3472; Percent complete: 86.8%; Average loss: 2.6877 Iteration: 3473; Percent complete: 86.8%; Average loss: 2.7586 Iteration: 3474; Percent complete: 86.9%; Average loss: 2.9320 Iteration: 3475; Percent complete: 86.9%; Average loss: 2.7742 Iteration: 3476; Percent complete: 86.9%; Average loss: 2.7361 Iteration: 3477; Percent complete: 86.9%; Average loss: 2.6517 Iteration: 3478; Percent complete: 87.0%; Average loss: 2.6035 Iteration: 3479; Percent complete: 87.0%; Average loss: 2.8535 Iteration: 3480; Percent complete: 87.0%; Average loss: 2.7606 Iteration: 3481; Percent complete: 87.0%; Average loss: 2.7198 Iteration: 3482; Percent complete: 87.1%; Average loss: 2.6336 Iteration: 3483; Percent complete: 87.1%; Average loss: 2.5265 Iteration: 3484; Percent complete: 87.1%; Average loss: 2.8110 Iteration: 3485; Percent complete: 87.1%; Average loss: 3.0392 Iteration: 3486; Percent complete: 87.2%; Average loss: 2.7315 Iteration: 3487; Percent complete: 87.2%; Average loss: 2.8930 Iteration: 3488; Percent complete: 87.2%; Average loss: 2.7499 Iteration: 3489; Percent complete: 87.2%; Average loss: 2.9192 Iteration: 3490; Percent complete: 87.2%; Average loss: 2.4131 Iteration: 3491; Percent complete: 87.3%; Average loss: 2.8195 Iteration: 3492; Percent complete: 87.3%; Average loss: 2.6261 Iteration: 3493; Percent complete: 87.3%; Average loss: 2.6877 Iteration: 3494; Percent complete: 87.4%; Average loss: 2.9134 Iteration: 3495; Percent complete: 87.4%; Average loss: 3.0331 Iteration: 3496; Percent complete: 87.4%; Average loss: 2.8177 Iteration: 3497; Percent complete: 87.4%; Average loss: 2.7524 Iteration: 3498; Percent complete: 87.5%; Average loss: 2.8604 Iteration: 3499; Percent complete: 87.5%; Average loss: 2.7868 Iteration: 3500; Percent complete: 87.5%; Average loss: 2.8101 Iteration: 3501; Percent complete: 87.5%; Average loss: 2.6919 Iteration: 3502; Percent complete: 87.5%; Average loss: 2.6421 Iteration: 3503; Percent complete: 87.6%; Average loss: 2.7526 Iteration: 3504; Percent complete: 87.6%; Average loss: 2.8567 Iteration: 3505; Percent complete: 87.6%; Average loss: 2.8137 Iteration: 3506; Percent complete: 87.6%; Average loss: 2.7189 Iteration: 3507; Percent complete: 87.7%; Average loss: 2.8170 Iteration: 3508; Percent complete: 87.7%; Average loss: 2.6677 Iteration: 3509; Percent complete: 87.7%; Average loss: 2.7455 Iteration: 3510; Percent complete: 87.8%; Average loss: 2.6448 Iteration: 3511; Percent complete: 87.8%; Average loss: 2.7990 Iteration: 3512; Percent complete: 87.8%; Average loss: 2.7411 Iteration: 3513; Percent complete: 87.8%; Average loss: 2.8946 Iteration: 3514; Percent complete: 87.8%; Average loss: 2.5619 Iteration: 3515; Percent complete: 87.9%; Average loss: 2.7954 Iteration: 3516; Percent complete: 87.9%; Average loss: 2.7614 Iteration: 3517; Percent complete: 87.9%; Average loss: 2.8210 Iteration: 3518; Percent complete: 87.9%; Average loss: 2.5165 Iteration: 3519; Percent complete: 88.0%; Average loss: 2.9331 Iteration: 3520; Percent complete: 88.0%; Average loss: 2.8895 Iteration: 3521; Percent complete: 88.0%; Average loss: 2.6333 Iteration: 3522; Percent complete: 88.0%; Average loss: 2.7865 Iteration: 3523; Percent complete: 88.1%; Average loss: 2.8571 Iteration: 3524; Percent complete: 88.1%; Average loss: 2.7569 Iteration: 3525; Percent complete: 88.1%; Average loss: 2.7661 Iteration: 3526; Percent complete: 88.1%; Average loss: 2.8462 Iteration: 3527; Percent complete: 88.2%; Average loss: 2.9266 Iteration: 3528; Percent complete: 88.2%; Average loss: 2.5955 Iteration: 3529; Percent complete: 88.2%; Average loss: 2.7975 Iteration: 3530; Percent complete: 88.2%; Average loss: 2.8836 Iteration: 3531; Percent complete: 88.3%; Average loss: 2.7054 Iteration: 3532; Percent complete: 88.3%; Average loss: 2.7210 Iteration: 3533; Percent complete: 88.3%; Average loss: 2.6846 Iteration: 3534; Percent complete: 88.3%; Average loss: 2.6646 Iteration: 3535; Percent complete: 88.4%; Average loss: 2.8414 Iteration: 3536; Percent complete: 88.4%; Average loss: 2.6424 Iteration: 3537; Percent complete: 88.4%; Average loss: 2.8802 Iteration: 3538; Percent complete: 88.4%; Average loss: 2.6915 Iteration: 3539; Percent complete: 88.5%; Average loss: 2.8430 Iteration: 3540; Percent complete: 88.5%; Average loss: 2.9332 Iteration: 3541; Percent complete: 88.5%; Average loss: 2.8082 Iteration: 3542; Percent complete: 88.5%; Average loss: 2.7314 Iteration: 3543; Percent complete: 88.6%; Average loss: 2.5753 Iteration: 3544; Percent complete: 88.6%; Average loss: 2.9192 Iteration: 3545; Percent complete: 88.6%; Average loss: 2.6688 Iteration: 3546; Percent complete: 88.6%; Average loss: 2.6843 Iteration: 3547; Percent complete: 88.7%; Average loss: 2.8122 Iteration: 3548; Percent complete: 88.7%; Average loss: 2.6978 Iteration: 3549; Percent complete: 88.7%; Average loss: 2.7366 Iteration: 3550; Percent complete: 88.8%; Average loss: 2.8988 Iteration: 3551; Percent complete: 88.8%; Average loss: 2.6629 Iteration: 3552; Percent complete: 88.8%; Average loss: 2.8440 Iteration: 3553; Percent complete: 88.8%; Average loss: 2.7957 Iteration: 3554; Percent complete: 88.8%; Average loss: 3.0217 Iteration: 3555; Percent complete: 88.9%; Average loss: 2.7490 Iteration: 3556; Percent complete: 88.9%; Average loss: 2.8884 Iteration: 3557; Percent complete: 88.9%; Average loss: 2.9393 Iteration: 3558; Percent complete: 88.9%; Average loss: 2.8683 Iteration: 3559; Percent complete: 89.0%; Average loss: 2.9548 Iteration: 3560; Percent complete: 89.0%; Average loss: 2.6712 Iteration: 3561; Percent complete: 89.0%; Average loss: 2.6861 Iteration: 3562; Percent complete: 89.0%; Average loss: 2.4478 Iteration: 3563; Percent complete: 89.1%; Average loss: 2.7216 Iteration: 3564; Percent complete: 89.1%; Average loss: 2.7814 Iteration: 3565; Percent complete: 89.1%; Average loss: 2.8691 Iteration: 3566; Percent complete: 89.1%; Average loss: 2.7201 Iteration: 3567; Percent complete: 89.2%; Average loss: 2.8213 Iteration: 3568; Percent complete: 89.2%; Average loss: 2.8969 Iteration: 3569; Percent complete: 89.2%; Average loss: 2.7880 Iteration: 3570; Percent complete: 89.2%; Average loss: 2.8587 Iteration: 3571; Percent complete: 89.3%; Average loss: 2.7468 Iteration: 3572; Percent complete: 89.3%; Average loss: 2.7664 Iteration: 3573; Percent complete: 89.3%; Average loss: 2.6145 Iteration: 3574; Percent complete: 89.3%; Average loss: 2.7998 Iteration: 3575; Percent complete: 89.4%; Average loss: 2.9450 Iteration: 3576; Percent complete: 89.4%; Average loss: 2.7980 Iteration: 3577; Percent complete: 89.4%; Average loss: 2.8749 Iteration: 3578; Percent complete: 89.5%; Average loss: 2.6039 Iteration: 3579; Percent complete: 89.5%; Average loss: 2.7327 Iteration: 3580; Percent complete: 89.5%; Average loss: 2.7637 Iteration: 3581; Percent complete: 89.5%; Average loss: 2.5677 Iteration: 3582; Percent complete: 89.5%; Average loss: 2.6359 Iteration: 3583; Percent complete: 89.6%; Average loss: 2.8550 Iteration: 3584; Percent complete: 89.6%; Average loss: 2.7408 Iteration: 3585; Percent complete: 89.6%; Average loss: 2.6209 Iteration: 3586; Percent complete: 89.6%; Average loss: 2.7148 Iteration: 3587; Percent complete: 89.7%; Average loss: 2.8705 Iteration: 3588; Percent complete: 89.7%; Average loss: 2.9079 Iteration: 3589; Percent complete: 89.7%; Average loss: 2.8261 Iteration: 3590; Percent complete: 89.8%; Average loss: 2.7317 Iteration: 3591; Percent complete: 89.8%; Average loss: 2.5852 Iteration: 3592; Percent complete: 89.8%; Average loss: 2.5261 Iteration: 3593; Percent complete: 89.8%; Average loss: 2.8429 Iteration: 3594; Percent complete: 89.8%; Average loss: 2.8071 Iteration: 3595; Percent complete: 89.9%; Average loss: 2.5704 Iteration: 3596; Percent complete: 89.9%; Average loss: 2.6715 Iteration: 3597; Percent complete: 89.9%; Average loss: 2.7706 Iteration: 3598; Percent complete: 90.0%; Average loss: 2.7544 Iteration: 3599; Percent complete: 90.0%; Average loss: 2.6661 Iteration: 3600; Percent complete: 90.0%; Average loss: 2.7660 Iteration: 3601; Percent complete: 90.0%; Average loss: 2.6500 Iteration: 3602; Percent complete: 90.0%; Average loss: 2.6437 Iteration: 3603; Percent complete: 90.1%; Average loss: 2.6360 Iteration: 3604; Percent complete: 90.1%; Average loss: 2.7389 Iteration: 3605; Percent complete: 90.1%; Average loss: 2.8445 Iteration: 3606; Percent complete: 90.1%; Average loss: 2.6361 Iteration: 3607; Percent complete: 90.2%; Average loss: 2.7655 Iteration: 3608; Percent complete: 90.2%; Average loss: 2.5592 Iteration: 3609; Percent complete: 90.2%; Average loss: 2.7872 Iteration: 3610; Percent complete: 90.2%; Average loss: 2.5624 Iteration: 3611; Percent complete: 90.3%; Average loss: 2.7962 Iteration: 3612; Percent complete: 90.3%; Average loss: 2.7204 Iteration: 3613; Percent complete: 90.3%; Average loss: 2.7911 Iteration: 3614; Percent complete: 90.3%; Average loss: 2.6070 Iteration: 3615; Percent complete: 90.4%; Average loss: 2.7184 Iteration: 3616; Percent complete: 90.4%; Average loss: 2.7040 Iteration: 3617; Percent complete: 90.4%; Average loss: 2.6034 Iteration: 3618; Percent complete: 90.5%; Average loss: 2.5909 Iteration: 3619; Percent complete: 90.5%; Average loss: 2.7891 Iteration: 3620; Percent complete: 90.5%; Average loss: 2.7941 Iteration: 3621; Percent complete: 90.5%; Average loss: 2.7387 Iteration: 3622; Percent complete: 90.5%; Average loss: 2.8106 Iteration: 3623; Percent complete: 90.6%; Average loss: 2.8161 Iteration: 3624; Percent complete: 90.6%; Average loss: 2.6772 Iteration: 3625; Percent complete: 90.6%; Average loss: 2.8693 Iteration: 3626; Percent complete: 90.6%; Average loss: 2.6609 Iteration: 3627; Percent complete: 90.7%; Average loss: 2.2583 Iteration: 3628; Percent complete: 90.7%; Average loss: 2.5550 Iteration: 3629; Percent complete: 90.7%; Average loss: 2.6270 Iteration: 3630; Percent complete: 90.8%; Average loss: 2.6131 Iteration: 3631; Percent complete: 90.8%; Average loss: 2.7411 Iteration: 3632; Percent complete: 90.8%; Average loss: 2.8756 Iteration: 3633; Percent complete: 90.8%; Average loss: 2.7440 Iteration: 3634; Percent complete: 90.8%; Average loss: 2.6204 Iteration: 3635; Percent complete: 90.9%; Average loss: 2.7481 Iteration: 3636; Percent complete: 90.9%; Average loss: 2.4324 Iteration: 3637; Percent complete: 90.9%; Average loss: 2.6290 Iteration: 3638; Percent complete: 91.0%; Average loss: 2.5657 Iteration: 3639; Percent complete: 91.0%; Average loss: 2.7277 Iteration: 3640; Percent complete: 91.0%; Average loss: 2.8937 Iteration: 3641; Percent complete: 91.0%; Average loss: 2.3627 Iteration: 3642; Percent complete: 91.0%; Average loss: 2.7155 Iteration: 3643; Percent complete: 91.1%; Average loss: 2.7697 Iteration: 3644; Percent complete: 91.1%; Average loss: 2.8136 Iteration: 3645; Percent complete: 91.1%; Average loss: 2.8715 Iteration: 3646; Percent complete: 91.1%; Average loss: 2.6719 Iteration: 3647; Percent complete: 91.2%; Average loss: 2.6676 Iteration: 3648; Percent complete: 91.2%; Average loss: 2.5993 Iteration: 3649; Percent complete: 91.2%; Average loss: 2.7738 Iteration: 3650; Percent complete: 91.2%; Average loss: 2.8756 Iteration: 3651; Percent complete: 91.3%; Average loss: 2.8535 Iteration: 3652; Percent complete: 91.3%; Average loss: 2.7332 Iteration: 3653; Percent complete: 91.3%; Average loss: 2.9763 Iteration: 3654; Percent complete: 91.3%; Average loss: 2.6252 Iteration: 3655; Percent complete: 91.4%; Average loss: 2.6605 Iteration: 3656; Percent complete: 91.4%; Average loss: 2.6673 Iteration: 3657; Percent complete: 91.4%; Average loss: 2.7444 Iteration: 3658; Percent complete: 91.5%; Average loss: 2.5654 Iteration: 3659; Percent complete: 91.5%; Average loss: 2.3670 Iteration: 3660; Percent complete: 91.5%; Average loss: 2.9667 Iteration: 3661; Percent complete: 91.5%; Average loss: 2.7091 Iteration: 3662; Percent complete: 91.5%; Average loss: 2.6378 Iteration: 3663; Percent complete: 91.6%; Average loss: 2.5715 Iteration: 3664; Percent complete: 91.6%; Average loss: 2.4035 Iteration: 3665; Percent complete: 91.6%; Average loss: 2.6691 Iteration: 3666; Percent complete: 91.6%; Average loss: 2.8368 Iteration: 3667; Percent complete: 91.7%; Average loss: 2.5952 Iteration: 3668; Percent complete: 91.7%; Average loss: 3.0498 Iteration: 3669; Percent complete: 91.7%; Average loss: 2.6671 Iteration: 3670; Percent complete: 91.8%; Average loss: 2.7720 Iteration: 3671; Percent complete: 91.8%; Average loss: 2.7269 Iteration: 3672; Percent complete: 91.8%; Average loss: 2.8617 Iteration: 3673; Percent complete: 91.8%; Average loss: 2.6387 Iteration: 3674; Percent complete: 91.8%; Average loss: 2.7025 Iteration: 3675; Percent complete: 91.9%; Average loss: 2.7508 Iteration: 3676; Percent complete: 91.9%; Average loss: 2.7247 Iteration: 3677; Percent complete: 91.9%; Average loss: 3.0049 Iteration: 3678; Percent complete: 92.0%; Average loss: 2.6115 Iteration: 3679; Percent complete: 92.0%; Average loss: 2.7385 Iteration: 3680; Percent complete: 92.0%; Average loss: 2.5090 Iteration: 3681; Percent complete: 92.0%; Average loss: 2.7184 Iteration: 3682; Percent complete: 92.0%; Average loss: 2.5811 Iteration: 3683; Percent complete: 92.1%; Average loss: 2.5861 Iteration: 3684; Percent complete: 92.1%; Average loss: 2.6464 Iteration: 3685; Percent complete: 92.1%; Average loss: 2.4681 Iteration: 3686; Percent complete: 92.2%; Average loss: 2.7413 Iteration: 3687; Percent complete: 92.2%; Average loss: 2.6202 Iteration: 3688; Percent complete: 92.2%; Average loss: 2.6965 Iteration: 3689; Percent complete: 92.2%; Average loss: 2.5116 Iteration: 3690; Percent complete: 92.2%; Average loss: 2.7124 Iteration: 3691; Percent complete: 92.3%; Average loss: 2.8565 Iteration: 3692; Percent complete: 92.3%; Average loss: 2.8379 Iteration: 3693; Percent complete: 92.3%; Average loss: 2.7236 Iteration: 3694; Percent complete: 92.3%; Average loss: 2.9059 Iteration: 3695; Percent complete: 92.4%; Average loss: 2.6912 Iteration: 3696; Percent complete: 92.4%; Average loss: 2.7722 Iteration: 3697; Percent complete: 92.4%; Average loss: 2.7468 Iteration: 3698; Percent complete: 92.5%; Average loss: 2.7573 Iteration: 3699; Percent complete: 92.5%; Average loss: 2.5251 Iteration: 3700; Percent complete: 92.5%; Average loss: 2.5587 Iteration: 3701; Percent complete: 92.5%; Average loss: 2.8658 Iteration: 3702; Percent complete: 92.5%; Average loss: 2.7020 Iteration: 3703; Percent complete: 92.6%; Average loss: 2.6218 Iteration: 3704; Percent complete: 92.6%; Average loss: 2.7208 Iteration: 3705; Percent complete: 92.6%; Average loss: 2.5934 Iteration: 3706; Percent complete: 92.7%; Average loss: 2.8950 Iteration: 3707; Percent complete: 92.7%; Average loss: 2.4966 Iteration: 3708; Percent complete: 92.7%; Average loss: 2.5085 Iteration: 3709; Percent complete: 92.7%; Average loss: 2.7147 Iteration: 3710; Percent complete: 92.8%; Average loss: 2.5579 Iteration: 3711; Percent complete: 92.8%; Average loss: 2.8422 Iteration: 3712; Percent complete: 92.8%; Average loss: 2.6113 Iteration: 3713; Percent complete: 92.8%; Average loss: 2.6717 Iteration: 3714; Percent complete: 92.8%; Average loss: 2.5341 Iteration: 3715; Percent complete: 92.9%; Average loss: 2.6218 Iteration: 3716; Percent complete: 92.9%; Average loss: 2.6519 Iteration: 3717; Percent complete: 92.9%; Average loss: 2.4849 Iteration: 3718; Percent complete: 93.0%; Average loss: 2.6524 Iteration: 3719; Percent complete: 93.0%; Average loss: 2.4739 Iteration: 3720; Percent complete: 93.0%; Average loss: 2.7633 Iteration: 3721; Percent complete: 93.0%; Average loss: 2.4064 Iteration: 3722; Percent complete: 93.0%; Average loss: 2.7249 Iteration: 3723; Percent complete: 93.1%; Average loss: 2.5313 Iteration: 3724; Percent complete: 93.1%; Average loss: 2.6195 Iteration: 3725; Percent complete: 93.1%; Average loss: 2.7112 Iteration: 3726; Percent complete: 93.2%; Average loss: 2.9204 Iteration: 3727; Percent complete: 93.2%; Average loss: 2.6928 Iteration: 3728; Percent complete: 93.2%; Average loss: 2.6325 Iteration: 3729; Percent complete: 93.2%; Average loss: 2.6230 Iteration: 3730; Percent complete: 93.2%; Average loss: 2.7028 Iteration: 3731; Percent complete: 93.3%; Average loss: 2.7023 Iteration: 3732; Percent complete: 93.3%; Average loss: 2.7067 Iteration: 3733; Percent complete: 93.3%; Average loss: 2.7280 Iteration: 3734; Percent complete: 93.3%; Average loss: 2.6250 Iteration: 3735; Percent complete: 93.4%; Average loss: 2.3916 Iteration: 3736; Percent complete: 93.4%; Average loss: 2.4029 Iteration: 3737; Percent complete: 93.4%; Average loss: 2.4762 Iteration: 3738; Percent complete: 93.5%; Average loss: 2.6466 Iteration: 3739; Percent complete: 93.5%; Average loss: 2.6428 Iteration: 3740; Percent complete: 93.5%; Average loss: 2.6117 Iteration: 3741; Percent complete: 93.5%; Average loss: 2.5662 Iteration: 3742; Percent complete: 93.5%; Average loss: 2.6060 Iteration: 3743; Percent complete: 93.6%; Average loss: 2.6598 Iteration: 3744; Percent complete: 93.6%; Average loss: 2.6716 Iteration: 3745; Percent complete: 93.6%; Average loss: 2.6250 Iteration: 3746; Percent complete: 93.7%; Average loss: 2.5523 Iteration: 3747; Percent complete: 93.7%; Average loss: 2.6362 Iteration: 3748; Percent complete: 93.7%; Average loss: 2.7859 Iteration: 3749; Percent complete: 93.7%; Average loss: 2.7047 Iteration: 3750; Percent complete: 93.8%; Average loss: 2.6173 Iteration: 3751; Percent complete: 93.8%; Average loss: 2.5027 Iteration: 3752; Percent complete: 93.8%; Average loss: 2.6918 Iteration: 3753; Percent complete: 93.8%; Average loss: 3.0341 Iteration: 3754; Percent complete: 93.8%; Average loss: 2.6777 Iteration: 3755; Percent complete: 93.9%; Average loss: 2.5369 Iteration: 3756; Percent complete: 93.9%; Average loss: 2.6036 Iteration: 3757; Percent complete: 93.9%; Average loss: 2.8520 Iteration: 3758; Percent complete: 94.0%; Average loss: 2.6455 Iteration: 3759; Percent complete: 94.0%; Average loss: 2.7668 Iteration: 3760; Percent complete: 94.0%; Average loss: 2.4912 Iteration: 3761; Percent complete: 94.0%; Average loss: 2.7218 Iteration: 3762; Percent complete: 94.0%; Average loss: 2.9150 Iteration: 3763; Percent complete: 94.1%; Average loss: 2.8503 Iteration: 3764; Percent complete: 94.1%; Average loss: 2.8348 Iteration: 3765; Percent complete: 94.1%; Average loss: 2.6052 Iteration: 3766; Percent complete: 94.2%; Average loss: 2.5784 Iteration: 3767; Percent complete: 94.2%; Average loss: 2.6383 Iteration: 3768; Percent complete: 94.2%; Average loss: 2.5500 Iteration: 3769; Percent complete: 94.2%; Average loss: 2.4657 Iteration: 3770; Percent complete: 94.2%; Average loss: 2.6778 Iteration: 3771; Percent complete: 94.3%; Average loss: 2.5614 Iteration: 3772; Percent complete: 94.3%; Average loss: 2.7583 Iteration: 3773; Percent complete: 94.3%; Average loss: 2.5660 Iteration: 3774; Percent complete: 94.3%; Average loss: 2.6402 Iteration: 3775; Percent complete: 94.4%; Average loss: 2.6572 Iteration: 3776; Percent complete: 94.4%; Average loss: 2.7160 Iteration: 3777; Percent complete: 94.4%; Average loss: 2.7647 Iteration: 3778; Percent complete: 94.5%; Average loss: 2.7438 Iteration: 3779; Percent complete: 94.5%; Average loss: 2.6466 Iteration: 3780; Percent complete: 94.5%; Average loss: 2.5878 Iteration: 3781; Percent complete: 94.5%; Average loss: 2.4100 Iteration: 3782; Percent complete: 94.5%; Average loss: 2.6912 Iteration: 3783; Percent complete: 94.6%; Average loss: 2.6480 Iteration: 3784; Percent complete: 94.6%; Average loss: 2.7136 Iteration: 3785; Percent complete: 94.6%; Average loss: 2.5206 Iteration: 3786; Percent complete: 94.7%; Average loss: 2.6126 Iteration: 3787; Percent complete: 94.7%; Average loss: 2.7213 Iteration: 3788; Percent complete: 94.7%; Average loss: 2.4696 Iteration: 3789; Percent complete: 94.7%; Average loss: 2.7261 Iteration: 3790; Percent complete: 94.8%; Average loss: 2.6836 Iteration: 3791; Percent complete: 94.8%; Average loss: 2.7555 Iteration: 3792; Percent complete: 94.8%; Average loss: 2.8439 Iteration: 3793; Percent complete: 94.8%; Average loss: 2.4288 Iteration: 3794; Percent complete: 94.8%; Average loss: 2.8137 Iteration: 3795; Percent complete: 94.9%; Average loss: 2.8264 Iteration: 3796; Percent complete: 94.9%; Average loss: 2.4946 Iteration: 3797; Percent complete: 94.9%; Average loss: 2.8219 Iteration: 3798; Percent complete: 95.0%; Average loss: 2.5639 Iteration: 3799; Percent complete: 95.0%; Average loss: 2.8002 Iteration: 3800; Percent complete: 95.0%; Average loss: 2.6819 Iteration: 3801; Percent complete: 95.0%; Average loss: 2.7871 Iteration: 3802; Percent complete: 95.0%; Average loss: 2.5175 Iteration: 3803; Percent complete: 95.1%; Average loss: 2.5202 Iteration: 3804; Percent complete: 95.1%; Average loss: 2.6545 Iteration: 3805; Percent complete: 95.1%; Average loss: 2.6961 Iteration: 3806; Percent complete: 95.2%; Average loss: 2.5205 Iteration: 3807; Percent complete: 95.2%; Average loss: 2.6703 Iteration: 3808; Percent complete: 95.2%; Average loss: 2.6644 Iteration: 3809; Percent complete: 95.2%; Average loss: 2.5069 Iteration: 3810; Percent complete: 95.2%; Average loss: 2.6489 Iteration: 3811; Percent complete: 95.3%; Average loss: 2.9117 Iteration: 3812; Percent complete: 95.3%; Average loss: 2.7419 Iteration: 3813; Percent complete: 95.3%; Average loss: 2.6565 Iteration: 3814; Percent complete: 95.3%; Average loss: 2.6431 Iteration: 3815; Percent complete: 95.4%; Average loss: 2.4354 Iteration: 3816; Percent complete: 95.4%; Average loss: 2.3492 Iteration: 3817; Percent complete: 95.4%; Average loss: 2.5975 Iteration: 3818; Percent complete: 95.5%; Average loss: 2.8249 Iteration: 3819; Percent complete: 95.5%; Average loss: 2.8128 Iteration: 3820; Percent complete: 95.5%; Average loss: 2.6498 Iteration: 3821; Percent complete: 95.5%; Average loss: 2.7396 Iteration: 3822; Percent complete: 95.5%; Average loss: 2.6298 Iteration: 3823; Percent complete: 95.6%; Average loss: 2.7593 Iteration: 3824; Percent complete: 95.6%; Average loss: 2.4111 Iteration: 3825; Percent complete: 95.6%; Average loss: 2.5952 Iteration: 3826; Percent complete: 95.7%; Average loss: 2.5228 Iteration: 3827; Percent complete: 95.7%; Average loss: 2.5803 Iteration: 3828; Percent complete: 95.7%; Average loss: 2.5323 Iteration: 3829; Percent complete: 95.7%; Average loss: 2.5916 Iteration: 3830; Percent complete: 95.8%; Average loss: 2.5924 Iteration: 3831; Percent complete: 95.8%; Average loss: 2.4993 Iteration: 3832; Percent complete: 95.8%; Average loss: 2.8020 Iteration: 3833; Percent complete: 95.8%; Average loss: 2.9128 Iteration: 3834; Percent complete: 95.9%; Average loss: 2.6753 Iteration: 3835; Percent complete: 95.9%; Average loss: 2.5177 Iteration: 3836; Percent complete: 95.9%; Average loss: 2.6892 Iteration: 3837; Percent complete: 95.9%; Average loss: 2.5647 Iteration: 3838; Percent complete: 96.0%; Average loss: 2.7260 Iteration: 3839; Percent complete: 96.0%; Average loss: 2.7435 Iteration: 3840; Percent complete: 96.0%; Average loss: 2.6940 Iteration: 3841; Percent complete: 96.0%; Average loss: 2.7308 Iteration: 3842; Percent complete: 96.0%; Average loss: 2.4757 Iteration: 3843; Percent complete: 96.1%; Average loss: 2.9029 Iteration: 3844; Percent complete: 96.1%; Average loss: 2.4683 Iteration: 3845; Percent complete: 96.1%; Average loss: 2.4489 Iteration: 3846; Percent complete: 96.2%; Average loss: 2.6549 Iteration: 3847; Percent complete: 96.2%; Average loss: 2.6728 Iteration: 3848; Percent complete: 96.2%; Average loss: 2.6637 Iteration: 3849; Percent complete: 96.2%; Average loss: 2.5499 Iteration: 3850; Percent complete: 96.2%; Average loss: 2.4308 Iteration: 3851; Percent complete: 96.3%; Average loss: 2.6762 Iteration: 3852; Percent complete: 96.3%; Average loss: 2.5643 Iteration: 3853; Percent complete: 96.3%; Average loss: 2.7090 Iteration: 3854; Percent complete: 96.4%; Average loss: 2.5415 Iteration: 3855; Percent complete: 96.4%; Average loss: 2.7316 Iteration: 3856; Percent complete: 96.4%; Average loss: 2.8635 Iteration: 3857; Percent complete: 96.4%; Average loss: 2.5885 Iteration: 3858; Percent complete: 96.5%; Average loss: 2.5194 Iteration: 3859; Percent complete: 96.5%; Average loss: 2.5740 Iteration: 3860; Percent complete: 96.5%; Average loss: 2.4419 Iteration: 3861; Percent complete: 96.5%; Average loss: 2.7156 Iteration: 3862; Percent complete: 96.5%; Average loss: 2.7024 Iteration: 3863; Percent complete: 96.6%; Average loss: 2.5988 Iteration: 3864; Percent complete: 96.6%; Average loss: 2.5606 Iteration: 3865; Percent complete: 96.6%; Average loss: 2.6909 Iteration: 3866; Percent complete: 96.7%; Average loss: 2.7817 Iteration: 3867; Percent complete: 96.7%; Average loss: 2.4599 Iteration: 3868; Percent complete: 96.7%; Average loss: 2.5629 Iteration: 3869; Percent complete: 96.7%; Average loss: 2.6829 Iteration: 3870; Percent complete: 96.8%; Average loss: 2.5845 Iteration: 3871; Percent complete: 96.8%; Average loss: 2.7375 Iteration: 3872; Percent complete: 96.8%; Average loss: 2.6716 Iteration: 3873; Percent complete: 96.8%; Average loss: 2.6114 Iteration: 3874; Percent complete: 96.9%; Average loss: 2.6976 Iteration: 3875; Percent complete: 96.9%; Average loss: 2.5456 Iteration: 3876; Percent complete: 96.9%; Average loss: 2.7874 Iteration: 3877; Percent complete: 96.9%; Average loss: 2.8360 Iteration: 3878; Percent complete: 97.0%; Average loss: 2.7745 Iteration: 3879; Percent complete: 97.0%; Average loss: 2.9649 Iteration: 3880; Percent complete: 97.0%; Average loss: 2.6135 Iteration: 3881; Percent complete: 97.0%; Average loss: 2.5413 Iteration: 3882; Percent complete: 97.0%; Average loss: 2.9062 Iteration: 3883; Percent complete: 97.1%; Average loss: 2.7175 Iteration: 3884; Percent complete: 97.1%; Average loss: 2.9140 Iteration: 3885; Percent complete: 97.1%; Average loss: 2.7382 Iteration: 3886; Percent complete: 97.2%; Average loss: 2.4912 Iteration: 3887; Percent complete: 97.2%; Average loss: 2.7386 Iteration: 3888; Percent complete: 97.2%; Average loss: 2.6930 Iteration: 3889; Percent complete: 97.2%; Average loss: 2.5423 Iteration: 3890; Percent complete: 97.2%; Average loss: 2.6614 Iteration: 3891; Percent complete: 97.3%; Average loss: 2.8658 Iteration: 3892; Percent complete: 97.3%; Average loss: 2.7217 Iteration: 3893; Percent complete: 97.3%; Average loss: 2.5815 Iteration: 3894; Percent complete: 97.4%; Average loss: 2.4968 Iteration: 3895; Percent complete: 97.4%; Average loss: 2.6752 Iteration: 3896; Percent complete: 97.4%; Average loss: 2.7592 Iteration: 3897; Percent complete: 97.4%; Average loss: 2.7382 Iteration: 3898; Percent complete: 97.5%; Average loss: 2.7946 Iteration: 3899; Percent complete: 97.5%; Average loss: 2.6107 Iteration: 3900; Percent complete: 97.5%; Average loss: 2.6435 Iteration: 3901; Percent complete: 97.5%; Average loss: 2.6608 Iteration: 3902; Percent complete: 97.5%; Average loss: 2.5030 Iteration: 3903; Percent complete: 97.6%; Average loss: 2.6083 Iteration: 3904; Percent complete: 97.6%; Average loss: 2.5874 Iteration: 3905; Percent complete: 97.6%; Average loss: 2.6836 Iteration: 3906; Percent complete: 97.7%; Average loss: 2.6553 Iteration: 3907; Percent complete: 97.7%; Average loss: 2.7086 Iteration: 3908; Percent complete: 97.7%; Average loss: 2.5893 Iteration: 3909; Percent complete: 97.7%; Average loss: 2.6382 Iteration: 3910; Percent complete: 97.8%; Average loss: 2.5498 Iteration: 3911; Percent complete: 97.8%; Average loss: 2.4810 Iteration: 3912; Percent complete: 97.8%; Average loss: 2.6768 Iteration: 3913; Percent complete: 97.8%; Average loss: 2.4554 Iteration: 3914; Percent complete: 97.9%; Average loss: 2.6367 Iteration: 3915; Percent complete: 97.9%; Average loss: 2.5724 Iteration: 3916; Percent complete: 97.9%; Average loss: 2.6495 Iteration: 3917; Percent complete: 97.9%; Average loss: 2.4811 Iteration: 3918; Percent complete: 98.0%; Average loss: 2.4607 Iteration: 3919; Percent complete: 98.0%; Average loss: 2.8312 Iteration: 3920; Percent complete: 98.0%; Average loss: 2.7209 Iteration: 3921; Percent complete: 98.0%; Average loss: 2.7160 Iteration: 3922; Percent complete: 98.0%; Average loss: 2.8307 Iteration: 3923; Percent complete: 98.1%; Average loss: 2.7633 Iteration: 3924; Percent complete: 98.1%; Average loss: 2.9466 Iteration: 3925; Percent complete: 98.1%; Average loss: 2.5440 Iteration: 3926; Percent complete: 98.2%; Average loss: 2.4603 Iteration: 3927; Percent complete: 98.2%; Average loss: 2.5636 Iteration: 3928; Percent complete: 98.2%; Average loss: 2.9691 Iteration: 3929; Percent complete: 98.2%; Average loss: 2.6196 Iteration: 3930; Percent complete: 98.2%; Average loss: 2.5323 Iteration: 3931; Percent complete: 98.3%; Average loss: 2.3430 Iteration: 3932; Percent complete: 98.3%; Average loss: 2.9050 Iteration: 3933; Percent complete: 98.3%; Average loss: 2.5606 Iteration: 3934; Percent complete: 98.4%; Average loss: 2.7507 Iteration: 3935; Percent complete: 98.4%; Average loss: 2.7561 Iteration: 3936; Percent complete: 98.4%; Average loss: 2.6409 Iteration: 3937; Percent complete: 98.4%; Average loss: 2.8119 Iteration: 3938; Percent complete: 98.5%; Average loss: 2.5014 Iteration: 3939; Percent complete: 98.5%; Average loss: 2.7470 Iteration: 3940; Percent complete: 98.5%; Average loss: 2.6831 Iteration: 3941; Percent complete: 98.5%; Average loss: 2.6739 Iteration: 3942; Percent complete: 98.6%; Average loss: 2.5648 Iteration: 3943; Percent complete: 98.6%; Average loss: 2.7060 Iteration: 3944; Percent complete: 98.6%; Average loss: 2.7278 Iteration: 3945; Percent complete: 98.6%; Average loss: 2.7134 Iteration: 3946; Percent complete: 98.7%; Average loss: 2.5586 Iteration: 3947; Percent complete: 98.7%; Average loss: 2.6631 Iteration: 3948; Percent complete: 98.7%; Average loss: 2.4562 Iteration: 3949; Percent complete: 98.7%; Average loss: 2.6221 Iteration: 3950; Percent complete: 98.8%; Average loss: 2.5409 Iteration: 3951; Percent complete: 98.8%; Average loss: 2.7434 Iteration: 3952; Percent complete: 98.8%; Average loss: 2.6169 Iteration: 3953; Percent complete: 98.8%; Average loss: 2.3909 Iteration: 3954; Percent complete: 98.9%; Average loss: 2.6249 Iteration: 3955; Percent complete: 98.9%; Average loss: 2.7647 Iteration: 3956; Percent complete: 98.9%; Average loss: 2.6571 Iteration: 3957; Percent complete: 98.9%; Average loss: 2.4930 Iteration: 3958; Percent complete: 99.0%; Average loss: 2.4042 Iteration: 3959; Percent complete: 99.0%; Average loss: 2.9591 Iteration: 3960; Percent complete: 99.0%; Average loss: 2.6932 Iteration: 3961; Percent complete: 99.0%; Average loss: 2.5330 Iteration: 3962; Percent complete: 99.1%; Average loss: 2.7501 Iteration: 3963; Percent complete: 99.1%; Average loss: 2.7160 Iteration: 3964; Percent complete: 99.1%; Average loss: 2.5589 Iteration: 3965; Percent complete: 99.1%; Average loss: 2.5702 Iteration: 3966; Percent complete: 99.2%; Average loss: 2.7847 Iteration: 3967; Percent complete: 99.2%; Average loss: 2.6711 Iteration: 3968; Percent complete: 99.2%; Average loss: 2.6549 Iteration: 3969; Percent complete: 99.2%; Average loss: 2.7142 Iteration: 3970; Percent complete: 99.2%; Average loss: 2.5803 Iteration: 3971; Percent complete: 99.3%; Average loss: 2.5902 Iteration: 3972; Percent complete: 99.3%; Average loss: 2.5613 Iteration: 3973; Percent complete: 99.3%; Average loss: 2.6055 Iteration: 3974; Percent complete: 99.4%; Average loss: 2.6768 Iteration: 3975; Percent complete: 99.4%; Average loss: 2.5305 Iteration: 3976; Percent complete: 99.4%; Average loss: 2.8735 Iteration: 3977; Percent complete: 99.4%; Average loss: 2.6139 Iteration: 3978; Percent complete: 99.5%; Average loss: 2.5500 Iteration: 3979; Percent complete: 99.5%; Average loss: 2.3924 Iteration: 3980; Percent complete: 99.5%; Average loss: 2.4942 Iteration: 3981; Percent complete: 99.5%; Average loss: 2.5486 Iteration: 3982; Percent complete: 99.6%; Average loss: 2.6154 Iteration: 3983; Percent complete: 99.6%; Average loss: 2.5299 Iteration: 3984; Percent complete: 99.6%; Average loss: 2.5456 Iteration: 3985; Percent complete: 99.6%; Average loss: 2.6009 Iteration: 3986; Percent complete: 99.7%; Average loss: 2.8824 Iteration: 3987; Percent complete: 99.7%; Average loss: 2.6323 Iteration: 3988; Percent complete: 99.7%; Average loss: 2.5172 Iteration: 3989; Percent complete: 99.7%; Average loss: 2.6723 Iteration: 3990; Percent complete: 99.8%; Average loss: 2.7110 Iteration: 3991; Percent complete: 99.8%; Average loss: 2.7118 Iteration: 3992; Percent complete: 99.8%; Average loss: 2.4058 Iteration: 3993; Percent complete: 99.8%; Average loss: 2.3599 Iteration: 3994; Percent complete: 99.9%; Average loss: 2.4839 Iteration: 3995; Percent complete: 99.9%; Average loss: 2.4875 Iteration: 3996; Percent complete: 99.9%; Average loss: 2.5872 Iteration: 3997; Percent complete: 99.9%; Average loss: 2.6011 Iteration: 3998; Percent complete: 100.0%; Average loss: 2.6643 Iteration: 3999; Percent complete: 100.0%; Average loss: 2.6179 Iteration: 4000; Percent complete: 100.0%; Average loss: 2.4628 \u8fd0\u884c\u8bc4\u4f30 \u00b6 \u8981\u548c\u8fd9\u4e2a\u6a21\u578b\u804a\u5929\uff0c\u8fd0\u884c\u4e0b\u9762\u7684\u4ee3\u7801\u5757\u3002 1 2 3 4 5 6 7 8 9 # Set dropout layers to eval mode encoder . eval () decoder . eval () # Initialize search module searcher = GreedySearchDecoder ( encoder , decoder ) # Begin chatting (uncomment and run the following line to begin) # evaluateInput(encoder, decoder, searcher, voc) \u7ed3\u8bba \u00b6 \u4f19\u8ba1\u4eec\uff0c\u8fd9\u5c31\u662f\u8fd9\u4e00\u5207\u3002\u606d\u559c\u4f60\uff0c\u73b0\u5728\u77e5\u9053\u4e86\u6784\u5efa\u751f\u6210\u804a\u5929\u673a\u5668\u4eba\u6a21\u578b\u7684\u57fa\u7840\u77e5\u8bc6\uff01\u5982\u679c\u6709\u5174\u8da3\uff0c\u53ef\u4ee5\u5c1d\u8bd5\u901a\u8fc7\u8c03\u6574\u6a21\u578b\u548c\u8bad\u7ec3\u53c2\u6570\u4ee5\u53ca\u81ea\u5b9a\u4e49\u8bad\u7ec3\u6a21\u578b\u7684\u6570\u636e\u6765\u5b9a\u5236\u804a\u5929\u673a\u5668\u4eba\u7684\u884c\u4e3a\u3002 \u67e5\u770b\u5176\u4ed6\u6559\u7a0b\uff0c\u4e86\u89e3PyTorch\u4e2d\u66f4\u9177\u7684\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528\u7a0b\u5e8f\uff01","title":"\u804a\u5929\u673a\u5668\u4eba\u6559\u7a0b"},{"location":"beginner/chatbot_tutorial/#\u804a\u5929\u673a\u5668\u4eba\u6559\u7a0b","text":"\u4f5c\u8005\uff1a Matthew Inkawhich \u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u63a2\u7d22\u4e00\u4e2a\u6709\u8da3\u7684\u5e8f\u5217\u5230\u5e8f\u5217(Seq2Seq)\u6a21\u578b\u7684\u7528\u4f8b\u3002\u6211\u4eec\u5c06\u4f7f\u7528 Cornell Movie-Dialogs Corpus \u7684\u7535\u5f71\u811a\u672c\u8bad\u7ec3\u4e00\u4e2a\u7b80\u5355\u7684\u804a\u5929\u673a\u5668\u4eba\u3002 \u5bf9\u8bdd\u6a21\u578b\u662f\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u7684\u70ed\u95e8\u8bdd\u9898\u3002 \u804a\u5929\u673a\u5668\u4eba\u5728\u5305\u62ec\u5ba2\u6237\u670d\u52a1\u5e94\u7528\u548c\u5728\u7ebf\u5e2e\u52a9\u53f0\u5728\u5185\u7684\u5404\u79cd\u573a\u666f\u90fd\u7528\u5e94\u7528\u3002 \u8fd9\u4e9b\u673a\u5668\u4eba\u901a\u5e38\u4f7f\u7531\u57fa\u4e8e\u7d22\u5f15\u7684\u6a21\u578b\u9a71\u52a8\u7684\uff0c\u5b83\u4eec\u54cd\u5e94\u7279\u5b9a\u7684\u95ee\u9898\uff0c\u8f93\u51fa\u9884\u5b9a\u4e49\u7684\u54cd\u5e94\u3002\u5728\u50cf\u516c\u53f8IT\u5e2e\u52a9\u53f0\u8fd9\u6837\u9ad8\u5ea6\u53d7\u9650\u7684\u9886\u57df\u4e2d\uff0c\u8fd9\u4e9b\u6a21\u578b\u53ef\u80fd\u662f\u4f4f\u591f\u4e86\uff0c\u4e0d\u8fc7\u5728\u901a\u5e38\u7684\u72b6\u51b5\u4e0b\uff0c\u4ed6\u4eec\u662f\u4e0d\u591f\u5065\u58ee\u7684\u3002 \u6559\u4e00\u53f0\u673a\u5668\u4e0e\u4eba\u7c7b\u5728\u591a\u4e2a\u9886\u57df\u5185\u8fdb\u884c\u6709\u610f\u4e49\u7684\u5bf9\u8bdd\u662f\u4e00\u4e2a\u8fdc\u672a\u89e3\u51b3\u7684\u7814\u7a76\u6027\u95ee\u9898\u3002 \u5728\u6700\u8fd1\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u70ed\u6f6e\u4e2d\uff0c\u50cf Google Neural Conversational Model \u8fd9\u6837\u7684\u5f3a\u5927\u7684\u751f\u6210\u6a21\u578b\u7684\u51fa\u73b0\uff0c\u6807\u5fd7\u7740\u591a\u9886\u57df\u751f\u6210\u5bf9\u8bdd\u6a21\u578b\u7684\u4e00\u5927\u8fdb\u6b65\u3002 \u5728\u8fd9\u4e2a\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u7528 PyTorch \u5b9e\u73b0\u8fd9\u79cd\u6a21\u578b\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 > hello? Bot: hello . > where am I? Bot: you re in a hospital . > who are you? Bot: i m a lawyer . > how are you doing? Bot: i m fine . > are you my friend? Bot: no . > you ' re under arrest Bot: i m trying to help you ! > i ' m just kidding Bot: i m sorry . > where are you from? Bot: san francisco . > it ' s time for me to leave Bot: i know . > goodbye Bot: goodbye . \u6559\u7a0b\u8981\u70b9\uff1a \u52a0\u8f7d\u548c\u9884\u5904\u7406 Cornell Movie-Dialogs Corpus \u6570\u636e\u96c6 \u4f7f\u7528 Luong attention mechanism(s) \u5b9e\u73b0\u4e00\u4e2a\u5e8f\u5217\u5230\u5e8f\u5217\u6a21\u578b \u4f7f\u7528\u5c0f\u6279\u6b21\u8054\u5408\u8bad\u7ec3\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u6a21\u578b \u5b9e\u73b0\u8d2a\u5a6a\u641c\u7d22\u89e3\u7801\u6a21\u5757 \u4e0e\u8bad\u7ec3\u540e\u7684\u804a\u5929\u673a\u5668\u4eba\u4e92\u52a8 \u81f4\u8c22\uff1a \u672c\u6559\u7a0b\u501f\u7528\u4ee5\u4e0b\u6765\u6e90\u7684\u4ee3\u7801\uff1a Yuan-Kuei Wu \u7684 pytorch-chatbot \u5b9e\u73b0: https://github.com/ywk991112/pytorch-chatbot Sean Robertson \u7684 practical-pytorch seq2seq-translation \u4f8b\u5b50: https://github.com/spro/practical-pytorch/tree/master/seq2seq-translation FloydHub \u7684 Cornell Movie Corpus \u9884\u5904\u7406\u4ee3\u7801: https://github.com/floydhub/textutil-preprocess-cornell-movie-corpus","title":"\u804a\u5929\u673a\u5668\u4eba\u6559\u7a0b"},{"location":"beginner/chatbot_tutorial/#\u51c6\u5907\u5de5\u4f5c","text":"\u9996\u5148\uff0c\u4ece https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html \u4e0b\u8f7dZIP\u6587\u4ef6\u89e3\u538b\u7f29\u5230\u5f53\u524d\u76ee\u5f55\u7684 data \u5b50\u76ee\u5f55\u4e2d\u3002 \u7136\u540e\uff0c\u6211\u4eec\u8fdb\u884c\u4e00\u4e9b\u5fc5\u8981\u7684\u5f15\u7528\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from __future__ import absolute_import , division , print_function , unicode_literals import torch from torch.jit import script , trace import torch.nn as nn from torch import optim import torch.nn.functional as F import csv import random import re import os import unicodedata import codecs from io import open import itertools import math USE_CUDA = torch . cuda . is_available () device = torch . device ( \"cuda\" if USE_CUDA else \"cpu\" ) print ( device ) 1 cuda","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"beginner/chatbot_tutorial/#\u6570\u636e\u52a0\u8f7d\u4e0e\u9884\u5904\u7406","text":"\u4e0b\u4e00\u6b65\uff0c\u5c06\u6211\u4eec\u4e4b\u524d\u4e0b\u8f7d\u7684\u6570\u636e\u91cd\u65b0\u683c\u5f0f\u5316\uff0c\u5e76\u628a\u6570\u636e\u52a0\u8f7d\u5230\u6211\u4eec\u6240\u8981\u4f7f\u7528\u7684\u7ed3\u6784\u4e2d\u3002 Cornell Movie-Dialogs Corpus \u662f\u4e00\u4e2a\u5185\u5bb9\u4e30\u5bcc\u7684\u7535\u5f71\u89d2\u8272\u5bf9\u8bdd\u6570\u636e\u96c6\uff1a \u6765\u81ea10,292\u5bf9\u7535\u5f71\u89d2\u8272\u7684220,579\u4e2a\u5bf9\u8bdd \u6765\u81ea617\u90e8\u7535\u5f71\u76849,035\u4e2a\u89d2\u8272 \u603b\u5171304,713\u53e5\u8bdd \u8fd9\u4e2a\u6570\u636e\u96c6\u5e9e\u5927\u4e14\u591a\u6837\uff0c\u5176\u8bed\u8a00\u5f62\u5f0f\u3001\u65f6\u95f4\u6bb5\u3001\u60c5\u611f\u90fd\u6709\u5f88\u5927\u53d8\u5316\u3002\u6211\u4eec\u5e0c\u671b\u8fd9\u79cd\u591a\u6837\u6027\u4f7f\u6211\u4eec\u7684\u6a21\u578b\u80fd\u591f\u9002\u5e94\u591a\u79cd\u5f62\u5f0f\u7684\u8f93\u5165\u548c\u8be2\u95ee\u3002 \u9996\u5148\uff0c\u6211\u4eec\u770b\u51e0\u884c\u6570\u636e\u6587\u4ef6\uff0c\u4e86\u89e3\u5176\u539f\u59cb\u683c\u5f0f\u3002 1 2 3 4 5 6 7 8 9 10 corpus_name = \"cornell movie-dialogs corpus\" corpus = os . path . join ( \"data\" , corpus_name ) def printLines ( file , n = 10 ): with open ( file , encoding = 'iso-8859-1' ) as datafile : for i , line in enumerate ( datafile ): if i < n : print ( line ) printLines ( os . path . join ( corpus , \"movie_lines.txt\" )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not! L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to! L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so. L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay? L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go. L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie. L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I'm kidding. You know how sometimes you just become this \"persona\"? And you don't know how to quit? L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?","title":"\u6570\u636e\u52a0\u8f7d\u4e0e\u9884\u5904\u7406"},{"location":"beginner/chatbot_tutorial/#\u521b\u5efa\u683c\u5f0f\u5316\u6570\u636e\u6587\u4ef6","text":"\u4e3a\u65b9\u4fbf\u8d77\u89c1\uff0c\u6211\u4eec\u5c06\u521b\u5efa\u4e00\u4e2a\u683c\u5f0f\u826f\u597d\u7684\u6570\u636e\u6587\u4ef6\uff0c\u5176\u4e2d\u6bcf\u4e00\u884c\u5305\u542b\u4e00\u4e2a\u4ee5\u5236\u8868\u7b26\u5206\u9694\u7684 \u8be2\u95ee\u8bed\u53e5 \u548c\u4e00\u4e2a \u54cd\u5e94\u8bed\u53e5 \u5bf9\u3002 \u4ee5\u4e0b\u51fd\u6570\u7528\u4e8e\u5206\u6790 movie_lines.txt \u6570\u636e\u6587\u4ef6\u7684\u539f\u59cb\u884c\u6570\u636e loadLines \u5c06\u6bcf\u4e00\u884c\u5206\u5272\u6210\u5177\u6709\u591a\u4e2a\u5b57\u6bb5(lineID, characterID, movieID, character, text)\u7684\u5b57\u5178 loadConversations \u5c06\u6765\u81ea loadLines \u7684\uff0c\u5305\u542b\u591a\u4e2a\u5b57\u6bb5\u7684\u884c\uff0c\u6839\u636e movie_conversations.txt \u7ec4\u6210\u5bf9\u8bdd\u3002 extractSentencePairs \u4ece\u5bf9\u8bdd\u4e2d\u63d0\u53d6\u201c\u8be2\u95ee-\u56de\u7b54\u201d\u53e5\u5b50\u5bf9 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 # Splits each line of the file into a dictionary of fields def loadLines ( fileName , fields ): lines = {} with open ( fileName , encoding = 'iso-8859-1' ) as f : for line in f : values = line . split ( \" +++$+++ \" ) # Extract fields lineObj = {} for i , field in enumerate ( fields ): lineObj [ field ] = values [ i ] lines [ lineObj [ 'lineID' ]] = lineObj return lines # Groups fields of lines from `loadLines` into conversations based on *movie_conversations.txt* def loadConversations ( fileName , lines , fields ): conversations = [] with open ( fileName , encoding = 'iso-8859-1' ) as f : for line in f : values = line . split ( \" +++$+++ \" ) # Extract fields convObj = {} for i , field in enumerate ( fields ): convObj [ field ] = values [ i ] # Convert string to list (convObj[\"utteranceIDs\"] == \"['L598485', 'L598486', ...]\") lineIds = eval ( convObj [ \"utteranceIDs\" ]) # Reassemble lines convObj [ \"lines\" ] = [] for lineId in lineIds : convObj [ \"lines\" ] . append ( lines [ lineId ]) conversations . append ( convObj ) return conversations # Extracts pairs of sentences from conversations def extractSentencePairs ( conversations ): qa_pairs = [] for conversation in conversations : # Iterate over all the lines of the conversation for i in range ( len ( conversation [ \"lines\" ]) - 1 ): # We ignore the last line (no answer for it) inputLine = conversation [ \"lines\" ][ i ][ \"text\" ] . strip () targetLine = conversation [ \"lines\" ][ i + 1 ][ \"text\" ] . strip () # Filter wrong samples (if one of the lists is empty) if inputLine and targetLine : qa_pairs . append ([ inputLine , targetLine ]) return qa_pairs \u73b0\u5728\uff0c\u8c03\u7528\u8fd9\u4e9b\u51fd\u6570\uff0c\u5e76\u521b\u5efa\u6587\u4ef6\uff0c\u6587\u4ef6\u540d\u662f formatted_movie_lines.txt \u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # Define path to new file datafile = os . path . join ( corpus , \"formatted_movie_lines.txt\" ) delimiter = ' \\t ' # Unescape the delimiter delimiter = str ( codecs . decode ( delimiter , \"unicode_escape\" )) # Initialize lines dict, conversations list, and field ids lines = {} conversations = [] MOVIE_LINES_FIELDS = [ \"lineID\" , \"characterID\" , \"movieID\" , \"character\" , \"text\" ] MOVIE_CONVERSATIONS_FIELDS = [ \"character1ID\" , \"character2ID\" , \"movieID\" , \"utteranceIDs\" ] # Load lines and process conversations print ( \" \\n Processing corpus...\" ) lines = loadLines ( os . path . join ( corpus , \"movie_lines.txt\" ), MOVIE_LINES_FIELDS ) print ( \" \\n Loading conversations...\" ) conversations = loadConversations ( os . path . join ( corpus , \"movie_conversations.txt\" ), lines , MOVIE_CONVERSATIONS_FIELDS ) # Write new csv file print ( \" \\n Writing newly formatted file...\" ) with open ( datafile , 'w' , encoding = 'utf-8' ) as outputfile : writer = csv . writer ( outputfile , delimiter = delimiter , lineterminator = ' \\n ' ) for pair in extractSentencePairs ( conversations ): writer . writerow ( pair ) # Print a sample of lines print ( \" \\n Sample lines from file:\" ) printLines ( datafile ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 Processing corpus... Loading conversations... Writing newly formatted file... Sample lines from file: Can we make this quick? Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad. Again. Well, I thought we'd start with pronunciation, if that's okay with you. Well, I thought we'd start with pronunciation, if that's okay with you. Not the hacking and gagging and spitting part. Please. Not the hacking and gagging and spitting part. Please. Okay... then how 'bout we try out some French cuisine. Saturday? Night? You're asking me out. That's so cute. What's your name again? Forget it. No, no, it's my fault -- we didn't have a proper introduction --- Cameron. Cameron. The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser. My sister. I can't date until she does. The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser. My sister. I can't date until she does. Seems like she could get a date easy enough... Why? Unsolved mystery. She used to be really popular when she started high school, then it was just like she got sick of it or something. Unsolved mystery. She used to be really popular when she started high school, then it was just like she got sick of it or something. That's a shame. Gosh, if only we could find Kat a boyfriend... Let me see what I can do.","title":"\u521b\u5efa\u683c\u5f0f\u5316\u6570\u636e\u6587\u4ef6"},{"location":"beginner/chatbot_tutorial/#\u6570\u636e\u52a0\u8f7d\u548c\u4fee\u526a","text":"\u6211\u4eec\u7684\u4e0b\u4e00\u4e2a\u4efb\u52a1\u662f\u521b\u5efa\u8bcd\u6c47\u8868\u5e76\u5c06\u8be2\u95ee/\u54cd\u5e94\u53e5\u5b50\u5bf9\u52a0\u8f7d\u5230\u5185\u5b58\u4e2d\u3002 \u6ce8\u610f\u6211\u4eec\u5904\u7406\u7684\u662f \u8bcd \u5e8f\u5217\uff0c\u5b83\u4eec\u6ca1\u6709\u9690\u5f0f\u6620\u5c04\u5230\u79bb\u6563\u6570\u503c\u7a7a\u95f4\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5fc5\u987b\u521b\u5efa\u4e00\u4e2a\u5c06\u6211\u4eec\u5728\u6570\u636e\u96c6\u4e2d\u9047\u5230\u7684\u6bcf\u4e2a\u552f\u4e00\u8bcd\u6620\u5c04\u5230\u7d22\u5f15\u503c\u7684\u6620\u5c04(Mapping)\u3002 \u4e3a\u6b64\uff0c\u6211\u4eec\u5b9a\u4e49\u4e00\u4e2a Voc \u7c7b\uff0c\u5b83\u4fdd\u5b58\u4ece\u8bcd\u5230\u7d22\u5f15\u503c\u7684\u6620\u5c04\uff0c\u4ee5\u53ca\u4ece\u7d22\u5f15\u503c\u5230\u8bcd\u7684\u53cd\u5411\u6620\u5c04\uff0c\u552f\u4e00\u8bcd\u7684\u6570\u91cf\uff0c\u603b\u8bcd\u6570\u3002 \u8fd9\u4e2a\u7c7b\u63d0\u4f9b\u4e86\u50cf\u8bcd\u8868\u6dfb\u52a0\u8bcd\u7684\u65b9\u6cd5( addWord )\uff0c\u6dfb\u52a0\u53e5\u5b50\u4e2d\u6240\u6709\u8bcd\u7684\u65b9\u6cd5( addSentence )\uff0c\u4ee5\u53ca\u524a\u51cf\u4e0d\u5e38\u89c1\u8bcd\u7684\u65b9\u6cd5( trim )\u3002 More on trimming later. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 # Default word tokens PAD_token = 0 # Used for padding short sentences SOS_token = 1 # Start-of-sentence token EOS_token = 2 # End-of-sentence token class Voc : def __init__ ( self , name ): self . name = name self . trimmed = False self . word2index = {} self . word2count = {} self . index2word = { PAD_token : \"PAD\" , SOS_token : \"SOS\" , EOS_token : \"EOS\" } self . num_words = 3 # Count SOS, EOS, PAD def addSentence ( self , sentence ): for word in sentence . split ( ' ' ): self . addWord ( word ) def addWord ( self , word ): if word not in self . word2index : self . word2index [ word ] = self . num_words self . word2count [ word ] = 1 self . index2word [ self . num_words ] = word self . num_words += 1 else : self . word2count [ word ] += 1 # Remove words below a certain count threshold def trim ( self , min_count ): if self . trimmed : return self . trimmed = True keep_words = [] for k , v in self . word2count . items (): if v >= min_count : keep_words . append ( k ) print ( 'keep_words {} / {} = {:.4f}' . format ( len ( keep_words ), len ( self . word2index ), len ( keep_words ) / len ( self . word2index ) )) # Reinitialize dictionaries self . word2index = {} self . word2count = {} self . index2word = { PAD_token : \"PAD\" , SOS_token : \"SOS\" , EOS_token : \"EOS\" } self . num_words = 3 # Count default tokens for word in keep_words : self . addWord ( word ) \u73b0\u5728\uff0c\u53ef\u4ee5\u5f00\u59cb\u88c5\u914d\u8bcd\u8868\u548c\u8be2\u95ee/\u56de\u590d\u53e5\u5b50\u5bf9\u3002\u5728\u51c6\u5907\u597d\u4f7f\u7528\u8fd9\u4e9b\u6570\u636e\u4e4b\u524d\uff0c\u6211\u4eec\u8fd8\u5f97\u6267\u884c\u4e00\u4e9b\u9884\u5904\u7406\u3002 \u9996\u5148\uff0c\u6211\u4eec\u5fc5\u987b\u4f7f\u7528 unicodeToAscii \u5c06\u5b57\u7b26\u4e32\u4ece Unicode \u8f6c\u4e3a ASCII \u3002 \u5176\u6b21\uff0c\u6211\u4eec\u5e94\u8be5\u628a\u6240\u6709\u5b57\u6bcd\u8f6c\u4e3a\u5c0f\u5199\uff0c\u5e76\u4e14\u88c1\u526a\u6389\u6240\u6709\u7684\u9664\u57fa\u672c\u6807\u70b9\u7b26\u53f7\u4e4b\u5916\u7684\u6240\u6709\u975e\u5b57\u6bcd\u5b57\u7b26( normalizeString )\u3002 \u6700\u540e\uff0c\u4e3a\u4e86\u8bad\u7ec3\u66f4\u5feb\u7684\u6536\u655b\uff0c\u6211\u4eec\u5c06\u8fc7\u6ee4\u6389( filterPairs )\u957f\u5ea6\u5927\u4e8e MAX_LENGTH \u9608\u503c\u7684\u53e5\u5b50\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 MAX_LENGTH = 10 # Maximum sentence length to consider # Turn a Unicode string to plain ASCII, thanks to # https://stackoverflow.com/a/518232/2809427 def unicodeToAscii ( s ): return '' . join ( c for c in unicodedata . normalize ( 'NFD' , s ) if unicodedata . category ( c ) != 'Mn' ) # Lowercase, trim, and remove non-letter characters def normalizeString ( s ): s = unicodeToAscii ( s . lower () . strip ()) s = re . sub ( r \"([.!?])\" , r \" \\1\" , s ) s = re . sub ( r \"[^a-zA-Z.!?]+\" , r \" \" , s ) s = re . sub ( r \"\\s+\" , r \" \" , s ) . strip () return s # Read query/response pairs and return a voc object def readVocs ( datafile , corpus_name ): print ( \"Reading lines...\" ) # Read the file and split into lines lines = open ( datafile , encoding = 'utf-8' ) . \\ read () . strip () . split ( ' \\n ' ) # Split every line into pairs and normalize pairs = [[ normalizeString ( s ) for s in l . split ( ' \\t ' )] for l in lines ] voc = Voc ( corpus_name ) return voc , pairs # Returns True if both sentences in a pair 'p' are under the MAX_LENGTH threshold def filterPair ( p ): # Input sequences need to preserve the last word for EOS token return all ( len ( s . split ( ' ' )) < MAX_LENGTH for s in p ) # Filter pairs using filterPair condition def filterPairs ( pairs ): return [ pair for pair in pairs if filterPair ( pair )] # Using the functions defined above, return a populated voc object and pairs list def loadPrepareData ( corpus , corpus_name , datafile , save_dir ): print ( \"Start preparing training data ...\" ) voc , pairs = readVocs ( datafile , corpus_name ) print ( \"Read {!s} sentence pairs\" . format ( len ( pairs ))) pairs = filterPairs ( pairs ) print ( \"Trimmed to {!s} sentence pairs\" . format ( len ( pairs ))) print ( \"Counting words...\" ) for pair in pairs : voc . addSentence ( pair [ 0 ]) voc . addSentence ( pair [ 1 ]) print ( \"Counted words:\" , voc . num_words ) return voc , pairs # Load/Assemble voc and pairs save_dir = os . path . join ( \"data\" , \"save\" ) voc , pairs = loadPrepareData ( corpus , corpus_name , datafile , save_dir ) # Print some pairs to validate print ( \" \\n pairs:\" ) for pair in pairs [: 10 ]: print ( pair ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 Start preparing training data ... Reading lines... Read 221282 sentence pairs Trimmed to 64217 sentence pairs Counting words... Counted words: 17996 pairs: ['there .', 'where ?'] ['you have my word . as a gentleman', 'you re sweet .'] ['hi .', 'looks like things worked out tonight huh ?'] ['you know chastity ?', 'i believe we share an art instructor'] ['have fun tonight ?', 'tons'] ['well no . . .', 'then that s all you had to say .'] ['then that s all you had to say .', 'but'] ['but', 'you always been this selfish ?'] ['do you listen to this crap ?', 'what crap ?'] ['what good stuff ?', 'the real you .'] \u53e6\u4e00\u79cd\u6709\u5229\u4e8e\u5728\u8bad\u7ec3\u671f\u95f4\u5b9e\u73b0\u66f4\u5feb\u6536\u655b\u7684\u7b56\u7565\u662f\u4fee\u526a\u6389\u6211\u4eec\u8bcd\u6c47\u8868\u4e2d\u5f88\u5c11\u4f7f\u7528\u7684\u5355\u8bcd\u3002\u51cf\u5c0f\u7279\u5f81\u7a7a\u95f4\u4e5f\u4f1a\u7f13\u548c\u6a21\u578b\u903c\u8fd1\u7684\u96be\u5ea6\u3002\u6211\u4eec\u5c06\u901a\u8fc7\u4e24\u4e2a\u6b65\u9aa4\u5b8c\u6210\u6b64\u64cd\u4f5c\uff1a \u7528 voc.trim \u4fee\u526a\u6389\u6570\u91cf\u5c0f\u4e8e MIN_COUNT \u9608\u503c\u7684\u8bcd\u3002 \u8fc7\u6ee4\u6389\u542b\u6709\u88ab\u4fee\u526a\u8bcd\u7684\u53e5\u5b50\u5bf9\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 MIN_COUNT = 3 # Minimum word count threshold for trimming def trimRareWords ( voc , pairs , MIN_COUNT ): # Trim words used under the MIN_COUNT from the voc voc . trim ( MIN_COUNT ) # Filter out pairs with trimmed words keep_pairs = [] for pair in pairs : input_sentence = pair [ 0 ] output_sentence = pair [ 1 ] keep_input = True keep_output = True # Check input sentence for word in input_sentence . split ( ' ' ): if word not in voc . word2index : keep_input = False break # Check output sentence for word in output_sentence . split ( ' ' ): if word not in voc . word2index : keep_output = False break # Only keep pairs that do not contain trimmed word(s) in their input or output sentence if keep_input and keep_output : keep_pairs . append ( pair ) print ( \"Trimmed from {} pairs to {}, {:.4f} of total\" . format ( len ( pairs ), len ( keep_pairs ), len ( keep_pairs ) / len ( pairs ))) return keep_pairs # Trim voc and pairs pairs = trimRareWords ( voc , pairs , MIN_COUNT ) 1 2 keep_words 7820 / 17993 = 0.4346 Trimmed from 64217 pairs to 53120, 0.8272 of total","title":"\u6570\u636e\u52a0\u8f7d\u548c\u4fee\u526a"},{"location":"beginner/chatbot_tutorial/#\u4e3a\u6a21\u578b\u51c6\u5907\u6570\u636e","text":"\u5c3d\u7ba1\u6211\u4eec\u5df2\u7ecf\u6295\u5165\u5f88\u5927\u7cbe\u529b\u6765\u51c6\u5907\u548c\u6574\u7406\u6570\u636e\uff0c\u628a\u5b83\u4eec\u653e\u5230\u4e86\u6f02\u4eae\u7684\u8bcd\u8868\u5bf9\u8c61\u548c\u53e5\u5b50\u5bf9\u5217\u8868\uff0c\u4f46\u6211\u4eec\u7684\u6a21\u578b\u6700\u7ec8\u9700\u8981\u7684\u7684\u8f93\u5165\u5374\u662fTensor\u7684\u5f20\u91cf\u6570\u5b57\u3002 \u53ef\u4ee5\u5728 Seq2Seq\u7ffb\u8bd1\u6559\u7a0b \u4e2d\u627e\u5230\u4e3a\u6a21\u578b\u51c6\u5907\u9884\u5904\u7406\u6570\u636e\u7684\u65b9\u6cd5\u3002 \u5728\u90a3\u4e2a\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528\u7684\u6279\u6b21\u503c\u4e3a1\uff0c\u8fd9\u610f\u5473\u7740\u6211\u4eec\u6240\u8981\u505a\u7684\u5c31\u662f\u5c06\u53e5\u5b50\u5bf9\u4e2d\u7684\u5355\u8bcd\u8f6c\u6362\u4e3a\u8bcd\u6c47\u8868\u4e2d\u5bf9\u5e94\u7684\u7d22\u5f15\u503c\uff0c\u5e76\u5c06\u5176\u63d0\u4f9b\u7ed9\u6a21\u578b\u3002 \u53e6\u5916\uff0c\u5982\u679c\u4f60\u5bf9\u52a0\u901f\u57f9\u8bad\u548c/\u6216\u60f3\u8981\u5229\u7528GPU\u5e76\u884c\u5316\u529f\u80fd\u611f\u5174\u8da3\uff0c\u5219\u9700\u8981\u4f7f\u7528\u5c0f\u6279\u91cf\u57f9\u8bad\u3002 \u4f7f\u7528\u5c0f\u6279\u91cf\u4e5f\u610f\u5473\u7740\u6211\u4eec\u5fc5\u987b\u6ce8\u610f\u6279\u91cf\u4e2d\u53e5\u5b50\u957f\u5ea6\u7684\u53d8\u5316\u3002 \u4e3a\u4e86\u5bb9\u7eb3\u540c\u4e00\u6279\u6b21\u4e2d\u4e0d\u540c\u5927\u5c0f\u7684\u53e5\u5b50\uff0c\u6211\u4eec\u8981\u8ba9\u6279\u91cf\u8f93\u5165\u5f20\u91cf\u7684\u5f62\u72b6 (max_length\uff0cbatch_size) \u4e2d\u7684\u77ed\u4e8e max_length \u7684\u53e5\u5b50\u5728 EOS_token \u4e4b\u540e\u7528\u96f6\u586b\u5145\u3002 \u5982\u679c\u6211\u4eec\u53ea\u662f\u7b80\u5355\u5730\u901a\u8fc7\u5c06\u5355\u8bcd\u8f6c\u6362\u4e3a\u5176\u7d22\u5f15\u503c( indexesFromSentence )\u548c\u96f6\u586b\u5145\u7684\u65b9\u6cd5\u5c06\u82f1\u8bed\u53e5\u5b50\u8f6c\u6362\u4e3a\u5f20\u91cf\uff0c\u5f20\u91cf\u7684\u5f62\u72b6\u5c06\u662f (batch_size\uff0cmax_length) \uff0c\u5e76\u4e14\u5728\u7b2c\u4e00\u7ef4\u4e0a\u7684\u7d22\u5f15\u5c06\u5728\u6240\u6709\u65f6\u95f4\u6b65\u9aa4\u4e2d\u8fd4\u56de\u5b8c\u6574\u5e8f\u5217\u3002 \u4f46\u662f\uff0c\u6211\u4eec\u9700\u8981\u80fd\u591f\u6cbf\u7740\u65f6\u95f4\u3001\u8de8\u6279\u6b21\u3001\u5728\u6240\u6709\u5e8f\u5217\u4e0a\u8fdb\u884c\u7d22\u5f15\u3002 \u56e0\u6b64\uff0c\u6211\u4eec\u5c06\u8f93\u5165\u6279\u5904\u7406\u7684\u5f62\u72b6\u8f6c\u7f6e\u4e3a (max_length\uff0cbatch_size) \uff0c\u4ee5\u4fbf\u8de8\u7b2c\u4e00\u7ef4\u7684\u7d22\u5f15\u8fd4\u56de\u6279\u4e2d\u6240\u6709\u53e5\u5b50\u7684\u65f6\u95f4\u6b65\u957f\u3002 \u6211\u4eec\u5728 zeroPadding \u51fd\u6570\u4e2d\u9690\u5f0f\u5904\u7406\u8fd9\u4e2a\u8f6c\u7f6e\u3002 \u51fd\u6570 inputVar \u5904\u7406\u53e5\u5b50\u5230\u5f20\u91cf\u7684\u8f6c\u6362\u8fc7\u7a0b\uff0c\u6700\u7ec8\u521b\u5efa\u4e00\u4e2a\u5f62\u72b6\u6b63\u786e\u7684\u96f6\u586b\u5145\u5f20\u91cf\u3002 \u5b83\u8fd8\u8fd4\u56de\u6279\u6b21\u4e2d\u6bcf\u4e2a\u5e8f\u5217\u7684\u957f\u5ea6( lengths )\u7684\u5f20\u91cf\uff0c\u5b83\u7a0d\u540e\u4f1a\u88ab\u4f20\u7ed9\u7f16\u7801\u5668\u3002 \u51fd\u6570 outputVar \u7684\u6267\u884c\u8fc7\u7a0b\u4e0e inputVar \u7c7b\u4f3c\uff0c\u4f46\u662f\u4e0d\u8fd4\u56de\u957f\u5ea6\uff08 lenghts \uff09\u5f20\u91cf\uff0c\u800c\u662f\u8fd4\u56de\u4e8c\u8fdb\u5236\u7684\u63a9\u7801\u5f20\u91cf\u548c\u76ee\u6807\u53e5\u5b50\u7684\u6700\u5927\u957f\u5ea6\u3002 \u4e8c\u8fdb\u5236\u63a9\u7801\u4e0e\u8f93\u51fa\u76ee\u6807\u5f20\u91cf\u5177\u6709\u540c\u6837\u7684\u5f62\u72b6\uff0c\u4f46\u662f\u5176\u4e2d\u7684\u6bcf\u4e2a PAD_token \u5143\u7d20\u90fd\u4e3a0\uff0c\u5176\u5b83\u6240\u6709\u5143\u7d20\u90fd\u4e3a1\u3002 batch2TrainData \u7b80\u5355\u7684\u4f7f\u7528\u4e00\u5806\u53e5\u5b50\u5bf9\u5e76\u4f7f\u7528\u4e0a\u8ff0\u51fd\u6570\u8fd4\u56de\u8f93\u5165\u548c\u76ee\u6807\u5f20\u91cf\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 def indexesFromSentence ( voc , sentence ): return [ voc . word2index [ word ] for word in sentence . split ( ' ' )] + [ EOS_token ] def zeroPadding ( l , fillvalue = PAD_token ): return list ( itertools . zip_longest ( * l , fillvalue = fillvalue )) def binaryMatrix ( l , value = PAD_token ): m = [] for i , seq in enumerate ( l ): m . append ([]) for token in seq : if token == PAD_token : m [ i ] . append ( 0 ) else : m [ i ] . append ( 1 ) return m # Returns padded input sequence tensor and lengths def inputVar ( l , voc ): indexes_batch = [ indexesFromSentence ( voc , sentence ) for sentence in l ] lengths = torch . tensor ([ len ( indexes ) for indexes in indexes_batch ]) padList = zeroPadding ( indexes_batch ) padVar = torch . LongTensor ( padList ) return padVar , lengths # Returns padded target sequence tensor, padding mask, and max target length def outputVar ( l , voc ): indexes_batch = [ indexesFromSentence ( voc , sentence ) for sentence in l ] max_target_len = max ([ len ( indexes ) for indexes in indexes_batch ]) padList = zeroPadding ( indexes_batch ) mask = binaryMatrix ( padList ) mask = torch . ByteTensor ( mask ) padVar = torch . LongTensor ( padList ) return padVar , mask , max_target_len # Returns all items for a given batch of pairs def batch2TrainData ( voc , pair_batch ): pair_batch . sort ( key = lambda x : len ( x [ 0 ] . split ( \" \" )), reverse = True ) input_batch , output_batch = [], [] for pair in pair_batch : input_batch . append ( pair [ 0 ]) output_batch . append ( pair [ 1 ]) inp , lengths = inputVar ( input_batch , voc ) output , mask , max_target_len = outputVar ( output_batch , voc ) return inp , lengths , output , mask , max_target_len # Example for validation small_batch_size = 5 batches = batch2TrainData ( voc , [ random . choice ( pairs ) for _ in range ( small_batch_size )]) input_variable , lengths , target_variable , mask , max_target_len = batches print ( \"input_variable:\" , input_variable ) print ( \"lengths:\" , lengths ) print ( \"target_variable:\" , target_variable ) print ( \"mask:\" , mask ) print ( \"max_target_len:\" , max_target_len ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 input_variable: tensor([[122, 443, 70, 16, 111], [ 7, 45, 519, 997, 66], [ 26, 571, 3, 4, 2], [ 36, 4, 2, 2, 0], [ 4, 2, 0, 0, 0], [ 2, 0, 0, 0, 0]]) lengths: tensor([6, 5, 4, 4, 3]) target_variable: tensor([[ 192, 785, 7319, 88, 95], [ 4, 12, 4, 2556, 4], [ 2, 786, 2, 4, 4], [ 0, 2, 0, 2, 4], [ 0, 0, 0, 0, 2]]) mask: tensor([[1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [0, 1, 0, 1, 1], [0, 0, 0, 0, 1]], dtype=torch.uint8) max_target_len: 5","title":"\u4e3a\u6a21\u578b\u51c6\u5907\u6570\u636e"},{"location":"beginner/chatbot_tutorial/#\u5b9a\u4e49\u6a21\u578b","text":"","title":"\u5b9a\u4e49\u6a21\u578b"},{"location":"beginner/chatbot_tutorial/#seq2seq-\u6a21\u578b","text":"\u6211\u4eec\u8fd9\u4e2a\u804a\u5929\u673a\u5668\u4eba\u7684\u5927\u8111\u662f\u5e8f\u5217\u5230\u5e8f\u5217\uff08seq2seq\uff09\u6a21\u578b\u3002seq2seq \u6a21\u578b\u7684\u76ee\u6807\u662f\u5c06\u53ef\u53d8\u957f\u5ea6\u5e8f\u5217\u4f5c\u4e3a\u8f93\u5165\uff0c\u5e76\u4f7f\u7528\u56fa\u5b9a\u5927\u5c0f\u7684\u6a21\u578b\u5c06\u53ef\u53d8\u957f\u5ea6\u5e8f\u5217\u4f5c\u4e3a\u8f93\u51fa\u8fd4\u56de\u3002 Sutskever \u7b49\u4eba \u53d1\u73b0\u901a\u8fc7\u4f7f\u7528\u4e24\u4e2a\u72ec\u7acb\u7684\u9012\u5f52\u795e\u7ecf\u7f51\u7edc(RNN)\uff0c\u6211\u4eec\u53ef\u4ee5\u5b8c\u6210\u8fd9\u9879\u4efb\u52a1\u3002 \u7b2c\u4e00\u4e2a RNN \u5145\u5f53 \u7f16\u7801\u5668 \uff0c\u5b83\u5c06\u53ef\u53d8\u957f\u5ea6\u8f93\u5165\u5e8f\u5217\u7f16\u7801\u6210\u56fa\u5b9a\u957f\u5ea6\u7684\u4e0a\u4e0b\u6587\u5411\u91cf\u3002 \u7406\u8bba\u4e0a\uff0c\u8fd9\u4e2a\u4e0a\u4e0b\u6587\u5411\u91cf\uff08RNN\u7684\u6700\u7ec8\u9690\u85cf\u5c42\uff09\u5c06\u5305\u542b\u5173\u4e8e\u8f93\u5165\u7ed9\u673a\u5668\u4eba\u7684\u8be2\u95ee\u53e5\u7684\u8bed\u4e49\u4fe1\u606f\u3002 \u7b2c\u4e8c\u4e2a RNN \u662f \u89e3\u7801\u5668 \uff0c\u5b83\u91c7\u7528\u8f93\u5165\u8bcd\u548c\u4e0a\u4e0b\u6587\u5411\u91cf\uff0c\u5e76\u8fd4\u56de\u5e8f\u5217\u4e2d\u540e\u7eed\u8bcd\u7684\u731c\u6d4b\u503c\uff0c\u4ee5\u53ca\u7528\u4e8e\u4e0b\u6b21\u8fed\u4ee3\u7684\u9690\u85cf\u5c42\u3002 \u56fe\u7247\u6765\u6e90\uff1a https://jeddy92.github.io/JEddy92.github.io/ts_seq2seq_intro/","title":"Seq2Seq \u6a21\u578b"},{"location":"beginner/chatbot_tutorial/#\u7f16\u7801\u5668","text":"\u7f16\u7801\u5668RNN\u4e00\u6b21\u8fed\u4ee3\u8f93\u5165\u53e5\u5b50\u7684\u4e00\u4e2a\u6807\u8bb0(token)\uff0c\u6bcf\u4e2a\u65f6\u95f4\u6b65\u9aa4\u8f93\u51fa\u4e00\u4e2a\u201c\u8f93\u51fa\u201d\u5411\u91cf\u548c\u4e00\u4e2a\u201c\u9690\u85cf\u72b6\u6001\u201d\u5411\u91cf\u3002 \u7136\u540e\u5c06\u9690\u85cf\u72b6\u6001\u5411\u91cf\u4f20\u9012\u7ed9\u4e0b\u4e00\u4e2a\u65f6\u95f4\u6b65\u9aa4\uff0c\u540c\u65f6\u8bb0\u5f55\u8f93\u51fa\u5411\u91cf\u3002 \u7f16\u7801\u5668\u5c06\u5176\u5728\u5e8f\u5217\u4e2d\u7684\u6bcf\u4e2a\u70b9\u770b\u5230\u7684\u4e0a\u4e0b\u6587\u8f6c\u6362\u4e3a\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u7684\u4e00\u7ec4\u70b9\uff0c\u89e3\u7801\u5668\u5c06\u4f7f\u7528\u8fd9\u7ec4\u70b9\u6765\u4e3a\u7ed9\u5b9a\u4efb\u52a1\u751f\u6210\u6709\u610f\u4e49\u7684\u8f93\u51fa\u3002 \u6211\u4eec\u8fd9\u4e2a\u7f16\u7801\u5668\u7684\u7684\u6838\u5fc3\u662f\u591a\u5c42\u95e8\u63a7\u5355\u5143(GRU)\uff0c\u7531 Cho \u7b49\u4eba \u4e8e2014\u5e74\u53d1\u660e\u3002 \u6211\u4eec\u5c06\u4f7f\u7528GRU\u7684\u4e00\u79cd\u53d8\u79cd\u2014\u2014\u53cc\u5411GRU\uff0c\u5b83\u4f7f\u7528\u4e24\u79cd\u72ec\u7acb\u7684RNN\uff1a\u4e00\u4e2a\u4ee5\u6b63\u5e38\u7684\u987a\u5e8f\u63a5\u6536\u8f93\u5165\u5e8f\u5217\uff0c\u53e6\u4e00\u4e2a\u4ee5\u53cd\u65b9\u5411\u63a5\u6536\u8f93\u5165\u5e8f\u5217\u3002 \u5728\u540c\u4e00\u65f6\u95f4\u6b65\u9aa4\u4e2d\u5bf9\u6bcf\u4e2a\u7f51\u7edc\u7684\u8f93\u51fa\u6c42\u548c\u3002 \u4f7f\u7528\u53cc\u5411GRU\u8bb2\u7ed9\u6211\u4eec\u5e26\u6765\u5bf9\u8fc7\u53bb\u548c\u672a\u6765\u4e0a\u4e0b\u6587\u8fdb\u884c\u7f16\u7801\u7684\u4f18\u52bf\u3002 \u53cc\u5411RNN: \u56fe\u7247\u6765\u6e90\uff1a https://colah.github.io/posts/2015-09-NN-Types-FP/ \u6ce8\u610f\uff0c\u4e00\u4e2a embedding \u5c42\u7528\u4e8e\u5728\u4efb\u610f\u5927\u5c0f\u7684\u7279\u5f81\u7a7a\u95f4\u4e2d\u7f16\u7801\u6211\u4eec\u7684\u5355\u8bcd\u7d22\u5f15\u3002 \u5bf9\u4e8e\u6211\u4eec\u7684\u6a21\u578b\uff0c\u8fd9\u4e2a\u5c42\u4f1a\u5c06\u6bcf\u4e2a\u8bcd\u6620\u5c04\u5230\u5927\u5c0f\u4e3a hidden_size \u7684\u7279\u5f81\u7a7a\u95f4\u3002 \u8bad\u7ec3\u540e\uff0c\u8fd9\u4e9b\u503c\u5e94\u8be5\u7f16\u7801\u4e86\u8fd1\u4e49\u8bcd\u4e4b\u95f4\u7684\u8bed\u4e49\u76f8\u4f3c\u5ea6\u3002 \u6700\u6709\uff0c\u5982\u679c\u8981\u586b\u5145\u540e\u7684\u4e00\u6279\u5e8f\u5217\u4f20\u5165RNN\u6a21\u5757\uff0c\u6211\u4eec\u5fc5\u987b\u56f4\u7ed5RNN\u8fdb\u884c\u6253\u5305\u548c\u89e3\u5305\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5206\u522b\u662f\uff1a - nn.utils.rnn.pack_padded_sequence - nn.utils.rnn.pad_packed_sequence \u8ba1\u7b97\u56fe\uff1a \u5c06\u8bcd\u7684\u7d22\u5f15\u503c\u8f6c\u4e3a\u5d4c\u5165 \u4e3aRNN\u6a21\u5757\u6253\u5305\u586b\u5145\u540e\u7684\u5e8f\u5217\u6279\u6b21 \u901a\u8fc7GRU\u524d\u5411\u4f20\u9012 \u89e3\u5305\u586b\u5145 \u53cc\u5411GRU\u8f93\u51fa\u6c42\u548c \u8fd4\u56de\u8f93\u51fa\u548c\u6700\u7ec8\u9690\u85cf\u72b6\u6001 \u8f93\u5165\uff1a input_seq : \u8f93\u5165\u53e5\u5b50\u6279\u6b21\uff1b\u5f62\u72b6= (max_length,batch_size) input_lengths : \u7531\u6279\u6b21\u4e2d\u6bcf\u4e2a\u53e5\u5b50\u7684\u957f\u5ea6\u7684\u6240\u6784\u6210\u7684\u5217\u8868\uff1b\u5f62\u72b6= (batch_size) hidden : \u9690\u85cf\u72b6\u6001\uff1b\u5f62\u72b6= (n_layers x num_directions, batch_size, hidden_size) \u8f93\u51fa\uff1a outputs : \u4eceGRN\u6700\u7ec8\u9690\u85cf\u5c42\u7684\u8f93\u51fa\u7279\u5f81\uff1b\u5f62\u72b6= (max_length, batch_size, hidden_size) hidden : \u4eceGRU\u66f4\u65b0\u7684\u9690\u85cf\u72b6\u6001\uff1b\u5f62\u72b6= (n_layers x num_directions, batch_size, hidden_size) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class EncoderRNN ( nn . Module ): def __init__ ( self , hidden_size , embedding , n_layers = 1 , dropout = 0 ): super ( EncoderRNN , self ) . __init__ () self . n_layers = n_layers self . hidden_size = hidden_size self . embedding = embedding # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size' # because our input size is a word embedding with number of features == hidden_size self . gru = nn . GRU ( hidden_size , hidden_size , n_layers , dropout = ( 0 if n_layers == 1 else dropout ), bidirectional = True ) def forward ( self , input_seq , input_lengths , hidden = None ): # Convert word indexes to embeddings embedded = self . embedding ( input_seq ) # Pack padded batch of sequences for RNN module packed = nn . utils . rnn . pack_padded_sequence ( embedded , input_lengths ) # Forward pass through GRU outputs , hidden = self . gru ( packed , hidden ) # Unpack padding outputs , _ = nn . utils . rnn . pad_packed_sequence ( outputs ) # Sum bidirectional GRU outputs outputs = outputs [:, :, : self . hidden_size ] + outputs [:, : , self . hidden_size :] # Return output and final hidden state return outputs , hidden","title":"\u7f16\u7801\u5668"},{"location":"beginner/chatbot_tutorial/#\u89e3\u7801\u5668","text":"\u89e3\u7801\u5668RNN\u4ee5\u201c\u4e00\u4e2a\u6807\u8bb0\u63a5\u4e00\u4e2a\u6807\u8bb0(token-by-token)\u201d\u7684\u5f62\u5f0f\u751f\u6210\u56de\u590d\u53e5\u5b50\u3002 \u5b83\u4f7f\u7528\u7f16\u7801\u5668\u7684\u4e0a\u4e0b\u6587\u5411\u91cf\u548c\u5185\u7f6e\u9690\u85cf\u72b6\u6001\u751f\u6210\u5e8f\u5217\u4e2d\u7684\u540e\u7eed\u8bcd\u3002 \u5b83\u6301\u7eed\u7684\u751f\u6210\u8bcd\uff0c\u76f4\u5230\u8f93\u51fa EOS_token \u2014\u2014\u8868\u793a\u53e5\u5b50\u7684\u7ed3\u675f\u3002 \u5bfb\u5e38\u7684Seq2Seq\u89e3\u7801\u5668\u5e38\u5e38\u9047\u5230\u7684\u4e00\u4e2a\u95ee\u9898\u5c31\u662f\uff0c\u5982\u679c\u6211\u4eec\u4f9d\u8d56\u4e8e\u4e0a\u4e0b\u6587\u5411\u91cf\u6765\u7f16\u7801\u6574\u4e2a\u8f93\u5165\u7684\u8bed\u4e49\uff0c\u90a3\u4e48\u6211\u4eec\u5f88\u53ef\u80fd\u4e22\u5931\u4fe1\u606f\u3002 \u5728\u5904\u7406\u957f\u8f93\u5165\u5e8f\u5217\u7684\u65f6\u5019\u5c24\u5176\u5982\u6b64\uff0c\u8fd9\u6781\u5927\u7684\u9650\u5236\u4e86\u6211\u4eec\u8fd9\u4e2a\u89e3\u7801\u5668\u7684\u80fd\u529b\u3002 \u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c Bahdanau \u7b49\u4eba \u521b\u5efa\u4e86\u201c\u6ce8\u610f\u529b\u673a\u5236\u201d\uff0c\u5141\u8bb8\u89e3\u7801\u5668\u53ea\u5173\u6ce8\u8f93\u5165\u5e8f\u5217\u7684\u67d0\u4e9b\u90e8\u5206\uff0c\u800c\u4e0d\u662f\u5728\u6bcf\u4e00\u6b65\u90fd\u4f7f\u7528\u6574\u4e2a\u56fa\u5b9a\u7684\u4e0a\u4e0b\u6587\u3002 \u5728\u4e0a\u5c42\uff0c\u7528\u89e3\u7801\u5668\u7684\u5f53\u524d\u9690\u85cf\u72b6\u6001\u548c\u7f16\u7801\u5668\u7684\u8f93\u51fa\u8ba1\u7b97\u6ce8\u610f\u529b\u3002 \u8f93\u51fa\u6ce8\u610f\u529b\u6743\u91cd\u548c\u8f93\u5165\u5e8f\u5217\u5177\u6709\u76f8\u540c\u7684\u5f62\u72b6\uff0c\u8fd9\u8ba9\u6211\u4eec\u53ef\u4ee5\u5c06\u5b83\u548c\u7f16\u7801\u5668\u8f93\u51fa\u76f8\u4e58\uff0c\u5f97\u5230\u7f16\u7801\u5668\u8f93\u51fa\u4e2d\u7684\u8981\u88ab\u52a0\u4ee5\u6ce8\u610f\u529b\u7684\u90e8\u5206\u7684\u52a0\u6743\u548c\u3002 Sean Robertson \u7684\u56fe\u793a\u5f88\u597d\u7684\u63cf\u8ff0\u4e86\u8fd9\u70b9\uff1a Luong \u7b49\u4eba \u521b\u5efa\u4e86\u201c\u5168\u5c40\u6ce8\u610f\u529b\u201d\u6765\u6539\u8fdb Bahdanau \u7b49\u4eba \u7684\u57fa\u7840\u5de5\u4f5c\u3002 \u201c\u5168\u5c40\u6ce8\u610f\u529b\u201d\u6700\u5173\u952e\u7684\u4e0d\u540c\u4e4b\u5904\u5728\u4e8e\uff1a\u5b83\u4f1a\u8003\u8651\u6240\u6709\u7684\u7f16\u7801\u5668\u9690\u85cf\u72b6\u6001\uff0c\u800c\u4e0d\u662f Bahdanau \u7b49\u4eba \u7684\u53ea\u8003\u8651\u5f53\u524d\u65f6\u95f4\u6b65\u9aa4\u4e2d\u7684\u7f16\u7801\u5668\u9690\u85cf\u72b6\u6001\u7684\u201c\u5c40\u90e8\u6ce8\u610f\u529b\u201d\u65b9\u5f0f\u3002 \u53e6\u4e00\u4e2a\u4e0d\u540c\u4e4b\u5904\u5728\u4e8e\uff0c\u4f7f\u7528\u201c\u5168\u5c40\u6ce8\u610f\u529b\u201d\uff0c\u6211\u4eec\u4ec5\u4ec5\u4f7f\u7528\u5f53\u524d\u65f6\u95f4\u6b65\u9aa4\u7684\u7f16\u7801\u5668\u7684\u9690\u85cf\u72b6\u6001\u6765\u8ba1\u7b97\u6ce8\u610f\u529b\u7684\u6743\u91cd\u6216\u80fd\u91cf\u503c\u3002 Bahdanau \u7b49\u4eba \u7684\u6ce8\u610f\u529b\u8ba1\u7b97\u9700\u8981\u4e86\u89e3\u4e0a\u4e00\u4e2a\u65f6\u95f4\u6b65\u9aa4\u4e2d\u7f16\u7801\u5668\u7684\u72b6\u6001\u3002 \u6b64\u5916\uff0c Luong \u7b49\u4eba \u63d0\u4f9b\u4e86\u7528\u4e8e\u8ba1\u7b97\u7f16\u7801\u5668\u8f93\u51fa\u548c\u89e3\u7801\u5668\u8f93\u51fa\u4e4b\u95f4\u7684\u6ce8\u610f\u529b\u7684\u7684\u591a\u79cd\u65b9\u6cd5\uff0c\u4ed6\u4eec\u88ab\u6210\u4e3a\u201c\u5f97\u5206\u51fd\u6570\u201d\uff08score functions\uff09\uff1a \u5176\u4e2d\uff0c - h_t h_t \u4e3a\u5f53\u524d\u76ee\u6807\u89e3\u7801\u5668\u72b6\u7684\u6001 - \\bar{h}_s \\bar{h}_s \u4e3a\u6240\u6709\u7f16\u7801\u5668\u7684\u72b6\u6001 \u603b\u4f53\u800c\u8a00\uff0c\u5168\u5c40\u6ce8\u610f\u529b\u673a\u5236\u53ef\u4ee5\u901a\u8fc7\u4e0b\u56fe\u6765\u603b\u7ed3\u3002 \u6ce8\u610f\u6211\u4eec\u5c06\u5728\u88ab\u79f0\u4f5c Attn \u7684\u5206\u79bb\u7684 nn.Module \u4e2d\u5b9e\u73b0\u201c\u6ce8\u610f\u529b\u5c42\u201d\u3002 \u8fd9\u4e2a\u6a21\u5757\u7684\u8f93\u51fa\u662f\u4e00\u4e2a softmax \u6807\u51c6\u5316\u6743\u91cd\u5f20\u91cf\uff0c\u5176\u5f62\u72b6\u662f (batch_size, 1, max_length) \u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 # Luong attention layer class Attn ( nn . Module ): def __init__ ( self , method , hidden_size ): super ( Attn , self ) . __init__ () self . method = method if self . method not in [ 'dot' , 'general' , 'concat' ]: raise ValueError ( self . method , \"is not an appropriate attention method.\" ) self . hidden_size = hidden_size if self . method == 'general' : self . attn = nn . Linear ( self . hidden_size , hidden_size ) elif self . method == 'concat' : self . attn = nn . Linear ( self . hidden_size * 2 , hidden_size ) self . v = nn . Parameter ( torch . FloatTensor ( hidden_size )) def dot_score ( self , hidden , encoder_output ): return torch . sum ( hidden * encoder_output , dim = 2 ) def general_score ( self , hidden , encoder_output ): energy = self . attn ( encoder_output ) return torch . sum ( hidden * energy , dim = 2 ) def concat_score ( self , hidden , encoder_output ): energy = self . attn ( torch . cat (( hidden . expand ( encoder_output . size ( 0 ), - 1 , - 1 ), encoder_output ), 2 )) . tanh () return torch . sum ( self . v * energy , dim = 2 ) def forward ( self , hidden , encoder_outputs ): # Calculate the attention weights (energies) based on the given method if self . method == 'general' : attn_energies = self . general_score ( hidden , encoder_outputs ) elif self . method == 'concat' : attn_energies = self . concat_score ( hidden , encoder_outputs ) elif self . method == 'dot' : attn_energies = self . dot_score ( hidden , encoder_outputs ) # Transpose max_length and batch_size dimensions attn_energies = attn_energies . t () # Return the softmax normalized probability scores (with added dimension) return F . softmax ( attn_energies , dim = 1 ) . unsqueeze ( 1 ) \u73b0\u5728\uff0c\u6211\u4eec\u5df2\u7ecf\u5b9a\u4e49\u4e86\u6ce8\u610f\u529b\u5b50\u6a21\u5757\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5b9e\u9645\u7684\u89e3\u7801\u5668\u6a21\u5757\u4e86\u3002 \u5bf9\u4e8e\u89e3\u7801\u5668\uff0c\u6211\u4eec\u5c06\u624b\u52a8\u7684\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u9aa4\u4e2d\u63d0\u4f9b\u6279\u6570\u636e\u3002 \u8fd9\u610f\u5473\u7740\u6211\u4eec\u7684\u5d4c\u5165\u8bcd\u5f20\u91cf\u548cGRU\u8f93\u51fa\u7684\u5f62\u72b6\u90fd\u662f (1, batch_size, hidden_size) \u3002 \u8ba1\u7b97\u56fe\uff1a \u83b7\u5f97\u5f53\u524d\u8f93\u5165\u8bcd\u7684\u5f3a\u5982\u5d4c\u5165\u3002 \u5355\u5411GRU\u524d\u5411\u3002 \u6709\u7b2c\u4e8c\u6b65\u7684GRU\u8f93\u51fa\u8ba1\u7b97\u6ce8\u610f\u529b\u6743\u91cd\u3002 \u6ce8\u610f\u529b\u6743\u91cd\u4e0e\u7f16\u7801\u5668\u8f93\u51fa\u76f8\u4e58\uff0c\u5f97\u5230\u201c\u52a0\u6743\u548c\u201d(weighted sum)\u4e0a\u4e0b\u6587\u5411\u91cf\u3002 \u4f7f\u7528 Luong \u7b49\u4eba\u7684\u65b9\u6cd5\u5c06\u52a0\u6743\u4e0a\u4e0b\u6587\u5411\u91cf\u548cGRU\u8f93\u51fa\u76f8\u52a0\u3002 \u4f7f\u7528 Luong \u7b49\u4eba\u7684\u65b9\u6cd5(\u4e0d\u7528 softmax)\u9884\u6d4b\u540e\u7eed\u8bcd\u3002 \u8fd4\u56de\u8f93\u51fa\u548c\u6700\u7ec8\u9690\u85cf\u5c42\u3002 \u8f93\u5165\uff1a input_step : \u8f93\u5165\u5e8f\u5217\u6279\u7684\u4e00\u4e2a\u65f6\u95f4\u6b65\u9aa4 (\u4e00\u4e2a\u8bcd)\uff1b\u5f62\u72b6= (1, batch_size) last_hidden : GRU\u7684\u6700\u7ec8\u9690\u85cf\u5c42\uff1b\u5f62\u72b6=(n_layers x num_directions, batch_size, hidden_size)* encoder_outputs : \u7f16\u7801\u5668\u7684\u6a21\u578b\u8f93\u51fa; \u6027\u8f6c= (max_length, batch_size, hidden_size) \u8f93\u51fa\uff1a \u8f93\u51fa : softmax \u6b63\u89c4\u5316\u5f20\u91cf\uff0c\u7ed9\u51fa\u4e86\u88ab\u89e3\u7801\u5e8f\u5217\u4e2d\u6bcf\u4e2a\u8bcd\u662f\u6b63\u786e\u7684\u540e\u7eed\u8bcd\u7684\u6982\u7387; \u5f62\u72b6= (batch_size, voc.num_words) hidden : GRU\u7684\u6700\u7ec8\u9690\u85cf\u72b6\u6001; \u5f62\u72b6= (n_layers x num_directions, batch_size, hidden_size) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 class LuongAttnDecoderRNN ( nn . Module ): def __init__ ( self , attn_model , embedding , hidden_size , output_size , n_layers = 1 , dropout = 0.1 ): super ( LuongAttnDecoderRNN , self ) . __init__ () # Keep for reference self . attn_model = attn_model self . hidden_size = hidden_size self . output_size = output_size self . n_layers = n_layers self . dropout = dropout # Define layers self . embedding = embedding self . embedding_dropout = nn . Dropout ( dropout ) self . gru = nn . GRU ( hidden_size , hidden_size , n_layers , dropout = ( 0 if n_layers == 1 else dropout )) self . concat = nn . Linear ( hidden_size * 2 , hidden_size ) self . out = nn . Linear ( hidden_size , output_size ) self . attn = Attn ( attn_model , hidden_size ) def forward ( self , input_step , last_hidden , encoder_outputs ): # Note: we run this one step (word) at a time # Get embedding of current input word embedded = self . embedding ( input_step ) embedded = self . embedding_dropout ( embedded ) # Forward through unidirectional GRU rnn_output , hidden = self . gru ( embedded , last_hidden ) # Calculate attention weights from the current GRU output attn_weights = self . attn ( rnn_output , encoder_outputs ) # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector context = attn_weights . bmm ( encoder_outputs . transpose ( 0 , 1 )) # Concatenate weighted context vector and GRU output using Luong eq. 5 rnn_output = rnn_output . squeeze ( 0 ) context = context . squeeze ( 1 ) concat_input = torch . cat (( rnn_output , context ), 1 ) concat_output = torch . tanh ( self . concat ( concat_input )) # Predict next word using Luong eq. 6 output = self . out ( concat_output ) output = F . softmax ( output , dim = 1 ) # Return output and final hidden state return output , hidden","title":"\u89e3\u7801\u5668"},{"location":"beginner/chatbot_tutorial/#\u5b9a\u4e49\u8bad\u7ec3\u8fc7\u7a0b","text":"","title":"\u5b9a\u4e49\u8bad\u7ec3\u8fc7\u7a0b"},{"location":"beginner/chatbot_tutorial/#\u635f\u5931\u63a9\u7801","text":"\u7531\u4e8e\u6211\u4eec\u5904\u7406\u7684\u662f\u6279\u91cf\u586b\u5145\u5e8f\u5217\uff0c\u56e0\u6b64\u5728\u8ba1\u7b97\u635f\u5931\u65f6\u6211\u4eec\u4e0d\u80fd\u7b80\u5355\u5730\u4ec5\u8003\u8651\u5f20\u91cf\u7684\u5168\u90e8\u5143\u7d20\u3002 \u800c\u662f\u901a\u8fc7\u5b9a\u4e49 maskNLLLoss \u635f\u5931\u51fd\u6570\uff0c\u57fa\u4e8e\u89e3\u7801\u5668\u8f93\u51fa\u5f20\u91cf\uff0c\u76ee\u6807\u5f20\u91cf\u548c\u63cf\u8ff0\u76ee\u6807\u5f20\u91cf\u586b\u5145\u7684\u4e8c\u8fdb\u5236\u63a9\u7801\u5f20\u91cf\uff0c\u6765\u8ba1\u7b97\u635f\u5931\u3002 \u8be5\u635f\u5931\u51fd\u6570\u8ba1\u7b97\u4e86\u5bf9\u5e94\u4e8e\u63a9\u7801\u5411\u91cf\u4e2d 1 \u7684\u5143\u7d20\u7684\u8d1f\u5bf9\u6570\u76f8\u4f3c\u5ea6\u3002 1 2 3 4 5 6 def maskNLLLoss ( inp , target , mask ): nTotal = mask . sum () crossEntropy = - torch . log ( torch . gather ( inp , 1 , target . view ( - 1 , 1 )) . squeeze ( 1 )) loss = crossEntropy . masked_select ( mask ) . mean () loss = loss . to ( device ) return loss , nTotal . item ()","title":"\u635f\u5931\u63a9\u7801"},{"location":"beginner/chatbot_tutorial/#\u5355\u6b21\u8bad\u7ec3\u7684\u8fed\u4ee3","text":"\u51fd\u6570 train \u5305\u542b\u4e00\u4e2a\u5355\u6b21\u8bad\u7ec3\u8fed\u4ee3\uff08\u5355\u4e2a\u8f93\u5165\u6279\u6b21\uff09\u7b97\u6cd5\u3002 \u6211\u4eec\u7528\u4e00\u4e9b\u5de7\u5999\u7684\u6280\u5de7\u6765\u4fc3\u8fdb\u6536\u655b\uff1a \u7b2c\u4e00\u4e2a\u6280\u5de7\u662f\u4f7f\u7528 tacher forcing \u3002 \u8fd9\u610f\u5473\u7740\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u8bbe\u7f6e teacher_forcing_ratio \uff0c\u6211\u4eec\u4f7f\u7528\u5f53\u524d\u76ee\u6807\u8bcd\uff0c\u800c\u4e0d\u662f\u4f7f\u7528\u89e3\u7801\u5668\u7684\u5f53\u524d\u731c\u6d4b\u7ed3\u679c\uff0c\u4f5c\u4e3a\u89e3\u7801\u5668\u7684\u540e\u7eed\u8f93\u5165\u3002 \u8fd9\u9879\u6280\u672f\u662f\u8bad\u7ec3\u89e3\u7801\u5668\u7684\u8f6e\u5b50\uff0c\u6709\u52a9\u4e8e\u66f4\u6709\u6548\u7684\u8bad\u7ec3\u3002 \u201ctacher forcing\u201d \u4f1a\u5bfc\u81f4\u6a21\u578b\u5728\u63a8\u7406\u671f\u95f4\u4e0d\u7a33\u5b9a\uff0c\u8fd9\u662f\u56e0\u4e3a\u89e3\u7801\u5668\u5728\u8bad\u7ec3\u4e2d\u6ca1\u6709\u8db3\u591f\u7684\u673a\u4f1a\u771f\u6b63\u7684\u5236\u4f5c\u5b83\u81ea\u5df1\u7684\u8f93\u51fa\u5e8f\u5217\u3002 \u6240\u6709\uff0c\u6211\u4eec\u5fc5\u987b\u6ce8\u610f\u5730\u5982\u4f55\u8bbe\u7f6e teacher_forcing_ratio \uff0c\u4e0d\u8981\u88ab\u5feb\u901f\u6536\u655b\u6b3a\u9a97\u3002 \u6211\u4eec\u8981\u5b9e\u73b0\u7684\u7b2c\u4e8c\u4e2a\u6280\u5de7\u662f \u68af\u5ea6\u526a\u5207 \u3002 \u8fd9\u662f\u4e00\u79cd\u7528\u4e8e\u5bf9\u6297\u201c\u68af\u5ea6\u7206\u70b8\u201d\u7684\u5e38\u7528\u6280\u672f\u3002 \u672c\u8d28\u4e0a\uff0c\u901a\u8fc7\u526a\u5207\u6216\u8005\u6700\u5927\u9608\u503c\uff0c\u6211\u4eec\u9632\u6b62\u68af\u5ea6\u7206\u70b8\u6027\u589e\u957f\u548c\u6ea2\u51fa(NaN)\uff0c\u6216\u8005\u5728\u6210\u672c\u51fd\u6570\u4e2d\u4ece\u60ac\u5d16\u8dcc\u843d\u3002 \u56fe\u50cf\u6765\u6e90: Goodfellow \u7b49\u4eba Deep Learning . 2016. \u8fd0\u7b97\u987a\u5e8f\uff1a \u901a\u8fc7\u7f16\u7801\u5668\u5411\u524d\u4f20\u9012\u6574\u4e2a\u8f93\u5165\u6279\u6b21\u3002 \u5c06\u89e3\u7801\u5668\u8f93\u5165\u521d\u59cb\u5316\u4e3a SOS_token \u548c\u7f16\u7801\u5668\u6700\u7ec8\u9690\u85cf\u5c42\u7684\u9690\u85cf\u72b6\u6001\u3002 \u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u9aa4\u4e2d\uff0c\u901a\u8fc7\u89e3\u7801\u5668\u5411\u524d\u4f20\u9012\u8f93\u5165\u6279\u7684\u5e8f\u5217\u3002 \u5982\u679c\u7528\u5230\u4e86 \u201ctacher forcing\u201d\uff1a\u628a\u4e0b\u4e00\u4e2a\u89e3\u7801\u5668\u8f93\u5165\u4f5c\u4e3a\u5f53\u524d\u76ee\u6807\uff1b\u5176\u5b83\uff1a\u7528\u4e0b\u4e00\u4e2a\u89e3\u7801\u5668\u8f93\u5165\u4f5c\u4e3a\u5f53\u524d\u89e3\u7801\u5668\u8f93\u51fa\u3002 \u8ba1\u7b97\u548c\u7d2f\u79ef\u635f\u5931\u3002 \u8fdb\u884c\u53cd\u5411\u4f20\u64ad \u526a\u5207\u68af\u5ea6\u3002 \u66f4\u65b0\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u6a21\u578b\u7684\u53c2\u6570\u3002 \u6ce8\u610f \u53ea\u9700\u5c06 PyTorch RNN \u6a21\u5757( RNN , LSTM , GRU )\u7684\u6574\u4e2a\u8f93\u5165\u5e8f\u5217\uff08\u6216\u6279\u6b21\u7684\u5e8f\u5217\uff09\u4f20\u5165\uff0c\u5b83\u4eec\u5c31\u53ef\u4ee5\u88ab\u7528\u4e8e\u4efb\u4f55\u5176\u5b83\u7c7b\u4f3c\u7684\u975e\u9012\u5f52\u5c42\u3002 \u6211\u4eec\u5728 encoder \u4e2d\u8fd9\u6837\u4f7f\u7528 GUR \u5c42\u3002 \u5b9e\u9645\u60c5\u51b5\u662f\uff0c\u5728\u5e95\u5c42\uff0c\u6bcf\u4e00\u4e2a\u65f6\u95f4\u6b65\u9aa4\u4e0a\u90fd\u6709\u4e00\u4e2a\u4ea4\u4e92\u8fc7\u7a0b\u8fed\u4ee3\u8ba1\u7b97\u9690\u85cf\u72b6\u6001\u3002 \u53e6\u5916\uff0c\u4e5f\u53ef\u4ee5\u6bcf\u4e2a\u65f6\u95f4\u6b65\u9aa4\u8fd0\u884c\u8fd9\u4e9b\u6a21\u578b\u3002 \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u5728\u8bad\u7ec3\u8fc7\u7a0b\u91cc\u624b\u52a8\u5faa\u73af\u904d\u5386\u5e8f\u5217\uff0c\u5c31\u50cf\u5bf9 decoder \u6a21\u578b\u505a\u7684\u90a3\u6837\u3002 \u53ea\u8981\u7ef4\u62a4\u597d\u8fd9\u4e9b\u6a21\u5757\u6b63\u786e\u7684\u6982\u5ff5\u6a21\u578b\uff0c\u5b9e\u73b0\u5e8f\u5217\u6a21\u578b\u5c31\u53ef\u4ee5\u975e\u5e38\u7b80\u5355\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def train ( input_variable , lengths , target_variable , mask , max_target_len , encoder , decoder , embedding , encoder_optimizer , decoder_optimizer , batch_size , clip , max_length = MAX_LENGTH ): # Zero gradients encoder_optimizer . zero_grad () decoder_optimizer . zero_grad () # Set device options input_variable = input_variable . to ( device ) lengths = lengths . to ( device ) target_variable = target_variable . to ( device ) mask = mask . to ( device ) # Initialize variables loss = 0 print_losses = [] n_totals = 0 # Forward pass through encoder encoder_outputs , encoder_hidden = encoder ( input_variable , lengths ) # Create initial decoder input (start with SOS tokens for each sentence) decoder_input = torch . LongTensor ([[ SOS_token for _ in range ( batch_size )]]) decoder_input = decoder_input . to ( device ) # Set initial decoder hidden state to the encoder's final hidden state decoder_hidden = encoder_hidden [: decoder . n_layers ] # Determine if we are using teacher forcing this iteration use_teacher_forcing = True if random . random () < teacher_forcing_ratio else False # Forward batch of sequences through decoder one time step at a time if use_teacher_forcing : for t in range ( max_target_len ): decoder_output , decoder_hidden = decoder ( decoder_input , decoder_hidden , encoder_outputs ) # Teacher forcing: next input is current target decoder_input = target_variable [ t ] . view ( 1 , - 1 ) # Calculate and accumulate loss mask_loss , nTotal = maskNLLLoss ( decoder_output , target_variable [ t ], mask [ t ]) loss += mask_loss print_losses . append ( mask_loss . item () * nTotal ) n_totals += nTotal else : for t in range ( max_target_len ): decoder_output , decoder_hidden = decoder ( decoder_input , decoder_hidden , encoder_outputs ) # No teacher forcing: next input is decoder's own current output _ , topi = decoder_output . topk ( 1 ) decoder_input = torch . LongTensor ([[ topi [ i ][ 0 ] for i in range ( batch_size )]]) decoder_input = decoder_input . to ( device ) # Calculate and accumulate loss mask_loss , nTotal = maskNLLLoss ( decoder_output , target_variable [ t ], mask [ t ]) loss += mask_loss print_losses . append ( mask_loss . item () * nTotal ) n_totals += nTotal # Perform backpropatation loss . backward () # Clip gradients: gradients are modified in place _ = nn . utils . clip_grad_norm_ ( encoder . parameters (), clip ) _ = nn . utils . clip_grad_norm_ ( decoder . parameters (), clip ) # Adjust model weights encoder_optimizer . step () decoder_optimizer . step () return sum ( print_losses ) / n_totals","title":"\u5355\u6b21\u8bad\u7ec3\u7684\u8fed\u4ee3"},{"location":"beginner/chatbot_tutorial/#\u591a\u6b21\u8bad\u7ec3\u7684\u8fed\u4ee3\u5668","text":"\u73b0\u5728\u662f\u65f6\u5019\u628a\u5b8c\u6574\u548c\u7684\u8bad\u7ec3\u8fc7\u7a0b\u548c\u6570\u636e\u7ed3\u5408\u5728\u4e00\u8d77\u4e86\u3002 \u51fd\u6570 trainIters \u8d1f\u8d23\u8fd0\u884c\u628a\u6a21\u578b\u3001\u4f18\u5316\u5668\u3001\u6570\u636e\u7b49\u4f20\u9012\u7ed9 n_iterations \u5e76\u8fd0\u884c\u3002 \u8fd9\u4e2a\u51fd\u6570\u5f88\u5bb9\u6613\u89e3\u91ca\u7684\uff0c\u56e0\u4e3a\u6211\u4eec\u5df2\u7ecf\u5bf9 train \u51fd\u6570\u505a\u4e86\u7e41\u591a\u7684\u5de5\u4f5c\u3002 \u6709\u4e00\u4e2a\u8981\u6ce8\u610f\u7684\u4e8b\u60c5\uff1a\u5f53\u6211\u4eec\u4fdd\u5b58\u6a21\u578b\u65f6\uff0c\u6240\u6709\u7684\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u72b6\u6001\u5b57\u5178(\u53c2\u6570)\u3001\u4f18\u5316\u5668\u7684\u72b6\u6001\u5b57\u5178\u3001\u635f\u5931\u3001\u8fed\u4ee3\u5668\u3001\u7b49\u7b49\uff0c\u90fd\u88ab\u4fdd\u5b58\u5728\u4e00\u4e2atarball\u4e2d\u3002 \u88ab\u8fd9\u6837\u4fdd\u5b58\u7684\u6a21\u578b\u53ef\u4ee5\u4e3a\u6211\u4eec\u63d0\u4f9b\u4e86\u5177\u6709\u6700\u5927\u7075\u6d3b\u6027\u7684\u68c0\u67e5\u70b9\u3002 \u4ece\u68c0\u67e5\u70b9\u52a0\u8f7d\u6a21\u578b\u4e4b\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u6a21\u578b\u53c2\u6570\u8fdb\u884c\u9884\u6d4b\uff0c\u6216\u8005\u4ece\u4e0a\u6b21\u5bf9\u51fa\u7684\u5730\u65b9\u7ee7\u7eed\u8bad\u7ec3\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def trainIters ( model_name , voc , pairs , encoder , decoder , encoder_optimizer , decoder_optimizer , embedding , encoder_n_layers , decoder_n_layers , save_dir , n_iteration , batch_size , print_every , save_every , clip , corpus_name , loadFilename ): # Load batches for each iteration training_batches = [ batch2TrainData ( voc , [ random . choice ( pairs ) for _ in range ( batch_size )]) for _ in range ( n_iteration )] # Initializations print ( 'Initializing ...' ) start_iteration = 1 print_loss = 0 if loadFilename : start_iteration = checkpoint [ 'iteration' ] + 1 # Training loop print ( \"Training...\" ) for iteration in range ( start_iteration , n_iteration + 1 ): training_batch = training_batches [ iteration - 1 ] # Extract fields from batch input_variable , lengths , target_variable , mask , max_target_len = training_batch # Run a training iteration with batch loss = train ( input_variable , lengths , target_variable , mask , max_target_len , encoder , decoder , embedding , encoder_optimizer , decoder_optimizer , batch_size , clip ) print_loss += loss # Print progress if iteration % print_every == 0 : print_loss_avg = print_loss / print_every print ( \"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\" . format ( iteration , iteration / n_iteration * 100 , print_loss_avg )) print_loss = 0 # Save checkpoint if ( iteration % save_every == 0 ): directory = os . path . join ( save_dir , model_name , corpus_name , '{}-{}_{}' . format ( encoder_n_layers , decoder_n_layers , hidden_size )) if not os . path . exists ( directory ): os . makedirs ( directory ) torch . save ({ 'iteration' : iteration , 'en' : encoder . state_dict (), 'de' : decoder . state_dict (), 'en_opt' : encoder_optimizer . state_dict (), 'de_opt' : decoder_optimizer . state_dict (), 'loss' : loss , 'voc_dict' : voc . __dict__ , 'embedding' : embedding . state_dict () }, os . path . join ( directory , '{}_{}.tar' . format ( iteration , 'checkpoint' )))","title":"\u591a\u6b21\u8bad\u7ec3\u7684\u8fed\u4ee3\u5668"},{"location":"beginner/chatbot_tutorial/#\u5b9a\u4e49\u8bc4\u4f30\u8fc7\u7a0b","text":"\u5728\u8bad\u7ec3\u6a21\u578b\u540e\uff0c\u6211\u4eec\u8981\u4e0e\u673a\u5668\u4eba\u4ea4\u8c08\u3002 \u9996\u5148\uff0c\u6211\u4eec\u5fc5\u987b\u5b9a\u4e49\u6a21\u578b\u5982\u4f55\u5bf9\u8f93\u5165\u8fdb\u884c\u7f16\u89e3\u7801\u3002","title":"\u5b9a\u4e49\u8bc4\u4f30\u8fc7\u7a0b"},{"location":"beginner/chatbot_tutorial/#greedy-decoding","text":"\u201cGreedy decoding\u201d \u662f\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d \u4e0d \u4f7f\u7528 \u201ctacher forcing\u201d \u7684\u60c5\u51b5\u4e0b\u4f7f\u7528\u7684\u4e00\u79cd\u89e3\u7801\u65b9\u5f0f\u3002 \u6362\u800c\u8a00\u4e4b\uff0c\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u9aa4\uff0c\u6211\u4eec\u53ea\u9700\u4ece decoder_output \u9009\u5b9asoftmax\u503c\u6700\u9ad8\u7684\u8bcd\u3002 \u8fd9\u4e2a\u89e3\u7801\u65b9\u5f0f\u5728\u5355\u65f6\u95f4\u6b65\u9aa4\u7ea7\u522b\u4e0a\u662f\u6700\u4f73\u7684\u3002 \u4e3a\u4e86\u65b9\u4fbf\u5730\u4f7f\u7528 \u201cGreedy decoding\u201d\uff0c\u6211\u4eec\u5b9a\u4e49\u4e00\u4e2a GreedySearchDecoder \u7c7b\u3002 \u5728\u8fd0\u884c\u65f6\uff0c\u8fd9\u4e2a\u7c7b\u7684\u5bf9\u8c61\u91c7\u7528\u7684\u53c2\u6570\u662f\uff1a \u5f62\u72b6\u4e3a (input_seq length, 1) \u7684\u8f93\u5165\u5e8f\u5217( input_seq ) \u6807\u91cf\u8f93\u5165\u957f\u5ea6( input_length ) tensor \u7b54\u590d\u53e5\u5b50\u957f\u5ea6\u9650\u5236\u503c max_length \u4f7f\u7528\u4ee5\u4e0b\u8ba1\u7b97\u56fe\u5bf9\u8f93\u5165\u53e5\u5b50\u8fdb\u884c\u8bc4\u4f30\u8f93\u5165\uff1a \u8ba1\u7b97\u56fe\uff1a \u901a\u8fc7\u7f16\u7801\u5668\u6a21\u578b\u8f6c\u53d1\u8f93\u5165\u3002 \u5c06\u7f16\u7801\u5668\u7684\u6700\u7ec8\u9690\u85cf\u5c42\u7528\u4f5c\u89e3\u7801\u5668\u7684\u7b2c\u4e00\u4e2a\u9690\u85cf\u8f93\u5165\u3002 \u5c06\u89e3\u7801\u5668\u7684\u7b2c\u4e00\u4e2a\u8f93\u5165\u521d\u59cb\u5316\u4e3a SOS_token \u521d\u59cb\u5316\u5f20\u91cf\uff0c\u5728\u5176\u672b\u5c3e\u9644\u52a0\u9644\u52a0\u89e3\u7801\u540e\u7684\u8bcd \u4e00\u6b21\u8fed\u4ee3\u89e3\u7801\u4e00\u4e2a\u8bcd\u7684\u6807\u8bb0(token)\uff1a \u901a\u8fc7\u89e3\u7801\u5668\u524d\u5411\u4f20\u64ad\u3002 \u83b7\u5f97\u53ef\u80fd\u6027\u6700\u5927\u8bcd\u7684token\u548csoftmax\u5206\u503c(score)\u3002 \u8bb0\u5f55token\u548cscore \u5c06\u5f53\u524dtoken\u7528\u4f5c\u4e0b\u4e00\u4e2a\u89e3\u7801\u5668\u7684\u8f93\u5165 \u8fd4\u56de\u6536\u96c6\u5230\u7684\u8bcd\u7684\u6807\u8bb0(token)\u548csoftmax\u5206\u503c(score)\u5217\u8868 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class GreedySearchDecoder ( nn . Module ): def __init__ ( self , encoder , decoder ): super ( GreedySearchDecoder , self ) . __init__ () self . encoder = encoder self . decoder = decoder def forward ( self , input_seq , input_length , max_length ): # Forward input through encoder model encoder_outputs , encoder_hidden = self . encoder ( input_seq , input_length ) # Prepare encoder's final hidden layer to be first hidden input to the decoder decoder_hidden = encoder_hidden [: decoder . n_layers ] # Initialize decoder input with SOS_token decoder_input = torch . ones ( 1 , 1 , device = device , dtype = torch . long ) * SOS_token # Initialize tensors to append decoded words to all_tokens = torch . zeros ([ 0 ], device = device , dtype = torch . long ) all_scores = torch . zeros ([ 0 ], device = device ) # Iteratively decode one word token at a time for _ in range ( max_length ): # Forward pass through decoder decoder_output , decoder_hidden = self . decoder ( decoder_input , decoder_hidden , encoder_outputs ) # Obtain most likely word token and its softmax score decoder_scores , decoder_input = torch . max ( decoder_output , dim = 1 ) # Record token and score all_tokens = torch . cat (( all_tokens , decoder_input ), dim = 0 ) all_scores = torch . cat (( all_scores , decoder_scores ), dim = 0 ) # Prepare current token to be next decoder input (add a dimension) decoder_input = torch . unsqueeze ( decoder_input , 0 ) # Return collections of word tokens and scores return all_tokens , all_scores","title":"Greedy decoding"},{"location":"beginner/chatbot_tutorial/#\u8bc4\u4f30\u7528\u6237\u8f93\u5165\u6587\u672c","text":"\u73b0\u5728\u6211\u4eec\u5df2\u7ecf\u5b9a\u4e49\u4e86\u89e3\u7801\u65b9\u6cd5\uff0c\u53ef\u4ee5\u7f16\u5199\u5bf9\u8f93\u5165\u53e5\u5b50\u6587\u672c\u8fdb\u884c\u8bc4\u4f30\u7684\u51fd\u6570\u3002 \u51fd\u6570 evaluate \u7528\u4e8e\u7ba1\u7406\u5904\u7406\u8f93\u5165\u53e5\u5b50\u7684\u5e95\u5c42\u8fc7\u7a0b\u3002 \u9996\u5148\u5c06\u53e5\u5b50\u683c\u5f0f\u5316\u4e3a batch_size==1 \u7684\u8bcd\u7d22\u5f15\u503c\u8f93\u5165\u6279\u3002 \u8981\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff0c\u9700\u8981\u5c06\u53e5\u5b50\u4e2d\u7684\u8bcd\u8f6c\u4e3a\u4ed6\u4eec\u5bf9\u5e94\u7684\u7d22\u5f15\u503c\uff0c\u5e76\u8c03\u6362\u5750\u6807\u8f74\u4ee5\u751f\u6210\u6211\u4eec\u6a21\u578b\u6240\u9700\u7684\u5f20\u91cf\u3002 \u8fd8\u8981\u521b\u5efa\u4e00\u4e2a lengths \u5f20\u91cf\uff0c\u7528\u4e8e\u8bb0\u5f55\u8f93\u5165\u53e5\u5b50\u7684\u957f\u5ea6\u3002 \u6b64\u65f6\uff0c lengths \u662f\u4e00\u4e2a\u6807\u91cf\uff0c\u56e0\u4e3a\u6211\u4eec\u4e00\u6b21\u53ea\u8bc4\u4f30\u4e00\u4e2a\u53e5\u5b50( batch_size==1 )\u3002 \u7136\u540e\uff0c\u901a\u8fc7 GreedySearchDecoder \u5bf9\u8c61( searcher )\uff0c\u6211\u4eec\u53ef\u4ee5\u83b7\u5f97\u89e3\u7801\u540e\u7684\u56de\u590d\u53e5\u7684\u5f20\u91cf\u3002 \u6700\u540e\uff0c\u5c06\u56de\u590d\u7684\u7d22\u5f15\u503c\u8f6c\u4e3a\u8bcd\uff0c\u5e76\u8fd4\u56de\u89e3\u7801\u51fa\u6765\u7684\u8bcd\u5217\u8868\u3002 evaluateInput acts as the user interface for our chatbot. evaluateInput \u5145\u5f53\u804a\u5929\u673a\u5668\u4eba\u7528\u6237\u754c\u9762\u3002 \u8c03\u7528\u5b83\u7684\u65f6\u5019\uff0c\u63d0\u4f9b\u4e00\u4e2a\u6587\u672c\u8f93\u5165\u754c\u9762\uff0c\u6211\u4eec\u5728\u8fd9\u91cc\u8f93\u5165\u81ea\u5df1\u7684\u8be2\u95ee\u53e5\u3002 \u6309 \u56de\u8f66 \u5b8c\u6210\u540e\uff0c\u7528\u548c\u5904\u7406\u8bad\u7ec3\u6570\u636e\u4e00\u81f4\u7684\u65b9\u6cd5\u5bf9\u8f93\u5165\u6587\u672c\u8fdb\u884c\u6b63\u89c4\u5316\u5904\u7406\uff0c\u4f20\u9012\u7ed9 evaluate \u51fd\u6570\uff0c\u83b7\u5f97\u89e3\u7801\u540e\u7684\u8f93\u51fa\u53e5\u5b50\u3002 \u5faa\u73af\u6267\u884c\u8fd9\u4e2a\u8fc7\u7a0b\uff0c\u8fd9\u6837\u5c31\u53ef\u4ee5\u548c\u673a\u5668\u4eba\u4e0d\u505c\u5730\u804a\u5929\u3002\u8f93\u5165\u201cq\u201d\u6216\u8005\u201dquit\u201d\u9000\u51fa\u3002 \u6700\u540e\uff0c\u5982\u679c\u53e5\u5b50\u4e2d\u542b\u6709\u8bcd\u8868\u4e2d\u4e0d\u5b58\u5728\u7684\u8bcd\uff0c\u5e94\u8be5\u8fdb\u884c\u4f18\u96c5\u7684\u9519\u8bef\u5904\u7406\uff1a\u8f93\u51fa\u9519\u8bef\u63d0\u793a\u4fe1\u606f\uff0c\u63d0\u9192\u7528\u6237\u8f93\u5165\u5176\u5b83\u8bcd\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def evaluate ( encoder , decoder , searcher , voc , sentence , max_length = MAX_LENGTH ): ### Format input sentence as a batch # words -> indexes indexes_batch = [ indexesFromSentence ( voc , sentence )] # Create lengths tensor lengths = torch . tensor ([ len ( indexes ) for indexes in indexes_batch ]) # Transpose dimensions of batch to match models' expectations input_batch = torch . LongTensor ( indexes_batch ) . transpose ( 0 , 1 ) # Use appropriate device input_batch = input_batch . to ( device ) lengths = lengths . to ( device ) # Decode sentence with searcher tokens , scores = searcher ( input_batch , lengths , max_length ) # indexes -> words decoded_words = [ voc . index2word [ token . item ()] for token in tokens ] return decoded_words def evaluateInput ( encoder , decoder , searcher , voc ): input_sentence = '' while ( 1 ): try : # Get input sentence input_sentence = input ( '> ' ) # Check if it is quit case if input_sentence == 'q' or input_sentence == 'quit' : break # Normalize sentence input_sentence = normalizeString ( input_sentence ) # Evaluate sentence output_words = evaluate ( encoder , decoder , searcher , voc , input_sentence ) # Format and print response sentence output_words [:] = [ x for x in output_words if not ( x == 'EOS' or x == 'PAD' )] print ( 'Bot:' , ' ' . join ( output_words )) except KeyError : print ( \"Error: Encountered unknown word.\" )","title":"\u8bc4\u4f30\u7528\u6237\u8f93\u5165\u6587\u672c"},{"location":"beginner/chatbot_tutorial/#\u8fd0\u884c\u6a21\u578b","text":"\u6700\u540e\uff0c\u662f\u65f6\u5019\u8fd0\u884c\u6211\u4eec\u7684\u6a21\u578b\u4e86\uff01 \u4e0d\u7ba1\u8bad\u7ec3\u8fd8\u662f\u6d4b\u8bd5\uff0c\u6211\u4eec\u90fd\u5f97\u521d\u59cb\u5316\u72ec\u7acb\u7684\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u6a21\u578b\u3002 \u5728\u4e0b\u9762\u7684\u4ee3\u7801\u5757\u4e2d\uff0c\u6211\u4eec\u8bbe\u7f6e\u4e86\u6240\u9700\u914d\u7f6e\uff0c\u9009\u62e9\u4ece\u5934\u5f00\u59cb\u6216\u8005\u4ece\u68c0\u67e5\u70b9\u52a0\u8f7d\u5e76\u6784\u5efa\u548c\u521d\u59cb\u5316\u6a21\u578b\u3002 \u53ef\u4ee5\u81ea\u7531\u5730\u4f7f\u7528\u4e0d\u540c\u7684\u6a21\u578b\u8bbe\u7f6e\u6765\u4f18\u5316\u6027\u80fd\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 # Configure models model_name = 'cb_model' attn_model = 'dot' #attn_model = 'general' #attn_model = 'concat' hidden_size = 500 encoder_n_layers = 2 decoder_n_layers = 2 dropout = 0.1 batch_size = 64 # Set checkpoint to load from; set to None if starting from scratch loadFilename = None checkpoint_iter = 4000 #loadFilename = os.path.join(save_dir, model_name, corpus_name, # '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size), # '{}_checkpoint.tar'.format(checkpoint_iter)) # Load model if a loadFilename is provided if loadFilename : # If loading on same machine the model was trained on checkpoint = torch . load ( loadFilename ) # If loading a model trained on GPU to CPU #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu')) encoder_sd = checkpoint [ 'en' ] decoder_sd = checkpoint [ 'de' ] encoder_optimizer_sd = checkpoint [ 'en_opt' ] decoder_optimizer_sd = checkpoint [ 'de_opt' ] embedding_sd = checkpoint [ 'embedding' ] voc . __dict__ = checkpoint [ 'voc_dict' ] print ( 'Building encoder and decoder ...' ) # Initialize word embeddings embedding = nn . Embedding ( voc . num_words , hidden_size ) if loadFilename : embedding . load_state_dict ( embedding_sd ) # Initialize encoder & decoder models encoder = EncoderRNN ( hidden_size , embedding , encoder_n_layers , dropout ) decoder = LuongAttnDecoderRNN ( attn_model , embedding , hidden_size , voc . num_words , decoder_n_layers , dropout ) if loadFilename : encoder . load_state_dict ( encoder_sd ) decoder . load_state_dict ( decoder_sd ) # Use appropriate device encoder = encoder . to ( device ) decoder = decoder . to ( device ) print ( 'Models built and ready to go!' ) 1 2 Building encoder and decoder ... Models built and ready to go!","title":"\u8fd0\u884c\u6a21\u578b"},{"location":"beginner/chatbot_tutorial/#\u8fd0\u884c\u8bad\u7ec3","text":"\u8fd0\u884c\u4ee5\u4e0b\u4ee3\u7801\u5757\u8bad\u7ec3\u6a21\u578b\u3002 \u9996\u5148\u662f\u8bbe\u7f6e\u8bad\u7ec3\u53c2\u6570 \u7136\u540e\u521d\u59cb\u5316\u4f18\u5316\u5668(optimizer) \u6700\u540e\u8c03\u7528 trainIters \u51fd\u6570\u8fd0\u884c\u8bad\u7ec3\u8fed\u4ee3\u5668\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Configure training/optimization clip = 50.0 teacher_forcing_ratio = 1.0 learning_rate = 0.0001 decoder_learning_ratio = 5.0 n_iteration = 4000 print_every = 1 save_every = 500 # Ensure dropout layers are in train mode encoder . train () decoder . train () # Initialize optimizers print ( 'Building optimizers ...' ) encoder_optimizer = optim . Adam ( encoder . parameters (), lr = learning_rate ) decoder_optimizer = optim . Adam ( decoder . parameters (), lr = learning_rate * decoder_learning_ratio ) if loadFilename : encoder_optimizer . load_state_dict ( encoder_optimizer_sd ) decoder_optimizer . load_state_dict ( decoder_optimizer_sd ) # Run training iterations print ( \"Starting Training!\" ) trainIters ( model_name , voc , pairs , encoder , decoder , encoder_optimizer , decoder_optimizer , embedding , encoder_n_layers , decoder_n_layers , save_dir , n_iteration , batch_size , print_every , save_every , clip , corpus_name , loadFilename ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026 2027 2028 2029 2030 2031 2032 2033 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067 2068 2069 2070 2071 2072 2073 2074 2075 2076 2077 2078 2079 2080 2081 2082 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 2097 2098 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 2222 2223 2224 2225 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236 2237 2238 2239 2240 2241 2242 2243 2244 2245 2246 2247 2248 2249 2250 2251 2252 2253 2254 2255 2256 2257 2258 2259 2260 2261 2262 2263 2264 2265 2266 2267 2268 2269 2270 2271 2272 2273 2274 2275 2276 2277 2278 2279 2280 2281 2282 2283 2284 2285 2286 2287 2288 2289 2290 2291 2292 2293 2294 2295 2296 2297 2298 2299 2300 2301 2302 2303 2304 2305 2306 2307 2308 2309 2310 2311 2312 2313 2314 2315 2316 2317 2318 2319 2320 2321 2322 2323 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333 2334 2335 2336 2337 2338 2339 2340 2341 2342 2343 2344 2345 2346 2347 2348 2349 2350 2351 2352 2353 2354 2355 2356 2357 2358 2359 2360 2361 2362 2363 2364 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 2375 2376 2377 2378 2379 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390 2391 2392 2393 2394 2395 2396 2397 2398 2399 2400 2401 2402 2403 2404 2405 2406 2407 2408 2409 2410 2411 2412 2413 2414 2415 2416 2417 2418 2419 2420 2421 2422 2423 2424 2425 2426 2427 2428 2429 2430 2431 2432 2433 2434 2435 2436 2437 2438 2439 2440 2441 2442 2443 2444 2445 2446 2447 2448 2449 2450 2451 2452 2453 2454 2455 2456 2457 2458 2459 2460 2461 2462 2463 2464 2465 2466 2467 2468 2469 2470 2471 2472 2473 2474 2475 2476 2477 2478 2479 2480 2481 2482 2483 2484 2485 2486 2487 2488 2489 2490 2491 2492 2493 2494 2495 2496 2497 2498 2499 2500 2501 2502 2503 2504 2505 2506 2507 2508 2509 2510 2511 2512 2513 2514 2515 2516 2517 2518 2519 2520 2521 2522 2523 2524 2525 2526 2527 2528 2529 2530 2531 2532 2533 2534 2535 2536 2537 2538 2539 2540 2541 2542 2543 2544 2545 2546 2547 2548 2549 2550 2551 2552 2553 2554 2555 2556 2557 2558 2559 2560 2561 2562 2563 2564 2565 2566 2567 2568 2569 2570 2571 2572 2573 2574 2575 2576 2577 2578 2579 2580 2581 2582 2583 2584 2585 2586 2587 2588 2589 2590 2591 2592 2593 2594 2595 2596 2597 2598 2599 2600 2601 2602 2603 2604 2605 2606 2607 2608 2609 2610 2611 2612 2613 2614 2615 2616 2617 2618 2619 2620 2621 2622 2623 2624 2625 2626 2627 2628 2629 2630 2631 2632 2633 2634 2635 2636 2637 2638 2639 2640 2641 2642 2643 2644 2645 2646 2647 2648 2649 2650 2651 2652 2653 2654 2655 2656 2657 2658 2659 2660 2661 2662 2663 2664 2665 2666 2667 2668 2669 2670 2671 2672 2673 2674 2675 2676 2677 2678 2679 2680 2681 2682 2683 2684 2685 2686 2687 2688 2689 2690 2691 2692 2693 2694 2695 2696 2697 2698 2699 2700 2701 2702 2703 2704 2705 2706 2707 2708 2709 2710 2711 2712 2713 2714 2715 2716 2717 2718 2719 2720 2721 2722 2723 2724 2725 2726 2727 2728 2729 2730 2731 2732 2733 2734 2735 2736 2737 2738 2739 2740 2741 2742 2743 2744 2745 2746 2747 2748 2749 2750 2751 2752 2753 2754 2755 2756 2757 2758 2759 2760 2761 2762 2763 2764 2765 2766 2767 2768 2769 2770 2771 2772 2773 2774 2775 2776 2777 2778 2779 2780 2781 2782 2783 2784 2785 2786 2787 2788 2789 2790 2791 2792 2793 2794 2795 2796 2797 2798 2799 2800 2801 2802 2803 2804 2805 2806 2807 2808 2809 2810 2811 2812 2813 2814 2815 2816 2817 2818 2819 2820 2821 2822 2823 2824 2825 2826 2827 2828 2829 2830 2831 2832 2833 2834 2835 2836 2837 2838 2839 2840 2841 2842 2843 2844 2845 2846 2847 2848 2849 2850 2851 2852 2853 2854 2855 2856 2857 2858 2859 2860 2861 2862 2863 2864 2865 2866 2867 2868 2869 2870 2871 2872 2873 2874 2875 2876 2877 2878 2879 2880 2881 2882 2883 2884 2885 2886 2887 2888 2889 2890 2891 2892 2893 2894 2895 2896 2897 2898 2899 2900 2901 2902 2903 2904 2905 2906 2907 2908 2909 2910 2911 2912 2913 2914 2915 2916 2917 2918 2919 2920 2921 2922 2923 2924 2925 2926 2927 2928 2929 2930 2931 2932 2933 2934 2935 2936 2937 2938 2939 2940 2941 2942 2943 2944 2945 2946 2947 2948 2949 2950 2951 2952 2953 2954 2955 2956 2957 2958 2959 2960 2961 2962 2963 2964 2965 2966 2967 2968 2969 2970 2971 2972 2973 2974 2975 2976 2977 2978 2979 2980 2981 2982 2983 2984 2985 2986 2987 2988 2989 2990 2991 2992 2993 2994 2995 2996 2997 2998 2999 3000 3001 3002 3003 3004 3005 3006 3007 3008 3009 3010 3011 3012 3013 3014 3015 3016 3017 3018 3019 3020 3021 3022 3023 3024 3025 3026 3027 3028 3029 3030 3031 3032 3033 3034 3035 3036 3037 3038 3039 3040 3041 3042 3043 3044 3045 3046 3047 3048 3049 3050 3051 3052 3053 3054 3055 3056 3057 3058 3059 3060 3061 3062 3063 3064 3065 3066 3067 3068 3069 3070 3071 3072 3073 3074 3075 3076 3077 3078 3079 3080 3081 3082 3083 3084 3085 3086 3087 3088 3089 3090 3091 3092 3093 3094 3095 3096 3097 3098 3099 3100 3101 3102 3103 3104 3105 3106 3107 3108 3109 3110 3111 3112 3113 3114 3115 3116 3117 3118 3119 3120 3121 3122 3123 3124 3125 3126 3127 3128 3129 3130 3131 3132 3133 3134 3135 3136 3137 3138 3139 3140 3141 3142 3143 3144 3145 3146 3147 3148 3149 3150 3151 3152 3153 3154 3155 3156 3157 3158 3159 3160 3161 3162 3163 3164 3165 3166 3167 3168 3169 3170 3171 3172 3173 3174 3175 3176 3177 3178 3179 3180 3181 3182 3183 3184 3185 3186 3187 3188 3189 3190 3191 3192 3193 3194 3195 3196 3197 3198 3199 3200 3201 3202 3203 3204 3205 3206 3207 3208 3209 3210 3211 3212 3213 3214 3215 3216 3217 3218 3219 3220 3221 3222 3223 3224 3225 3226 3227 3228 3229 3230 3231 3232 3233 3234 3235 3236 3237 3238 3239 3240 3241 3242 3243 3244 3245 3246 3247 3248 3249 3250 3251 3252 3253 3254 3255 3256 3257 3258 3259 3260 3261 3262 3263 3264 3265 3266 3267 3268 3269 3270 3271 3272 3273 3274 3275 3276 3277 3278 3279 3280 3281 3282 3283 3284 3285 3286 3287 3288 3289 3290 3291 3292 3293 3294 3295 3296 3297 3298 3299 3300 3301 3302 3303 3304 3305 3306 3307 3308 3309 3310 3311 3312 3313 3314 3315 3316 3317 3318 3319 3320 3321 3322 3323 3324 3325 3326 3327 3328 3329 3330 3331 3332 3333 3334 3335 3336 3337 3338 3339 3340 3341 3342 3343 3344 3345 3346 3347 3348 3349 3350 3351 3352 3353 3354 3355 3356 3357 3358 3359 3360 3361 3362 3363 3364 3365 3366 3367 3368 3369 3370 3371 3372 3373 3374 3375 3376 3377 3378 3379 3380 3381 3382 3383 3384 3385 3386 3387 3388 3389 3390 3391 3392 3393 3394 3395 3396 3397 3398 3399 3400 3401 3402 3403 3404 3405 3406 3407 3408 3409 3410 3411 3412 3413 3414 3415 3416 3417 3418 3419 3420 3421 3422 3423 3424 3425 3426 3427 3428 3429 3430 3431 3432 3433 3434 3435 3436 3437 3438 3439 3440 3441 3442 3443 3444 3445 3446 3447 3448 3449 3450 3451 3452 3453 3454 3455 3456 3457 3458 3459 3460 3461 3462 3463 3464 3465 3466 3467 3468 3469 3470 3471 3472 3473 3474 3475 3476 3477 3478 3479 3480 3481 3482 3483 3484 3485 3486 3487 3488 3489 3490 3491 3492 3493 3494 3495 3496 3497 3498 3499 3500 3501 3502 3503 3504 3505 3506 3507 3508 3509 3510 3511 3512 3513 3514 3515 3516 3517 3518 3519 3520 3521 3522 3523 3524 3525 3526 3527 3528 3529 3530 3531 3532 3533 3534 3535 3536 3537 3538 3539 3540 3541 3542 3543 3544 3545 3546 3547 3548 3549 3550 3551 3552 3553 3554 3555 3556 3557 3558 3559 3560 3561 3562 3563 3564 3565 3566 3567 3568 3569 3570 3571 3572 3573 3574 3575 3576 3577 3578 3579 3580 3581 3582 3583 3584 3585 3586 3587 3588 3589 3590 3591 3592 3593 3594 3595 3596 3597 3598 3599 3600 3601 3602 3603 3604 3605 3606 3607 3608 3609 3610 3611 3612 3613 3614 3615 3616 3617 3618 3619 3620 3621 3622 3623 3624 3625 3626 3627 3628 3629 3630 3631 3632 3633 3634 3635 3636 3637 3638 3639 3640 3641 3642 3643 3644 3645 3646 3647 3648 3649 3650 3651 3652 3653 3654 3655 3656 3657 3658 3659 3660 3661 3662 3663 3664 3665 3666 3667 3668 3669 3670 3671 3672 3673 3674 3675 3676 3677 3678 3679 3680 3681 3682 3683 3684 3685 3686 3687 3688 3689 3690 3691 3692 3693 3694 3695 3696 3697 3698 3699 3700 3701 3702 3703 3704 3705 3706 3707 3708 3709 3710 3711 3712 3713 3714 3715 3716 3717 3718 3719 3720 3721 3722 3723 3724 3725 3726 3727 3728 3729 3730 3731 3732 3733 3734 3735 3736 3737 3738 3739 3740 3741 3742 3743 3744 3745 3746 3747 3748 3749 3750 3751 3752 3753 3754 3755 3756 3757 3758 3759 3760 3761 3762 3763 3764 3765 3766 3767 3768 3769 3770 3771 3772 3773 3774 3775 3776 3777 3778 3779 3780 3781 3782 3783 3784 3785 3786 3787 3788 3789 3790 3791 3792 3793 3794 3795 3796 3797 3798 3799 3800 3801 3802 3803 3804 3805 3806 3807 3808 3809 3810 3811 3812 3813 3814 3815 3816 3817 3818 3819 3820 3821 3822 3823 3824 3825 3826 3827 3828 3829 3830 3831 3832 3833 3834 3835 3836 3837 3838 3839 3840 3841 3842 3843 3844 3845 3846 3847 3848 3849 3850 3851 3852 3853 3854 3855 3856 3857 3858 3859 3860 3861 3862 3863 3864 3865 3866 3867 3868 3869 3870 3871 3872 3873 3874 3875 3876 3877 3878 3879 3880 3881 3882 3883 3884 3885 3886 3887 3888 3889 3890 3891 3892 3893 3894 3895 3896 3897 3898 3899 3900 3901 3902 3903 3904 3905 3906 3907 3908 3909 3910 3911 3912 3913 3914 3915 3916 3917 3918 3919 3920 3921 3922 3923 3924 3925 3926 3927 3928 3929 3930 3931 3932 3933 3934 3935 3936 3937 3938 3939 3940 3941 3942 3943 3944 3945 3946 3947 3948 3949 3950 3951 3952 3953 3954 3955 3956 3957 3958 3959 3960 3961 3962 3963 3964 3965 3966 3967 3968 3969 3970 3971 3972 3973 3974 3975 3976 3977 3978 3979 3980 3981 3982 3983 3984 3985 3986 3987 3988 3989 3990 3991 3992 3993 3994 3995 3996 3997 3998 3999 4000 4001 4002 4003 4004 Building optimizers ... Starting Training! Initializing ... Training... Iteration: 1; Percent complete: 0.0%; Average loss: 8.9628 Iteration: 2; Percent complete: 0.1%; Average loss: 8.8343 Iteration: 3; Percent complete: 0.1%; Average loss: 8.6592 Iteration: 4; Percent complete: 0.1%; Average loss: 8.3297 Iteration: 5; Percent complete: 0.1%; Average loss: 7.9770 Iteration: 6; Percent complete: 0.1%; Average loss: 7.3291 Iteration: 7; Percent complete: 0.2%; Average loss: 6.7586 Iteration: 8; Percent complete: 0.2%; Average loss: 6.9260 Iteration: 9; Percent complete: 0.2%; Average loss: 6.8659 Iteration: 10; Percent complete: 0.2%; Average loss: 6.6029 Iteration: 11; Percent complete: 0.3%; Average loss: 6.2245 Iteration: 12; Percent complete: 0.3%; Average loss: 6.1180 Iteration: 13; Percent complete: 0.3%; Average loss: 5.6463 Iteration: 14; Percent complete: 0.4%; Average loss: 5.6409 Iteration: 15; Percent complete: 0.4%; Average loss: 5.4941 Iteration: 16; Percent complete: 0.4%; Average loss: 5.6748 Iteration: 17; Percent complete: 0.4%; Average loss: 5.3386 Iteration: 18; Percent complete: 0.4%; Average loss: 5.3936 Iteration: 19; Percent complete: 0.5%; Average loss: 4.8958 Iteration: 20; Percent complete: 0.5%; Average loss: 4.9486 Iteration: 21; Percent complete: 0.5%; Average loss: 4.8804 Iteration: 22; Percent complete: 0.5%; Average loss: 4.8959 Iteration: 23; Percent complete: 0.6%; Average loss: 4.8363 Iteration: 24; Percent complete: 0.6%; Average loss: 4.9209 Iteration: 25; Percent complete: 0.6%; Average loss: 4.9964 Iteration: 26; Percent complete: 0.7%; Average loss: 4.8157 Iteration: 27; Percent complete: 0.7%; Average loss: 4.8140 Iteration: 28; Percent complete: 0.7%; Average loss: 4.9719 Iteration: 29; Percent complete: 0.7%; Average loss: 4.6794 Iteration: 30; Percent complete: 0.8%; Average loss: 4.7190 Iteration: 31; Percent complete: 0.8%; Average loss: 4.6346 Iteration: 32; Percent complete: 0.8%; Average loss: 4.9546 Iteration: 33; Percent complete: 0.8%; Average loss: 4.7239 Iteration: 34; Percent complete: 0.9%; Average loss: 4.8934 Iteration: 35; Percent complete: 0.9%; Average loss: 4.9053 Iteration: 36; Percent complete: 0.9%; Average loss: 4.8707 Iteration: 37; Percent complete: 0.9%; Average loss: 4.7875 Iteration: 38; Percent complete: 0.9%; Average loss: 4.6638 Iteration: 39; Percent complete: 1.0%; Average loss: 4.8682 Iteration: 40; Percent complete: 1.0%; Average loss: 4.5905 Iteration: 41; Percent complete: 1.0%; Average loss: 4.5374 Iteration: 42; Percent complete: 1.1%; Average loss: 4.6670 Iteration: 43; Percent complete: 1.1%; Average loss: 4.8148 Iteration: 44; Percent complete: 1.1%; Average loss: 4.6093 Iteration: 45; Percent complete: 1.1%; Average loss: 4.6322 Iteration: 46; Percent complete: 1.1%; Average loss: 4.5139 Iteration: 47; Percent complete: 1.2%; Average loss: 4.6834 Iteration: 48; Percent complete: 1.2%; Average loss: 4.5035 Iteration: 49; Percent complete: 1.2%; Average loss: 4.8119 Iteration: 50; Percent complete: 1.2%; Average loss: 4.5661 Iteration: 51; Percent complete: 1.3%; Average loss: 4.8288 Iteration: 52; Percent complete: 1.3%; Average loss: 4.5483 Iteration: 53; Percent complete: 1.3%; Average loss: 4.6328 Iteration: 54; Percent complete: 1.4%; Average loss: 4.5040 Iteration: 55; Percent complete: 1.4%; Average loss: 4.6982 Iteration: 56; Percent complete: 1.4%; Average loss: 4.5518 Iteration: 57; Percent complete: 1.4%; Average loss: 4.5344 Iteration: 58; Percent complete: 1.5%; Average loss: 4.5677 Iteration: 59; Percent complete: 1.5%; Average loss: 4.7938 Iteration: 60; Percent complete: 1.5%; Average loss: 4.5126 Iteration: 61; Percent complete: 1.5%; Average loss: 4.4967 Iteration: 62; Percent complete: 1.6%; Average loss: 4.6333 Iteration: 63; Percent complete: 1.6%; Average loss: 4.5888 Iteration: 64; Percent complete: 1.6%; Average loss: 4.3075 Iteration: 65; Percent complete: 1.6%; Average loss: 4.5133 Iteration: 66; Percent complete: 1.7%; Average loss: 4.5084 Iteration: 67; Percent complete: 1.7%; Average loss: 4.8161 Iteration: 68; Percent complete: 1.7%; Average loss: 4.5948 Iteration: 69; Percent complete: 1.7%; Average loss: 4.6278 Iteration: 70; Percent complete: 1.8%; Average loss: 4.4582 Iteration: 71; Percent complete: 1.8%; Average loss: 4.4484 Iteration: 72; Percent complete: 1.8%; Average loss: 4.4757 Iteration: 73; Percent complete: 1.8%; Average loss: 4.5937 Iteration: 74; Percent complete: 1.8%; Average loss: 4.3474 Iteration: 75; Percent complete: 1.9%; Average loss: 4.4921 Iteration: 76; Percent complete: 1.9%; Average loss: 4.6886 Iteration: 77; Percent complete: 1.9%; Average loss: 4.3638 Iteration: 78; Percent complete: 1.9%; Average loss: 4.6839 Iteration: 79; Percent complete: 2.0%; Average loss: 4.4069 Iteration: 80; Percent complete: 2.0%; Average loss: 4.4496 Iteration: 81; Percent complete: 2.0%; Average loss: 4.5592 Iteration: 82; Percent complete: 2.1%; Average loss: 4.7000 Iteration: 83; Percent complete: 2.1%; Average loss: 4.3022 Iteration: 84; Percent complete: 2.1%; Average loss: 4.5013 Iteration: 85; Percent complete: 2.1%; Average loss: 4.3784 Iteration: 86; Percent complete: 2.1%; Average loss: 4.5025 Iteration: 87; Percent complete: 2.2%; Average loss: 4.5840 Iteration: 88; Percent complete: 2.2%; Average loss: 4.4873 Iteration: 89; Percent complete: 2.2%; Average loss: 4.2982 Iteration: 90; Percent complete: 2.2%; Average loss: 4.3686 Iteration: 91; Percent complete: 2.3%; Average loss: 4.3155 Iteration: 92; Percent complete: 2.3%; Average loss: 4.2115 Iteration: 93; Percent complete: 2.3%; Average loss: 4.2979 Iteration: 94; Percent complete: 2.4%; Average loss: 4.3565 Iteration: 95; Percent complete: 2.4%; Average loss: 4.3283 Iteration: 96; Percent complete: 2.4%; Average loss: 4.4177 Iteration: 97; Percent complete: 2.4%; Average loss: 4.3689 Iteration: 98; Percent complete: 2.5%; Average loss: 4.4567 Iteration: 99; Percent complete: 2.5%; Average loss: 4.3242 Iteration: 100; Percent complete: 2.5%; Average loss: 4.4249 Iteration: 101; Percent complete: 2.5%; Average loss: 4.1899 Iteration: 102; Percent complete: 2.5%; Average loss: 4.5148 Iteration: 103; Percent complete: 2.6%; Average loss: 4.2772 Iteration: 104; Percent complete: 2.6%; Average loss: 4.2775 Iteration: 105; Percent complete: 2.6%; Average loss: 4.5088 Iteration: 106; Percent complete: 2.6%; Average loss: 4.5121 Iteration: 107; Percent complete: 2.7%; Average loss: 4.3327 Iteration: 108; Percent complete: 2.7%; Average loss: 4.5252 Iteration: 109; Percent complete: 2.7%; Average loss: 4.1831 Iteration: 110; Percent complete: 2.8%; Average loss: 4.1833 Iteration: 111; Percent complete: 2.8%; Average loss: 4.3274 Iteration: 112; Percent complete: 2.8%; Average loss: 4.5191 Iteration: 113; Percent complete: 2.8%; Average loss: 4.3452 Iteration: 114; Percent complete: 2.9%; Average loss: 4.4353 Iteration: 115; Percent complete: 2.9%; Average loss: 4.3186 Iteration: 116; Percent complete: 2.9%; Average loss: 4.3743 Iteration: 117; Percent complete: 2.9%; Average loss: 4.3036 Iteration: 118; Percent complete: 2.9%; Average loss: 4.4618 Iteration: 119; Percent complete: 3.0%; Average loss: 4.4117 Iteration: 120; Percent complete: 3.0%; Average loss: 4.1575 Iteration: 121; Percent complete: 3.0%; Average loss: 4.3188 Iteration: 122; Percent complete: 3.0%; Average loss: 4.4365 Iteration: 123; Percent complete: 3.1%; Average loss: 4.1238 Iteration: 124; Percent complete: 3.1%; Average loss: 4.3645 Iteration: 125; Percent complete: 3.1%; Average loss: 4.2155 Iteration: 126; Percent complete: 3.1%; Average loss: 4.7236 Iteration: 127; Percent complete: 3.2%; Average loss: 4.1829 Iteration: 128; Percent complete: 3.2%; Average loss: 4.2557 Iteration: 129; Percent complete: 3.2%; Average loss: 4.4819 Iteration: 130; Percent complete: 3.2%; Average loss: 4.4005 Iteration: 131; Percent complete: 3.3%; Average loss: 4.2772 Iteration: 132; Percent complete: 3.3%; Average loss: 4.2948 Iteration: 133; Percent complete: 3.3%; Average loss: 4.2153 Iteration: 134; Percent complete: 3.4%; Average loss: 4.3869 Iteration: 135; Percent complete: 3.4%; Average loss: 4.3561 Iteration: 136; Percent complete: 3.4%; Average loss: 4.2666 Iteration: 137; Percent complete: 3.4%; Average loss: 4.2705 Iteration: 138; Percent complete: 3.5%; Average loss: 4.3425 Iteration: 139; Percent complete: 3.5%; Average loss: 4.3308 Iteration: 140; Percent complete: 3.5%; Average loss: 4.2482 Iteration: 141; Percent complete: 3.5%; Average loss: 4.2002 Iteration: 142; Percent complete: 3.5%; Average loss: 4.0395 Iteration: 143; Percent complete: 3.6%; Average loss: 4.4818 Iteration: 144; Percent complete: 3.6%; Average loss: 4.0831 Iteration: 145; Percent complete: 3.6%; Average loss: 4.1008 Iteration: 146; Percent complete: 3.6%; Average loss: 4.2408 Iteration: 147; Percent complete: 3.7%; Average loss: 4.1107 Iteration: 148; Percent complete: 3.7%; Average loss: 4.3374 Iteration: 149; Percent complete: 3.7%; Average loss: 4.0637 Iteration: 150; Percent complete: 3.8%; Average loss: 4.2662 Iteration: 151; Percent complete: 3.8%; Average loss: 4.2423 Iteration: 152; Percent complete: 3.8%; Average loss: 4.1396 Iteration: 153; Percent complete: 3.8%; Average loss: 4.2338 Iteration: 154; Percent complete: 3.9%; Average loss: 4.1130 Iteration: 155; Percent complete: 3.9%; Average loss: 4.2585 Iteration: 156; Percent complete: 3.9%; Average loss: 4.2221 Iteration: 157; Percent complete: 3.9%; Average loss: 4.1251 Iteration: 158; Percent complete: 4.0%; Average loss: 4.2255 Iteration: 159; Percent complete: 4.0%; Average loss: 4.3224 Iteration: 160; Percent complete: 4.0%; Average loss: 4.0658 Iteration: 161; Percent complete: 4.0%; Average loss: 4.2790 Iteration: 162; Percent complete: 4.0%; Average loss: 4.1332 Iteration: 163; Percent complete: 4.1%; Average loss: 3.9183 Iteration: 164; Percent complete: 4.1%; Average loss: 4.0841 Iteration: 165; Percent complete: 4.1%; Average loss: 3.7811 Iteration: 166; Percent complete: 4.2%; Average loss: 4.2144 Iteration: 167; Percent complete: 4.2%; Average loss: 3.9774 Iteration: 168; Percent complete: 4.2%; Average loss: 4.1725 Iteration: 169; Percent complete: 4.2%; Average loss: 4.2217 Iteration: 170; Percent complete: 4.2%; Average loss: 4.2590 Iteration: 171; Percent complete: 4.3%; Average loss: 4.2249 Iteration: 172; Percent complete: 4.3%; Average loss: 4.1298 Iteration: 173; Percent complete: 4.3%; Average loss: 4.2693 Iteration: 174; Percent complete: 4.3%; Average loss: 4.0924 Iteration: 175; Percent complete: 4.4%; Average loss: 3.8366 Iteration: 176; Percent complete: 4.4%; Average loss: 4.0131 Iteration: 177; Percent complete: 4.4%; Average loss: 4.0707 Iteration: 178; Percent complete: 4.5%; Average loss: 4.3873 Iteration: 179; Percent complete: 4.5%; Average loss: 3.9353 Iteration: 180; Percent complete: 4.5%; Average loss: 4.1841 Iteration: 181; Percent complete: 4.5%; Average loss: 3.9691 Iteration: 182; Percent complete: 4.5%; Average loss: 4.2445 Iteration: 183; Percent complete: 4.6%; Average loss: 3.9909 Iteration: 184; Percent complete: 4.6%; Average loss: 4.2100 Iteration: 185; Percent complete: 4.6%; Average loss: 4.0717 Iteration: 186; Percent complete: 4.7%; Average loss: 4.1351 Iteration: 187; Percent complete: 4.7%; Average loss: 3.9354 Iteration: 188; Percent complete: 4.7%; Average loss: 3.8134 Iteration: 189; Percent complete: 4.7%; Average loss: 4.3966 Iteration: 190; Percent complete: 4.8%; Average loss: 4.0509 Iteration: 191; Percent complete: 4.8%; Average loss: 3.9912 Iteration: 192; Percent complete: 4.8%; Average loss: 3.9772 Iteration: 193; Percent complete: 4.8%; Average loss: 3.9212 Iteration: 194; Percent complete: 4.9%; Average loss: 4.0303 Iteration: 195; Percent complete: 4.9%; Average loss: 4.1305 Iteration: 196; Percent complete: 4.9%; Average loss: 4.3996 Iteration: 197; Percent complete: 4.9%; Average loss: 4.3042 Iteration: 198; Percent complete: 5.0%; Average loss: 4.3489 Iteration: 199; Percent complete: 5.0%; Average loss: 4.2896 Iteration: 200; Percent complete: 5.0%; Average loss: 3.8091 Iteration: 201; Percent complete: 5.0%; Average loss: 4.0971 Iteration: 202; Percent complete: 5.1%; Average loss: 3.9714 Iteration: 203; Percent complete: 5.1%; Average loss: 4.0337 Iteration: 204; Percent complete: 5.1%; Average loss: 4.1092 Iteration: 205; Percent complete: 5.1%; Average loss: 4.0293 Iteration: 206; Percent complete: 5.1%; Average loss: 4.2051 Iteration: 207; Percent complete: 5.2%; Average loss: 4.1471 Iteration: 208; Percent complete: 5.2%; Average loss: 4.2239 Iteration: 209; Percent complete: 5.2%; Average loss: 3.8455 Iteration: 210; Percent complete: 5.2%; Average loss: 3.9024 Iteration: 211; Percent complete: 5.3%; Average loss: 4.2336 Iteration: 212; Percent complete: 5.3%; Average loss: 4.0724 Iteration: 213; Percent complete: 5.3%; Average loss: 4.0936 Iteration: 214; Percent complete: 5.3%; Average loss: 3.9975 Iteration: 215; Percent complete: 5.4%; Average loss: 3.9113 Iteration: 216; Percent complete: 5.4%; Average loss: 4.0042 Iteration: 217; Percent complete: 5.4%; Average loss: 4.0473 Iteration: 218; Percent complete: 5.5%; Average loss: 3.9720 Iteration: 219; Percent complete: 5.5%; Average loss: 3.8636 Iteration: 220; Percent complete: 5.5%; Average loss: 3.7938 Iteration: 221; Percent complete: 5.5%; Average loss: 3.9970 Iteration: 222; Percent complete: 5.5%; Average loss: 3.9214 Iteration: 223; Percent complete: 5.6%; Average loss: 3.9747 Iteration: 224; Percent complete: 5.6%; Average loss: 4.4310 Iteration: 225; Percent complete: 5.6%; Average loss: 4.2264 Iteration: 226; Percent complete: 5.7%; Average loss: 4.1461 Iteration: 227; Percent complete: 5.7%; Average loss: 3.8592 Iteration: 228; Percent complete: 5.7%; Average loss: 3.8012 Iteration: 229; Percent complete: 5.7%; Average loss: 4.0243 Iteration: 230; Percent complete: 5.8%; Average loss: 4.1253 Iteration: 231; Percent complete: 5.8%; Average loss: 4.0399 Iteration: 232; Percent complete: 5.8%; Average loss: 3.8528 Iteration: 233; Percent complete: 5.8%; Average loss: 4.2015 Iteration: 234; Percent complete: 5.9%; Average loss: 3.8354 Iteration: 235; Percent complete: 5.9%; Average loss: 4.0100 Iteration: 236; Percent complete: 5.9%; Average loss: 4.1361 Iteration: 237; Percent complete: 5.9%; Average loss: 4.0682 Iteration: 238; Percent complete: 5.9%; Average loss: 3.8058 Iteration: 239; Percent complete: 6.0%; Average loss: 4.0169 Iteration: 240; Percent complete: 6.0%; Average loss: 3.8472 Iteration: 241; Percent complete: 6.0%; Average loss: 4.0268 Iteration: 242; Percent complete: 6.0%; Average loss: 3.8856 Iteration: 243; Percent complete: 6.1%; Average loss: 3.9712 Iteration: 244; Percent complete: 6.1%; Average loss: 4.0298 Iteration: 245; Percent complete: 6.1%; Average loss: 3.9250 Iteration: 246; Percent complete: 6.2%; Average loss: 4.0529 Iteration: 247; Percent complete: 6.2%; Average loss: 4.0036 Iteration: 248; Percent complete: 6.2%; Average loss: 4.2044 Iteration: 249; Percent complete: 6.2%; Average loss: 4.1692 Iteration: 250; Percent complete: 6.2%; Average loss: 3.9902 Iteration: 251; Percent complete: 6.3%; Average loss: 3.9847 Iteration: 252; Percent complete: 6.3%; Average loss: 3.9312 Iteration: 253; Percent complete: 6.3%; Average loss: 4.0968 Iteration: 254; Percent complete: 6.3%; Average loss: 3.7440 Iteration: 255; Percent complete: 6.4%; Average loss: 3.7814 Iteration: 256; Percent complete: 6.4%; Average loss: 3.8778 Iteration: 257; Percent complete: 6.4%; Average loss: 3.9462 Iteration: 258; Percent complete: 6.5%; Average loss: 3.8670 Iteration: 259; Percent complete: 6.5%; Average loss: 3.9583 Iteration: 260; Percent complete: 6.5%; Average loss: 3.8852 Iteration: 261; Percent complete: 6.5%; Average loss: 3.9413 Iteration: 262; Percent complete: 6.6%; Average loss: 4.1769 Iteration: 263; Percent complete: 6.6%; Average loss: 3.9672 Iteration: 264; Percent complete: 6.6%; Average loss: 4.0755 Iteration: 265; Percent complete: 6.6%; Average loss: 3.6813 Iteration: 266; Percent complete: 6.7%; Average loss: 3.6722 Iteration: 267; Percent complete: 6.7%; Average loss: 4.1432 Iteration: 268; Percent complete: 6.7%; Average loss: 4.3056 Iteration: 269; Percent complete: 6.7%; Average loss: 4.2333 Iteration: 270; Percent complete: 6.8%; Average loss: 3.9264 Iteration: 271; Percent complete: 6.8%; Average loss: 4.1835 Iteration: 272; Percent complete: 6.8%; Average loss: 4.2209 Iteration: 273; Percent complete: 6.8%; Average loss: 3.8886 Iteration: 274; Percent complete: 6.9%; Average loss: 3.9627 Iteration: 275; Percent complete: 6.9%; Average loss: 4.0668 Iteration: 276; Percent complete: 6.9%; Average loss: 4.1140 Iteration: 277; Percent complete: 6.9%; Average loss: 3.9334 Iteration: 278; Percent complete: 7.0%; Average loss: 3.9210 Iteration: 279; Percent complete: 7.0%; Average loss: 3.9074 Iteration: 280; Percent complete: 7.0%; Average loss: 3.6459 Iteration: 281; Percent complete: 7.0%; Average loss: 3.4367 Iteration: 282; Percent complete: 7.0%; Average loss: 4.0782 Iteration: 283; Percent complete: 7.1%; Average loss: 3.8470 Iteration: 284; Percent complete: 7.1%; Average loss: 3.8382 Iteration: 285; Percent complete: 7.1%; Average loss: 4.2122 Iteration: 286; Percent complete: 7.1%; Average loss: 4.0994 Iteration: 287; Percent complete: 7.2%; Average loss: 3.8104 Iteration: 288; Percent complete: 7.2%; Average loss: 4.0726 Iteration: 289; Percent complete: 7.2%; Average loss: 3.9776 Iteration: 290; Percent complete: 7.2%; Average loss: 3.9142 Iteration: 291; Percent complete: 7.3%; Average loss: 3.9161 Iteration: 292; Percent complete: 7.3%; Average loss: 4.1642 Iteration: 293; Percent complete: 7.3%; Average loss: 4.1852 Iteration: 294; Percent complete: 7.3%; Average loss: 3.9292 Iteration: 295; Percent complete: 7.4%; Average loss: 4.2290 Iteration: 296; Percent complete: 7.4%; Average loss: 3.6722 Iteration: 297; Percent complete: 7.4%; Average loss: 3.9260 Iteration: 298; Percent complete: 7.4%; Average loss: 3.8653 Iteration: 299; Percent complete: 7.5%; Average loss: 3.9611 Iteration: 300; Percent complete: 7.5%; Average loss: 4.0588 Iteration: 301; Percent complete: 7.5%; Average loss: 3.8498 Iteration: 302; Percent complete: 7.5%; Average loss: 3.8037 Iteration: 303; Percent complete: 7.6%; Average loss: 4.0109 Iteration: 304; Percent complete: 7.6%; Average loss: 3.7357 Iteration: 305; Percent complete: 7.6%; Average loss: 4.0324 Iteration: 306; Percent complete: 7.6%; Average loss: 3.7437 Iteration: 307; Percent complete: 7.7%; Average loss: 3.9824 Iteration: 308; Percent complete: 7.7%; Average loss: 4.0195 Iteration: 309; Percent complete: 7.7%; Average loss: 3.6357 Iteration: 310; Percent complete: 7.8%; Average loss: 4.2062 Iteration: 311; Percent complete: 7.8%; Average loss: 3.9330 Iteration: 312; Percent complete: 7.8%; Average loss: 4.0195 Iteration: 313; Percent complete: 7.8%; Average loss: 3.8258 Iteration: 314; Percent complete: 7.8%; Average loss: 3.6191 Iteration: 315; Percent complete: 7.9%; Average loss: 3.7524 Iteration: 316; Percent complete: 7.9%; Average loss: 3.7817 Iteration: 317; Percent complete: 7.9%; Average loss: 3.7227 Iteration: 318; Percent complete: 8.0%; Average loss: 3.6559 Iteration: 319; Percent complete: 8.0%; Average loss: 3.9009 Iteration: 320; Percent complete: 8.0%; Average loss: 4.0995 Iteration: 321; Percent complete: 8.0%; Average loss: 3.9927 Iteration: 322; Percent complete: 8.1%; Average loss: 4.1229 Iteration: 323; Percent complete: 8.1%; Average loss: 3.8154 Iteration: 324; Percent complete: 8.1%; Average loss: 4.0058 Iteration: 325; Percent complete: 8.1%; Average loss: 3.5169 Iteration: 326; Percent complete: 8.2%; Average loss: 3.8350 Iteration: 327; Percent complete: 8.2%; Average loss: 3.8259 Iteration: 328; Percent complete: 8.2%; Average loss: 3.9748 Iteration: 329; Percent complete: 8.2%; Average loss: 3.7931 Iteration: 330; Percent complete: 8.2%; Average loss: 3.8386 Iteration: 331; Percent complete: 8.3%; Average loss: 3.8571 Iteration: 332; Percent complete: 8.3%; Average loss: 3.8165 Iteration: 333; Percent complete: 8.3%; Average loss: 3.8858 Iteration: 334; Percent complete: 8.3%; Average loss: 3.6886 Iteration: 335; Percent complete: 8.4%; Average loss: 3.6719 Iteration: 336; Percent complete: 8.4%; Average loss: 3.7104 Iteration: 337; Percent complete: 8.4%; Average loss: 3.5886 Iteration: 338; Percent complete: 8.5%; Average loss: 3.7231 Iteration: 339; Percent complete: 8.5%; Average loss: 3.8111 Iteration: 340; Percent complete: 8.5%; Average loss: 4.0626 Iteration: 341; Percent complete: 8.5%; Average loss: 3.8089 Iteration: 342; Percent complete: 8.6%; Average loss: 3.9092 Iteration: 343; Percent complete: 8.6%; Average loss: 3.8564 Iteration: 344; Percent complete: 8.6%; Average loss: 3.7870 Iteration: 345; Percent complete: 8.6%; Average loss: 4.0120 Iteration: 346; Percent complete: 8.6%; Average loss: 3.7148 Iteration: 347; Percent complete: 8.7%; Average loss: 3.8811 Iteration: 348; Percent complete: 8.7%; Average loss: 3.8372 Iteration: 349; Percent complete: 8.7%; Average loss: 3.7754 Iteration: 350; Percent complete: 8.8%; Average loss: 4.1021 Iteration: 351; Percent complete: 8.8%; Average loss: 3.9062 Iteration: 352; Percent complete: 8.8%; Average loss: 3.7697 Iteration: 353; Percent complete: 8.8%; Average loss: 3.6772 Iteration: 354; Percent complete: 8.8%; Average loss: 4.0363 Iteration: 355; Percent complete: 8.9%; Average loss: 3.9751 Iteration: 356; Percent complete: 8.9%; Average loss: 3.7483 Iteration: 357; Percent complete: 8.9%; Average loss: 3.9562 Iteration: 358; Percent complete: 8.9%; Average loss: 3.8029 Iteration: 359; Percent complete: 9.0%; Average loss: 3.9083 Iteration: 360; Percent complete: 9.0%; Average loss: 3.8005 Iteration: 361; Percent complete: 9.0%; Average loss: 3.8683 Iteration: 362; Percent complete: 9.0%; Average loss: 3.8801 Iteration: 363; Percent complete: 9.1%; Average loss: 3.8870 Iteration: 364; Percent complete: 9.1%; Average loss: 3.8691 Iteration: 365; Percent complete: 9.1%; Average loss: 3.6718 Iteration: 366; Percent complete: 9.2%; Average loss: 4.0133 Iteration: 367; Percent complete: 9.2%; Average loss: 3.6297 Iteration: 368; Percent complete: 9.2%; Average loss: 3.8958 Iteration: 369; Percent complete: 9.2%; Average loss: 3.7986 Iteration: 370; Percent complete: 9.2%; Average loss: 3.7870 Iteration: 371; Percent complete: 9.3%; Average loss: 3.9622 Iteration: 372; Percent complete: 9.3%; Average loss: 3.8877 Iteration: 373; Percent complete: 9.3%; Average loss: 3.7196 Iteration: 374; Percent complete: 9.3%; Average loss: 3.7833 Iteration: 375; Percent complete: 9.4%; Average loss: 3.9449 Iteration: 376; Percent complete: 9.4%; Average loss: 3.9530 Iteration: 377; Percent complete: 9.4%; Average loss: 3.4805 Iteration: 378; Percent complete: 9.4%; Average loss: 3.6414 Iteration: 379; Percent complete: 9.5%; Average loss: 3.6594 Iteration: 380; Percent complete: 9.5%; Average loss: 3.7621 Iteration: 381; Percent complete: 9.5%; Average loss: 3.9670 Iteration: 382; Percent complete: 9.6%; Average loss: 3.5196 Iteration: 383; Percent complete: 9.6%; Average loss: 3.5022 Iteration: 384; Percent complete: 9.6%; Average loss: 3.9580 Iteration: 385; Percent complete: 9.6%; Average loss: 3.7315 Iteration: 386; Percent complete: 9.7%; Average loss: 4.0758 Iteration: 387; Percent complete: 9.7%; Average loss: 3.8051 Iteration: 388; Percent complete: 9.7%; Average loss: 3.8562 Iteration: 389; Percent complete: 9.7%; Average loss: 3.7532 Iteration: 390; Percent complete: 9.8%; Average loss: 4.1445 Iteration: 391; Percent complete: 9.8%; Average loss: 3.9031 Iteration: 392; Percent complete: 9.8%; Average loss: 3.6054 Iteration: 393; Percent complete: 9.8%; Average loss: 4.0023 Iteration: 394; Percent complete: 9.8%; Average loss: 3.8720 Iteration: 395; Percent complete: 9.9%; Average loss: 3.8136 Iteration: 396; Percent complete: 9.9%; Average loss: 3.7782 Iteration: 397; Percent complete: 9.9%; Average loss: 3.8818 Iteration: 398; Percent complete: 10.0%; Average loss: 3.6830 Iteration: 399; Percent complete: 10.0%; Average loss: 3.6042 Iteration: 400; Percent complete: 10.0%; Average loss: 3.6033 Iteration: 401; Percent complete: 10.0%; Average loss: 4.0543 Iteration: 402; Percent complete: 10.1%; Average loss: 3.8603 Iteration: 403; Percent complete: 10.1%; Average loss: 3.7808 Iteration: 404; Percent complete: 10.1%; Average loss: 4.1854 Iteration: 405; Percent complete: 10.1%; Average loss: 3.7163 Iteration: 406; Percent complete: 10.2%; Average loss: 3.7211 Iteration: 407; Percent complete: 10.2%; Average loss: 4.0517 Iteration: 408; Percent complete: 10.2%; Average loss: 3.9546 Iteration: 409; Percent complete: 10.2%; Average loss: 3.7168 Iteration: 410; Percent complete: 10.2%; Average loss: 3.8187 Iteration: 411; Percent complete: 10.3%; Average loss: 3.7692 Iteration: 412; Percent complete: 10.3%; Average loss: 3.7195 Iteration: 413; Percent complete: 10.3%; Average loss: 3.8198 Iteration: 414; Percent complete: 10.3%; Average loss: 3.9124 Iteration: 415; Percent complete: 10.4%; Average loss: 4.0625 Iteration: 416; Percent complete: 10.4%; Average loss: 3.7785 Iteration: 417; Percent complete: 10.4%; Average loss: 3.5422 Iteration: 418; Percent complete: 10.4%; Average loss: 3.5326 Iteration: 419; Percent complete: 10.5%; Average loss: 3.7042 Iteration: 420; Percent complete: 10.5%; Average loss: 4.1719 Iteration: 421; Percent complete: 10.5%; Average loss: 3.8295 Iteration: 422; Percent complete: 10.5%; Average loss: 3.8512 Iteration: 423; Percent complete: 10.6%; Average loss: 3.9442 Iteration: 424; Percent complete: 10.6%; Average loss: 3.8014 Iteration: 425; Percent complete: 10.6%; Average loss: 3.6830 Iteration: 426; Percent complete: 10.7%; Average loss: 3.8851 Iteration: 427; Percent complete: 10.7%; Average loss: 3.4497 Iteration: 428; Percent complete: 10.7%; Average loss: 3.8967 Iteration: 429; Percent complete: 10.7%; Average loss: 3.8292 Iteration: 430; Percent complete: 10.8%; Average loss: 4.0449 Iteration: 431; Percent complete: 10.8%; Average loss: 3.7376 Iteration: 432; Percent complete: 10.8%; Average loss: 3.8036 Iteration: 433; Percent complete: 10.8%; Average loss: 4.0134 Iteration: 434; Percent complete: 10.8%; Average loss: 3.7928 Iteration: 435; Percent complete: 10.9%; Average loss: 3.7534 Iteration: 436; Percent complete: 10.9%; Average loss: 3.6854 Iteration: 437; Percent complete: 10.9%; Average loss: 3.6860 Iteration: 438; Percent complete: 10.9%; Average loss: 3.6491 Iteration: 439; Percent complete: 11.0%; Average loss: 3.6968 Iteration: 440; Percent complete: 11.0%; Average loss: 3.7408 Iteration: 441; Percent complete: 11.0%; Average loss: 4.0733 Iteration: 442; Percent complete: 11.1%; Average loss: 4.0408 Iteration: 443; Percent complete: 11.1%; Average loss: 3.7522 Iteration: 444; Percent complete: 11.1%; Average loss: 3.7933 Iteration: 445; Percent complete: 11.1%; Average loss: 3.8746 Iteration: 446; Percent complete: 11.2%; Average loss: 3.7394 Iteration: 447; Percent complete: 11.2%; Average loss: 3.6027 Iteration: 448; Percent complete: 11.2%; Average loss: 3.6644 Iteration: 449; Percent complete: 11.2%; Average loss: 3.7040 Iteration: 450; Percent complete: 11.2%; Average loss: 3.6673 Iteration: 451; Percent complete: 11.3%; Average loss: 3.8281 Iteration: 452; Percent complete: 11.3%; Average loss: 3.7541 Iteration: 453; Percent complete: 11.3%; Average loss: 3.6738 Iteration: 454; Percent complete: 11.3%; Average loss: 3.6599 Iteration: 455; Percent complete: 11.4%; Average loss: 3.7282 Iteration: 456; Percent complete: 11.4%; Average loss: 3.5735 Iteration: 457; Percent complete: 11.4%; Average loss: 3.7592 Iteration: 458; Percent complete: 11.5%; Average loss: 3.7065 Iteration: 459; Percent complete: 11.5%; Average loss: 3.8106 Iteration: 460; Percent complete: 11.5%; Average loss: 3.5145 Iteration: 461; Percent complete: 11.5%; Average loss: 3.5765 Iteration: 462; Percent complete: 11.6%; Average loss: 3.8841 Iteration: 463; Percent complete: 11.6%; Average loss: 3.8804 Iteration: 464; Percent complete: 11.6%; Average loss: 3.7238 Iteration: 465; Percent complete: 11.6%; Average loss: 4.0917 Iteration: 466; Percent complete: 11.7%; Average loss: 3.6467 Iteration: 467; Percent complete: 11.7%; Average loss: 3.7112 Iteration: 468; Percent complete: 11.7%; Average loss: 3.9512 Iteration: 469; Percent complete: 11.7%; Average loss: 3.6835 Iteration: 470; Percent complete: 11.8%; Average loss: 3.4151 Iteration: 471; Percent complete: 11.8%; Average loss: 3.7216 Iteration: 472; Percent complete: 11.8%; Average loss: 3.7019 Iteration: 473; Percent complete: 11.8%; Average loss: 3.6703 Iteration: 474; Percent complete: 11.8%; Average loss: 3.9414 Iteration: 475; Percent complete: 11.9%; Average loss: 3.7452 Iteration: 476; Percent complete: 11.9%; Average loss: 3.9532 Iteration: 477; Percent complete: 11.9%; Average loss: 3.8105 Iteration: 478; Percent complete: 11.9%; Average loss: 3.8009 Iteration: 479; Percent complete: 12.0%; Average loss: 3.7122 Iteration: 480; Percent complete: 12.0%; Average loss: 3.7430 Iteration: 481; Percent complete: 12.0%; Average loss: 3.8451 Iteration: 482; Percent complete: 12.0%; Average loss: 3.7796 Iteration: 483; Percent complete: 12.1%; Average loss: 3.6644 Iteration: 484; Percent complete: 12.1%; Average loss: 3.7532 Iteration: 485; Percent complete: 12.1%; Average loss: 3.8663 Iteration: 486; Percent complete: 12.2%; Average loss: 3.5834 Iteration: 487; Percent complete: 12.2%; Average loss: 3.9960 Iteration: 488; Percent complete: 12.2%; Average loss: 3.6028 Iteration: 489; Percent complete: 12.2%; Average loss: 3.9242 Iteration: 490; Percent complete: 12.2%; Average loss: 3.5886 Iteration: 491; Percent complete: 12.3%; Average loss: 3.7678 Iteration: 492; Percent complete: 12.3%; Average loss: 3.7378 Iteration: 493; Percent complete: 12.3%; Average loss: 3.6492 Iteration: 494; Percent complete: 12.3%; Average loss: 3.7200 Iteration: 495; Percent complete: 12.4%; Average loss: 3.7285 Iteration: 496; Percent complete: 12.4%; Average loss: 3.6411 Iteration: 497; Percent complete: 12.4%; Average loss: 3.7766 Iteration: 498; Percent complete: 12.4%; Average loss: 3.5712 Iteration: 499; Percent complete: 12.5%; Average loss: 3.6934 Iteration: 500; Percent complete: 12.5%; Average loss: 3.4050 Iteration: 501; Percent complete: 12.5%; Average loss: 3.7637 Iteration: 502; Percent complete: 12.6%; Average loss: 3.6943 Iteration: 503; Percent complete: 12.6%; Average loss: 4.0403 Iteration: 504; Percent complete: 12.6%; Average loss: 3.8772 Iteration: 505; Percent complete: 12.6%; Average loss: 3.6806 Iteration: 506; Percent complete: 12.7%; Average loss: 3.9077 Iteration: 507; Percent complete: 12.7%; Average loss: 3.8560 Iteration: 508; Percent complete: 12.7%; Average loss: 3.7142 Iteration: 509; Percent complete: 12.7%; Average loss: 3.7543 Iteration: 510; Percent complete: 12.8%; Average loss: 3.8389 Iteration: 511; Percent complete: 12.8%; Average loss: 3.7334 Iteration: 512; Percent complete: 12.8%; Average loss: 3.7529 Iteration: 513; Percent complete: 12.8%; Average loss: 3.7681 Iteration: 514; Percent complete: 12.8%; Average loss: 3.9264 Iteration: 515; Percent complete: 12.9%; Average loss: 3.9280 Iteration: 516; Percent complete: 12.9%; Average loss: 3.5613 Iteration: 517; Percent complete: 12.9%; Average loss: 3.7961 Iteration: 518; Percent complete: 13.0%; Average loss: 4.2622 Iteration: 519; Percent complete: 13.0%; Average loss: 3.7063 Iteration: 520; Percent complete: 13.0%; Average loss: 3.7242 Iteration: 521; Percent complete: 13.0%; Average loss: 3.6466 Iteration: 522; Percent complete: 13.1%; Average loss: 3.6605 Iteration: 523; Percent complete: 13.1%; Average loss: 3.6951 Iteration: 524; Percent complete: 13.1%; Average loss: 3.8780 Iteration: 525; Percent complete: 13.1%; Average loss: 3.6828 Iteration: 526; Percent complete: 13.2%; Average loss: 3.6776 Iteration: 527; Percent complete: 13.2%; Average loss: 3.6464 Iteration: 528; Percent complete: 13.2%; Average loss: 3.8845 Iteration: 529; Percent complete: 13.2%; Average loss: 3.5866 Iteration: 530; Percent complete: 13.2%; Average loss: 3.6426 Iteration: 531; Percent complete: 13.3%; Average loss: 3.5789 Iteration: 532; Percent complete: 13.3%; Average loss: 3.9215 Iteration: 533; Percent complete: 13.3%; Average loss: 3.7895 Iteration: 534; Percent complete: 13.4%; Average loss: 3.7361 Iteration: 535; Percent complete: 13.4%; Average loss: 3.5429 Iteration: 536; Percent complete: 13.4%; Average loss: 3.6919 Iteration: 537; Percent complete: 13.4%; Average loss: 3.6720 Iteration: 538; Percent complete: 13.5%; Average loss: 3.4389 Iteration: 539; Percent complete: 13.5%; Average loss: 3.6855 Iteration: 540; Percent complete: 13.5%; Average loss: 3.5914 Iteration: 541; Percent complete: 13.5%; Average loss: 3.6102 Iteration: 542; Percent complete: 13.6%; Average loss: 3.5880 Iteration: 543; Percent complete: 13.6%; Average loss: 3.6183 Iteration: 544; Percent complete: 13.6%; Average loss: 3.6859 Iteration: 545; Percent complete: 13.6%; Average loss: 3.7343 Iteration: 546; Percent complete: 13.7%; Average loss: 3.8276 Iteration: 547; Percent complete: 13.7%; Average loss: 3.8533 Iteration: 548; Percent complete: 13.7%; Average loss: 3.6370 Iteration: 549; Percent complete: 13.7%; Average loss: 3.7400 Iteration: 550; Percent complete: 13.8%; Average loss: 3.4673 Iteration: 551; Percent complete: 13.8%; Average loss: 3.6513 Iteration: 552; Percent complete: 13.8%; Average loss: 3.8294 Iteration: 553; Percent complete: 13.8%; Average loss: 3.8554 Iteration: 554; Percent complete: 13.9%; Average loss: 3.7860 Iteration: 555; Percent complete: 13.9%; Average loss: 3.7085 Iteration: 556; Percent complete: 13.9%; Average loss: 3.6445 Iteration: 557; Percent complete: 13.9%; Average loss: 3.5229 Iteration: 558; Percent complete: 14.0%; Average loss: 3.9394 Iteration: 559; Percent complete: 14.0%; Average loss: 3.8552 Iteration: 560; Percent complete: 14.0%; Average loss: 3.8065 Iteration: 561; Percent complete: 14.0%; Average loss: 3.7205 Iteration: 562; Percent complete: 14.1%; Average loss: 3.7204 Iteration: 563; Percent complete: 14.1%; Average loss: 3.6498 Iteration: 564; Percent complete: 14.1%; Average loss: 3.6531 Iteration: 565; Percent complete: 14.1%; Average loss: 3.6938 Iteration: 566; Percent complete: 14.1%; Average loss: 3.4877 Iteration: 567; Percent complete: 14.2%; Average loss: 3.7694 Iteration: 568; Percent complete: 14.2%; Average loss: 3.6035 Iteration: 569; Percent complete: 14.2%; Average loss: 3.6045 Iteration: 570; Percent complete: 14.2%; Average loss: 3.7176 Iteration: 571; Percent complete: 14.3%; Average loss: 3.6273 Iteration: 572; Percent complete: 14.3%; Average loss: 3.6440 Iteration: 573; Percent complete: 14.3%; Average loss: 3.5800 Iteration: 574; Percent complete: 14.3%; Average loss: 3.7222 Iteration: 575; Percent complete: 14.4%; Average loss: 3.4439 Iteration: 576; Percent complete: 14.4%; Average loss: 3.4557 Iteration: 577; Percent complete: 14.4%; Average loss: 3.8653 Iteration: 578; Percent complete: 14.4%; Average loss: 3.4434 Iteration: 579; Percent complete: 14.5%; Average loss: 4.0301 Iteration: 580; Percent complete: 14.5%; Average loss: 3.7929 Iteration: 581; Percent complete: 14.5%; Average loss: 3.6134 Iteration: 582; Percent complete: 14.5%; Average loss: 3.7934 Iteration: 583; Percent complete: 14.6%; Average loss: 3.5687 Iteration: 584; Percent complete: 14.6%; Average loss: 3.3886 Iteration: 585; Percent complete: 14.6%; Average loss: 3.8155 Iteration: 586; Percent complete: 14.6%; Average loss: 3.8307 Iteration: 587; Percent complete: 14.7%; Average loss: 3.5291 Iteration: 588; Percent complete: 14.7%; Average loss: 3.3265 Iteration: 589; Percent complete: 14.7%; Average loss: 3.4991 Iteration: 590; Percent complete: 14.8%; Average loss: 3.7146 Iteration: 591; Percent complete: 14.8%; Average loss: 3.8169 Iteration: 592; Percent complete: 14.8%; Average loss: 3.4924 Iteration: 593; Percent complete: 14.8%; Average loss: 3.4134 Iteration: 594; Percent complete: 14.8%; Average loss: 3.5661 Iteration: 595; Percent complete: 14.9%; Average loss: 3.6817 Iteration: 596; Percent complete: 14.9%; Average loss: 3.6530 Iteration: 597; Percent complete: 14.9%; Average loss: 3.3046 Iteration: 598; Percent complete: 14.9%; Average loss: 3.8228 Iteration: 599; Percent complete: 15.0%; Average loss: 3.9314 Iteration: 600; Percent complete: 15.0%; Average loss: 3.7030 Iteration: 601; Percent complete: 15.0%; Average loss: 3.5892 Iteration: 602; Percent complete: 15.0%; Average loss: 3.5931 Iteration: 603; Percent complete: 15.1%; Average loss: 3.7933 Iteration: 604; Percent complete: 15.1%; Average loss: 3.5734 Iteration: 605; Percent complete: 15.1%; Average loss: 3.5999 Iteration: 606; Percent complete: 15.2%; Average loss: 3.4313 Iteration: 607; Percent complete: 15.2%; Average loss: 3.6277 Iteration: 608; Percent complete: 15.2%; Average loss: 3.7649 Iteration: 609; Percent complete: 15.2%; Average loss: 3.6757 Iteration: 610; Percent complete: 15.2%; Average loss: 3.9065 Iteration: 611; Percent complete: 15.3%; Average loss: 3.5413 Iteration: 612; Percent complete: 15.3%; Average loss: 3.5871 Iteration: 613; Percent complete: 15.3%; Average loss: 3.8822 Iteration: 614; Percent complete: 15.3%; Average loss: 3.6300 Iteration: 615; Percent complete: 15.4%; Average loss: 3.8569 Iteration: 616; Percent complete: 15.4%; Average loss: 3.7766 Iteration: 617; Percent complete: 15.4%; Average loss: 3.7841 Iteration: 618; Percent complete: 15.4%; Average loss: 3.8145 Iteration: 619; Percent complete: 15.5%; Average loss: 3.8911 Iteration: 620; Percent complete: 15.5%; Average loss: 3.4458 Iteration: 621; Percent complete: 15.5%; Average loss: 3.3701 Iteration: 622; Percent complete: 15.6%; Average loss: 3.7175 Iteration: 623; Percent complete: 15.6%; Average loss: 3.5337 Iteration: 624; Percent complete: 15.6%; Average loss: 4.0521 Iteration: 625; Percent complete: 15.6%; Average loss: 3.8515 Iteration: 626; Percent complete: 15.7%; Average loss: 3.7054 Iteration: 627; Percent complete: 15.7%; Average loss: 3.5507 Iteration: 628; Percent complete: 15.7%; Average loss: 3.6639 Iteration: 629; Percent complete: 15.7%; Average loss: 3.5412 Iteration: 630; Percent complete: 15.8%; Average loss: 3.6610 Iteration: 631; Percent complete: 15.8%; Average loss: 3.6869 Iteration: 632; Percent complete: 15.8%; Average loss: 3.9102 Iteration: 633; Percent complete: 15.8%; Average loss: 3.4345 Iteration: 634; Percent complete: 15.8%; Average loss: 3.7326 Iteration: 635; Percent complete: 15.9%; Average loss: 3.8203 Iteration: 636; Percent complete: 15.9%; Average loss: 3.3605 Iteration: 637; Percent complete: 15.9%; Average loss: 3.8944 Iteration: 638; Percent complete: 16.0%; Average loss: 3.5699 Iteration: 639; Percent complete: 16.0%; Average loss: 3.5902 Iteration: 640; Percent complete: 16.0%; Average loss: 3.8047 Iteration: 641; Percent complete: 16.0%; Average loss: 3.4144 Iteration: 642; Percent complete: 16.1%; Average loss: 3.6899 Iteration: 643; Percent complete: 16.1%; Average loss: 3.8405 Iteration: 644; Percent complete: 16.1%; Average loss: 3.5504 Iteration: 645; Percent complete: 16.1%; Average loss: 3.4787 Iteration: 646; Percent complete: 16.2%; Average loss: 3.5888 Iteration: 647; Percent complete: 16.2%; Average loss: 3.5492 Iteration: 648; Percent complete: 16.2%; Average loss: 3.5877 Iteration: 649; Percent complete: 16.2%; Average loss: 3.8612 Iteration: 650; Percent complete: 16.2%; Average loss: 3.4469 Iteration: 651; Percent complete: 16.3%; Average loss: 3.5281 Iteration: 652; Percent complete: 16.3%; Average loss: 3.4904 Iteration: 653; Percent complete: 16.3%; Average loss: 3.5642 Iteration: 654; Percent complete: 16.4%; Average loss: 3.3360 Iteration: 655; Percent complete: 16.4%; Average loss: 4.0082 Iteration: 656; Percent complete: 16.4%; Average loss: 3.6184 Iteration: 657; Percent complete: 16.4%; Average loss: 3.6745 Iteration: 658; Percent complete: 16.4%; Average loss: 3.6890 Iteration: 659; Percent complete: 16.5%; Average loss: 3.6936 Iteration: 660; Percent complete: 16.5%; Average loss: 3.4461 Iteration: 661; Percent complete: 16.5%; Average loss: 3.4395 Iteration: 662; Percent complete: 16.6%; Average loss: 3.3765 Iteration: 663; Percent complete: 16.6%; Average loss: 3.6121 Iteration: 664; Percent complete: 16.6%; Average loss: 3.5725 Iteration: 665; Percent complete: 16.6%; Average loss: 3.5572 Iteration: 666; Percent complete: 16.7%; Average loss: 3.8594 Iteration: 667; Percent complete: 16.7%; Average loss: 3.4797 Iteration: 668; Percent complete: 16.7%; Average loss: 3.6888 Iteration: 669; Percent complete: 16.7%; Average loss: 3.6844 Iteration: 670; Percent complete: 16.8%; Average loss: 3.4451 Iteration: 671; Percent complete: 16.8%; Average loss: 3.7818 Iteration: 672; Percent complete: 16.8%; Average loss: 3.4658 Iteration: 673; Percent complete: 16.8%; Average loss: 3.2167 Iteration: 674; Percent complete: 16.9%; Average loss: 3.3791 Iteration: 675; Percent complete: 16.9%; Average loss: 3.5105 Iteration: 676; Percent complete: 16.9%; Average loss: 3.5353 Iteration: 677; Percent complete: 16.9%; Average loss: 3.4139 Iteration: 678; Percent complete: 17.0%; Average loss: 3.7559 Iteration: 679; Percent complete: 17.0%; Average loss: 3.9090 Iteration: 680; Percent complete: 17.0%; Average loss: 3.7739 Iteration: 681; Percent complete: 17.0%; Average loss: 3.5693 Iteration: 682; Percent complete: 17.1%; Average loss: 3.8804 Iteration: 683; Percent complete: 17.1%; Average loss: 3.6567 Iteration: 684; Percent complete: 17.1%; Average loss: 3.5583 Iteration: 685; Percent complete: 17.1%; Average loss: 3.5636 Iteration: 686; Percent complete: 17.2%; Average loss: 3.3426 Iteration: 687; Percent complete: 17.2%; Average loss: 3.6325 Iteration: 688; Percent complete: 17.2%; Average loss: 3.5755 Iteration: 689; Percent complete: 17.2%; Average loss: 3.9421 Iteration: 690; Percent complete: 17.2%; Average loss: 3.7110 Iteration: 691; Percent complete: 17.3%; Average loss: 3.6895 Iteration: 692; Percent complete: 17.3%; Average loss: 3.6401 Iteration: 693; Percent complete: 17.3%; Average loss: 3.9377 Iteration: 694; Percent complete: 17.3%; Average loss: 3.5453 Iteration: 695; Percent complete: 17.4%; Average loss: 3.7622 Iteration: 696; Percent complete: 17.4%; Average loss: 3.5246 Iteration: 697; Percent complete: 17.4%; Average loss: 3.6749 Iteration: 698; Percent complete: 17.4%; Average loss: 3.5956 Iteration: 699; Percent complete: 17.5%; Average loss: 3.6051 Iteration: 700; Percent complete: 17.5%; Average loss: 3.9431 Iteration: 701; Percent complete: 17.5%; Average loss: 3.5296 Iteration: 702; Percent complete: 17.5%; Average loss: 3.8589 Iteration: 703; Percent complete: 17.6%; Average loss: 3.3820 Iteration: 704; Percent complete: 17.6%; Average loss: 3.4485 Iteration: 705; Percent complete: 17.6%; Average loss: 3.3530 Iteration: 706; Percent complete: 17.6%; Average loss: 3.6531 Iteration: 707; Percent complete: 17.7%; Average loss: 3.4037 Iteration: 708; Percent complete: 17.7%; Average loss: 3.5056 Iteration: 709; Percent complete: 17.7%; Average loss: 3.7576 Iteration: 710; Percent complete: 17.8%; Average loss: 3.3733 Iteration: 711; Percent complete: 17.8%; Average loss: 3.5497 Iteration: 712; Percent complete: 17.8%; Average loss: 3.4299 Iteration: 713; Percent complete: 17.8%; Average loss: 3.6077 Iteration: 714; Percent complete: 17.8%; Average loss: 3.6160 Iteration: 715; Percent complete: 17.9%; Average loss: 3.6875 Iteration: 716; Percent complete: 17.9%; Average loss: 3.8424 Iteration: 717; Percent complete: 17.9%; Average loss: 3.5597 Iteration: 718; Percent complete: 17.9%; Average loss: 3.3719 Iteration: 719; Percent complete: 18.0%; Average loss: 3.7144 Iteration: 720; Percent complete: 18.0%; Average loss: 3.7321 Iteration: 721; Percent complete: 18.0%; Average loss: 3.7388 Iteration: 722; Percent complete: 18.1%; Average loss: 3.4843 Iteration: 723; Percent complete: 18.1%; Average loss: 3.4233 Iteration: 724; Percent complete: 18.1%; Average loss: 3.5898 Iteration: 725; Percent complete: 18.1%; Average loss: 3.8557 Iteration: 726; Percent complete: 18.1%; Average loss: 3.8385 Iteration: 727; Percent complete: 18.2%; Average loss: 3.7131 Iteration: 728; Percent complete: 18.2%; Average loss: 3.7178 Iteration: 729; Percent complete: 18.2%; Average loss: 4.1074 Iteration: 730; Percent complete: 18.2%; Average loss: 3.5278 Iteration: 731; Percent complete: 18.3%; Average loss: 3.7509 Iteration: 732; Percent complete: 18.3%; Average loss: 3.6628 Iteration: 733; Percent complete: 18.3%; Average loss: 3.5518 Iteration: 734; Percent complete: 18.4%; Average loss: 3.6094 Iteration: 735; Percent complete: 18.4%; Average loss: 3.5243 Iteration: 736; Percent complete: 18.4%; Average loss: 3.7603 Iteration: 737; Percent complete: 18.4%; Average loss: 3.3238 Iteration: 738; Percent complete: 18.4%; Average loss: 3.5140 Iteration: 739; Percent complete: 18.5%; Average loss: 3.5051 Iteration: 740; Percent complete: 18.5%; Average loss: 3.6614 Iteration: 741; Percent complete: 18.5%; Average loss: 3.7263 Iteration: 742; Percent complete: 18.6%; Average loss: 3.7878 Iteration: 743; Percent complete: 18.6%; Average loss: 3.7736 Iteration: 744; Percent complete: 18.6%; Average loss: 3.3851 Iteration: 745; Percent complete: 18.6%; Average loss: 3.6624 Iteration: 746; Percent complete: 18.6%; Average loss: 3.5097 Iteration: 747; Percent complete: 18.7%; Average loss: 3.8095 Iteration: 748; Percent complete: 18.7%; Average loss: 3.3399 Iteration: 749; Percent complete: 18.7%; Average loss: 3.7161 Iteration: 750; Percent complete: 18.8%; Average loss: 3.6133 Iteration: 751; Percent complete: 18.8%; Average loss: 3.7416 Iteration: 752; Percent complete: 18.8%; Average loss: 3.7949 Iteration: 753; Percent complete: 18.8%; Average loss: 3.5560 Iteration: 754; Percent complete: 18.9%; Average loss: 3.7997 Iteration: 755; Percent complete: 18.9%; Average loss: 3.4895 Iteration: 756; Percent complete: 18.9%; Average loss: 3.7303 Iteration: 757; Percent complete: 18.9%; Average loss: 3.5593 Iteration: 758; Percent complete: 18.9%; Average loss: 3.5854 Iteration: 759; Percent complete: 19.0%; Average loss: 3.3611 Iteration: 760; Percent complete: 19.0%; Average loss: 3.5116 Iteration: 761; Percent complete: 19.0%; Average loss: 3.6401 Iteration: 762; Percent complete: 19.1%; Average loss: 3.6278 Iteration: 763; Percent complete: 19.1%; Average loss: 3.6157 Iteration: 764; Percent complete: 19.1%; Average loss: 3.4133 Iteration: 765; Percent complete: 19.1%; Average loss: 3.6686 Iteration: 766; Percent complete: 19.1%; Average loss: 3.5565 Iteration: 767; Percent complete: 19.2%; Average loss: 3.5312 Iteration: 768; Percent complete: 19.2%; Average loss: 3.6221 Iteration: 769; Percent complete: 19.2%; Average loss: 3.6621 Iteration: 770; Percent complete: 19.2%; Average loss: 3.6298 Iteration: 771; Percent complete: 19.3%; Average loss: 3.4978 Iteration: 772; Percent complete: 19.3%; Average loss: 3.5311 Iteration: 773; Percent complete: 19.3%; Average loss: 3.4143 Iteration: 774; Percent complete: 19.4%; Average loss: 3.1761 Iteration: 775; Percent complete: 19.4%; Average loss: 3.5972 Iteration: 776; Percent complete: 19.4%; Average loss: 3.5982 Iteration: 777; Percent complete: 19.4%; Average loss: 3.5905 Iteration: 778; Percent complete: 19.4%; Average loss: 3.4382 Iteration: 779; Percent complete: 19.5%; Average loss: 3.4797 Iteration: 780; Percent complete: 19.5%; Average loss: 3.4883 Iteration: 781; Percent complete: 19.5%; Average loss: 3.6410 Iteration: 782; Percent complete: 19.6%; Average loss: 3.7022 Iteration: 783; Percent complete: 19.6%; Average loss: 3.6563 Iteration: 784; Percent complete: 19.6%; Average loss: 3.5219 Iteration: 785; Percent complete: 19.6%; Average loss: 3.3917 Iteration: 786; Percent complete: 19.7%; Average loss: 3.5456 Iteration: 787; Percent complete: 19.7%; Average loss: 3.8979 Iteration: 788; Percent complete: 19.7%; Average loss: 3.5261 Iteration: 789; Percent complete: 19.7%; Average loss: 3.6247 Iteration: 790; Percent complete: 19.8%; Average loss: 3.6166 Iteration: 791; Percent complete: 19.8%; Average loss: 3.3911 Iteration: 792; Percent complete: 19.8%; Average loss: 3.5538 Iteration: 793; Percent complete: 19.8%; Average loss: 3.6301 Iteration: 794; Percent complete: 19.9%; Average loss: 3.5063 Iteration: 795; Percent complete: 19.9%; Average loss: 3.6609 Iteration: 796; Percent complete: 19.9%; Average loss: 3.5223 Iteration: 797; Percent complete: 19.9%; Average loss: 3.7565 Iteration: 798; Percent complete: 20.0%; Average loss: 3.5414 Iteration: 799; Percent complete: 20.0%; Average loss: 3.4539 Iteration: 800; Percent complete: 20.0%; Average loss: 3.5645 Iteration: 801; Percent complete: 20.0%; Average loss: 3.4538 Iteration: 802; Percent complete: 20.1%; Average loss: 3.6022 Iteration: 803; Percent complete: 20.1%; Average loss: 3.5889 Iteration: 804; Percent complete: 20.1%; Average loss: 3.4370 Iteration: 805; Percent complete: 20.1%; Average loss: 3.4511 Iteration: 806; Percent complete: 20.2%; Average loss: 3.5771 Iteration: 807; Percent complete: 20.2%; Average loss: 3.5379 Iteration: 808; Percent complete: 20.2%; Average loss: 3.3270 Iteration: 809; Percent complete: 20.2%; Average loss: 3.1621 Iteration: 810; Percent complete: 20.2%; Average loss: 3.6326 Iteration: 811; Percent complete: 20.3%; Average loss: 3.7955 Iteration: 812; Percent complete: 20.3%; Average loss: 3.5261 Iteration: 813; Percent complete: 20.3%; Average loss: 3.5820 Iteration: 814; Percent complete: 20.3%; Average loss: 3.3628 Iteration: 815; Percent complete: 20.4%; Average loss: 3.6605 Iteration: 816; Percent complete: 20.4%; Average loss: 3.5128 Iteration: 817; Percent complete: 20.4%; Average loss: 3.7156 Iteration: 818; Percent complete: 20.4%; Average loss: 3.9293 Iteration: 819; Percent complete: 20.5%; Average loss: 3.4814 Iteration: 820; Percent complete: 20.5%; Average loss: 3.4905 Iteration: 821; Percent complete: 20.5%; Average loss: 3.3458 Iteration: 822; Percent complete: 20.5%; Average loss: 3.4010 Iteration: 823; Percent complete: 20.6%; Average loss: 3.7499 Iteration: 824; Percent complete: 20.6%; Average loss: 3.4270 Iteration: 825; Percent complete: 20.6%; Average loss: 3.5985 Iteration: 826; Percent complete: 20.6%; Average loss: 3.3891 Iteration: 827; Percent complete: 20.7%; Average loss: 3.5869 Iteration: 828; Percent complete: 20.7%; Average loss: 3.4493 Iteration: 829; Percent complete: 20.7%; Average loss: 3.8053 Iteration: 830; Percent complete: 20.8%; Average loss: 3.5465 Iteration: 831; Percent complete: 20.8%; Average loss: 3.3203 Iteration: 832; Percent complete: 20.8%; Average loss: 3.7204 Iteration: 833; Percent complete: 20.8%; Average loss: 3.2725 Iteration: 834; Percent complete: 20.8%; Average loss: 3.5377 Iteration: 835; Percent complete: 20.9%; Average loss: 3.5848 Iteration: 836; Percent complete: 20.9%; Average loss: 3.5857 Iteration: 837; Percent complete: 20.9%; Average loss: 3.5773 Iteration: 838; Percent complete: 20.9%; Average loss: 3.3663 Iteration: 839; Percent complete: 21.0%; Average loss: 3.6197 Iteration: 840; Percent complete: 21.0%; Average loss: 3.6037 Iteration: 841; Percent complete: 21.0%; Average loss: 3.5231 Iteration: 842; Percent complete: 21.1%; Average loss: 3.5001 Iteration: 843; Percent complete: 21.1%; Average loss: 3.5841 Iteration: 844; Percent complete: 21.1%; Average loss: 3.6276 Iteration: 845; Percent complete: 21.1%; Average loss: 3.7463 Iteration: 846; Percent complete: 21.1%; Average loss: 3.4006 Iteration: 847; Percent complete: 21.2%; Average loss: 3.3359 Iteration: 848; Percent complete: 21.2%; Average loss: 3.3949 Iteration: 849; Percent complete: 21.2%; Average loss: 3.5315 Iteration: 850; Percent complete: 21.2%; Average loss: 3.6480 Iteration: 851; Percent complete: 21.3%; Average loss: 3.5539 Iteration: 852; Percent complete: 21.3%; Average loss: 3.6276 Iteration: 853; Percent complete: 21.3%; Average loss: 3.4616 Iteration: 854; Percent complete: 21.3%; Average loss: 3.3897 Iteration: 855; Percent complete: 21.4%; Average loss: 3.5216 Iteration: 856; Percent complete: 21.4%; Average loss: 3.3250 Iteration: 857; Percent complete: 21.4%; Average loss: 3.5170 Iteration: 858; Percent complete: 21.4%; Average loss: 3.6663 Iteration: 859; Percent complete: 21.5%; Average loss: 3.6312 Iteration: 860; Percent complete: 21.5%; Average loss: 3.4810 Iteration: 861; Percent complete: 21.5%; Average loss: 3.4135 Iteration: 862; Percent complete: 21.6%; Average loss: 3.5074 Iteration: 863; Percent complete: 21.6%; Average loss: 3.4013 Iteration: 864; Percent complete: 21.6%; Average loss: 3.4131 Iteration: 865; Percent complete: 21.6%; Average loss: 3.4332 Iteration: 866; Percent complete: 21.6%; Average loss: 3.6537 Iteration: 867; Percent complete: 21.7%; Average loss: 3.5194 Iteration: 868; Percent complete: 21.7%; Average loss: 3.5412 Iteration: 869; Percent complete: 21.7%; Average loss: 3.8455 Iteration: 870; Percent complete: 21.8%; Average loss: 3.5537 Iteration: 871; Percent complete: 21.8%; Average loss: 3.4311 Iteration: 872; Percent complete: 21.8%; Average loss: 3.5531 Iteration: 873; Percent complete: 21.8%; Average loss: 3.6795 Iteration: 874; Percent complete: 21.9%; Average loss: 3.9202 Iteration: 875; Percent complete: 21.9%; Average loss: 3.4337 Iteration: 876; Percent complete: 21.9%; Average loss: 3.6008 Iteration: 877; Percent complete: 21.9%; Average loss: 3.6333 Iteration: 878; Percent complete: 21.9%; Average loss: 3.5520 Iteration: 879; Percent complete: 22.0%; Average loss: 3.7014 Iteration: 880; Percent complete: 22.0%; Average loss: 3.4167 Iteration: 881; Percent complete: 22.0%; Average loss: 3.4480 Iteration: 882; Percent complete: 22.1%; Average loss: 3.3562 Iteration: 883; Percent complete: 22.1%; Average loss: 3.7513 Iteration: 884; Percent complete: 22.1%; Average loss: 3.6109 Iteration: 885; Percent complete: 22.1%; Average loss: 3.4533 Iteration: 886; Percent complete: 22.1%; Average loss: 3.5784 Iteration: 887; Percent complete: 22.2%; Average loss: 3.8337 Iteration: 888; Percent complete: 22.2%; Average loss: 3.6128 Iteration: 889; Percent complete: 22.2%; Average loss: 3.5425 Iteration: 890; Percent complete: 22.2%; Average loss: 3.5607 Iteration: 891; Percent complete: 22.3%; Average loss: 3.3575 Iteration: 892; Percent complete: 22.3%; Average loss: 3.4576 Iteration: 893; Percent complete: 22.3%; Average loss: 3.3373 Iteration: 894; Percent complete: 22.4%; Average loss: 3.5568 Iteration: 895; Percent complete: 22.4%; Average loss: 3.6575 Iteration: 896; Percent complete: 22.4%; Average loss: 3.7700 Iteration: 897; Percent complete: 22.4%; Average loss: 3.3262 Iteration: 898; Percent complete: 22.4%; Average loss: 3.6849 Iteration: 899; Percent complete: 22.5%; Average loss: 3.3986 Iteration: 900; Percent complete: 22.5%; Average loss: 3.7253 Iteration: 901; Percent complete: 22.5%; Average loss: 3.3032 Iteration: 902; Percent complete: 22.6%; Average loss: 3.6491 Iteration: 903; Percent complete: 22.6%; Average loss: 3.6549 Iteration: 904; Percent complete: 22.6%; Average loss: 3.4655 Iteration: 905; Percent complete: 22.6%; Average loss: 3.4510 Iteration: 906; Percent complete: 22.7%; Average loss: 3.4310 Iteration: 907; Percent complete: 22.7%; Average loss: 3.4675 Iteration: 908; Percent complete: 22.7%; Average loss: 3.4511 Iteration: 909; Percent complete: 22.7%; Average loss: 3.4042 Iteration: 910; Percent complete: 22.8%; Average loss: 3.3496 Iteration: 911; Percent complete: 22.8%; Average loss: 3.5439 Iteration: 912; Percent complete: 22.8%; Average loss: 3.4406 Iteration: 913; Percent complete: 22.8%; Average loss: 3.3593 Iteration: 914; Percent complete: 22.9%; Average loss: 3.5348 Iteration: 915; Percent complete: 22.9%; Average loss: 3.7245 Iteration: 916; Percent complete: 22.9%; Average loss: 3.6471 Iteration: 917; Percent complete: 22.9%; Average loss: 3.6803 Iteration: 918; Percent complete: 22.9%; Average loss: 3.4830 Iteration: 919; Percent complete: 23.0%; Average loss: 3.2568 Iteration: 920; Percent complete: 23.0%; Average loss: 3.7267 Iteration: 921; Percent complete: 23.0%; Average loss: 3.6653 Iteration: 922; Percent complete: 23.1%; Average loss: 3.4491 Iteration: 923; Percent complete: 23.1%; Average loss: 3.6209 Iteration: 924; Percent complete: 23.1%; Average loss: 3.6114 Iteration: 925; Percent complete: 23.1%; Average loss: 3.1260 Iteration: 926; Percent complete: 23.2%; Average loss: 3.7398 Iteration: 927; Percent complete: 23.2%; Average loss: 3.3786 Iteration: 928; Percent complete: 23.2%; Average loss: 3.5939 Iteration: 929; Percent complete: 23.2%; Average loss: 3.2974 Iteration: 930; Percent complete: 23.2%; Average loss: 3.5081 Iteration: 931; Percent complete: 23.3%; Average loss: 3.6164 Iteration: 932; Percent complete: 23.3%; Average loss: 3.4276 Iteration: 933; Percent complete: 23.3%; Average loss: 3.4698 Iteration: 934; Percent complete: 23.4%; Average loss: 3.4674 Iteration: 935; Percent complete: 23.4%; Average loss: 3.4487 Iteration: 936; Percent complete: 23.4%; Average loss: 3.8329 Iteration: 937; Percent complete: 23.4%; Average loss: 3.4054 Iteration: 938; Percent complete: 23.4%; Average loss: 3.6054 Iteration: 939; Percent complete: 23.5%; Average loss: 3.4963 Iteration: 940; Percent complete: 23.5%; Average loss: 3.6354 Iteration: 941; Percent complete: 23.5%; Average loss: 3.6042 Iteration: 942; Percent complete: 23.5%; Average loss: 3.3475 Iteration: 943; Percent complete: 23.6%; Average loss: 3.3167 Iteration: 944; Percent complete: 23.6%; Average loss: 3.3889 Iteration: 945; Percent complete: 23.6%; Average loss: 3.2977 Iteration: 946; Percent complete: 23.6%; Average loss: 3.4713 Iteration: 947; Percent complete: 23.7%; Average loss: 3.4775 Iteration: 948; Percent complete: 23.7%; Average loss: 3.5051 Iteration: 949; Percent complete: 23.7%; Average loss: 3.1252 Iteration: 950; Percent complete: 23.8%; Average loss: 3.6533 Iteration: 951; Percent complete: 23.8%; Average loss: 3.5571 Iteration: 952; Percent complete: 23.8%; Average loss: 3.5177 Iteration: 953; Percent complete: 23.8%; Average loss: 3.3169 Iteration: 954; Percent complete: 23.8%; Average loss: 3.7467 Iteration: 955; Percent complete: 23.9%; Average loss: 3.4274 Iteration: 956; Percent complete: 23.9%; Average loss: 3.5838 Iteration: 957; Percent complete: 23.9%; Average loss: 3.3769 Iteration: 958; Percent complete: 23.9%; Average loss: 3.4067 Iteration: 959; Percent complete: 24.0%; Average loss: 3.4789 Iteration: 960; Percent complete: 24.0%; Average loss: 3.4014 Iteration: 961; Percent complete: 24.0%; Average loss: 3.5607 Iteration: 962; Percent complete: 24.1%; Average loss: 3.3497 Iteration: 963; Percent complete: 24.1%; Average loss: 3.3740 Iteration: 964; Percent complete: 24.1%; Average loss: 3.3644 Iteration: 965; Percent complete: 24.1%; Average loss: 3.6642 Iteration: 966; Percent complete: 24.1%; Average loss: 3.5393 Iteration: 967; Percent complete: 24.2%; Average loss: 3.5352 Iteration: 968; Percent complete: 24.2%; Average loss: 3.4962 Iteration: 969; Percent complete: 24.2%; Average loss: 3.4899 Iteration: 970; Percent complete: 24.2%; Average loss: 3.5454 Iteration: 971; Percent complete: 24.3%; Average loss: 3.4335 Iteration: 972; Percent complete: 24.3%; Average loss: 3.4028 Iteration: 973; Percent complete: 24.3%; Average loss: 3.4868 Iteration: 974; Percent complete: 24.3%; Average loss: 3.3918 Iteration: 975; Percent complete: 24.4%; Average loss: 3.4541 Iteration: 976; Percent complete: 24.4%; Average loss: 3.2493 Iteration: 977; Percent complete: 24.4%; Average loss: 3.4464 Iteration: 978; Percent complete: 24.4%; Average loss: 3.4639 Iteration: 979; Percent complete: 24.5%; Average loss: 3.3465 Iteration: 980; Percent complete: 24.5%; Average loss: 3.2723 Iteration: 981; Percent complete: 24.5%; Average loss: 3.2729 Iteration: 982; Percent complete: 24.6%; Average loss: 3.6451 Iteration: 983; Percent complete: 24.6%; Average loss: 3.4533 Iteration: 984; Percent complete: 24.6%; Average loss: 3.5463 Iteration: 985; Percent complete: 24.6%; Average loss: 3.4242 Iteration: 986; Percent complete: 24.6%; Average loss: 3.5540 Iteration: 987; Percent complete: 24.7%; Average loss: 3.4739 Iteration: 988; Percent complete: 24.7%; Average loss: 3.4252 Iteration: 989; Percent complete: 24.7%; Average loss: 3.3033 Iteration: 990; Percent complete: 24.8%; Average loss: 3.4102 Iteration: 991; Percent complete: 24.8%; Average loss: 3.5793 Iteration: 992; Percent complete: 24.8%; Average loss: 3.4535 Iteration: 993; Percent complete: 24.8%; Average loss: 3.4009 Iteration: 994; Percent complete: 24.9%; Average loss: 3.5699 Iteration: 995; Percent complete: 24.9%; Average loss: 3.4716 Iteration: 996; Percent complete: 24.9%; Average loss: 3.5791 Iteration: 997; Percent complete: 24.9%; Average loss: 3.2969 Iteration: 998; Percent complete: 24.9%; Average loss: 3.1974 Iteration: 999; Percent complete: 25.0%; Average loss: 3.2873 Iteration: 1000; Percent complete: 25.0%; Average loss: 3.6855 Iteration: 1001; Percent complete: 25.0%; Average loss: 3.2325 Iteration: 1002; Percent complete: 25.1%; Average loss: 3.5274 Iteration: 1003; Percent complete: 25.1%; Average loss: 3.7042 Iteration: 1004; Percent complete: 25.1%; Average loss: 3.6182 Iteration: 1005; Percent complete: 25.1%; Average loss: 3.2686 Iteration: 1006; Percent complete: 25.1%; Average loss: 3.2909 Iteration: 1007; Percent complete: 25.2%; Average loss: 3.4840 Iteration: 1008; Percent complete: 25.2%; Average loss: 3.5833 Iteration: 1009; Percent complete: 25.2%; Average loss: 3.4722 Iteration: 1010; Percent complete: 25.2%; Average loss: 3.4814 Iteration: 1011; Percent complete: 25.3%; Average loss: 3.6635 Iteration: 1012; Percent complete: 25.3%; Average loss: 3.6570 Iteration: 1013; Percent complete: 25.3%; Average loss: 3.3822 Iteration: 1014; Percent complete: 25.4%; Average loss: 3.4184 Iteration: 1015; Percent complete: 25.4%; Average loss: 3.5297 Iteration: 1016; Percent complete: 25.4%; Average loss: 3.0870 Iteration: 1017; Percent complete: 25.4%; Average loss: 3.4151 Iteration: 1018; Percent complete: 25.4%; Average loss: 3.1984 Iteration: 1019; Percent complete: 25.5%; Average loss: 3.5597 Iteration: 1020; Percent complete: 25.5%; Average loss: 3.5073 Iteration: 1021; Percent complete: 25.5%; Average loss: 3.4539 Iteration: 1022; Percent complete: 25.6%; Average loss: 3.4809 Iteration: 1023; Percent complete: 25.6%; Average loss: 3.3777 Iteration: 1024; Percent complete: 25.6%; Average loss: 3.7249 Iteration: 1025; Percent complete: 25.6%; Average loss: 3.4981 Iteration: 1026; Percent complete: 25.7%; Average loss: 3.4003 Iteration: 1027; Percent complete: 25.7%; Average loss: 3.2648 Iteration: 1028; Percent complete: 25.7%; Average loss: 3.5892 Iteration: 1029; Percent complete: 25.7%; Average loss: 3.6561 Iteration: 1030; Percent complete: 25.8%; Average loss: 3.2470 Iteration: 1031; Percent complete: 25.8%; Average loss: 3.3200 Iteration: 1032; Percent complete: 25.8%; Average loss: 3.5689 Iteration: 1033; Percent complete: 25.8%; Average loss: 3.3992 Iteration: 1034; Percent complete: 25.9%; Average loss: 3.5077 Iteration: 1035; Percent complete: 25.9%; Average loss: 3.3738 Iteration: 1036; Percent complete: 25.9%; Average loss: 3.5188 Iteration: 1037; Percent complete: 25.9%; Average loss: 3.3653 Iteration: 1038; Percent complete: 25.9%; Average loss: 3.3464 Iteration: 1039; Percent complete: 26.0%; Average loss: 3.4738 Iteration: 1040; Percent complete: 26.0%; Average loss: 3.3141 Iteration: 1041; Percent complete: 26.0%; Average loss: 3.3878 Iteration: 1042; Percent complete: 26.1%; Average loss: 3.4279 Iteration: 1043; Percent complete: 26.1%; Average loss: 3.4310 Iteration: 1044; Percent complete: 26.1%; Average loss: 3.3725 Iteration: 1045; Percent complete: 26.1%; Average loss: 3.8419 Iteration: 1046; Percent complete: 26.2%; Average loss: 3.3358 Iteration: 1047; Percent complete: 26.2%; Average loss: 3.6724 Iteration: 1048; Percent complete: 26.2%; Average loss: 3.3505 Iteration: 1049; Percent complete: 26.2%; Average loss: 3.2956 Iteration: 1050; Percent complete: 26.2%; Average loss: 3.2860 Iteration: 1051; Percent complete: 26.3%; Average loss: 3.4088 Iteration: 1052; Percent complete: 26.3%; Average loss: 3.6164 Iteration: 1053; Percent complete: 26.3%; Average loss: 3.7105 Iteration: 1054; Percent complete: 26.4%; Average loss: 3.6051 Iteration: 1055; Percent complete: 26.4%; Average loss: 3.3170 Iteration: 1056; Percent complete: 26.4%; Average loss: 3.4801 Iteration: 1057; Percent complete: 26.4%; Average loss: 3.4772 Iteration: 1058; Percent complete: 26.5%; Average loss: 3.3950 Iteration: 1059; Percent complete: 26.5%; Average loss: 3.1410 Iteration: 1060; Percent complete: 26.5%; Average loss: 3.4419 Iteration: 1061; Percent complete: 26.5%; Average loss: 3.6071 Iteration: 1062; Percent complete: 26.6%; Average loss: 3.4191 Iteration: 1063; Percent complete: 26.6%; Average loss: 3.4889 Iteration: 1064; Percent complete: 26.6%; Average loss: 3.7739 Iteration: 1065; Percent complete: 26.6%; Average loss: 3.5487 Iteration: 1066; Percent complete: 26.7%; Average loss: 3.4202 Iteration: 1067; Percent complete: 26.7%; Average loss: 3.6634 Iteration: 1068; Percent complete: 26.7%; Average loss: 3.2873 Iteration: 1069; Percent complete: 26.7%; Average loss: 3.5069 Iteration: 1070; Percent complete: 26.8%; Average loss: 3.2480 Iteration: 1071; Percent complete: 26.8%; Average loss: 3.4556 Iteration: 1072; Percent complete: 26.8%; Average loss: 3.5446 Iteration: 1073; Percent complete: 26.8%; Average loss: 3.5157 Iteration: 1074; Percent complete: 26.9%; Average loss: 3.4550 Iteration: 1075; Percent complete: 26.9%; Average loss: 3.5131 Iteration: 1076; Percent complete: 26.9%; Average loss: 3.4119 Iteration: 1077; Percent complete: 26.9%; Average loss: 3.4230 Iteration: 1078; Percent complete: 27.0%; Average loss: 3.5849 Iteration: 1079; Percent complete: 27.0%; Average loss: 3.1230 Iteration: 1080; Percent complete: 27.0%; Average loss: 3.1496 Iteration: 1081; Percent complete: 27.0%; Average loss: 3.3635 Iteration: 1082; Percent complete: 27.1%; Average loss: 3.7272 Iteration: 1083; Percent complete: 27.1%; Average loss: 3.2388 Iteration: 1084; Percent complete: 27.1%; Average loss: 3.5667 Iteration: 1085; Percent complete: 27.1%; Average loss: 3.3594 Iteration: 1086; Percent complete: 27.2%; Average loss: 3.3956 Iteration: 1087; Percent complete: 27.2%; Average loss: 3.1149 Iteration: 1088; Percent complete: 27.2%; Average loss: 3.3598 Iteration: 1089; Percent complete: 27.2%; Average loss: 3.8926 Iteration: 1090; Percent complete: 27.3%; Average loss: 3.7141 Iteration: 1091; Percent complete: 27.3%; Average loss: 3.6096 Iteration: 1092; Percent complete: 27.3%; Average loss: 3.4429 Iteration: 1093; Percent complete: 27.3%; Average loss: 3.6357 Iteration: 1094; Percent complete: 27.4%; Average loss: 3.3907 Iteration: 1095; Percent complete: 27.4%; Average loss: 3.3197 Iteration: 1096; Percent complete: 27.4%; Average loss: 3.1119 Iteration: 1097; Percent complete: 27.4%; Average loss: 3.0901 Iteration: 1098; Percent complete: 27.5%; Average loss: 3.5611 Iteration: 1099; Percent complete: 27.5%; Average loss: 3.7190 Iteration: 1100; Percent complete: 27.5%; Average loss: 3.6275 Iteration: 1101; Percent complete: 27.5%; Average loss: 3.5717 Iteration: 1102; Percent complete: 27.6%; Average loss: 3.1786 Iteration: 1103; Percent complete: 27.6%; Average loss: 3.7171 Iteration: 1104; Percent complete: 27.6%; Average loss: 3.3975 Iteration: 1105; Percent complete: 27.6%; Average loss: 3.4284 Iteration: 1106; Percent complete: 27.7%; Average loss: 3.2208 Iteration: 1107; Percent complete: 27.7%; Average loss: 3.5802 Iteration: 1108; Percent complete: 27.7%; Average loss: 3.2912 Iteration: 1109; Percent complete: 27.7%; Average loss: 3.5566 Iteration: 1110; Percent complete: 27.8%; Average loss: 3.1842 Iteration: 1111; Percent complete: 27.8%; Average loss: 3.5535 Iteration: 1112; Percent complete: 27.8%; Average loss: 3.2853 Iteration: 1113; Percent complete: 27.8%; Average loss: 3.5890 Iteration: 1114; Percent complete: 27.9%; Average loss: 3.2193 Iteration: 1115; Percent complete: 27.9%; Average loss: 3.2593 Iteration: 1116; Percent complete: 27.9%; Average loss: 3.5051 Iteration: 1117; Percent complete: 27.9%; Average loss: 3.3173 Iteration: 1118; Percent complete: 28.0%; Average loss: 3.3634 Iteration: 1119; Percent complete: 28.0%; Average loss: 3.4383 Iteration: 1120; Percent complete: 28.0%; Average loss: 3.2621 Iteration: 1121; Percent complete: 28.0%; Average loss: 3.2845 Iteration: 1122; Percent complete: 28.1%; Average loss: 3.3336 Iteration: 1123; Percent complete: 28.1%; Average loss: 3.2216 Iteration: 1124; Percent complete: 28.1%; Average loss: 3.2762 Iteration: 1125; Percent complete: 28.1%; Average loss: 3.5245 Iteration: 1126; Percent complete: 28.1%; Average loss: 3.1087 Iteration: 1127; Percent complete: 28.2%; Average loss: 3.6523 Iteration: 1128; Percent complete: 28.2%; Average loss: 3.3426 Iteration: 1129; Percent complete: 28.2%; Average loss: 3.3690 Iteration: 1130; Percent complete: 28.2%; Average loss: 3.4334 Iteration: 1131; Percent complete: 28.3%; Average loss: 3.2746 Iteration: 1132; Percent complete: 28.3%; Average loss: 3.5091 Iteration: 1133; Percent complete: 28.3%; Average loss: 3.5834 Iteration: 1134; Percent complete: 28.3%; Average loss: 3.2084 Iteration: 1135; Percent complete: 28.4%; Average loss: 3.5185 Iteration: 1136; Percent complete: 28.4%; Average loss: 3.4850 Iteration: 1137; Percent complete: 28.4%; Average loss: 3.1962 Iteration: 1138; Percent complete: 28.4%; Average loss: 3.4021 Iteration: 1139; Percent complete: 28.5%; Average loss: 3.6018 Iteration: 1140; Percent complete: 28.5%; Average loss: 3.5646 Iteration: 1141; Percent complete: 28.5%; Average loss: 3.2766 Iteration: 1142; Percent complete: 28.5%; Average loss: 3.3545 Iteration: 1143; Percent complete: 28.6%; Average loss: 3.2054 Iteration: 1144; Percent complete: 28.6%; Average loss: 3.5684 Iteration: 1145; Percent complete: 28.6%; Average loss: 3.3730 Iteration: 1146; Percent complete: 28.6%; Average loss: 3.3580 Iteration: 1147; Percent complete: 28.7%; Average loss: 3.5411 Iteration: 1148; Percent complete: 28.7%; Average loss: 3.5103 Iteration: 1149; Percent complete: 28.7%; Average loss: 3.5753 Iteration: 1150; Percent complete: 28.7%; Average loss: 3.2917 Iteration: 1151; Percent complete: 28.8%; Average loss: 3.3203 Iteration: 1152; Percent complete: 28.8%; Average loss: 3.2972 Iteration: 1153; Percent complete: 28.8%; Average loss: 3.4172 Iteration: 1154; Percent complete: 28.8%; Average loss: 3.3854 Iteration: 1155; Percent complete: 28.9%; Average loss: 3.6790 Iteration: 1156; Percent complete: 28.9%; Average loss: 3.6520 Iteration: 1157; Percent complete: 28.9%; Average loss: 3.5157 Iteration: 1158; Percent complete: 28.9%; Average loss: 3.1978 Iteration: 1159; Percent complete: 29.0%; Average loss: 3.3821 Iteration: 1160; Percent complete: 29.0%; Average loss: 3.7369 Iteration: 1161; Percent complete: 29.0%; Average loss: 3.2006 Iteration: 1162; Percent complete: 29.0%; Average loss: 3.1708 Iteration: 1163; Percent complete: 29.1%; Average loss: 3.4499 Iteration: 1164; Percent complete: 29.1%; Average loss: 3.3072 Iteration: 1165; Percent complete: 29.1%; Average loss: 3.4391 Iteration: 1166; Percent complete: 29.1%; Average loss: 3.4102 Iteration: 1167; Percent complete: 29.2%; Average loss: 3.5171 Iteration: 1168; Percent complete: 29.2%; Average loss: 3.2941 Iteration: 1169; Percent complete: 29.2%; Average loss: 3.4647 Iteration: 1170; Percent complete: 29.2%; Average loss: 3.4245 Iteration: 1171; Percent complete: 29.3%; Average loss: 3.4860 Iteration: 1172; Percent complete: 29.3%; Average loss: 3.2974 Iteration: 1173; Percent complete: 29.3%; Average loss: 3.3110 Iteration: 1174; Percent complete: 29.3%; Average loss: 3.2935 Iteration: 1175; Percent complete: 29.4%; Average loss: 3.6118 Iteration: 1176; Percent complete: 29.4%; Average loss: 3.3962 Iteration: 1177; Percent complete: 29.4%; Average loss: 3.3924 Iteration: 1178; Percent complete: 29.4%; Average loss: 3.3507 Iteration: 1179; Percent complete: 29.5%; Average loss: 3.5898 Iteration: 1180; Percent complete: 29.5%; Average loss: 3.5854 Iteration: 1181; Percent complete: 29.5%; Average loss: 3.2520 Iteration: 1182; Percent complete: 29.5%; Average loss: 3.3434 Iteration: 1183; Percent complete: 29.6%; Average loss: 3.3307 Iteration: 1184; Percent complete: 29.6%; Average loss: 3.2823 Iteration: 1185; Percent complete: 29.6%; Average loss: 3.4077 Iteration: 1186; Percent complete: 29.6%; Average loss: 3.2617 Iteration: 1187; Percent complete: 29.7%; Average loss: 3.3286 Iteration: 1188; Percent complete: 29.7%; Average loss: 3.6794 Iteration: 1189; Percent complete: 29.7%; Average loss: 3.3547 Iteration: 1190; Percent complete: 29.8%; Average loss: 3.4249 Iteration: 1191; Percent complete: 29.8%; Average loss: 3.3854 Iteration: 1192; Percent complete: 29.8%; Average loss: 3.2782 Iteration: 1193; Percent complete: 29.8%; Average loss: 3.2950 Iteration: 1194; Percent complete: 29.8%; Average loss: 3.3090 Iteration: 1195; Percent complete: 29.9%; Average loss: 3.3662 Iteration: 1196; Percent complete: 29.9%; Average loss: 3.5265 Iteration: 1197; Percent complete: 29.9%; Average loss: 3.3203 Iteration: 1198; Percent complete: 29.9%; Average loss: 3.2653 Iteration: 1199; Percent complete: 30.0%; Average loss: 3.3040 Iteration: 1200; Percent complete: 30.0%; Average loss: 3.4843 Iteration: 1201; Percent complete: 30.0%; Average loss: 3.5059 Iteration: 1202; Percent complete: 30.0%; Average loss: 3.2906 Iteration: 1203; Percent complete: 30.1%; Average loss: 3.5463 Iteration: 1204; Percent complete: 30.1%; Average loss: 3.6588 Iteration: 1205; Percent complete: 30.1%; Average loss: 3.2958 Iteration: 1206; Percent complete: 30.1%; Average loss: 3.4777 Iteration: 1207; Percent complete: 30.2%; Average loss: 3.2783 Iteration: 1208; Percent complete: 30.2%; Average loss: 3.4041 Iteration: 1209; Percent complete: 30.2%; Average loss: 3.3784 Iteration: 1210; Percent complete: 30.2%; Average loss: 3.4538 Iteration: 1211; Percent complete: 30.3%; Average loss: 3.2769 Iteration: 1212; Percent complete: 30.3%; Average loss: 3.4682 Iteration: 1213; Percent complete: 30.3%; Average loss: 3.1071 Iteration: 1214; Percent complete: 30.3%; Average loss: 3.4931 Iteration: 1215; Percent complete: 30.4%; Average loss: 3.5234 Iteration: 1216; Percent complete: 30.4%; Average loss: 3.3680 Iteration: 1217; Percent complete: 30.4%; Average loss: 3.3327 Iteration: 1218; Percent complete: 30.4%; Average loss: 3.4088 Iteration: 1219; Percent complete: 30.5%; Average loss: 3.6000 Iteration: 1220; Percent complete: 30.5%; Average loss: 3.5628 Iteration: 1221; Percent complete: 30.5%; Average loss: 3.2329 Iteration: 1222; Percent complete: 30.6%; Average loss: 3.4396 Iteration: 1223; Percent complete: 30.6%; Average loss: 3.3859 Iteration: 1224; Percent complete: 30.6%; Average loss: 3.2597 Iteration: 1225; Percent complete: 30.6%; Average loss: 3.2235 Iteration: 1226; Percent complete: 30.6%; Average loss: 3.5944 Iteration: 1227; Percent complete: 30.7%; Average loss: 3.2327 Iteration: 1228; Percent complete: 30.7%; Average loss: 3.2696 Iteration: 1229; Percent complete: 30.7%; Average loss: 3.1657 Iteration: 1230; Percent complete: 30.8%; Average loss: 3.4409 Iteration: 1231; Percent complete: 30.8%; Average loss: 3.2590 Iteration: 1232; Percent complete: 30.8%; Average loss: 3.5405 Iteration: 1233; Percent complete: 30.8%; Average loss: 3.4164 Iteration: 1234; Percent complete: 30.9%; Average loss: 3.2695 Iteration: 1235; Percent complete: 30.9%; Average loss: 3.2286 Iteration: 1236; Percent complete: 30.9%; Average loss: 3.3534 Iteration: 1237; Percent complete: 30.9%; Average loss: 3.5711 Iteration: 1238; Percent complete: 30.9%; Average loss: 3.4164 Iteration: 1239; Percent complete: 31.0%; Average loss: 3.3308 Iteration: 1240; Percent complete: 31.0%; Average loss: 3.4876 Iteration: 1241; Percent complete: 31.0%; Average loss: 3.5128 Iteration: 1242; Percent complete: 31.1%; Average loss: 3.4155 Iteration: 1243; Percent complete: 31.1%; Average loss: 3.3458 Iteration: 1244; Percent complete: 31.1%; Average loss: 3.5423 Iteration: 1245; Percent complete: 31.1%; Average loss: 3.4959 Iteration: 1246; Percent complete: 31.1%; Average loss: 3.2350 Iteration: 1247; Percent complete: 31.2%; Average loss: 3.1694 Iteration: 1248; Percent complete: 31.2%; Average loss: 3.4860 Iteration: 1249; Percent complete: 31.2%; Average loss: 3.3868 Iteration: 1250; Percent complete: 31.2%; Average loss: 3.3078 Iteration: 1251; Percent complete: 31.3%; Average loss: 3.6386 Iteration: 1252; Percent complete: 31.3%; Average loss: 3.5785 Iteration: 1253; Percent complete: 31.3%; Average loss: 3.7612 Iteration: 1254; Percent complete: 31.4%; Average loss: 3.2868 Iteration: 1255; Percent complete: 31.4%; Average loss: 3.4709 Iteration: 1256; Percent complete: 31.4%; Average loss: 3.3390 Iteration: 1257; Percent complete: 31.4%; Average loss: 3.4660 Iteration: 1258; Percent complete: 31.4%; Average loss: 3.2997 Iteration: 1259; Percent complete: 31.5%; Average loss: 3.2849 Iteration: 1260; Percent complete: 31.5%; Average loss: 3.2408 Iteration: 1261; Percent complete: 31.5%; Average loss: 3.3716 Iteration: 1262; Percent complete: 31.6%; Average loss: 3.4106 Iteration: 1263; Percent complete: 31.6%; Average loss: 3.3841 Iteration: 1264; Percent complete: 31.6%; Average loss: 3.4994 Iteration: 1265; Percent complete: 31.6%; Average loss: 3.5058 Iteration: 1266; Percent complete: 31.6%; Average loss: 3.2695 Iteration: 1267; Percent complete: 31.7%; Average loss: 3.2332 Iteration: 1268; Percent complete: 31.7%; Average loss: 3.5514 Iteration: 1269; Percent complete: 31.7%; Average loss: 3.3619 Iteration: 1270; Percent complete: 31.8%; Average loss: 3.3013 Iteration: 1271; Percent complete: 31.8%; Average loss: 3.5951 Iteration: 1272; Percent complete: 31.8%; Average loss: 3.5497 Iteration: 1273; Percent complete: 31.8%; Average loss: 3.4859 Iteration: 1274; Percent complete: 31.9%; Average loss: 3.2828 Iteration: 1275; Percent complete: 31.9%; Average loss: 3.5645 Iteration: 1276; Percent complete: 31.9%; Average loss: 3.4103 Iteration: 1277; Percent complete: 31.9%; Average loss: 3.3707 Iteration: 1278; Percent complete: 31.9%; Average loss: 3.4423 Iteration: 1279; Percent complete: 32.0%; Average loss: 3.3864 Iteration: 1280; Percent complete: 32.0%; Average loss: 3.4208 Iteration: 1281; Percent complete: 32.0%; Average loss: 3.6646 Iteration: 1282; Percent complete: 32.0%; Average loss: 3.5360 Iteration: 1283; Percent complete: 32.1%; Average loss: 3.3565 Iteration: 1284; Percent complete: 32.1%; Average loss: 3.2624 Iteration: 1285; Percent complete: 32.1%; Average loss: 3.1663 Iteration: 1286; Percent complete: 32.1%; Average loss: 3.4615 Iteration: 1287; Percent complete: 32.2%; Average loss: 3.4985 Iteration: 1288; Percent complete: 32.2%; Average loss: 3.3606 Iteration: 1289; Percent complete: 32.2%; Average loss: 3.2504 Iteration: 1290; Percent complete: 32.2%; Average loss: 3.1189 Iteration: 1291; Percent complete: 32.3%; Average loss: 3.1394 Iteration: 1292; Percent complete: 32.3%; Average loss: 3.5708 Iteration: 1293; Percent complete: 32.3%; Average loss: 3.3905 Iteration: 1294; Percent complete: 32.4%; Average loss: 3.6477 Iteration: 1295; Percent complete: 32.4%; Average loss: 3.3037 Iteration: 1296; Percent complete: 32.4%; Average loss: 3.5348 Iteration: 1297; Percent complete: 32.4%; Average loss: 3.2369 Iteration: 1298; Percent complete: 32.5%; Average loss: 3.2639 Iteration: 1299; Percent complete: 32.5%; Average loss: 3.3452 Iteration: 1300; Percent complete: 32.5%; Average loss: 3.1631 Iteration: 1301; Percent complete: 32.5%; Average loss: 3.3955 Iteration: 1302; Percent complete: 32.6%; Average loss: 3.2996 Iteration: 1303; Percent complete: 32.6%; Average loss: 3.3941 Iteration: 1304; Percent complete: 32.6%; Average loss: 3.4682 Iteration: 1305; Percent complete: 32.6%; Average loss: 3.2827 Iteration: 1306; Percent complete: 32.6%; Average loss: 3.4269 Iteration: 1307; Percent complete: 32.7%; Average loss: 3.5330 Iteration: 1308; Percent complete: 32.7%; Average loss: 3.3512 Iteration: 1309; Percent complete: 32.7%; Average loss: 3.1152 Iteration: 1310; Percent complete: 32.8%; Average loss: 3.3459 Iteration: 1311; Percent complete: 32.8%; Average loss: 3.4356 Iteration: 1312; Percent complete: 32.8%; Average loss: 3.5147 Iteration: 1313; Percent complete: 32.8%; Average loss: 3.3596 Iteration: 1314; Percent complete: 32.9%; Average loss: 3.5494 Iteration: 1315; Percent complete: 32.9%; Average loss: 3.2821 Iteration: 1316; Percent complete: 32.9%; Average loss: 3.3712 Iteration: 1317; Percent complete: 32.9%; Average loss: 3.5842 Iteration: 1318; Percent complete: 33.0%; Average loss: 3.3862 Iteration: 1319; Percent complete: 33.0%; Average loss: 3.5419 Iteration: 1320; Percent complete: 33.0%; Average loss: 3.3121 Iteration: 1321; Percent complete: 33.0%; Average loss: 3.4419 Iteration: 1322; Percent complete: 33.1%; Average loss: 3.3700 Iteration: 1323; Percent complete: 33.1%; Average loss: 3.2580 Iteration: 1324; Percent complete: 33.1%; Average loss: 3.2560 Iteration: 1325; Percent complete: 33.1%; Average loss: 3.1759 Iteration: 1326; Percent complete: 33.1%; Average loss: 3.3677 Iteration: 1327; Percent complete: 33.2%; Average loss: 3.4352 Iteration: 1328; Percent complete: 33.2%; Average loss: 3.1544 Iteration: 1329; Percent complete: 33.2%; Average loss: 3.2133 Iteration: 1330; Percent complete: 33.2%; Average loss: 3.1968 Iteration: 1331; Percent complete: 33.3%; Average loss: 3.1073 Iteration: 1332; Percent complete: 33.3%; Average loss: 3.3342 Iteration: 1333; Percent complete: 33.3%; Average loss: 3.3237 Iteration: 1334; Percent complete: 33.4%; Average loss: 3.5931 Iteration: 1335; Percent complete: 33.4%; Average loss: 3.5538 Iteration: 1336; Percent complete: 33.4%; Average loss: 3.4758 Iteration: 1337; Percent complete: 33.4%; Average loss: 3.2387 Iteration: 1338; Percent complete: 33.5%; Average loss: 3.1353 Iteration: 1339; Percent complete: 33.5%; Average loss: 3.3341 Iteration: 1340; Percent complete: 33.5%; Average loss: 3.4654 Iteration: 1341; Percent complete: 33.5%; Average loss: 3.1349 Iteration: 1342; Percent complete: 33.6%; Average loss: 3.2993 Iteration: 1343; Percent complete: 33.6%; Average loss: 3.1209 Iteration: 1344; Percent complete: 33.6%; Average loss: 3.3624 Iteration: 1345; Percent complete: 33.6%; Average loss: 3.3508 Iteration: 1346; Percent complete: 33.7%; Average loss: 3.0245 Iteration: 1347; Percent complete: 33.7%; Average loss: 3.3446 Iteration: 1348; Percent complete: 33.7%; Average loss: 3.4064 Iteration: 1349; Percent complete: 33.7%; Average loss: 3.3198 Iteration: 1350; Percent complete: 33.8%; Average loss: 3.4660 Iteration: 1351; Percent complete: 33.8%; Average loss: 3.1197 Iteration: 1352; Percent complete: 33.8%; Average loss: 3.4816 Iteration: 1353; Percent complete: 33.8%; Average loss: 3.4092 Iteration: 1354; Percent complete: 33.9%; Average loss: 3.1850 Iteration: 1355; Percent complete: 33.9%; Average loss: 3.3011 Iteration: 1356; Percent complete: 33.9%; Average loss: 3.3445 Iteration: 1357; Percent complete: 33.9%; Average loss: 3.5676 Iteration: 1358; Percent complete: 34.0%; Average loss: 3.3175 Iteration: 1359; Percent complete: 34.0%; Average loss: 3.5758 Iteration: 1360; Percent complete: 34.0%; Average loss: 3.0686 Iteration: 1361; Percent complete: 34.0%; Average loss: 3.1018 Iteration: 1362; Percent complete: 34.1%; Average loss: 3.4027 Iteration: 1363; Percent complete: 34.1%; Average loss: 3.4235 Iteration: 1364; Percent complete: 34.1%; Average loss: 3.2070 Iteration: 1365; Percent complete: 34.1%; Average loss: 3.4107 Iteration: 1366; Percent complete: 34.2%; Average loss: 3.2742 Iteration: 1367; Percent complete: 34.2%; Average loss: 3.3266 Iteration: 1368; Percent complete: 34.2%; Average loss: 3.4072 Iteration: 1369; Percent complete: 34.2%; Average loss: 3.4958 Iteration: 1370; Percent complete: 34.2%; Average loss: 3.2853 Iteration: 1371; Percent complete: 34.3%; Average loss: 3.5436 Iteration: 1372; Percent complete: 34.3%; Average loss: 3.5354 Iteration: 1373; Percent complete: 34.3%; Average loss: 3.2952 Iteration: 1374; Percent complete: 34.4%; Average loss: 3.4385 Iteration: 1375; Percent complete: 34.4%; Average loss: 3.3569 Iteration: 1376; Percent complete: 34.4%; Average loss: 3.0008 Iteration: 1377; Percent complete: 34.4%; Average loss: 3.8009 Iteration: 1378; Percent complete: 34.4%; Average loss: 3.3266 Iteration: 1379; Percent complete: 34.5%; Average loss: 3.1768 Iteration: 1380; Percent complete: 34.5%; Average loss: 3.5706 Iteration: 1381; Percent complete: 34.5%; Average loss: 3.4232 Iteration: 1382; Percent complete: 34.5%; Average loss: 3.5156 Iteration: 1383; Percent complete: 34.6%; Average loss: 3.4124 Iteration: 1384; Percent complete: 34.6%; Average loss: 3.2435 Iteration: 1385; Percent complete: 34.6%; Average loss: 3.3121 Iteration: 1386; Percent complete: 34.6%; Average loss: 3.4889 Iteration: 1387; Percent complete: 34.7%; Average loss: 3.3456 Iteration: 1388; Percent complete: 34.7%; Average loss: 3.4371 Iteration: 1389; Percent complete: 34.7%; Average loss: 3.3818 Iteration: 1390; Percent complete: 34.8%; Average loss: 3.4394 Iteration: 1391; Percent complete: 34.8%; Average loss: 3.1665 Iteration: 1392; Percent complete: 34.8%; Average loss: 3.4033 Iteration: 1393; Percent complete: 34.8%; Average loss: 3.5298 Iteration: 1394; Percent complete: 34.8%; Average loss: 3.2085 Iteration: 1395; Percent complete: 34.9%; Average loss: 3.0194 Iteration: 1396; Percent complete: 34.9%; Average loss: 3.3512 Iteration: 1397; Percent complete: 34.9%; Average loss: 3.3296 Iteration: 1398; Percent complete: 34.9%; Average loss: 3.0238 Iteration: 1399; Percent complete: 35.0%; Average loss: 3.1455 Iteration: 1400; Percent complete: 35.0%; Average loss: 2.9905 Iteration: 1401; Percent complete: 35.0%; Average loss: 3.1701 Iteration: 1402; Percent complete: 35.0%; Average loss: 3.4527 Iteration: 1403; Percent complete: 35.1%; Average loss: 3.2137 Iteration: 1404; Percent complete: 35.1%; Average loss: 3.1841 Iteration: 1405; Percent complete: 35.1%; Average loss: 3.2687 Iteration: 1406; Percent complete: 35.1%; Average loss: 3.4170 Iteration: 1407; Percent complete: 35.2%; Average loss: 3.2435 Iteration: 1408; Percent complete: 35.2%; Average loss: 3.5382 Iteration: 1409; Percent complete: 35.2%; Average loss: 3.2707 Iteration: 1410; Percent complete: 35.2%; Average loss: 3.4647 Iteration: 1411; Percent complete: 35.3%; Average loss: 3.3420 Iteration: 1412; Percent complete: 35.3%; Average loss: 3.1993 Iteration: 1413; Percent complete: 35.3%; Average loss: 3.5202 Iteration: 1414; Percent complete: 35.4%; Average loss: 3.2845 Iteration: 1415; Percent complete: 35.4%; Average loss: 3.2461 Iteration: 1416; Percent complete: 35.4%; Average loss: 3.3519 Iteration: 1417; Percent complete: 35.4%; Average loss: 3.3718 Iteration: 1418; Percent complete: 35.4%; Average loss: 3.4330 Iteration: 1419; Percent complete: 35.5%; Average loss: 3.4659 Iteration: 1420; Percent complete: 35.5%; Average loss: 3.4026 Iteration: 1421; Percent complete: 35.5%; Average loss: 3.2468 Iteration: 1422; Percent complete: 35.5%; Average loss: 3.3979 Iteration: 1423; Percent complete: 35.6%; Average loss: 3.1711 Iteration: 1424; Percent complete: 35.6%; Average loss: 3.4085 Iteration: 1425; Percent complete: 35.6%; Average loss: 3.2346 Iteration: 1426; Percent complete: 35.6%; Average loss: 3.3699 Iteration: 1427; Percent complete: 35.7%; Average loss: 3.3563 Iteration: 1428; Percent complete: 35.7%; Average loss: 3.4010 Iteration: 1429; Percent complete: 35.7%; Average loss: 3.3687 Iteration: 1430; Percent complete: 35.8%; Average loss: 3.1894 Iteration: 1431; Percent complete: 35.8%; Average loss: 3.4224 Iteration: 1432; Percent complete: 35.8%; Average loss: 3.1035 Iteration: 1433; Percent complete: 35.8%; Average loss: 3.2205 Iteration: 1434; Percent complete: 35.9%; Average loss: 3.0578 Iteration: 1435; Percent complete: 35.9%; Average loss: 3.3864 Iteration: 1436; Percent complete: 35.9%; Average loss: 3.4729 Iteration: 1437; Percent complete: 35.9%; Average loss: 3.4461 Iteration: 1438; Percent complete: 35.9%; Average loss: 3.3293 Iteration: 1439; Percent complete: 36.0%; Average loss: 3.2806 Iteration: 1440; Percent complete: 36.0%; Average loss: 3.2169 Iteration: 1441; Percent complete: 36.0%; Average loss: 3.4162 Iteration: 1442; Percent complete: 36.0%; Average loss: 3.2754 Iteration: 1443; Percent complete: 36.1%; Average loss: 3.5577 Iteration: 1444; Percent complete: 36.1%; Average loss: 3.1890 Iteration: 1445; Percent complete: 36.1%; Average loss: 3.2804 Iteration: 1446; Percent complete: 36.1%; Average loss: 3.4320 Iteration: 1447; Percent complete: 36.2%; Average loss: 3.2852 Iteration: 1448; Percent complete: 36.2%; Average loss: 3.5093 Iteration: 1449; Percent complete: 36.2%; Average loss: 3.1936 Iteration: 1450; Percent complete: 36.2%; Average loss: 3.1954 Iteration: 1451; Percent complete: 36.3%; Average loss: 3.3873 Iteration: 1452; Percent complete: 36.3%; Average loss: 3.2636 Iteration: 1453; Percent complete: 36.3%; Average loss: 3.2706 Iteration: 1454; Percent complete: 36.4%; Average loss: 3.1513 Iteration: 1455; Percent complete: 36.4%; Average loss: 3.4187 Iteration: 1456; Percent complete: 36.4%; Average loss: 3.4063 Iteration: 1457; Percent complete: 36.4%; Average loss: 3.2307 Iteration: 1458; Percent complete: 36.4%; Average loss: 3.3445 Iteration: 1459; Percent complete: 36.5%; Average loss: 3.2706 Iteration: 1460; Percent complete: 36.5%; Average loss: 3.0638 Iteration: 1461; Percent complete: 36.5%; Average loss: 2.9783 Iteration: 1462; Percent complete: 36.5%; Average loss: 3.4609 Iteration: 1463; Percent complete: 36.6%; Average loss: 3.1469 Iteration: 1464; Percent complete: 36.6%; Average loss: 3.3021 Iteration: 1465; Percent complete: 36.6%; Average loss: 3.2708 Iteration: 1466; Percent complete: 36.6%; Average loss: 3.1350 Iteration: 1467; Percent complete: 36.7%; Average loss: 3.1991 Iteration: 1468; Percent complete: 36.7%; Average loss: 3.3467 Iteration: 1469; Percent complete: 36.7%; Average loss: 3.2377 Iteration: 1470; Percent complete: 36.8%; Average loss: 3.1275 Iteration: 1471; Percent complete: 36.8%; Average loss: 3.3203 Iteration: 1472; Percent complete: 36.8%; Average loss: 3.6607 Iteration: 1473; Percent complete: 36.8%; Average loss: 3.4064 Iteration: 1474; Percent complete: 36.9%; Average loss: 3.1828 Iteration: 1475; Percent complete: 36.9%; Average loss: 3.3238 Iteration: 1476; Percent complete: 36.9%; Average loss: 3.4740 Iteration: 1477; Percent complete: 36.9%; Average loss: 3.0380 Iteration: 1478; Percent complete: 37.0%; Average loss: 3.6326 Iteration: 1479; Percent complete: 37.0%; Average loss: 3.6307 Iteration: 1480; Percent complete: 37.0%; Average loss: 3.3536 Iteration: 1481; Percent complete: 37.0%; Average loss: 3.3328 Iteration: 1482; Percent complete: 37.0%; Average loss: 3.5123 Iteration: 1483; Percent complete: 37.1%; Average loss: 3.2374 Iteration: 1484; Percent complete: 37.1%; Average loss: 3.2034 Iteration: 1485; Percent complete: 37.1%; Average loss: 3.4141 Iteration: 1486; Percent complete: 37.1%; Average loss: 3.3675 Iteration: 1487; Percent complete: 37.2%; Average loss: 3.1559 Iteration: 1488; Percent complete: 37.2%; Average loss: 3.1924 Iteration: 1489; Percent complete: 37.2%; Average loss: 3.3427 Iteration: 1490; Percent complete: 37.2%; Average loss: 3.5639 Iteration: 1491; Percent complete: 37.3%; Average loss: 3.3479 Iteration: 1492; Percent complete: 37.3%; Average loss: 3.2384 Iteration: 1493; Percent complete: 37.3%; Average loss: 3.1886 Iteration: 1494; Percent complete: 37.4%; Average loss: 3.6953 Iteration: 1495; Percent complete: 37.4%; Average loss: 3.5848 Iteration: 1496; Percent complete: 37.4%; Average loss: 3.2389 Iteration: 1497; Percent complete: 37.4%; Average loss: 3.3284 Iteration: 1498; Percent complete: 37.5%; Average loss: 3.1658 Iteration: 1499; Percent complete: 37.5%; Average loss: 3.3764 Iteration: 1500; Percent complete: 37.5%; Average loss: 3.1852 Iteration: 1501; Percent complete: 37.5%; Average loss: 3.2811 Iteration: 1502; Percent complete: 37.5%; Average loss: 3.3556 Iteration: 1503; Percent complete: 37.6%; Average loss: 3.0454 Iteration: 1504; Percent complete: 37.6%; Average loss: 3.2825 Iteration: 1505; Percent complete: 37.6%; Average loss: 3.1059 Iteration: 1506; Percent complete: 37.6%; Average loss: 3.2047 Iteration: 1507; Percent complete: 37.7%; Average loss: 3.1982 Iteration: 1508; Percent complete: 37.7%; Average loss: 2.9349 Iteration: 1509; Percent complete: 37.7%; Average loss: 3.3957 Iteration: 1510; Percent complete: 37.8%; Average loss: 2.9322 Iteration: 1511; Percent complete: 37.8%; Average loss: 3.1681 Iteration: 1512; Percent complete: 37.8%; Average loss: 3.5575 Iteration: 1513; Percent complete: 37.8%; Average loss: 2.9218 Iteration: 1514; Percent complete: 37.9%; Average loss: 3.2164 Iteration: 1515; Percent complete: 37.9%; Average loss: 3.3670 Iteration: 1516; Percent complete: 37.9%; Average loss: 3.2559 Iteration: 1517; Percent complete: 37.9%; Average loss: 3.2558 Iteration: 1518; Percent complete: 38.0%; Average loss: 3.3094 Iteration: 1519; Percent complete: 38.0%; Average loss: 3.5517 Iteration: 1520; Percent complete: 38.0%; Average loss: 3.2588 Iteration: 1521; Percent complete: 38.0%; Average loss: 3.1360 Iteration: 1522; Percent complete: 38.0%; Average loss: 3.2725 Iteration: 1523; Percent complete: 38.1%; Average loss: 3.3827 Iteration: 1524; Percent complete: 38.1%; Average loss: 3.2261 Iteration: 1525; Percent complete: 38.1%; Average loss: 3.2958 Iteration: 1526; Percent complete: 38.1%; Average loss: 3.1877 Iteration: 1527; Percent complete: 38.2%; Average loss: 3.1610 Iteration: 1528; Percent complete: 38.2%; Average loss: 3.1740 Iteration: 1529; Percent complete: 38.2%; Average loss: 3.4668 Iteration: 1530; Percent complete: 38.2%; Average loss: 3.3595 Iteration: 1531; Percent complete: 38.3%; Average loss: 3.3976 Iteration: 1532; Percent complete: 38.3%; Average loss: 3.3456 Iteration: 1533; Percent complete: 38.3%; Average loss: 3.2906 Iteration: 1534; Percent complete: 38.4%; Average loss: 3.3467 Iteration: 1535; Percent complete: 38.4%; Average loss: 3.2514 Iteration: 1536; Percent complete: 38.4%; Average loss: 3.0888 Iteration: 1537; Percent complete: 38.4%; Average loss: 3.0713 Iteration: 1538; Percent complete: 38.5%; Average loss: 3.2358 Iteration: 1539; Percent complete: 38.5%; Average loss: 3.2046 Iteration: 1540; Percent complete: 38.5%; Average loss: 3.6050 Iteration: 1541; Percent complete: 38.5%; Average loss: 3.4687 Iteration: 1542; Percent complete: 38.6%; Average loss: 3.2880 Iteration: 1543; Percent complete: 38.6%; Average loss: 3.3498 Iteration: 1544; Percent complete: 38.6%; Average loss: 3.2711 Iteration: 1545; Percent complete: 38.6%; Average loss: 3.0882 Iteration: 1546; Percent complete: 38.6%; Average loss: 3.4675 Iteration: 1547; Percent complete: 38.7%; Average loss: 3.3136 Iteration: 1548; Percent complete: 38.7%; Average loss: 3.4352 Iteration: 1549; Percent complete: 38.7%; Average loss: 3.2541 Iteration: 1550; Percent complete: 38.8%; Average loss: 3.2738 Iteration: 1551; Percent complete: 38.8%; Average loss: 3.2710 Iteration: 1552; Percent complete: 38.8%; Average loss: 3.2654 Iteration: 1553; Percent complete: 38.8%; Average loss: 3.4501 Iteration: 1554; Percent complete: 38.9%; Average loss: 3.2156 Iteration: 1555; Percent complete: 38.9%; Average loss: 3.3635 Iteration: 1556; Percent complete: 38.9%; Average loss: 3.4191 Iteration: 1557; Percent complete: 38.9%; Average loss: 3.1605 Iteration: 1558; Percent complete: 39.0%; Average loss: 3.2360 Iteration: 1559; Percent complete: 39.0%; Average loss: 3.4577 Iteration: 1560; Percent complete: 39.0%; Average loss: 3.1584 Iteration: 1561; Percent complete: 39.0%; Average loss: 3.4698 Iteration: 1562; Percent complete: 39.1%; Average loss: 3.4334 Iteration: 1563; Percent complete: 39.1%; Average loss: 3.2543 Iteration: 1564; Percent complete: 39.1%; Average loss: 3.3854 Iteration: 1565; Percent complete: 39.1%; Average loss: 3.2546 Iteration: 1566; Percent complete: 39.1%; Average loss: 3.3022 Iteration: 1567; Percent complete: 39.2%; Average loss: 3.3012 Iteration: 1568; Percent complete: 39.2%; Average loss: 3.5721 Iteration: 1569; Percent complete: 39.2%; Average loss: 3.0039 Iteration: 1570; Percent complete: 39.2%; Average loss: 3.2307 Iteration: 1571; Percent complete: 39.3%; Average loss: 3.2429 Iteration: 1572; Percent complete: 39.3%; Average loss: 3.2137 Iteration: 1573; Percent complete: 39.3%; Average loss: 3.4119 Iteration: 1574; Percent complete: 39.4%; Average loss: 3.3878 Iteration: 1575; Percent complete: 39.4%; Average loss: 3.1604 Iteration: 1576; Percent complete: 39.4%; Average loss: 3.1626 Iteration: 1577; Percent complete: 39.4%; Average loss: 3.3007 Iteration: 1578; Percent complete: 39.5%; Average loss: 3.2592 Iteration: 1579; Percent complete: 39.5%; Average loss: 3.2758 Iteration: 1580; Percent complete: 39.5%; Average loss: 3.6267 Iteration: 1581; Percent complete: 39.5%; Average loss: 3.4473 Iteration: 1582; Percent complete: 39.6%; Average loss: 3.4671 Iteration: 1583; Percent complete: 39.6%; Average loss: 3.4252 Iteration: 1584; Percent complete: 39.6%; Average loss: 3.3068 Iteration: 1585; Percent complete: 39.6%; Average loss: 3.5446 Iteration: 1586; Percent complete: 39.6%; Average loss: 3.0792 Iteration: 1587; Percent complete: 39.7%; Average loss: 3.2319 Iteration: 1588; Percent complete: 39.7%; Average loss: 3.1976 Iteration: 1589; Percent complete: 39.7%; Average loss: 3.4802 Iteration: 1590; Percent complete: 39.8%; Average loss: 3.2631 Iteration: 1591; Percent complete: 39.8%; Average loss: 3.0641 Iteration: 1592; Percent complete: 39.8%; Average loss: 3.4544 Iteration: 1593; Percent complete: 39.8%; Average loss: 3.3098 Iteration: 1594; Percent complete: 39.9%; Average loss: 3.2107 Iteration: 1595; Percent complete: 39.9%; Average loss: 3.4340 Iteration: 1596; Percent complete: 39.9%; Average loss: 3.1531 Iteration: 1597; Percent complete: 39.9%; Average loss: 3.3466 Iteration: 1598; Percent complete: 40.0%; Average loss: 3.1351 Iteration: 1599; Percent complete: 40.0%; Average loss: 3.4859 Iteration: 1600; Percent complete: 40.0%; Average loss: 3.3846 Iteration: 1601; Percent complete: 40.0%; Average loss: 3.2511 Iteration: 1602; Percent complete: 40.1%; Average loss: 3.2000 Iteration: 1603; Percent complete: 40.1%; Average loss: 3.3334 Iteration: 1604; Percent complete: 40.1%; Average loss: 3.1224 Iteration: 1605; Percent complete: 40.1%; Average loss: 3.2327 Iteration: 1606; Percent complete: 40.2%; Average loss: 3.3431 Iteration: 1607; Percent complete: 40.2%; Average loss: 3.2837 Iteration: 1608; Percent complete: 40.2%; Average loss: 3.3838 Iteration: 1609; Percent complete: 40.2%; Average loss: 3.3573 Iteration: 1610; Percent complete: 40.2%; Average loss: 3.1297 Iteration: 1611; Percent complete: 40.3%; Average loss: 3.4124 Iteration: 1612; Percent complete: 40.3%; Average loss: 3.1478 Iteration: 1613; Percent complete: 40.3%; Average loss: 3.0738 Iteration: 1614; Percent complete: 40.4%; Average loss: 3.2790 Iteration: 1615; Percent complete: 40.4%; Average loss: 3.2837 Iteration: 1616; Percent complete: 40.4%; Average loss: 3.1461 Iteration: 1617; Percent complete: 40.4%; Average loss: 3.3423 Iteration: 1618; Percent complete: 40.5%; Average loss: 3.3120 Iteration: 1619; Percent complete: 40.5%; Average loss: 3.3059 Iteration: 1620; Percent complete: 40.5%; Average loss: 3.5898 Iteration: 1621; Percent complete: 40.5%; Average loss: 3.0933 Iteration: 1622; Percent complete: 40.6%; Average loss: 2.9213 Iteration: 1623; Percent complete: 40.6%; Average loss: 3.4153 Iteration: 1624; Percent complete: 40.6%; Average loss: 3.3667 Iteration: 1625; Percent complete: 40.6%; Average loss: 3.3382 Iteration: 1626; Percent complete: 40.6%; Average loss: 3.3901 Iteration: 1627; Percent complete: 40.7%; Average loss: 3.3473 Iteration: 1628; Percent complete: 40.7%; Average loss: 3.3875 Iteration: 1629; Percent complete: 40.7%; Average loss: 3.1542 Iteration: 1630; Percent complete: 40.8%; Average loss: 3.3152 Iteration: 1631; Percent complete: 40.8%; Average loss: 3.1604 Iteration: 1632; Percent complete: 40.8%; Average loss: 3.0765 Iteration: 1633; Percent complete: 40.8%; Average loss: 3.2234 Iteration: 1634; Percent complete: 40.8%; Average loss: 3.1725 Iteration: 1635; Percent complete: 40.9%; Average loss: 3.2468 Iteration: 1636; Percent complete: 40.9%; Average loss: 3.1424 Iteration: 1637; Percent complete: 40.9%; Average loss: 3.0094 Iteration: 1638; Percent complete: 40.9%; Average loss: 3.1595 Iteration: 1639; Percent complete: 41.0%; Average loss: 3.4891 Iteration: 1640; Percent complete: 41.0%; Average loss: 3.1030 Iteration: 1641; Percent complete: 41.0%; Average loss: 3.1865 Iteration: 1642; Percent complete: 41.0%; Average loss: 3.2970 Iteration: 1643; Percent complete: 41.1%; Average loss: 3.2997 Iteration: 1644; Percent complete: 41.1%; Average loss: 3.4412 Iteration: 1645; Percent complete: 41.1%; Average loss: 3.1460 Iteration: 1646; Percent complete: 41.1%; Average loss: 3.3175 Iteration: 1647; Percent complete: 41.2%; Average loss: 3.2048 Iteration: 1648; Percent complete: 41.2%; Average loss: 3.3547 Iteration: 1649; Percent complete: 41.2%; Average loss: 3.2310 Iteration: 1650; Percent complete: 41.2%; Average loss: 3.0978 Iteration: 1651; Percent complete: 41.3%; Average loss: 3.3480 Iteration: 1652; Percent complete: 41.3%; Average loss: 3.2720 Iteration: 1653; Percent complete: 41.3%; Average loss: 3.0122 Iteration: 1654; Percent complete: 41.3%; Average loss: 3.1171 Iteration: 1655; Percent complete: 41.4%; Average loss: 3.1559 Iteration: 1656; Percent complete: 41.4%; Average loss: 3.3425 Iteration: 1657; Percent complete: 41.4%; Average loss: 3.1542 Iteration: 1658; Percent complete: 41.4%; Average loss: 3.2723 Iteration: 1659; Percent complete: 41.5%; Average loss: 3.0934 Iteration: 1660; Percent complete: 41.5%; Average loss: 3.2247 Iteration: 1661; Percent complete: 41.5%; Average loss: 2.9709 Iteration: 1662; Percent complete: 41.5%; Average loss: 3.2690 Iteration: 1663; Percent complete: 41.6%; Average loss: 3.1796 Iteration: 1664; Percent complete: 41.6%; Average loss: 3.3143 Iteration: 1665; Percent complete: 41.6%; Average loss: 3.2776 Iteration: 1666; Percent complete: 41.6%; Average loss: 3.2233 Iteration: 1667; Percent complete: 41.7%; Average loss: 3.2503 Iteration: 1668; Percent complete: 41.7%; Average loss: 3.1394 Iteration: 1669; Percent complete: 41.7%; Average loss: 3.2403 Iteration: 1670; Percent complete: 41.8%; Average loss: 3.3145 Iteration: 1671; Percent complete: 41.8%; Average loss: 3.3339 Iteration: 1672; Percent complete: 41.8%; Average loss: 3.1796 Iteration: 1673; Percent complete: 41.8%; Average loss: 3.1970 Iteration: 1674; Percent complete: 41.9%; Average loss: 3.0952 Iteration: 1675; Percent complete: 41.9%; Average loss: 3.4530 Iteration: 1676; Percent complete: 41.9%; Average loss: 3.1250 Iteration: 1677; Percent complete: 41.9%; Average loss: 3.3322 Iteration: 1678; Percent complete: 41.9%; Average loss: 3.3537 Iteration: 1679; Percent complete: 42.0%; Average loss: 3.3503 Iteration: 1680; Percent complete: 42.0%; Average loss: 3.3016 Iteration: 1681; Percent complete: 42.0%; Average loss: 3.0670 Iteration: 1682; Percent complete: 42.0%; Average loss: 3.2754 Iteration: 1683; Percent complete: 42.1%; Average loss: 3.4398 Iteration: 1684; Percent complete: 42.1%; Average loss: 3.1970 Iteration: 1685; Percent complete: 42.1%; Average loss: 3.2584 Iteration: 1686; Percent complete: 42.1%; Average loss: 3.2808 Iteration: 1687; Percent complete: 42.2%; Average loss: 3.3765 Iteration: 1688; Percent complete: 42.2%; Average loss: 3.2109 Iteration: 1689; Percent complete: 42.2%; Average loss: 3.1530 Iteration: 1690; Percent complete: 42.2%; Average loss: 3.3300 Iteration: 1691; Percent complete: 42.3%; Average loss: 3.3281 Iteration: 1692; Percent complete: 42.3%; Average loss: 3.2467 Iteration: 1693; Percent complete: 42.3%; Average loss: 3.3082 Iteration: 1694; Percent complete: 42.4%; Average loss: 3.3633 Iteration: 1695; Percent complete: 42.4%; Average loss: 3.2194 Iteration: 1696; Percent complete: 42.4%; Average loss: 3.2342 Iteration: 1697; Percent complete: 42.4%; Average loss: 3.1391 Iteration: 1698; Percent complete: 42.4%; Average loss: 3.3145 Iteration: 1699; Percent complete: 42.5%; Average loss: 3.2582 Iteration: 1700; Percent complete: 42.5%; Average loss: 3.1322 Iteration: 1701; Percent complete: 42.5%; Average loss: 2.9353 Iteration: 1702; Percent complete: 42.5%; Average loss: 2.9088 Iteration: 1703; Percent complete: 42.6%; Average loss: 3.4067 Iteration: 1704; Percent complete: 42.6%; Average loss: 3.0896 Iteration: 1705; Percent complete: 42.6%; Average loss: 3.1764 Iteration: 1706; Percent complete: 42.6%; Average loss: 3.2341 Iteration: 1707; Percent complete: 42.7%; Average loss: 3.4767 Iteration: 1708; Percent complete: 42.7%; Average loss: 3.1424 Iteration: 1709; Percent complete: 42.7%; Average loss: 3.1852 Iteration: 1710; Percent complete: 42.8%; Average loss: 3.2949 Iteration: 1711; Percent complete: 42.8%; Average loss: 3.6801 Iteration: 1712; Percent complete: 42.8%; Average loss: 3.4333 Iteration: 1713; Percent complete: 42.8%; Average loss: 3.2099 Iteration: 1714; Percent complete: 42.9%; Average loss: 3.2764 Iteration: 1715; Percent complete: 42.9%; Average loss: 3.3786 Iteration: 1716; Percent complete: 42.9%; Average loss: 3.3092 Iteration: 1717; Percent complete: 42.9%; Average loss: 3.1970 Iteration: 1718; Percent complete: 43.0%; Average loss: 3.2539 Iteration: 1719; Percent complete: 43.0%; Average loss: 3.2503 Iteration: 1720; Percent complete: 43.0%; Average loss: 3.4296 Iteration: 1721; Percent complete: 43.0%; Average loss: 3.3935 Iteration: 1722; Percent complete: 43.0%; Average loss: 3.3546 Iteration: 1723; Percent complete: 43.1%; Average loss: 3.1472 Iteration: 1724; Percent complete: 43.1%; Average loss: 3.3800 Iteration: 1725; Percent complete: 43.1%; Average loss: 3.2387 Iteration: 1726; Percent complete: 43.1%; Average loss: 3.2898 Iteration: 1727; Percent complete: 43.2%; Average loss: 3.3427 Iteration: 1728; Percent complete: 43.2%; Average loss: 3.4329 Iteration: 1729; Percent complete: 43.2%; Average loss: 3.0428 Iteration: 1730; Percent complete: 43.2%; Average loss: 3.3042 Iteration: 1731; Percent complete: 43.3%; Average loss: 3.2811 Iteration: 1732; Percent complete: 43.3%; Average loss: 3.2720 Iteration: 1733; Percent complete: 43.3%; Average loss: 2.9724 Iteration: 1734; Percent complete: 43.4%; Average loss: 3.0596 Iteration: 1735; Percent complete: 43.4%; Average loss: 3.2289 Iteration: 1736; Percent complete: 43.4%; Average loss: 3.2371 Iteration: 1737; Percent complete: 43.4%; Average loss: 3.1310 Iteration: 1738; Percent complete: 43.5%; Average loss: 3.3377 Iteration: 1739; Percent complete: 43.5%; Average loss: 3.0105 Iteration: 1740; Percent complete: 43.5%; Average loss: 3.2333 Iteration: 1741; Percent complete: 43.5%; Average loss: 3.0533 Iteration: 1742; Percent complete: 43.5%; Average loss: 3.3368 Iteration: 1743; Percent complete: 43.6%; Average loss: 3.2564 Iteration: 1744; Percent complete: 43.6%; Average loss: 3.1726 Iteration: 1745; Percent complete: 43.6%; Average loss: 3.3298 Iteration: 1746; Percent complete: 43.6%; Average loss: 3.3548 Iteration: 1747; Percent complete: 43.7%; Average loss: 3.2832 Iteration: 1748; Percent complete: 43.7%; Average loss: 3.2664 Iteration: 1749; Percent complete: 43.7%; Average loss: 3.3260 Iteration: 1750; Percent complete: 43.8%; Average loss: 3.2545 Iteration: 1751; Percent complete: 43.8%; Average loss: 3.3269 Iteration: 1752; Percent complete: 43.8%; Average loss: 3.1759 Iteration: 1753; Percent complete: 43.8%; Average loss: 3.2739 Iteration: 1754; Percent complete: 43.9%; Average loss: 3.3581 Iteration: 1755; Percent complete: 43.9%; Average loss: 3.1249 Iteration: 1756; Percent complete: 43.9%; Average loss: 3.0306 Iteration: 1757; Percent complete: 43.9%; Average loss: 3.2474 Iteration: 1758; Percent complete: 44.0%; Average loss: 3.2258 Iteration: 1759; Percent complete: 44.0%; Average loss: 3.2227 Iteration: 1760; Percent complete: 44.0%; Average loss: 3.0180 Iteration: 1761; Percent complete: 44.0%; Average loss: 3.1512 Iteration: 1762; Percent complete: 44.0%; Average loss: 3.3239 Iteration: 1763; Percent complete: 44.1%; Average loss: 3.1343 Iteration: 1764; Percent complete: 44.1%; Average loss: 3.2860 Iteration: 1765; Percent complete: 44.1%; Average loss: 3.0836 Iteration: 1766; Percent complete: 44.1%; Average loss: 3.1725 Iteration: 1767; Percent complete: 44.2%; Average loss: 3.1051 Iteration: 1768; Percent complete: 44.2%; Average loss: 3.2529 Iteration: 1769; Percent complete: 44.2%; Average loss: 3.1671 Iteration: 1770; Percent complete: 44.2%; Average loss: 3.2439 Iteration: 1771; Percent complete: 44.3%; Average loss: 3.2535 Iteration: 1772; Percent complete: 44.3%; Average loss: 3.3010 Iteration: 1773; Percent complete: 44.3%; Average loss: 3.2438 Iteration: 1774; Percent complete: 44.4%; Average loss: 3.3068 Iteration: 1775; Percent complete: 44.4%; Average loss: 3.1590 Iteration: 1776; Percent complete: 44.4%; Average loss: 3.2097 Iteration: 1777; Percent complete: 44.4%; Average loss: 2.9020 Iteration: 1778; Percent complete: 44.5%; Average loss: 3.2168 Iteration: 1779; Percent complete: 44.5%; Average loss: 3.1102 Iteration: 1780; Percent complete: 44.5%; Average loss: 3.4255 Iteration: 1781; Percent complete: 44.5%; Average loss: 3.3038 Iteration: 1782; Percent complete: 44.5%; Average loss: 3.1064 Iteration: 1783; Percent complete: 44.6%; Average loss: 3.1988 Iteration: 1784; Percent complete: 44.6%; Average loss: 3.3512 Iteration: 1785; Percent complete: 44.6%; Average loss: 3.0708 Iteration: 1786; Percent complete: 44.6%; Average loss: 3.2355 Iteration: 1787; Percent complete: 44.7%; Average loss: 3.0953 Iteration: 1788; Percent complete: 44.7%; Average loss: 2.9998 Iteration: 1789; Percent complete: 44.7%; Average loss: 3.4002 Iteration: 1790; Percent complete: 44.8%; Average loss: 3.0080 Iteration: 1791; Percent complete: 44.8%; Average loss: 3.2352 Iteration: 1792; Percent complete: 44.8%; Average loss: 3.2270 Iteration: 1793; Percent complete: 44.8%; Average loss: 3.2408 Iteration: 1794; Percent complete: 44.9%; Average loss: 3.3257 Iteration: 1795; Percent complete: 44.9%; Average loss: 3.1964 Iteration: 1796; Percent complete: 44.9%; Average loss: 3.2535 Iteration: 1797; Percent complete: 44.9%; Average loss: 3.1854 Iteration: 1798; Percent complete: 45.0%; Average loss: 3.1728 Iteration: 1799; Percent complete: 45.0%; Average loss: 3.1533 Iteration: 1800; Percent complete: 45.0%; Average loss: 3.4538 Iteration: 1801; Percent complete: 45.0%; Average loss: 3.1524 Iteration: 1802; Percent complete: 45.1%; Average loss: 3.1420 Iteration: 1803; Percent complete: 45.1%; Average loss: 3.2081 Iteration: 1804; Percent complete: 45.1%; Average loss: 3.3607 Iteration: 1805; Percent complete: 45.1%; Average loss: 3.3200 Iteration: 1806; Percent complete: 45.1%; Average loss: 3.2826 Iteration: 1807; Percent complete: 45.2%; Average loss: 3.4802 Iteration: 1808; Percent complete: 45.2%; Average loss: 3.4306 Iteration: 1809; Percent complete: 45.2%; Average loss: 3.2518 Iteration: 1810; Percent complete: 45.2%; Average loss: 3.3997 Iteration: 1811; Percent complete: 45.3%; Average loss: 3.3589 Iteration: 1812; Percent complete: 45.3%; Average loss: 3.3485 Iteration: 1813; Percent complete: 45.3%; Average loss: 3.0921 Iteration: 1814; Percent complete: 45.4%; Average loss: 3.3622 Iteration: 1815; Percent complete: 45.4%; Average loss: 3.3516 Iteration: 1816; Percent complete: 45.4%; Average loss: 2.9022 Iteration: 1817; Percent complete: 45.4%; Average loss: 3.2554 Iteration: 1818; Percent complete: 45.5%; Average loss: 3.3143 Iteration: 1819; Percent complete: 45.5%; Average loss: 3.1725 Iteration: 1820; Percent complete: 45.5%; Average loss: 2.9438 Iteration: 1821; Percent complete: 45.5%; Average loss: 3.0712 Iteration: 1822; Percent complete: 45.6%; Average loss: 3.0540 Iteration: 1823; Percent complete: 45.6%; Average loss: 3.5431 Iteration: 1824; Percent complete: 45.6%; Average loss: 3.1571 Iteration: 1825; Percent complete: 45.6%; Average loss: 3.4339 Iteration: 1826; Percent complete: 45.6%; Average loss: 2.8519 Iteration: 1827; Percent complete: 45.7%; Average loss: 3.0372 Iteration: 1828; Percent complete: 45.7%; Average loss: 3.1696 Iteration: 1829; Percent complete: 45.7%; Average loss: 3.4680 Iteration: 1830; Percent complete: 45.8%; Average loss: 3.2369 Iteration: 1831; Percent complete: 45.8%; Average loss: 2.9861 Iteration: 1832; Percent complete: 45.8%; Average loss: 3.1067 Iteration: 1833; Percent complete: 45.8%; Average loss: 3.0964 Iteration: 1834; Percent complete: 45.9%; Average loss: 3.1945 Iteration: 1835; Percent complete: 45.9%; Average loss: 3.5193 Iteration: 1836; Percent complete: 45.9%; Average loss: 3.0755 Iteration: 1837; Percent complete: 45.9%; Average loss: 3.1492 Iteration: 1838; Percent complete: 46.0%; Average loss: 3.0795 Iteration: 1839; Percent complete: 46.0%; Average loss: 3.2730 Iteration: 1840; Percent complete: 46.0%; Average loss: 3.1958 Iteration: 1841; Percent complete: 46.0%; Average loss: 3.2035 Iteration: 1842; Percent complete: 46.1%; Average loss: 3.3586 Iteration: 1843; Percent complete: 46.1%; Average loss: 3.0348 Iteration: 1844; Percent complete: 46.1%; Average loss: 3.2771 Iteration: 1845; Percent complete: 46.1%; Average loss: 3.0922 Iteration: 1846; Percent complete: 46.2%; Average loss: 2.9237 Iteration: 1847; Percent complete: 46.2%; Average loss: 3.4554 Iteration: 1848; Percent complete: 46.2%; Average loss: 2.8235 Iteration: 1849; Percent complete: 46.2%; Average loss: 3.3057 Iteration: 1850; Percent complete: 46.2%; Average loss: 3.2573 Iteration: 1851; Percent complete: 46.3%; Average loss: 3.1695 Iteration: 1852; Percent complete: 46.3%; Average loss: 2.8142 Iteration: 1853; Percent complete: 46.3%; Average loss: 3.2474 Iteration: 1854; Percent complete: 46.4%; Average loss: 3.1892 Iteration: 1855; Percent complete: 46.4%; Average loss: 3.1886 Iteration: 1856; Percent complete: 46.4%; Average loss: 3.1296 Iteration: 1857; Percent complete: 46.4%; Average loss: 3.0317 Iteration: 1858; Percent complete: 46.5%; Average loss: 3.4448 Iteration: 1859; Percent complete: 46.5%; Average loss: 3.3902 Iteration: 1860; Percent complete: 46.5%; Average loss: 3.3883 Iteration: 1861; Percent complete: 46.5%; Average loss: 3.4157 Iteration: 1862; Percent complete: 46.6%; Average loss: 3.4026 Iteration: 1863; Percent complete: 46.6%; Average loss: 3.2992 Iteration: 1864; Percent complete: 46.6%; Average loss: 3.6761 Iteration: 1865; Percent complete: 46.6%; Average loss: 3.0394 Iteration: 1866; Percent complete: 46.7%; Average loss: 3.1771 Iteration: 1867; Percent complete: 46.7%; Average loss: 3.0232 Iteration: 1868; Percent complete: 46.7%; Average loss: 3.3837 Iteration: 1869; Percent complete: 46.7%; Average loss: 2.9927 Iteration: 1870; Percent complete: 46.8%; Average loss: 3.0303 Iteration: 1871; Percent complete: 46.8%; Average loss: 3.1939 Iteration: 1872; Percent complete: 46.8%; Average loss: 3.2114 Iteration: 1873; Percent complete: 46.8%; Average loss: 3.2382 Iteration: 1874; Percent complete: 46.9%; Average loss: 3.0336 Iteration: 1875; Percent complete: 46.9%; Average loss: 3.2703 Iteration: 1876; Percent complete: 46.9%; Average loss: 3.0808 Iteration: 1877; Percent complete: 46.9%; Average loss: 3.0507 Iteration: 1878; Percent complete: 46.9%; Average loss: 3.0155 Iteration: 1879; Percent complete: 47.0%; Average loss: 2.9668 Iteration: 1880; Percent complete: 47.0%; Average loss: 3.0084 Iteration: 1881; Percent complete: 47.0%; Average loss: 3.3885 Iteration: 1882; Percent complete: 47.0%; Average loss: 3.2488 Iteration: 1883; Percent complete: 47.1%; Average loss: 3.2768 Iteration: 1884; Percent complete: 47.1%; Average loss: 3.0737 Iteration: 1885; Percent complete: 47.1%; Average loss: 3.1842 Iteration: 1886; Percent complete: 47.1%; Average loss: 3.3481 Iteration: 1887; Percent complete: 47.2%; Average loss: 3.1089 Iteration: 1888; Percent complete: 47.2%; Average loss: 3.4178 Iteration: 1889; Percent complete: 47.2%; Average loss: 3.0599 Iteration: 1890; Percent complete: 47.2%; Average loss: 3.3511 Iteration: 1891; Percent complete: 47.3%; Average loss: 3.1818 Iteration: 1892; Percent complete: 47.3%; Average loss: 3.0544 Iteration: 1893; Percent complete: 47.3%; Average loss: 3.0559 Iteration: 1894; Percent complete: 47.3%; Average loss: 3.4408 Iteration: 1895; Percent complete: 47.4%; Average loss: 3.3146 Iteration: 1896; Percent complete: 47.4%; Average loss: 3.3139 Iteration: 1897; Percent complete: 47.4%; Average loss: 3.0016 Iteration: 1898; Percent complete: 47.4%; Average loss: 3.0670 Iteration: 1899; Percent complete: 47.5%; Average loss: 3.0942 Iteration: 1900; Percent complete: 47.5%; Average loss: 3.2458 Iteration: 1901; Percent complete: 47.5%; Average loss: 3.1989 Iteration: 1902; Percent complete: 47.5%; Average loss: 3.1257 Iteration: 1903; Percent complete: 47.6%; Average loss: 3.5761 Iteration: 1904; Percent complete: 47.6%; Average loss: 3.2674 Iteration: 1905; Percent complete: 47.6%; Average loss: 3.1304 Iteration: 1906; Percent complete: 47.6%; Average loss: 3.0014 Iteration: 1907; Percent complete: 47.7%; Average loss: 3.2440 Iteration: 1908; Percent complete: 47.7%; Average loss: 3.1727 Iteration: 1909; Percent complete: 47.7%; Average loss: 3.0841 Iteration: 1910; Percent complete: 47.8%; Average loss: 3.2016 Iteration: 1911; Percent complete: 47.8%; Average loss: 3.2067 Iteration: 1912; Percent complete: 47.8%; Average loss: 3.0726 Iteration: 1913; Percent complete: 47.8%; Average loss: 3.2902 Iteration: 1914; Percent complete: 47.9%; Average loss: 3.1222 Iteration: 1915; Percent complete: 47.9%; Average loss: 3.5211 Iteration: 1916; Percent complete: 47.9%; Average loss: 3.2679 Iteration: 1917; Percent complete: 47.9%; Average loss: 2.8842 Iteration: 1918; Percent complete: 47.9%; Average loss: 3.1873 Iteration: 1919; Percent complete: 48.0%; Average loss: 3.3119 Iteration: 1920; Percent complete: 48.0%; Average loss: 3.3168 Iteration: 1921; Percent complete: 48.0%; Average loss: 2.9828 Iteration: 1922; Percent complete: 48.0%; Average loss: 3.2156 Iteration: 1923; Percent complete: 48.1%; Average loss: 3.1331 Iteration: 1924; Percent complete: 48.1%; Average loss: 3.0838 Iteration: 1925; Percent complete: 48.1%; Average loss: 3.2389 Iteration: 1926; Percent complete: 48.1%; Average loss: 3.3909 Iteration: 1927; Percent complete: 48.2%; Average loss: 3.4071 Iteration: 1928; Percent complete: 48.2%; Average loss: 3.0535 Iteration: 1929; Percent complete: 48.2%; Average loss: 3.1975 Iteration: 1930; Percent complete: 48.2%; Average loss: 3.2160 Iteration: 1931; Percent complete: 48.3%; Average loss: 3.1258 Iteration: 1932; Percent complete: 48.3%; Average loss: 3.2427 Iteration: 1933; Percent complete: 48.3%; Average loss: 3.0206 Iteration: 1934; Percent complete: 48.4%; Average loss: 3.0848 Iteration: 1935; Percent complete: 48.4%; Average loss: 3.2541 Iteration: 1936; Percent complete: 48.4%; Average loss: 3.0758 Iteration: 1937; Percent complete: 48.4%; Average loss: 3.3375 Iteration: 1938; Percent complete: 48.4%; Average loss: 3.1487 Iteration: 1939; Percent complete: 48.5%; Average loss: 3.1254 Iteration: 1940; Percent complete: 48.5%; Average loss: 3.1642 Iteration: 1941; Percent complete: 48.5%; Average loss: 3.2941 Iteration: 1942; Percent complete: 48.5%; Average loss: 3.2600 Iteration: 1943; Percent complete: 48.6%; Average loss: 3.2646 Iteration: 1944; Percent complete: 48.6%; Average loss: 3.0745 Iteration: 1945; Percent complete: 48.6%; Average loss: 3.0826 Iteration: 1946; Percent complete: 48.6%; Average loss: 3.2309 Iteration: 1947; Percent complete: 48.7%; Average loss: 3.0008 Iteration: 1948; Percent complete: 48.7%; Average loss: 3.1870 Iteration: 1949; Percent complete: 48.7%; Average loss: 3.0244 Iteration: 1950; Percent complete: 48.8%; Average loss: 3.0549 Iteration: 1951; Percent complete: 48.8%; Average loss: 3.2872 Iteration: 1952; Percent complete: 48.8%; Average loss: 3.3491 Iteration: 1953; Percent complete: 48.8%; Average loss: 3.1662 Iteration: 1954; Percent complete: 48.9%; Average loss: 3.3495 Iteration: 1955; Percent complete: 48.9%; Average loss: 3.1466 Iteration: 1956; Percent complete: 48.9%; Average loss: 3.2604 Iteration: 1957; Percent complete: 48.9%; Average loss: 3.0930 Iteration: 1958; Percent complete: 48.9%; Average loss: 3.0189 Iteration: 1959; Percent complete: 49.0%; Average loss: 3.4434 Iteration: 1960; Percent complete: 49.0%; Average loss: 3.2460 Iteration: 1961; Percent complete: 49.0%; Average loss: 3.2129 Iteration: 1962; Percent complete: 49.0%; Average loss: 3.0597 Iteration: 1963; Percent complete: 49.1%; Average loss: 3.2764 Iteration: 1964; Percent complete: 49.1%; Average loss: 3.3456 Iteration: 1965; Percent complete: 49.1%; Average loss: 3.1536 Iteration: 1966; Percent complete: 49.1%; Average loss: 3.0357 Iteration: 1967; Percent complete: 49.2%; Average loss: 3.2141 Iteration: 1968; Percent complete: 49.2%; Average loss: 3.4683 Iteration: 1969; Percent complete: 49.2%; Average loss: 3.1661 Iteration: 1970; Percent complete: 49.2%; Average loss: 3.4083 Iteration: 1971; Percent complete: 49.3%; Average loss: 3.3786 Iteration: 1972; Percent complete: 49.3%; Average loss: 2.9513 Iteration: 1973; Percent complete: 49.3%; Average loss: 3.2350 Iteration: 1974; Percent complete: 49.4%; Average loss: 3.2921 Iteration: 1975; Percent complete: 49.4%; Average loss: 3.4953 Iteration: 1976; Percent complete: 49.4%; Average loss: 3.3263 Iteration: 1977; Percent complete: 49.4%; Average loss: 3.2543 Iteration: 1978; Percent complete: 49.5%; Average loss: 3.1496 Iteration: 1979; Percent complete: 49.5%; Average loss: 3.1834 Iteration: 1980; Percent complete: 49.5%; Average loss: 3.1350 Iteration: 1981; Percent complete: 49.5%; Average loss: 3.2269 Iteration: 1982; Percent complete: 49.5%; Average loss: 2.9852 Iteration: 1983; Percent complete: 49.6%; Average loss: 3.1887 Iteration: 1984; Percent complete: 49.6%; Average loss: 3.0983 Iteration: 1985; Percent complete: 49.6%; Average loss: 3.0130 Iteration: 1986; Percent complete: 49.6%; Average loss: 3.1037 Iteration: 1987; Percent complete: 49.7%; Average loss: 3.1281 Iteration: 1988; Percent complete: 49.7%; Average loss: 3.2399 Iteration: 1989; Percent complete: 49.7%; Average loss: 3.1808 Iteration: 1990; Percent complete: 49.8%; Average loss: 3.1546 Iteration: 1991; Percent complete: 49.8%; Average loss: 3.0688 Iteration: 1992; Percent complete: 49.8%; Average loss: 3.0411 Iteration: 1993; Percent complete: 49.8%; Average loss: 3.2910 Iteration: 1994; Percent complete: 49.9%; Average loss: 3.0836 Iteration: 1995; Percent complete: 49.9%; Average loss: 3.3743 Iteration: 1996; Percent complete: 49.9%; Average loss: 3.2843 Iteration: 1997; Percent complete: 49.9%; Average loss: 3.1963 Iteration: 1998; Percent complete: 50.0%; Average loss: 3.2841 Iteration: 1999; Percent complete: 50.0%; Average loss: 3.1529 Iteration: 2000; Percent complete: 50.0%; Average loss: 3.2082 Iteration: 2001; Percent complete: 50.0%; Average loss: 3.1149 Iteration: 2002; Percent complete: 50.0%; Average loss: 3.4692 Iteration: 2003; Percent complete: 50.1%; Average loss: 3.1024 Iteration: 2004; Percent complete: 50.1%; Average loss: 3.1624 Iteration: 2005; Percent complete: 50.1%; Average loss: 3.1737 Iteration: 2006; Percent complete: 50.1%; Average loss: 3.1565 Iteration: 2007; Percent complete: 50.2%; Average loss: 3.3474 Iteration: 2008; Percent complete: 50.2%; Average loss: 3.0540 Iteration: 2009; Percent complete: 50.2%; Average loss: 2.9797 Iteration: 2010; Percent complete: 50.2%; Average loss: 3.2565 Iteration: 2011; Percent complete: 50.3%; Average loss: 3.3492 Iteration: 2012; Percent complete: 50.3%; Average loss: 3.1628 Iteration: 2013; Percent complete: 50.3%; Average loss: 3.1572 Iteration: 2014; Percent complete: 50.3%; Average loss: 3.2238 Iteration: 2015; Percent complete: 50.4%; Average loss: 3.2448 Iteration: 2016; Percent complete: 50.4%; Average loss: 3.2495 Iteration: 2017; Percent complete: 50.4%; Average loss: 2.9318 Iteration: 2018; Percent complete: 50.4%; Average loss: 3.2012 Iteration: 2019; Percent complete: 50.5%; Average loss: 3.2043 Iteration: 2020; Percent complete: 50.5%; Average loss: 3.4169 Iteration: 2021; Percent complete: 50.5%; Average loss: 3.2700 Iteration: 2022; Percent complete: 50.5%; Average loss: 3.0297 Iteration: 2023; Percent complete: 50.6%; Average loss: 2.9735 Iteration: 2024; Percent complete: 50.6%; Average loss: 2.7525 Iteration: 2025; Percent complete: 50.6%; Average loss: 3.3517 Iteration: 2026; Percent complete: 50.6%; Average loss: 3.2983 Iteration: 2027; Percent complete: 50.7%; Average loss: 2.9989 Iteration: 2028; Percent complete: 50.7%; Average loss: 2.8659 Iteration: 2029; Percent complete: 50.7%; Average loss: 3.2319 Iteration: 2030; Percent complete: 50.7%; Average loss: 3.1767 Iteration: 2031; Percent complete: 50.8%; Average loss: 3.0906 Iteration: 2032; Percent complete: 50.8%; Average loss: 2.9274 Iteration: 2033; Percent complete: 50.8%; Average loss: 3.3168 Iteration: 2034; Percent complete: 50.8%; Average loss: 3.0443 Iteration: 2035; Percent complete: 50.9%; Average loss: 3.3384 Iteration: 2036; Percent complete: 50.9%; Average loss: 3.1877 Iteration: 2037; Percent complete: 50.9%; Average loss: 3.2322 Iteration: 2038; Percent complete: 50.9%; Average loss: 3.0349 Iteration: 2039; Percent complete: 51.0%; Average loss: 3.1963 Iteration: 2040; Percent complete: 51.0%; Average loss: 2.9312 Iteration: 2041; Percent complete: 51.0%; Average loss: 3.0710 Iteration: 2042; Percent complete: 51.0%; Average loss: 3.2978 Iteration: 2043; Percent complete: 51.1%; Average loss: 3.0428 Iteration: 2044; Percent complete: 51.1%; Average loss: 3.1851 Iteration: 2045; Percent complete: 51.1%; Average loss: 3.4392 Iteration: 2046; Percent complete: 51.1%; Average loss: 3.0813 Iteration: 2047; Percent complete: 51.2%; Average loss: 3.0319 Iteration: 2048; Percent complete: 51.2%; Average loss: 3.1984 Iteration: 2049; Percent complete: 51.2%; Average loss: 3.1841 Iteration: 2050; Percent complete: 51.2%; Average loss: 3.0969 Iteration: 2051; Percent complete: 51.3%; Average loss: 3.2731 Iteration: 2052; Percent complete: 51.3%; Average loss: 2.9168 Iteration: 2053; Percent complete: 51.3%; Average loss: 3.1649 Iteration: 2054; Percent complete: 51.3%; Average loss: 3.1549 Iteration: 2055; Percent complete: 51.4%; Average loss: 3.2137 Iteration: 2056; Percent complete: 51.4%; Average loss: 3.0599 Iteration: 2057; Percent complete: 51.4%; Average loss: 3.0577 Iteration: 2058; Percent complete: 51.4%; Average loss: 3.0192 Iteration: 2059; Percent complete: 51.5%; Average loss: 3.0032 Iteration: 2060; Percent complete: 51.5%; Average loss: 3.3567 Iteration: 2061; Percent complete: 51.5%; Average loss: 3.0327 Iteration: 2062; Percent complete: 51.5%; Average loss: 3.0040 Iteration: 2063; Percent complete: 51.6%; Average loss: 3.1066 Iteration: 2064; Percent complete: 51.6%; Average loss: 3.3245 Iteration: 2065; Percent complete: 51.6%; Average loss: 3.1876 Iteration: 2066; Percent complete: 51.6%; Average loss: 3.1613 Iteration: 2067; Percent complete: 51.7%; Average loss: 2.9272 Iteration: 2068; Percent complete: 51.7%; Average loss: 3.3433 Iteration: 2069; Percent complete: 51.7%; Average loss: 3.1538 Iteration: 2070; Percent complete: 51.7%; Average loss: 2.9813 Iteration: 2071; Percent complete: 51.8%; Average loss: 3.0278 Iteration: 2072; Percent complete: 51.8%; Average loss: 3.4067 Iteration: 2073; Percent complete: 51.8%; Average loss: 3.1327 Iteration: 2074; Percent complete: 51.8%; Average loss: 3.1426 Iteration: 2075; Percent complete: 51.9%; Average loss: 3.0571 Iteration: 2076; Percent complete: 51.9%; Average loss: 3.0072 Iteration: 2077; Percent complete: 51.9%; Average loss: 3.0279 Iteration: 2078; Percent complete: 51.9%; Average loss: 3.1783 Iteration: 2079; Percent complete: 52.0%; Average loss: 3.2371 Iteration: 2080; Percent complete: 52.0%; Average loss: 3.1959 Iteration: 2081; Percent complete: 52.0%; Average loss: 3.2585 Iteration: 2082; Percent complete: 52.0%; Average loss: 3.0316 Iteration: 2083; Percent complete: 52.1%; Average loss: 3.2417 Iteration: 2084; Percent complete: 52.1%; Average loss: 3.1226 Iteration: 2085; Percent complete: 52.1%; Average loss: 3.0579 Iteration: 2086; Percent complete: 52.1%; Average loss: 3.0619 Iteration: 2087; Percent complete: 52.2%; Average loss: 3.0890 Iteration: 2088; Percent complete: 52.2%; Average loss: 3.1590 Iteration: 2089; Percent complete: 52.2%; Average loss: 3.2042 Iteration: 2090; Percent complete: 52.2%; Average loss: 3.0238 Iteration: 2091; Percent complete: 52.3%; Average loss: 3.2154 Iteration: 2092; Percent complete: 52.3%; Average loss: 3.0813 Iteration: 2093; Percent complete: 52.3%; Average loss: 3.0748 Iteration: 2094; Percent complete: 52.3%; Average loss: 3.0433 Iteration: 2095; Percent complete: 52.4%; Average loss: 3.1053 Iteration: 2096; Percent complete: 52.4%; Average loss: 3.1631 Iteration: 2097; Percent complete: 52.4%; Average loss: 3.2154 Iteration: 2098; Percent complete: 52.4%; Average loss: 3.1838 Iteration: 2099; Percent complete: 52.5%; Average loss: 3.3070 Iteration: 2100; Percent complete: 52.5%; Average loss: 3.1872 Iteration: 2101; Percent complete: 52.5%; Average loss: 3.2999 Iteration: 2102; Percent complete: 52.5%; Average loss: 3.4601 Iteration: 2103; Percent complete: 52.6%; Average loss: 3.0459 Iteration: 2104; Percent complete: 52.6%; Average loss: 3.0534 Iteration: 2105; Percent complete: 52.6%; Average loss: 2.9912 Iteration: 2106; Percent complete: 52.6%; Average loss: 3.0896 Iteration: 2107; Percent complete: 52.7%; Average loss: 3.1473 Iteration: 2108; Percent complete: 52.7%; Average loss: 3.0212 Iteration: 2109; Percent complete: 52.7%; Average loss: 3.4153 Iteration: 2110; Percent complete: 52.8%; Average loss: 3.0631 Iteration: 2111; Percent complete: 52.8%; Average loss: 3.3058 Iteration: 2112; Percent complete: 52.8%; Average loss: 3.0522 Iteration: 2113; Percent complete: 52.8%; Average loss: 3.1656 Iteration: 2114; Percent complete: 52.8%; Average loss: 3.1904 Iteration: 2115; Percent complete: 52.9%; Average loss: 3.2335 Iteration: 2116; Percent complete: 52.9%; Average loss: 3.4354 Iteration: 2117; Percent complete: 52.9%; Average loss: 3.0054 Iteration: 2118; Percent complete: 52.9%; Average loss: 3.0576 Iteration: 2119; Percent complete: 53.0%; Average loss: 3.1077 Iteration: 2120; Percent complete: 53.0%; Average loss: 3.0456 Iteration: 2121; Percent complete: 53.0%; Average loss: 3.2399 Iteration: 2122; Percent complete: 53.0%; Average loss: 3.1187 Iteration: 2123; Percent complete: 53.1%; Average loss: 3.2444 Iteration: 2124; Percent complete: 53.1%; Average loss: 3.1705 Iteration: 2125; Percent complete: 53.1%; Average loss: 2.8593 Iteration: 2126; Percent complete: 53.1%; Average loss: 3.0219 Iteration: 2127; Percent complete: 53.2%; Average loss: 3.2337 Iteration: 2128; Percent complete: 53.2%; Average loss: 3.2209 Iteration: 2129; Percent complete: 53.2%; Average loss: 3.1871 Iteration: 2130; Percent complete: 53.2%; Average loss: 2.9840 Iteration: 2131; Percent complete: 53.3%; Average loss: 3.0416 Iteration: 2132; Percent complete: 53.3%; Average loss: 3.1449 Iteration: 2133; Percent complete: 53.3%; Average loss: 3.0035 Iteration: 2134; Percent complete: 53.3%; Average loss: 3.1888 Iteration: 2135; Percent complete: 53.4%; Average loss: 3.3041 Iteration: 2136; Percent complete: 53.4%; Average loss: 3.1467 Iteration: 2137; Percent complete: 53.4%; Average loss: 3.2681 Iteration: 2138; Percent complete: 53.4%; Average loss: 3.0517 Iteration: 2139; Percent complete: 53.5%; Average loss: 2.9784 Iteration: 2140; Percent complete: 53.5%; Average loss: 3.2595 Iteration: 2141; Percent complete: 53.5%; Average loss: 3.0933 Iteration: 2142; Percent complete: 53.5%; Average loss: 3.0125 Iteration: 2143; Percent complete: 53.6%; Average loss: 3.0328 Iteration: 2144; Percent complete: 53.6%; Average loss: 3.0783 Iteration: 2145; Percent complete: 53.6%; Average loss: 3.0843 Iteration: 2146; Percent complete: 53.6%; Average loss: 3.1724 Iteration: 2147; Percent complete: 53.7%; Average loss: 3.2402 Iteration: 2148; Percent complete: 53.7%; Average loss: 2.9745 Iteration: 2149; Percent complete: 53.7%; Average loss: 3.1805 Iteration: 2150; Percent complete: 53.8%; Average loss: 3.0659 Iteration: 2151; Percent complete: 53.8%; Average loss: 3.1741 Iteration: 2152; Percent complete: 53.8%; Average loss: 3.2166 Iteration: 2153; Percent complete: 53.8%; Average loss: 3.0210 Iteration: 2154; Percent complete: 53.8%; Average loss: 3.1465 Iteration: 2155; Percent complete: 53.9%; Average loss: 3.1798 Iteration: 2156; Percent complete: 53.9%; Average loss: 2.9233 Iteration: 2157; Percent complete: 53.9%; Average loss: 3.2107 Iteration: 2158; Percent complete: 53.9%; Average loss: 3.1495 Iteration: 2159; Percent complete: 54.0%; Average loss: 3.0062 Iteration: 2160; Percent complete: 54.0%; Average loss: 3.3022 Iteration: 2161; Percent complete: 54.0%; Average loss: 3.0096 Iteration: 2162; Percent complete: 54.0%; Average loss: 3.1913 Iteration: 2163; Percent complete: 54.1%; Average loss: 3.1533 Iteration: 2164; Percent complete: 54.1%; Average loss: 3.0350 Iteration: 2165; Percent complete: 54.1%; Average loss: 3.1658 Iteration: 2166; Percent complete: 54.1%; Average loss: 3.2373 Iteration: 2167; Percent complete: 54.2%; Average loss: 3.2373 Iteration: 2168; Percent complete: 54.2%; Average loss: 3.0903 Iteration: 2169; Percent complete: 54.2%; Average loss: 3.0564 Iteration: 2170; Percent complete: 54.2%; Average loss: 2.9251 Iteration: 2171; Percent complete: 54.3%; Average loss: 3.2464 Iteration: 2172; Percent complete: 54.3%; Average loss: 3.3825 Iteration: 2173; Percent complete: 54.3%; Average loss: 3.3807 Iteration: 2174; Percent complete: 54.4%; Average loss: 3.2350 Iteration: 2175; Percent complete: 54.4%; Average loss: 2.9695 Iteration: 2176; Percent complete: 54.4%; Average loss: 2.9056 Iteration: 2177; Percent complete: 54.4%; Average loss: 3.0504 Iteration: 2178; Percent complete: 54.4%; Average loss: 3.0704 Iteration: 2179; Percent complete: 54.5%; Average loss: 3.2341 Iteration: 2180; Percent complete: 54.5%; Average loss: 2.9216 Iteration: 2181; Percent complete: 54.5%; Average loss: 2.9119 Iteration: 2182; Percent complete: 54.5%; Average loss: 3.1728 Iteration: 2183; Percent complete: 54.6%; Average loss: 3.2105 Iteration: 2184; Percent complete: 54.6%; Average loss: 2.9327 Iteration: 2185; Percent complete: 54.6%; Average loss: 3.1283 Iteration: 2186; Percent complete: 54.6%; Average loss: 3.2375 Iteration: 2187; Percent complete: 54.7%; Average loss: 3.1406 Iteration: 2188; Percent complete: 54.7%; Average loss: 3.2877 Iteration: 2189; Percent complete: 54.7%; Average loss: 3.0811 Iteration: 2190; Percent complete: 54.8%; Average loss: 2.9333 Iteration: 2191; Percent complete: 54.8%; Average loss: 3.0446 Iteration: 2192; Percent complete: 54.8%; Average loss: 3.1889 Iteration: 2193; Percent complete: 54.8%; Average loss: 3.2479 Iteration: 2194; Percent complete: 54.9%; Average loss: 3.2153 Iteration: 2195; Percent complete: 54.9%; Average loss: 3.3670 Iteration: 2196; Percent complete: 54.9%; Average loss: 2.9339 Iteration: 2197; Percent complete: 54.9%; Average loss: 3.0430 Iteration: 2198; Percent complete: 54.9%; Average loss: 3.0997 Iteration: 2199; Percent complete: 55.0%; Average loss: 3.2640 Iteration: 2200; Percent complete: 55.0%; Average loss: 3.1570 Iteration: 2201; Percent complete: 55.0%; Average loss: 3.0195 Iteration: 2202; Percent complete: 55.0%; Average loss: 3.0562 Iteration: 2203; Percent complete: 55.1%; Average loss: 2.9020 Iteration: 2204; Percent complete: 55.1%; Average loss: 3.1450 Iteration: 2205; Percent complete: 55.1%; Average loss: 3.1889 Iteration: 2206; Percent complete: 55.1%; Average loss: 3.1361 Iteration: 2207; Percent complete: 55.2%; Average loss: 3.2197 Iteration: 2208; Percent complete: 55.2%; Average loss: 3.1695 Iteration: 2209; Percent complete: 55.2%; Average loss: 3.1391 Iteration: 2210; Percent complete: 55.2%; Average loss: 3.0901 Iteration: 2211; Percent complete: 55.3%; Average loss: 3.1009 Iteration: 2212; Percent complete: 55.3%; Average loss: 3.0961 Iteration: 2213; Percent complete: 55.3%; Average loss: 3.1071 Iteration: 2214; Percent complete: 55.4%; Average loss: 3.3075 Iteration: 2215; Percent complete: 55.4%; Average loss: 3.1559 Iteration: 2216; Percent complete: 55.4%; Average loss: 3.0978 Iteration: 2217; Percent complete: 55.4%; Average loss: 3.2209 Iteration: 2218; Percent complete: 55.5%; Average loss: 2.8812 Iteration: 2219; Percent complete: 55.5%; Average loss: 3.0528 Iteration: 2220; Percent complete: 55.5%; Average loss: 2.9478 Iteration: 2221; Percent complete: 55.5%; Average loss: 3.2022 Iteration: 2222; Percent complete: 55.5%; Average loss: 3.0690 Iteration: 2223; Percent complete: 55.6%; Average loss: 3.0582 Iteration: 2224; Percent complete: 55.6%; Average loss: 2.9638 Iteration: 2225; Percent complete: 55.6%; Average loss: 3.2415 Iteration: 2226; Percent complete: 55.6%; Average loss: 3.5418 Iteration: 2227; Percent complete: 55.7%; Average loss: 2.9684 Iteration: 2228; Percent complete: 55.7%; Average loss: 2.9168 Iteration: 2229; Percent complete: 55.7%; Average loss: 3.0042 Iteration: 2230; Percent complete: 55.8%; Average loss: 3.3420 Iteration: 2231; Percent complete: 55.8%; Average loss: 2.7673 Iteration: 2232; Percent complete: 55.8%; Average loss: 3.3168 Iteration: 2233; Percent complete: 55.8%; Average loss: 3.0216 Iteration: 2234; Percent complete: 55.9%; Average loss: 3.1888 Iteration: 2235; Percent complete: 55.9%; Average loss: 2.9440 Iteration: 2236; Percent complete: 55.9%; Average loss: 3.0129 Iteration: 2237; Percent complete: 55.9%; Average loss: 3.2862 Iteration: 2238; Percent complete: 56.0%; Average loss: 3.3401 Iteration: 2239; Percent complete: 56.0%; Average loss: 3.0064 Iteration: 2240; Percent complete: 56.0%; Average loss: 2.9815 Iteration: 2241; Percent complete: 56.0%; Average loss: 3.1758 Iteration: 2242; Percent complete: 56.0%; Average loss: 3.1954 Iteration: 2243; Percent complete: 56.1%; Average loss: 3.0948 Iteration: 2244; Percent complete: 56.1%; Average loss: 3.2322 Iteration: 2245; Percent complete: 56.1%; Average loss: 2.8999 Iteration: 2246; Percent complete: 56.1%; Average loss: 3.1323 Iteration: 2247; Percent complete: 56.2%; Average loss: 3.2062 Iteration: 2248; Percent complete: 56.2%; Average loss: 3.2596 Iteration: 2249; Percent complete: 56.2%; Average loss: 3.0669 Iteration: 2250; Percent complete: 56.2%; Average loss: 3.1418 Iteration: 2251; Percent complete: 56.3%; Average loss: 3.0625 Iteration: 2252; Percent complete: 56.3%; Average loss: 2.8354 Iteration: 2253; Percent complete: 56.3%; Average loss: 3.0152 Iteration: 2254; Percent complete: 56.4%; Average loss: 3.3499 Iteration: 2255; Percent complete: 56.4%; Average loss: 3.1123 Iteration: 2256; Percent complete: 56.4%; Average loss: 3.2161 Iteration: 2257; Percent complete: 56.4%; Average loss: 2.9346 Iteration: 2258; Percent complete: 56.5%; Average loss: 3.4500 Iteration: 2259; Percent complete: 56.5%; Average loss: 2.9546 Iteration: 2260; Percent complete: 56.5%; Average loss: 3.2969 Iteration: 2261; Percent complete: 56.5%; Average loss: 3.0232 Iteration: 2262; Percent complete: 56.5%; Average loss: 3.0781 Iteration: 2263; Percent complete: 56.6%; Average loss: 2.6908 Iteration: 2264; Percent complete: 56.6%; Average loss: 3.0890 Iteration: 2265; Percent complete: 56.6%; Average loss: 3.0787 Iteration: 2266; Percent complete: 56.6%; Average loss: 2.8231 Iteration: 2267; Percent complete: 56.7%; Average loss: 2.8522 Iteration: 2268; Percent complete: 56.7%; Average loss: 2.8848 Iteration: 2269; Percent complete: 56.7%; Average loss: 3.2179 Iteration: 2270; Percent complete: 56.8%; Average loss: 2.9336 Iteration: 2271; Percent complete: 56.8%; Average loss: 3.1807 Iteration: 2272; Percent complete: 56.8%; Average loss: 3.2438 Iteration: 2273; Percent complete: 56.8%; Average loss: 3.1505 Iteration: 2274; Percent complete: 56.9%; Average loss: 2.8398 Iteration: 2275; Percent complete: 56.9%; Average loss: 3.0749 Iteration: 2276; Percent complete: 56.9%; Average loss: 3.2126 Iteration: 2277; Percent complete: 56.9%; Average loss: 3.1575 Iteration: 2278; Percent complete: 57.0%; Average loss: 3.0908 Iteration: 2279; Percent complete: 57.0%; Average loss: 3.2163 Iteration: 2280; Percent complete: 57.0%; Average loss: 3.0343 Iteration: 2281; Percent complete: 57.0%; Average loss: 3.1416 Iteration: 2282; Percent complete: 57.0%; Average loss: 3.2054 Iteration: 2283; Percent complete: 57.1%; Average loss: 2.8907 Iteration: 2284; Percent complete: 57.1%; Average loss: 3.1890 Iteration: 2285; Percent complete: 57.1%; Average loss: 3.0720 Iteration: 2286; Percent complete: 57.1%; Average loss: 2.8727 Iteration: 2287; Percent complete: 57.2%; Average loss: 3.2101 Iteration: 2288; Percent complete: 57.2%; Average loss: 2.7598 Iteration: 2289; Percent complete: 57.2%; Average loss: 3.0717 Iteration: 2290; Percent complete: 57.2%; Average loss: 2.9435 Iteration: 2291; Percent complete: 57.3%; Average loss: 2.9831 Iteration: 2292; Percent complete: 57.3%; Average loss: 3.0723 Iteration: 2293; Percent complete: 57.3%; Average loss: 3.0705 Iteration: 2294; Percent complete: 57.4%; Average loss: 2.9697 Iteration: 2295; Percent complete: 57.4%; Average loss: 2.8842 Iteration: 2296; Percent complete: 57.4%; Average loss: 3.1654 Iteration: 2297; Percent complete: 57.4%; Average loss: 3.0200 Iteration: 2298; Percent complete: 57.5%; Average loss: 3.0309 Iteration: 2299; Percent complete: 57.5%; Average loss: 3.1875 Iteration: 2300; Percent complete: 57.5%; Average loss: 3.1743 Iteration: 2301; Percent complete: 57.5%; Average loss: 3.1811 Iteration: 2302; Percent complete: 57.6%; Average loss: 3.0401 Iteration: 2303; Percent complete: 57.6%; Average loss: 2.9408 Iteration: 2304; Percent complete: 57.6%; Average loss: 3.2218 Iteration: 2305; Percent complete: 57.6%; Average loss: 3.1464 Iteration: 2306; Percent complete: 57.6%; Average loss: 2.9642 Iteration: 2307; Percent complete: 57.7%; Average loss: 2.8079 Iteration: 2308; Percent complete: 57.7%; Average loss: 2.9996 Iteration: 2309; Percent complete: 57.7%; Average loss: 3.0726 Iteration: 2310; Percent complete: 57.8%; Average loss: 3.3397 Iteration: 2311; Percent complete: 57.8%; Average loss: 3.1430 Iteration: 2312; Percent complete: 57.8%; Average loss: 3.3286 Iteration: 2313; Percent complete: 57.8%; Average loss: 2.9164 Iteration: 2314; Percent complete: 57.9%; Average loss: 3.2109 Iteration: 2315; Percent complete: 57.9%; Average loss: 3.3330 Iteration: 2316; Percent complete: 57.9%; Average loss: 3.1248 Iteration: 2317; Percent complete: 57.9%; Average loss: 3.0707 Iteration: 2318; Percent complete: 58.0%; Average loss: 3.0612 Iteration: 2319; Percent complete: 58.0%; Average loss: 2.8473 Iteration: 2320; Percent complete: 58.0%; Average loss: 3.2078 Iteration: 2321; Percent complete: 58.0%; Average loss: 3.0709 Iteration: 2322; Percent complete: 58.1%; Average loss: 3.0883 Iteration: 2323; Percent complete: 58.1%; Average loss: 3.0236 Iteration: 2324; Percent complete: 58.1%; Average loss: 2.9528 Iteration: 2325; Percent complete: 58.1%; Average loss: 3.2924 Iteration: 2326; Percent complete: 58.1%; Average loss: 2.7107 Iteration: 2327; Percent complete: 58.2%; Average loss: 2.9690 Iteration: 2328; Percent complete: 58.2%; Average loss: 3.0400 Iteration: 2329; Percent complete: 58.2%; Average loss: 2.9971 Iteration: 2330; Percent complete: 58.2%; Average loss: 3.0223 Iteration: 2331; Percent complete: 58.3%; Average loss: 3.2198 Iteration: 2332; Percent complete: 58.3%; Average loss: 3.0670 Iteration: 2333; Percent complete: 58.3%; Average loss: 2.8575 Iteration: 2334; Percent complete: 58.4%; Average loss: 3.1535 Iteration: 2335; Percent complete: 58.4%; Average loss: 2.8534 Iteration: 2336; Percent complete: 58.4%; Average loss: 3.1788 Iteration: 2337; Percent complete: 58.4%; Average loss: 3.3014 Iteration: 2338; Percent complete: 58.5%; Average loss: 3.1429 Iteration: 2339; Percent complete: 58.5%; Average loss: 3.0781 Iteration: 2340; Percent complete: 58.5%; Average loss: 2.9397 Iteration: 2341; Percent complete: 58.5%; Average loss: 2.9744 Iteration: 2342; Percent complete: 58.6%; Average loss: 3.0321 Iteration: 2343; Percent complete: 58.6%; Average loss: 3.1011 Iteration: 2344; Percent complete: 58.6%; Average loss: 3.0047 Iteration: 2345; Percent complete: 58.6%; Average loss: 2.8201 Iteration: 2346; Percent complete: 58.7%; Average loss: 3.3459 Iteration: 2347; Percent complete: 58.7%; Average loss: 3.0558 Iteration: 2348; Percent complete: 58.7%; Average loss: 2.8595 Iteration: 2349; Percent complete: 58.7%; Average loss: 2.9791 Iteration: 2350; Percent complete: 58.8%; Average loss: 2.9259 Iteration: 2351; Percent complete: 58.8%; Average loss: 2.9089 Iteration: 2352; Percent complete: 58.8%; Average loss: 2.8353 Iteration: 2353; Percent complete: 58.8%; Average loss: 3.2003 Iteration: 2354; Percent complete: 58.9%; Average loss: 3.3361 Iteration: 2355; Percent complete: 58.9%; Average loss: 3.0339 Iteration: 2356; Percent complete: 58.9%; Average loss: 3.0604 Iteration: 2357; Percent complete: 58.9%; Average loss: 3.0730 Iteration: 2358; Percent complete: 59.0%; Average loss: 3.0103 Iteration: 2359; Percent complete: 59.0%; Average loss: 3.1640 Iteration: 2360; Percent complete: 59.0%; Average loss: 2.8391 Iteration: 2361; Percent complete: 59.0%; Average loss: 3.0450 Iteration: 2362; Percent complete: 59.1%; Average loss: 3.2066 Iteration: 2363; Percent complete: 59.1%; Average loss: 3.0557 Iteration: 2364; Percent complete: 59.1%; Average loss: 3.1378 Iteration: 2365; Percent complete: 59.1%; Average loss: 3.0553 Iteration: 2366; Percent complete: 59.2%; Average loss: 2.9069 Iteration: 2367; Percent complete: 59.2%; Average loss: 2.8837 Iteration: 2368; Percent complete: 59.2%; Average loss: 3.0619 Iteration: 2369; Percent complete: 59.2%; Average loss: 3.2510 Iteration: 2370; Percent complete: 59.2%; Average loss: 3.3935 Iteration: 2371; Percent complete: 59.3%; Average loss: 2.9976 Iteration: 2372; Percent complete: 59.3%; Average loss: 3.2120 Iteration: 2373; Percent complete: 59.3%; Average loss: 2.9935 Iteration: 2374; Percent complete: 59.4%; Average loss: 2.9533 Iteration: 2375; Percent complete: 59.4%; Average loss: 2.8794 Iteration: 2376; Percent complete: 59.4%; Average loss: 3.0426 Iteration: 2377; Percent complete: 59.4%; Average loss: 2.9115 Iteration: 2378; Percent complete: 59.5%; Average loss: 2.9513 Iteration: 2379; Percent complete: 59.5%; Average loss: 3.1140 Iteration: 2380; Percent complete: 59.5%; Average loss: 2.8890 Iteration: 2381; Percent complete: 59.5%; Average loss: 3.1780 Iteration: 2382; Percent complete: 59.6%; Average loss: 2.7383 Iteration: 2383; Percent complete: 59.6%; Average loss: 2.9650 Iteration: 2384; Percent complete: 59.6%; Average loss: 2.8571 Iteration: 2385; Percent complete: 59.6%; Average loss: 3.1086 Iteration: 2386; Percent complete: 59.7%; Average loss: 3.0087 Iteration: 2387; Percent complete: 59.7%; Average loss: 3.0994 Iteration: 2388; Percent complete: 59.7%; Average loss: 3.2480 Iteration: 2389; Percent complete: 59.7%; Average loss: 3.1875 Iteration: 2390; Percent complete: 59.8%; Average loss: 2.8676 Iteration: 2391; Percent complete: 59.8%; Average loss: 2.9218 Iteration: 2392; Percent complete: 59.8%; Average loss: 3.1167 Iteration: 2393; Percent complete: 59.8%; Average loss: 2.9529 Iteration: 2394; Percent complete: 59.9%; Average loss: 2.8647 Iteration: 2395; Percent complete: 59.9%; Average loss: 3.0451 Iteration: 2396; Percent complete: 59.9%; Average loss: 3.0807 Iteration: 2397; Percent complete: 59.9%; Average loss: 3.1120 Iteration: 2398; Percent complete: 60.0%; Average loss: 2.9271 Iteration: 2399; Percent complete: 60.0%; Average loss: 3.1743 Iteration: 2400; Percent complete: 60.0%; Average loss: 3.0094 Iteration: 2401; Percent complete: 60.0%; Average loss: 2.8377 Iteration: 2402; Percent complete: 60.1%; Average loss: 3.2161 Iteration: 2403; Percent complete: 60.1%; Average loss: 2.9270 Iteration: 2404; Percent complete: 60.1%; Average loss: 2.8011 Iteration: 2405; Percent complete: 60.1%; Average loss: 3.1273 Iteration: 2406; Percent complete: 60.2%; Average loss: 2.8971 Iteration: 2407; Percent complete: 60.2%; Average loss: 2.8312 Iteration: 2408; Percent complete: 60.2%; Average loss: 3.0171 Iteration: 2409; Percent complete: 60.2%; Average loss: 3.0027 Iteration: 2410; Percent complete: 60.2%; Average loss: 2.6430 Iteration: 2411; Percent complete: 60.3%; Average loss: 2.8715 Iteration: 2412; Percent complete: 60.3%; Average loss: 2.9399 Iteration: 2413; Percent complete: 60.3%; Average loss: 3.1935 Iteration: 2414; Percent complete: 60.4%; Average loss: 2.9675 Iteration: 2415; Percent complete: 60.4%; Average loss: 3.0074 Iteration: 2416; Percent complete: 60.4%; Average loss: 2.9939 Iteration: 2417; Percent complete: 60.4%; Average loss: 3.3969 Iteration: 2418; Percent complete: 60.5%; Average loss: 2.9387 Iteration: 2419; Percent complete: 60.5%; Average loss: 2.7750 Iteration: 2420; Percent complete: 60.5%; Average loss: 3.0736 Iteration: 2421; Percent complete: 60.5%; Average loss: 2.9006 Iteration: 2422; Percent complete: 60.6%; Average loss: 3.1029 Iteration: 2423; Percent complete: 60.6%; Average loss: 3.0998 Iteration: 2424; Percent complete: 60.6%; Average loss: 3.1131 Iteration: 2425; Percent complete: 60.6%; Average loss: 3.0232 Iteration: 2426; Percent complete: 60.7%; Average loss: 3.0732 Iteration: 2427; Percent complete: 60.7%; Average loss: 3.3019 Iteration: 2428; Percent complete: 60.7%; Average loss: 3.2356 Iteration: 2429; Percent complete: 60.7%; Average loss: 3.0353 Iteration: 2430; Percent complete: 60.8%; Average loss: 3.2755 Iteration: 2431; Percent complete: 60.8%; Average loss: 3.0375 Iteration: 2432; Percent complete: 60.8%; Average loss: 3.1990 Iteration: 2433; Percent complete: 60.8%; Average loss: 3.0002 Iteration: 2434; Percent complete: 60.9%; Average loss: 3.0500 Iteration: 2435; Percent complete: 60.9%; Average loss: 3.2308 Iteration: 2436; Percent complete: 60.9%; Average loss: 2.9852 Iteration: 2437; Percent complete: 60.9%; Average loss: 3.0617 Iteration: 2438; Percent complete: 61.0%; Average loss: 2.8609 Iteration: 2439; Percent complete: 61.0%; Average loss: 2.9806 Iteration: 2440; Percent complete: 61.0%; Average loss: 3.3641 Iteration: 2441; Percent complete: 61.0%; Average loss: 2.9916 Iteration: 2442; Percent complete: 61.1%; Average loss: 3.3711 Iteration: 2443; Percent complete: 61.1%; Average loss: 3.1155 Iteration: 2444; Percent complete: 61.1%; Average loss: 3.0574 Iteration: 2445; Percent complete: 61.1%; Average loss: 3.1826 Iteration: 2446; Percent complete: 61.2%; Average loss: 2.9566 Iteration: 2447; Percent complete: 61.2%; Average loss: 2.7333 Iteration: 2448; Percent complete: 61.2%; Average loss: 3.1451 Iteration: 2449; Percent complete: 61.2%; Average loss: 3.0976 Iteration: 2450; Percent complete: 61.3%; Average loss: 3.0232 Iteration: 2451; Percent complete: 61.3%; Average loss: 3.0026 Iteration: 2452; Percent complete: 61.3%; Average loss: 3.1373 Iteration: 2453; Percent complete: 61.3%; Average loss: 2.9024 Iteration: 2454; Percent complete: 61.4%; Average loss: 3.0756 Iteration: 2455; Percent complete: 61.4%; Average loss: 3.1137 Iteration: 2456; Percent complete: 61.4%; Average loss: 2.9910 Iteration: 2457; Percent complete: 61.4%; Average loss: 3.0233 Iteration: 2458; Percent complete: 61.5%; Average loss: 3.1661 Iteration: 2459; Percent complete: 61.5%; Average loss: 2.8733 Iteration: 2460; Percent complete: 61.5%; Average loss: 3.1447 Iteration: 2461; Percent complete: 61.5%; Average loss: 3.0538 Iteration: 2462; Percent complete: 61.6%; Average loss: 2.9841 Iteration: 2463; Percent complete: 61.6%; Average loss: 2.9982 Iteration: 2464; Percent complete: 61.6%; Average loss: 2.8630 Iteration: 2465; Percent complete: 61.6%; Average loss: 2.9709 Iteration: 2466; Percent complete: 61.7%; Average loss: 3.0314 Iteration: 2467; Percent complete: 61.7%; Average loss: 3.1028 Iteration: 2468; Percent complete: 61.7%; Average loss: 3.0246 Iteration: 2469; Percent complete: 61.7%; Average loss: 3.0360 Iteration: 2470; Percent complete: 61.8%; Average loss: 2.8862 Iteration: 2471; Percent complete: 61.8%; Average loss: 2.9073 Iteration: 2472; Percent complete: 61.8%; Average loss: 3.1232 Iteration: 2473; Percent complete: 61.8%; Average loss: 2.7387 Iteration: 2474; Percent complete: 61.9%; Average loss: 3.0861 Iteration: 2475; Percent complete: 61.9%; Average loss: 2.9135 Iteration: 2476; Percent complete: 61.9%; Average loss: 3.0335 Iteration: 2477; Percent complete: 61.9%; Average loss: 3.3159 Iteration: 2478; Percent complete: 62.0%; Average loss: 2.8861 Iteration: 2479; Percent complete: 62.0%; Average loss: 3.0872 Iteration: 2480; Percent complete: 62.0%; Average loss: 3.0781 Iteration: 2481; Percent complete: 62.0%; Average loss: 2.9064 Iteration: 2482; Percent complete: 62.1%; Average loss: 3.0143 Iteration: 2483; Percent complete: 62.1%; Average loss: 3.2912 Iteration: 2484; Percent complete: 62.1%; Average loss: 3.0475 Iteration: 2485; Percent complete: 62.1%; Average loss: 3.0903 Iteration: 2486; Percent complete: 62.2%; Average loss: 2.8582 Iteration: 2487; Percent complete: 62.2%; Average loss: 2.8165 Iteration: 2488; Percent complete: 62.2%; Average loss: 2.8773 Iteration: 2489; Percent complete: 62.2%; Average loss: 3.0432 Iteration: 2490; Percent complete: 62.3%; Average loss: 3.1007 Iteration: 2491; Percent complete: 62.3%; Average loss: 2.7417 Iteration: 2492; Percent complete: 62.3%; Average loss: 3.0895 Iteration: 2493; Percent complete: 62.3%; Average loss: 3.1848 Iteration: 2494; Percent complete: 62.4%; Average loss: 3.1985 Iteration: 2495; Percent complete: 62.4%; Average loss: 2.9238 Iteration: 2496; Percent complete: 62.4%; Average loss: 2.7739 Iteration: 2497; Percent complete: 62.4%; Average loss: 2.9055 Iteration: 2498; Percent complete: 62.5%; Average loss: 3.3160 Iteration: 2499; Percent complete: 62.5%; Average loss: 3.3404 Iteration: 2500; Percent complete: 62.5%; Average loss: 2.6977 Iteration: 2501; Percent complete: 62.5%; Average loss: 2.9520 Iteration: 2502; Percent complete: 62.5%; Average loss: 3.0517 Iteration: 2503; Percent complete: 62.6%; Average loss: 2.8219 Iteration: 2504; Percent complete: 62.6%; Average loss: 2.8717 Iteration: 2505; Percent complete: 62.6%; Average loss: 3.0007 Iteration: 2506; Percent complete: 62.6%; Average loss: 3.0205 Iteration: 2507; Percent complete: 62.7%; Average loss: 2.9983 Iteration: 2508; Percent complete: 62.7%; Average loss: 3.1072 Iteration: 2509; Percent complete: 62.7%; Average loss: 3.0707 Iteration: 2510; Percent complete: 62.7%; Average loss: 3.0299 Iteration: 2511; Percent complete: 62.8%; Average loss: 3.0130 Iteration: 2512; Percent complete: 62.8%; Average loss: 3.1158 Iteration: 2513; Percent complete: 62.8%; Average loss: 2.9568 Iteration: 2514; Percent complete: 62.8%; Average loss: 2.8753 Iteration: 2515; Percent complete: 62.9%; Average loss: 2.9292 Iteration: 2516; Percent complete: 62.9%; Average loss: 3.1571 Iteration: 2517; Percent complete: 62.9%; Average loss: 3.0246 Iteration: 2518; Percent complete: 62.9%; Average loss: 2.8949 Iteration: 2519; Percent complete: 63.0%; Average loss: 2.8011 Iteration: 2520; Percent complete: 63.0%; Average loss: 2.9427 Iteration: 2521; Percent complete: 63.0%; Average loss: 2.9891 Iteration: 2522; Percent complete: 63.0%; Average loss: 3.0095 Iteration: 2523; Percent complete: 63.1%; Average loss: 3.0353 Iteration: 2524; Percent complete: 63.1%; Average loss: 3.2749 Iteration: 2525; Percent complete: 63.1%; Average loss: 2.9561 Iteration: 2526; Percent complete: 63.1%; Average loss: 3.2806 Iteration: 2527; Percent complete: 63.2%; Average loss: 3.0784 Iteration: 2528; Percent complete: 63.2%; Average loss: 3.1953 Iteration: 2529; Percent complete: 63.2%; Average loss: 2.8902 Iteration: 2530; Percent complete: 63.2%; Average loss: 2.9831 Iteration: 2531; Percent complete: 63.3%; Average loss: 3.0368 Iteration: 2532; Percent complete: 63.3%; Average loss: 2.9397 Iteration: 2533; Percent complete: 63.3%; Average loss: 3.1506 Iteration: 2534; Percent complete: 63.3%; Average loss: 2.8618 Iteration: 2535; Percent complete: 63.4%; Average loss: 3.0549 Iteration: 2536; Percent complete: 63.4%; Average loss: 3.1664 Iteration: 2537; Percent complete: 63.4%; Average loss: 2.7106 Iteration: 2538; Percent complete: 63.4%; Average loss: 3.1740 Iteration: 2539; Percent complete: 63.5%; Average loss: 3.0385 Iteration: 2540; Percent complete: 63.5%; Average loss: 3.2137 Iteration: 2541; Percent complete: 63.5%; Average loss: 3.0837 Iteration: 2542; Percent complete: 63.5%; Average loss: 3.0074 Iteration: 2543; Percent complete: 63.6%; Average loss: 2.6155 Iteration: 2544; Percent complete: 63.6%; Average loss: 3.2696 Iteration: 2545; Percent complete: 63.6%; Average loss: 2.8813 Iteration: 2546; Percent complete: 63.6%; Average loss: 3.0037 Iteration: 2547; Percent complete: 63.7%; Average loss: 2.9897 Iteration: 2548; Percent complete: 63.7%; Average loss: 3.0735 Iteration: 2549; Percent complete: 63.7%; Average loss: 3.0650 Iteration: 2550; Percent complete: 63.7%; Average loss: 3.1080 Iteration: 2551; Percent complete: 63.8%; Average loss: 2.8200 Iteration: 2552; Percent complete: 63.8%; Average loss: 2.9047 Iteration: 2553; Percent complete: 63.8%; Average loss: 3.2542 Iteration: 2554; Percent complete: 63.8%; Average loss: 2.9342 Iteration: 2555; Percent complete: 63.9%; Average loss: 3.0346 Iteration: 2556; Percent complete: 63.9%; Average loss: 2.8772 Iteration: 2557; Percent complete: 63.9%; Average loss: 3.1414 Iteration: 2558; Percent complete: 63.9%; Average loss: 3.0918 Iteration: 2559; Percent complete: 64.0%; Average loss: 3.0061 Iteration: 2560; Percent complete: 64.0%; Average loss: 2.8662 Iteration: 2561; Percent complete: 64.0%; Average loss: 3.2474 Iteration: 2562; Percent complete: 64.0%; Average loss: 3.0393 Iteration: 2563; Percent complete: 64.1%; Average loss: 3.0809 Iteration: 2564; Percent complete: 64.1%; Average loss: 2.8227 Iteration: 2565; Percent complete: 64.1%; Average loss: 3.2218 Iteration: 2566; Percent complete: 64.1%; Average loss: 2.9595 Iteration: 2567; Percent complete: 64.2%; Average loss: 2.9379 Iteration: 2568; Percent complete: 64.2%; Average loss: 2.9421 Iteration: 2569; Percent complete: 64.2%; Average loss: 3.0058 Iteration: 2570; Percent complete: 64.2%; Average loss: 3.0435 Iteration: 2571; Percent complete: 64.3%; Average loss: 3.0110 Iteration: 2572; Percent complete: 64.3%; Average loss: 2.8702 Iteration: 2573; Percent complete: 64.3%; Average loss: 2.9370 Iteration: 2574; Percent complete: 64.3%; Average loss: 3.0840 Iteration: 2575; Percent complete: 64.4%; Average loss: 2.9967 Iteration: 2576; Percent complete: 64.4%; Average loss: 3.0738 Iteration: 2577; Percent complete: 64.4%; Average loss: 2.9509 Iteration: 2578; Percent complete: 64.5%; Average loss: 2.8010 Iteration: 2579; Percent complete: 64.5%; Average loss: 2.8671 Iteration: 2580; Percent complete: 64.5%; Average loss: 3.0494 Iteration: 2581; Percent complete: 64.5%; Average loss: 3.2313 Iteration: 2582; Percent complete: 64.5%; Average loss: 2.8002 Iteration: 2583; Percent complete: 64.6%; Average loss: 2.8280 Iteration: 2584; Percent complete: 64.6%; Average loss: 2.7542 Iteration: 2585; Percent complete: 64.6%; Average loss: 3.0977 Iteration: 2586; Percent complete: 64.6%; Average loss: 3.0219 Iteration: 2587; Percent complete: 64.7%; Average loss: 3.0494 Iteration: 2588; Percent complete: 64.7%; Average loss: 2.8396 Iteration: 2589; Percent complete: 64.7%; Average loss: 3.1587 Iteration: 2590; Percent complete: 64.8%; Average loss: 3.2286 Iteration: 2591; Percent complete: 64.8%; Average loss: 3.1454 Iteration: 2592; Percent complete: 64.8%; Average loss: 2.9944 Iteration: 2593; Percent complete: 64.8%; Average loss: 3.0932 Iteration: 2594; Percent complete: 64.8%; Average loss: 2.8769 Iteration: 2595; Percent complete: 64.9%; Average loss: 2.9790 Iteration: 2596; Percent complete: 64.9%; Average loss: 3.0198 Iteration: 2597; Percent complete: 64.9%; Average loss: 2.8238 Iteration: 2598; Percent complete: 65.0%; Average loss: 3.0501 Iteration: 2599; Percent complete: 65.0%; Average loss: 3.1038 Iteration: 2600; Percent complete: 65.0%; Average loss: 2.9094 Iteration: 2601; Percent complete: 65.0%; Average loss: 2.9104 Iteration: 2602; Percent complete: 65.0%; Average loss: 3.0460 Iteration: 2603; Percent complete: 65.1%; Average loss: 3.0343 Iteration: 2604; Percent complete: 65.1%; Average loss: 3.2776 Iteration: 2605; Percent complete: 65.1%; Average loss: 3.1113 Iteration: 2606; Percent complete: 65.1%; Average loss: 3.2273 Iteration: 2607; Percent complete: 65.2%; Average loss: 3.2279 Iteration: 2608; Percent complete: 65.2%; Average loss: 3.0195 Iteration: 2609; Percent complete: 65.2%; Average loss: 3.2257 Iteration: 2610; Percent complete: 65.2%; Average loss: 3.0352 Iteration: 2611; Percent complete: 65.3%; Average loss: 3.0399 Iteration: 2612; Percent complete: 65.3%; Average loss: 2.9857 Iteration: 2613; Percent complete: 65.3%; Average loss: 2.9917 Iteration: 2614; Percent complete: 65.3%; Average loss: 3.0639 Iteration: 2615; Percent complete: 65.4%; Average loss: 2.6918 Iteration: 2616; Percent complete: 65.4%; Average loss: 3.0844 Iteration: 2617; Percent complete: 65.4%; Average loss: 2.8985 Iteration: 2618; Percent complete: 65.5%; Average loss: 2.9986 Iteration: 2619; Percent complete: 65.5%; Average loss: 2.8861 Iteration: 2620; Percent complete: 65.5%; Average loss: 3.1542 Iteration: 2621; Percent complete: 65.5%; Average loss: 2.9965 Iteration: 2622; Percent complete: 65.5%; Average loss: 2.7434 Iteration: 2623; Percent complete: 65.6%; Average loss: 3.2364 Iteration: 2624; Percent complete: 65.6%; Average loss: 3.0255 Iteration: 2625; Percent complete: 65.6%; Average loss: 2.9321 Iteration: 2626; Percent complete: 65.6%; Average loss: 3.0955 Iteration: 2627; Percent complete: 65.7%; Average loss: 2.8944 Iteration: 2628; Percent complete: 65.7%; Average loss: 3.0704 Iteration: 2629; Percent complete: 65.7%; Average loss: 2.8900 Iteration: 2630; Percent complete: 65.8%; Average loss: 2.8992 Iteration: 2631; Percent complete: 65.8%; Average loss: 2.8165 Iteration: 2632; Percent complete: 65.8%; Average loss: 2.9634 Iteration: 2633; Percent complete: 65.8%; Average loss: 3.3636 Iteration: 2634; Percent complete: 65.8%; Average loss: 3.0438 Iteration: 2635; Percent complete: 65.9%; Average loss: 2.8746 Iteration: 2636; Percent complete: 65.9%; Average loss: 3.1292 Iteration: 2637; Percent complete: 65.9%; Average loss: 2.8750 Iteration: 2638; Percent complete: 66.0%; Average loss: 2.7047 Iteration: 2639; Percent complete: 66.0%; Average loss: 2.9193 Iteration: 2640; Percent complete: 66.0%; Average loss: 2.9350 Iteration: 2641; Percent complete: 66.0%; Average loss: 2.8696 Iteration: 2642; Percent complete: 66.0%; Average loss: 3.2226 Iteration: 2643; Percent complete: 66.1%; Average loss: 2.8267 Iteration: 2644; Percent complete: 66.1%; Average loss: 2.9209 Iteration: 2645; Percent complete: 66.1%; Average loss: 3.1970 Iteration: 2646; Percent complete: 66.1%; Average loss: 2.9289 Iteration: 2647; Percent complete: 66.2%; Average loss: 2.9753 Iteration: 2648; Percent complete: 66.2%; Average loss: 3.0245 Iteration: 2649; Percent complete: 66.2%; Average loss: 2.7824 Iteration: 2650; Percent complete: 66.2%; Average loss: 2.8589 Iteration: 2651; Percent complete: 66.3%; Average loss: 3.2010 Iteration: 2652; Percent complete: 66.3%; Average loss: 3.1796 Iteration: 2653; Percent complete: 66.3%; Average loss: 3.1595 Iteration: 2654; Percent complete: 66.3%; Average loss: 2.9745 Iteration: 2655; Percent complete: 66.4%; Average loss: 3.2552 Iteration: 2656; Percent complete: 66.4%; Average loss: 3.0038 Iteration: 2657; Percent complete: 66.4%; Average loss: 2.9362 Iteration: 2658; Percent complete: 66.5%; Average loss: 2.8249 Iteration: 2659; Percent complete: 66.5%; Average loss: 3.0315 Iteration: 2660; Percent complete: 66.5%; Average loss: 3.2433 Iteration: 2661; Percent complete: 66.5%; Average loss: 2.8465 Iteration: 2662; Percent complete: 66.5%; Average loss: 3.0026 Iteration: 2663; Percent complete: 66.6%; Average loss: 2.7188 Iteration: 2664; Percent complete: 66.6%; Average loss: 2.9460 Iteration: 2665; Percent complete: 66.6%; Average loss: 3.0766 Iteration: 2666; Percent complete: 66.6%; Average loss: 2.9762 Iteration: 2667; Percent complete: 66.7%; Average loss: 2.9885 Iteration: 2668; Percent complete: 66.7%; Average loss: 2.9136 Iteration: 2669; Percent complete: 66.7%; Average loss: 2.9530 Iteration: 2670; Percent complete: 66.8%; Average loss: 3.1029 Iteration: 2671; Percent complete: 66.8%; Average loss: 2.9581 Iteration: 2672; Percent complete: 66.8%; Average loss: 2.8751 Iteration: 2673; Percent complete: 66.8%; Average loss: 3.0228 Iteration: 2674; Percent complete: 66.8%; Average loss: 2.9980 Iteration: 2675; Percent complete: 66.9%; Average loss: 3.1778 Iteration: 2676; Percent complete: 66.9%; Average loss: 3.0110 Iteration: 2677; Percent complete: 66.9%; Average loss: 2.9999 Iteration: 2678; Percent complete: 67.0%; Average loss: 2.8637 Iteration: 2679; Percent complete: 67.0%; Average loss: 2.7602 Iteration: 2680; Percent complete: 67.0%; Average loss: 2.9435 Iteration: 2681; Percent complete: 67.0%; Average loss: 2.9638 Iteration: 2682; Percent complete: 67.0%; Average loss: 2.9956 Iteration: 2683; Percent complete: 67.1%; Average loss: 2.6761 Iteration: 2684; Percent complete: 67.1%; Average loss: 2.9263 Iteration: 2685; Percent complete: 67.1%; Average loss: 2.8377 Iteration: 2686; Percent complete: 67.2%; Average loss: 2.8527 Iteration: 2687; Percent complete: 67.2%; Average loss: 2.9922 Iteration: 2688; Percent complete: 67.2%; Average loss: 2.9717 Iteration: 2689; Percent complete: 67.2%; Average loss: 3.0173 Iteration: 2690; Percent complete: 67.2%; Average loss: 2.9750 Iteration: 2691; Percent complete: 67.3%; Average loss: 3.0568 Iteration: 2692; Percent complete: 67.3%; Average loss: 2.9489 Iteration: 2693; Percent complete: 67.3%; Average loss: 3.0926 Iteration: 2694; Percent complete: 67.3%; Average loss: 2.7249 Iteration: 2695; Percent complete: 67.4%; Average loss: 3.0322 Iteration: 2696; Percent complete: 67.4%; Average loss: 2.9278 Iteration: 2697; Percent complete: 67.4%; Average loss: 2.8933 Iteration: 2698; Percent complete: 67.5%; Average loss: 3.0013 Iteration: 2699; Percent complete: 67.5%; Average loss: 2.9842 Iteration: 2700; Percent complete: 67.5%; Average loss: 2.8574 Iteration: 2701; Percent complete: 67.5%; Average loss: 2.9005 Iteration: 2702; Percent complete: 67.5%; Average loss: 2.7546 Iteration: 2703; Percent complete: 67.6%; Average loss: 2.9167 Iteration: 2704; Percent complete: 67.6%; Average loss: 2.9049 Iteration: 2705; Percent complete: 67.6%; Average loss: 3.0145 Iteration: 2706; Percent complete: 67.7%; Average loss: 2.8532 Iteration: 2707; Percent complete: 67.7%; Average loss: 3.0567 Iteration: 2708; Percent complete: 67.7%; Average loss: 3.2285 Iteration: 2709; Percent complete: 67.7%; Average loss: 2.8267 Iteration: 2710; Percent complete: 67.8%; Average loss: 3.0982 Iteration: 2711; Percent complete: 67.8%; Average loss: 2.8877 Iteration: 2712; Percent complete: 67.8%; Average loss: 2.9180 Iteration: 2713; Percent complete: 67.8%; Average loss: 3.1380 Iteration: 2714; Percent complete: 67.8%; Average loss: 2.7569 Iteration: 2715; Percent complete: 67.9%; Average loss: 3.1548 Iteration: 2716; Percent complete: 67.9%; Average loss: 2.8605 Iteration: 2717; Percent complete: 67.9%; Average loss: 3.0388 Iteration: 2718; Percent complete: 68.0%; Average loss: 2.8688 Iteration: 2719; Percent complete: 68.0%; Average loss: 2.9155 Iteration: 2720; Percent complete: 68.0%; Average loss: 2.9601 Iteration: 2721; Percent complete: 68.0%; Average loss: 3.1063 Iteration: 2722; Percent complete: 68.0%; Average loss: 2.9606 Iteration: 2723; Percent complete: 68.1%; Average loss: 3.0425 Iteration: 2724; Percent complete: 68.1%; Average loss: 3.0224 Iteration: 2725; Percent complete: 68.1%; Average loss: 2.9436 Iteration: 2726; Percent complete: 68.2%; Average loss: 2.9152 Iteration: 2727; Percent complete: 68.2%; Average loss: 2.7811 Iteration: 2728; Percent complete: 68.2%; Average loss: 2.9955 Iteration: 2729; Percent complete: 68.2%; Average loss: 3.0642 Iteration: 2730; Percent complete: 68.2%; Average loss: 2.7513 Iteration: 2731; Percent complete: 68.3%; Average loss: 2.9346 Iteration: 2732; Percent complete: 68.3%; Average loss: 2.8338 Iteration: 2733; Percent complete: 68.3%; Average loss: 2.9069 Iteration: 2734; Percent complete: 68.3%; Average loss: 3.0149 Iteration: 2735; Percent complete: 68.4%; Average loss: 3.1079 Iteration: 2736; Percent complete: 68.4%; Average loss: 2.9963 Iteration: 2737; Percent complete: 68.4%; Average loss: 2.9656 Iteration: 2738; Percent complete: 68.5%; Average loss: 3.0681 Iteration: 2739; Percent complete: 68.5%; Average loss: 2.9722 Iteration: 2740; Percent complete: 68.5%; Average loss: 2.9638 Iteration: 2741; Percent complete: 68.5%; Average loss: 3.0068 Iteration: 2742; Percent complete: 68.5%; Average loss: 2.8337 Iteration: 2743; Percent complete: 68.6%; Average loss: 2.9058 Iteration: 2744; Percent complete: 68.6%; Average loss: 3.0459 Iteration: 2745; Percent complete: 68.6%; Average loss: 2.5502 Iteration: 2746; Percent complete: 68.7%; Average loss: 3.0915 Iteration: 2747; Percent complete: 68.7%; Average loss: 2.9855 Iteration: 2748; Percent complete: 68.7%; Average loss: 2.9987 Iteration: 2749; Percent complete: 68.7%; Average loss: 2.8893 Iteration: 2750; Percent complete: 68.8%; Average loss: 2.7861 Iteration: 2751; Percent complete: 68.8%; Average loss: 3.0766 Iteration: 2752; Percent complete: 68.8%; Average loss: 2.8763 Iteration: 2753; Percent complete: 68.8%; Average loss: 3.0608 Iteration: 2754; Percent complete: 68.8%; Average loss: 2.8179 Iteration: 2755; Percent complete: 68.9%; Average loss: 2.5586 Iteration: 2756; Percent complete: 68.9%; Average loss: 2.8236 Iteration: 2757; Percent complete: 68.9%; Average loss: 3.0911 Iteration: 2758; Percent complete: 69.0%; Average loss: 3.0338 Iteration: 2759; Percent complete: 69.0%; Average loss: 2.9071 Iteration: 2760; Percent complete: 69.0%; Average loss: 3.0498 Iteration: 2761; Percent complete: 69.0%; Average loss: 2.9858 Iteration: 2762; Percent complete: 69.0%; Average loss: 2.8432 Iteration: 2763; Percent complete: 69.1%; Average loss: 2.9627 Iteration: 2764; Percent complete: 69.1%; Average loss: 3.0823 Iteration: 2765; Percent complete: 69.1%; Average loss: 2.8110 Iteration: 2766; Percent complete: 69.2%; Average loss: 3.2241 Iteration: 2767; Percent complete: 69.2%; Average loss: 2.8854 Iteration: 2768; Percent complete: 69.2%; Average loss: 2.9809 Iteration: 2769; Percent complete: 69.2%; Average loss: 2.6752 Iteration: 2770; Percent complete: 69.2%; Average loss: 2.7684 Iteration: 2771; Percent complete: 69.3%; Average loss: 2.6598 Iteration: 2772; Percent complete: 69.3%; Average loss: 3.0240 Iteration: 2773; Percent complete: 69.3%; Average loss: 2.9068 Iteration: 2774; Percent complete: 69.3%; Average loss: 2.9570 Iteration: 2775; Percent complete: 69.4%; Average loss: 2.9165 Iteration: 2776; Percent complete: 69.4%; Average loss: 2.9637 Iteration: 2777; Percent complete: 69.4%; Average loss: 3.2319 Iteration: 2778; Percent complete: 69.5%; Average loss: 3.2035 Iteration: 2779; Percent complete: 69.5%; Average loss: 2.9817 Iteration: 2780; Percent complete: 69.5%; Average loss: 2.9455 Iteration: 2781; Percent complete: 69.5%; Average loss: 2.9081 Iteration: 2782; Percent complete: 69.5%; Average loss: 2.9686 Iteration: 2783; Percent complete: 69.6%; Average loss: 2.8813 Iteration: 2784; Percent complete: 69.6%; Average loss: 3.0033 Iteration: 2785; Percent complete: 69.6%; Average loss: 2.8987 Iteration: 2786; Percent complete: 69.7%; Average loss: 3.1428 Iteration: 2787; Percent complete: 69.7%; Average loss: 2.8328 Iteration: 2788; Percent complete: 69.7%; Average loss: 2.8144 Iteration: 2789; Percent complete: 69.7%; Average loss: 3.1540 Iteration: 2790; Percent complete: 69.8%; Average loss: 3.1196 Iteration: 2791; Percent complete: 69.8%; Average loss: 3.0040 Iteration: 2792; Percent complete: 69.8%; Average loss: 3.0600 Iteration: 2793; Percent complete: 69.8%; Average loss: 2.9361 Iteration: 2794; Percent complete: 69.8%; Average loss: 3.0799 Iteration: 2795; Percent complete: 69.9%; Average loss: 3.0402 Iteration: 2796; Percent complete: 69.9%; Average loss: 2.7618 Iteration: 2797; Percent complete: 69.9%; Average loss: 2.7964 Iteration: 2798; Percent complete: 70.0%; Average loss: 2.8617 Iteration: 2799; Percent complete: 70.0%; Average loss: 2.9878 Iteration: 2800; Percent complete: 70.0%; Average loss: 2.7504 Iteration: 2801; Percent complete: 70.0%; Average loss: 2.8580 Iteration: 2802; Percent complete: 70.0%; Average loss: 2.8685 Iteration: 2803; Percent complete: 70.1%; Average loss: 2.7516 Iteration: 2804; Percent complete: 70.1%; Average loss: 2.7257 Iteration: 2805; Percent complete: 70.1%; Average loss: 2.9471 Iteration: 2806; Percent complete: 70.2%; Average loss: 2.8982 Iteration: 2807; Percent complete: 70.2%; Average loss: 2.9259 Iteration: 2808; Percent complete: 70.2%; Average loss: 3.0059 Iteration: 2809; Percent complete: 70.2%; Average loss: 2.7959 Iteration: 2810; Percent complete: 70.2%; Average loss: 2.6522 Iteration: 2811; Percent complete: 70.3%; Average loss: 2.9103 Iteration: 2812; Percent complete: 70.3%; Average loss: 3.0946 Iteration: 2813; Percent complete: 70.3%; Average loss: 2.8603 Iteration: 2814; Percent complete: 70.3%; Average loss: 2.8077 Iteration: 2815; Percent complete: 70.4%; Average loss: 2.9464 Iteration: 2816; Percent complete: 70.4%; Average loss: 2.8726 Iteration: 2817; Percent complete: 70.4%; Average loss: 3.2415 Iteration: 2818; Percent complete: 70.5%; Average loss: 2.9972 Iteration: 2819; Percent complete: 70.5%; Average loss: 2.8944 Iteration: 2820; Percent complete: 70.5%; Average loss: 3.0370 Iteration: 2821; Percent complete: 70.5%; Average loss: 2.8722 Iteration: 2822; Percent complete: 70.5%; Average loss: 3.0710 Iteration: 2823; Percent complete: 70.6%; Average loss: 2.7812 Iteration: 2824; Percent complete: 70.6%; Average loss: 2.9932 Iteration: 2825; Percent complete: 70.6%; Average loss: 3.0553 Iteration: 2826; Percent complete: 70.7%; Average loss: 3.1295 Iteration: 2827; Percent complete: 70.7%; Average loss: 3.0668 Iteration: 2828; Percent complete: 70.7%; Average loss: 3.0899 Iteration: 2829; Percent complete: 70.7%; Average loss: 2.7914 Iteration: 2830; Percent complete: 70.8%; Average loss: 2.8199 Iteration: 2831; Percent complete: 70.8%; Average loss: 2.9094 Iteration: 2832; Percent complete: 70.8%; Average loss: 2.9319 Iteration: 2833; Percent complete: 70.8%; Average loss: 3.0194 Iteration: 2834; Percent complete: 70.9%; Average loss: 2.6769 Iteration: 2835; Percent complete: 70.9%; Average loss: 2.8394 Iteration: 2836; Percent complete: 70.9%; Average loss: 3.0220 Iteration: 2837; Percent complete: 70.9%; Average loss: 2.8327 Iteration: 2838; Percent complete: 71.0%; Average loss: 2.7978 Iteration: 2839; Percent complete: 71.0%; Average loss: 2.8368 Iteration: 2840; Percent complete: 71.0%; Average loss: 2.9183 Iteration: 2841; Percent complete: 71.0%; Average loss: 3.0764 Iteration: 2842; Percent complete: 71.0%; Average loss: 2.9923 Iteration: 2843; Percent complete: 71.1%; Average loss: 2.9126 Iteration: 2844; Percent complete: 71.1%; Average loss: 2.7952 Iteration: 2845; Percent complete: 71.1%; Average loss: 3.1327 Iteration: 2846; Percent complete: 71.2%; Average loss: 2.9010 Iteration: 2847; Percent complete: 71.2%; Average loss: 2.7796 Iteration: 2848; Percent complete: 71.2%; Average loss: 3.0164 Iteration: 2849; Percent complete: 71.2%; Average loss: 2.9841 Iteration: 2850; Percent complete: 71.2%; Average loss: 2.9307 Iteration: 2851; Percent complete: 71.3%; Average loss: 2.9832 Iteration: 2852; Percent complete: 71.3%; Average loss: 2.9146 Iteration: 2853; Percent complete: 71.3%; Average loss: 2.8312 Iteration: 2854; Percent complete: 71.4%; Average loss: 2.9593 Iteration: 2855; Percent complete: 71.4%; Average loss: 2.7897 Iteration: 2856; Percent complete: 71.4%; Average loss: 2.8131 Iteration: 2857; Percent complete: 71.4%; Average loss: 2.8403 Iteration: 2858; Percent complete: 71.5%; Average loss: 2.7888 Iteration: 2859; Percent complete: 71.5%; Average loss: 2.8594 Iteration: 2860; Percent complete: 71.5%; Average loss: 2.8666 Iteration: 2861; Percent complete: 71.5%; Average loss: 2.9241 Iteration: 2862; Percent complete: 71.5%; Average loss: 2.7550 Iteration: 2863; Percent complete: 71.6%; Average loss: 2.7998 Iteration: 2864; Percent complete: 71.6%; Average loss: 3.0297 Iteration: 2865; Percent complete: 71.6%; Average loss: 2.8772 Iteration: 2866; Percent complete: 71.7%; Average loss: 3.0335 Iteration: 2867; Percent complete: 71.7%; Average loss: 2.9476 Iteration: 2868; Percent complete: 71.7%; Average loss: 3.0405 Iteration: 2869; Percent complete: 71.7%; Average loss: 2.7103 Iteration: 2870; Percent complete: 71.8%; Average loss: 2.8975 Iteration: 2871; Percent complete: 71.8%; Average loss: 3.2626 Iteration: 2872; Percent complete: 71.8%; Average loss: 2.6274 Iteration: 2873; Percent complete: 71.8%; Average loss: 3.0785 Iteration: 2874; Percent complete: 71.9%; Average loss: 3.1895 Iteration: 2875; Percent complete: 71.9%; Average loss: 2.9454 Iteration: 2876; Percent complete: 71.9%; Average loss: 2.9936 Iteration: 2877; Percent complete: 71.9%; Average loss: 2.6867 Iteration: 2878; Percent complete: 72.0%; Average loss: 2.8551 Iteration: 2879; Percent complete: 72.0%; Average loss: 3.0061 Iteration: 2880; Percent complete: 72.0%; Average loss: 2.8279 Iteration: 2881; Percent complete: 72.0%; Average loss: 2.9551 Iteration: 2882; Percent complete: 72.0%; Average loss: 3.1905 Iteration: 2883; Percent complete: 72.1%; Average loss: 2.7876 Iteration: 2884; Percent complete: 72.1%; Average loss: 2.7943 Iteration: 2885; Percent complete: 72.1%; Average loss: 3.0628 Iteration: 2886; Percent complete: 72.2%; Average loss: 2.9779 Iteration: 2887; Percent complete: 72.2%; Average loss: 3.0812 Iteration: 2888; Percent complete: 72.2%; Average loss: 2.9815 Iteration: 2889; Percent complete: 72.2%; Average loss: 2.8360 Iteration: 2890; Percent complete: 72.2%; Average loss: 2.7097 Iteration: 2891; Percent complete: 72.3%; Average loss: 2.9955 Iteration: 2892; Percent complete: 72.3%; Average loss: 2.9710 Iteration: 2893; Percent complete: 72.3%; Average loss: 2.9034 Iteration: 2894; Percent complete: 72.4%; Average loss: 2.8781 Iteration: 2895; Percent complete: 72.4%; Average loss: 2.9648 Iteration: 2896; Percent complete: 72.4%; Average loss: 2.8218 Iteration: 2897; Percent complete: 72.4%; Average loss: 2.9542 Iteration: 2898; Percent complete: 72.5%; Average loss: 2.8892 Iteration: 2899; Percent complete: 72.5%; Average loss: 3.1090 Iteration: 2900; Percent complete: 72.5%; Average loss: 2.9539 Iteration: 2901; Percent complete: 72.5%; Average loss: 2.9935 Iteration: 2902; Percent complete: 72.5%; Average loss: 2.6629 Iteration: 2903; Percent complete: 72.6%; Average loss: 2.9627 Iteration: 2904; Percent complete: 72.6%; Average loss: 2.8354 Iteration: 2905; Percent complete: 72.6%; Average loss: 2.9876 Iteration: 2906; Percent complete: 72.7%; Average loss: 2.8022 Iteration: 2907; Percent complete: 72.7%; Average loss: 3.1498 Iteration: 2908; Percent complete: 72.7%; Average loss: 2.9000 Iteration: 2909; Percent complete: 72.7%; Average loss: 3.0950 Iteration: 2910; Percent complete: 72.8%; Average loss: 2.7017 Iteration: 2911; Percent complete: 72.8%; Average loss: 2.8750 Iteration: 2912; Percent complete: 72.8%; Average loss: 2.9354 Iteration: 2913; Percent complete: 72.8%; Average loss: 3.0945 Iteration: 2914; Percent complete: 72.9%; Average loss: 3.0731 Iteration: 2915; Percent complete: 72.9%; Average loss: 2.9518 Iteration: 2916; Percent complete: 72.9%; Average loss: 2.8993 Iteration: 2917; Percent complete: 72.9%; Average loss: 2.7700 Iteration: 2918; Percent complete: 73.0%; Average loss: 2.9766 Iteration: 2919; Percent complete: 73.0%; Average loss: 2.7121 Iteration: 2920; Percent complete: 73.0%; Average loss: 2.9228 Iteration: 2921; Percent complete: 73.0%; Average loss: 3.0062 Iteration: 2922; Percent complete: 73.0%; Average loss: 2.9907 Iteration: 2923; Percent complete: 73.1%; Average loss: 2.7383 Iteration: 2924; Percent complete: 73.1%; Average loss: 2.8742 Iteration: 2925; Percent complete: 73.1%; Average loss: 2.7704 Iteration: 2926; Percent complete: 73.2%; Average loss: 3.0878 Iteration: 2927; Percent complete: 73.2%; Average loss: 3.0383 Iteration: 2928; Percent complete: 73.2%; Average loss: 3.0491 Iteration: 2929; Percent complete: 73.2%; Average loss: 2.8969 Iteration: 2930; Percent complete: 73.2%; Average loss: 2.8859 Iteration: 2931; Percent complete: 73.3%; Average loss: 2.8969 Iteration: 2932; Percent complete: 73.3%; Average loss: 3.0829 Iteration: 2933; Percent complete: 73.3%; Average loss: 2.8648 Iteration: 2934; Percent complete: 73.4%; Average loss: 2.7475 Iteration: 2935; Percent complete: 73.4%; Average loss: 3.4020 Iteration: 2936; Percent complete: 73.4%; Average loss: 2.7819 Iteration: 2937; Percent complete: 73.4%; Average loss: 2.9354 Iteration: 2938; Percent complete: 73.5%; Average loss: 2.8160 Iteration: 2939; Percent complete: 73.5%; Average loss: 2.7817 Iteration: 2940; Percent complete: 73.5%; Average loss: 2.6799 Iteration: 2941; Percent complete: 73.5%; Average loss: 2.9063 Iteration: 2942; Percent complete: 73.6%; Average loss: 3.0292 Iteration: 2943; Percent complete: 73.6%; Average loss: 2.6448 Iteration: 2944; Percent complete: 73.6%; Average loss: 3.1482 Iteration: 2945; Percent complete: 73.6%; Average loss: 2.8997 Iteration: 2946; Percent complete: 73.7%; Average loss: 2.9586 Iteration: 2947; Percent complete: 73.7%; Average loss: 2.8941 Iteration: 2948; Percent complete: 73.7%; Average loss: 2.8444 Iteration: 2949; Percent complete: 73.7%; Average loss: 3.0576 Iteration: 2950; Percent complete: 73.8%; Average loss: 3.0507 Iteration: 2951; Percent complete: 73.8%; Average loss: 2.7605 Iteration: 2952; Percent complete: 73.8%; Average loss: 3.0545 Iteration: 2953; Percent complete: 73.8%; Average loss: 2.6876 Iteration: 2954; Percent complete: 73.9%; Average loss: 2.8494 Iteration: 2955; Percent complete: 73.9%; Average loss: 3.0959 Iteration: 2956; Percent complete: 73.9%; Average loss: 2.7438 Iteration: 2957; Percent complete: 73.9%; Average loss: 2.7802 Iteration: 2958; Percent complete: 74.0%; Average loss: 2.5535 Iteration: 2959; Percent complete: 74.0%; Average loss: 3.0118 Iteration: 2960; Percent complete: 74.0%; Average loss: 2.7613 Iteration: 2961; Percent complete: 74.0%; Average loss: 2.8696 Iteration: 2962; Percent complete: 74.1%; Average loss: 2.9938 Iteration: 2963; Percent complete: 74.1%; Average loss: 2.9215 Iteration: 2964; Percent complete: 74.1%; Average loss: 2.9053 Iteration: 2965; Percent complete: 74.1%; Average loss: 3.0314 Iteration: 2966; Percent complete: 74.2%; Average loss: 2.9922 Iteration: 2967; Percent complete: 74.2%; Average loss: 3.0080 Iteration: 2968; Percent complete: 74.2%; Average loss: 2.8914 Iteration: 2969; Percent complete: 74.2%; Average loss: 2.7259 Iteration: 2970; Percent complete: 74.2%; Average loss: 2.7464 Iteration: 2971; Percent complete: 74.3%; Average loss: 3.0716 Iteration: 2972; Percent complete: 74.3%; Average loss: 3.0906 Iteration: 2973; Percent complete: 74.3%; Average loss: 3.0441 Iteration: 2974; Percent complete: 74.4%; Average loss: 2.9178 Iteration: 2975; Percent complete: 74.4%; Average loss: 2.8915 Iteration: 2976; Percent complete: 74.4%; Average loss: 2.6867 Iteration: 2977; Percent complete: 74.4%; Average loss: 2.9638 Iteration: 2978; Percent complete: 74.5%; Average loss: 2.7096 Iteration: 2979; Percent complete: 74.5%; Average loss: 2.7930 Iteration: 2980; Percent complete: 74.5%; Average loss: 3.0344 Iteration: 2981; Percent complete: 74.5%; Average loss: 2.9932 Iteration: 2982; Percent complete: 74.6%; Average loss: 2.9410 Iteration: 2983; Percent complete: 74.6%; Average loss: 2.6728 Iteration: 2984; Percent complete: 74.6%; Average loss: 2.6244 Iteration: 2985; Percent complete: 74.6%; Average loss: 2.9220 Iteration: 2986; Percent complete: 74.7%; Average loss: 2.8158 Iteration: 2987; Percent complete: 74.7%; Average loss: 3.0462 Iteration: 2988; Percent complete: 74.7%; Average loss: 2.8750 Iteration: 2989; Percent complete: 74.7%; Average loss: 2.6633 Iteration: 2990; Percent complete: 74.8%; Average loss: 3.1500 Iteration: 2991; Percent complete: 74.8%; Average loss: 2.8159 Iteration: 2992; Percent complete: 74.8%; Average loss: 2.7437 Iteration: 2993; Percent complete: 74.8%; Average loss: 2.9685 Iteration: 2994; Percent complete: 74.9%; Average loss: 2.9506 Iteration: 2995; Percent complete: 74.9%; Average loss: 3.0977 Iteration: 2996; Percent complete: 74.9%; Average loss: 3.0489 Iteration: 2997; Percent complete: 74.9%; Average loss: 3.0138 Iteration: 2998; Percent complete: 75.0%; Average loss: 2.9891 Iteration: 2999; Percent complete: 75.0%; Average loss: 2.9733 Iteration: 3000; Percent complete: 75.0%; Average loss: 2.9357 Iteration: 3001; Percent complete: 75.0%; Average loss: 2.8092 Iteration: 3002; Percent complete: 75.0%; Average loss: 2.8705 Iteration: 3003; Percent complete: 75.1%; Average loss: 2.9608 Iteration: 3004; Percent complete: 75.1%; Average loss: 2.8782 Iteration: 3005; Percent complete: 75.1%; Average loss: 2.9570 Iteration: 3006; Percent complete: 75.1%; Average loss: 2.8634 Iteration: 3007; Percent complete: 75.2%; Average loss: 2.8878 Iteration: 3008; Percent complete: 75.2%; Average loss: 2.8199 Iteration: 3009; Percent complete: 75.2%; Average loss: 3.0053 Iteration: 3010; Percent complete: 75.2%; Average loss: 2.9324 Iteration: 3011; Percent complete: 75.3%; Average loss: 2.9002 Iteration: 3012; Percent complete: 75.3%; Average loss: 3.0516 Iteration: 3013; Percent complete: 75.3%; Average loss: 2.9159 Iteration: 3014; Percent complete: 75.3%; Average loss: 2.9177 Iteration: 3015; Percent complete: 75.4%; Average loss: 2.9223 Iteration: 3016; Percent complete: 75.4%; Average loss: 2.9078 Iteration: 3017; Percent complete: 75.4%; Average loss: 3.0318 Iteration: 3018; Percent complete: 75.4%; Average loss: 2.9476 Iteration: 3019; Percent complete: 75.5%; Average loss: 2.8563 Iteration: 3020; Percent complete: 75.5%; Average loss: 2.9087 Iteration: 3021; Percent complete: 75.5%; Average loss: 3.1355 Iteration: 3022; Percent complete: 75.5%; Average loss: 2.9919 Iteration: 3023; Percent complete: 75.6%; Average loss: 2.8395 Iteration: 3024; Percent complete: 75.6%; Average loss: 2.9893 Iteration: 3025; Percent complete: 75.6%; Average loss: 3.2082 Iteration: 3026; Percent complete: 75.6%; Average loss: 2.8194 Iteration: 3027; Percent complete: 75.7%; Average loss: 2.6423 Iteration: 3028; Percent complete: 75.7%; Average loss: 2.9622 Iteration: 3029; Percent complete: 75.7%; Average loss: 2.7897 Iteration: 3030; Percent complete: 75.8%; Average loss: 2.7169 Iteration: 3031; Percent complete: 75.8%; Average loss: 2.7786 Iteration: 3032; Percent complete: 75.8%; Average loss: 2.8090 Iteration: 3033; Percent complete: 75.8%; Average loss: 3.0403 Iteration: 3034; Percent complete: 75.8%; Average loss: 2.6833 Iteration: 3035; Percent complete: 75.9%; Average loss: 2.7725 Iteration: 3036; Percent complete: 75.9%; Average loss: 2.7909 Iteration: 3037; Percent complete: 75.9%; Average loss: 2.9215 Iteration: 3038; Percent complete: 75.9%; Average loss: 2.8880 Iteration: 3039; Percent complete: 76.0%; Average loss: 3.1363 Iteration: 3040; Percent complete: 76.0%; Average loss: 2.9210 Iteration: 3041; Percent complete: 76.0%; Average loss: 3.1818 Iteration: 3042; Percent complete: 76.0%; Average loss: 2.8843 Iteration: 3043; Percent complete: 76.1%; Average loss: 2.6322 Iteration: 3044; Percent complete: 76.1%; Average loss: 2.8226 Iteration: 3045; Percent complete: 76.1%; Average loss: 2.7612 Iteration: 3046; Percent complete: 76.1%; Average loss: 2.8491 Iteration: 3047; Percent complete: 76.2%; Average loss: 2.8252 Iteration: 3048; Percent complete: 76.2%; Average loss: 2.8672 Iteration: 3049; Percent complete: 76.2%; Average loss: 3.0725 Iteration: 3050; Percent complete: 76.2%; Average loss: 2.6355 Iteration: 3051; Percent complete: 76.3%; Average loss: 3.0267 Iteration: 3052; Percent complete: 76.3%; Average loss: 2.7756 Iteration: 3053; Percent complete: 76.3%; Average loss: 2.8999 Iteration: 3054; Percent complete: 76.3%; Average loss: 2.7297 Iteration: 3055; Percent complete: 76.4%; Average loss: 2.9017 Iteration: 3056; Percent complete: 76.4%; Average loss: 2.8614 Iteration: 3057; Percent complete: 76.4%; Average loss: 2.9103 Iteration: 3058; Percent complete: 76.4%; Average loss: 2.9228 Iteration: 3059; Percent complete: 76.5%; Average loss: 2.9057 Iteration: 3060; Percent complete: 76.5%; Average loss: 2.8180 Iteration: 3061; Percent complete: 76.5%; Average loss: 2.8708 Iteration: 3062; Percent complete: 76.5%; Average loss: 2.8121 Iteration: 3063; Percent complete: 76.6%; Average loss: 2.8796 Iteration: 3064; Percent complete: 76.6%; Average loss: 2.8903 Iteration: 3065; Percent complete: 76.6%; Average loss: 2.9282 Iteration: 3066; Percent complete: 76.6%; Average loss: 2.7941 Iteration: 3067; Percent complete: 76.7%; Average loss: 2.8311 Iteration: 3068; Percent complete: 76.7%; Average loss: 2.7265 Iteration: 3069; Percent complete: 76.7%; Average loss: 2.8328 Iteration: 3070; Percent complete: 76.8%; Average loss: 2.6190 Iteration: 3071; Percent complete: 76.8%; Average loss: 2.8061 Iteration: 3072; Percent complete: 76.8%; Average loss: 2.8321 Iteration: 3073; Percent complete: 76.8%; Average loss: 3.2295 Iteration: 3074; Percent complete: 76.8%; Average loss: 2.9620 Iteration: 3075; Percent complete: 76.9%; Average loss: 2.6073 Iteration: 3076; Percent complete: 76.9%; Average loss: 2.6937 Iteration: 3077; Percent complete: 76.9%; Average loss: 2.7879 Iteration: 3078; Percent complete: 77.0%; Average loss: 2.7540 Iteration: 3079; Percent complete: 77.0%; Average loss: 2.8467 Iteration: 3080; Percent complete: 77.0%; Average loss: 2.9061 Iteration: 3081; Percent complete: 77.0%; Average loss: 2.8316 Iteration: 3082; Percent complete: 77.0%; Average loss: 2.6672 Iteration: 3083; Percent complete: 77.1%; Average loss: 2.8990 Iteration: 3084; Percent complete: 77.1%; Average loss: 2.7778 Iteration: 3085; Percent complete: 77.1%; Average loss: 2.9400 Iteration: 3086; Percent complete: 77.1%; Average loss: 2.6451 Iteration: 3087; Percent complete: 77.2%; Average loss: 2.6923 Iteration: 3088; Percent complete: 77.2%; Average loss: 2.8971 Iteration: 3089; Percent complete: 77.2%; Average loss: 2.7108 Iteration: 3090; Percent complete: 77.2%; Average loss: 2.9312 Iteration: 3091; Percent complete: 77.3%; Average loss: 2.8065 Iteration: 3092; Percent complete: 77.3%; Average loss: 2.6729 Iteration: 3093; Percent complete: 77.3%; Average loss: 2.7770 Iteration: 3094; Percent complete: 77.3%; Average loss: 2.9211 Iteration: 3095; Percent complete: 77.4%; Average loss: 2.8852 Iteration: 3096; Percent complete: 77.4%; Average loss: 2.7850 Iteration: 3097; Percent complete: 77.4%; Average loss: 2.7957 Iteration: 3098; Percent complete: 77.5%; Average loss: 2.7621 Iteration: 3099; Percent complete: 77.5%; Average loss: 2.8050 Iteration: 3100; Percent complete: 77.5%; Average loss: 2.7617 Iteration: 3101; Percent complete: 77.5%; Average loss: 2.9096 Iteration: 3102; Percent complete: 77.5%; Average loss: 2.7998 Iteration: 3103; Percent complete: 77.6%; Average loss: 2.7245 Iteration: 3104; Percent complete: 77.6%; Average loss: 2.9060 Iteration: 3105; Percent complete: 77.6%; Average loss: 2.7620 Iteration: 3106; Percent complete: 77.6%; Average loss: 3.0188 Iteration: 3107; Percent complete: 77.7%; Average loss: 2.6290 Iteration: 3108; Percent complete: 77.7%; Average loss: 2.7612 Iteration: 3109; Percent complete: 77.7%; Average loss: 2.8992 Iteration: 3110; Percent complete: 77.8%; Average loss: 3.1011 Iteration: 3111; Percent complete: 77.8%; Average loss: 2.8471 Iteration: 3112; Percent complete: 77.8%; Average loss: 2.8335 Iteration: 3113; Percent complete: 77.8%; Average loss: 2.9999 Iteration: 3114; Percent complete: 77.8%; Average loss: 3.1837 Iteration: 3115; Percent complete: 77.9%; Average loss: 2.7911 Iteration: 3116; Percent complete: 77.9%; Average loss: 2.8297 Iteration: 3117; Percent complete: 77.9%; Average loss: 2.8399 Iteration: 3118; Percent complete: 78.0%; Average loss: 2.7940 Iteration: 3119; Percent complete: 78.0%; Average loss: 2.6527 Iteration: 3120; Percent complete: 78.0%; Average loss: 2.9140 Iteration: 3121; Percent complete: 78.0%; Average loss: 2.6164 Iteration: 3122; Percent complete: 78.0%; Average loss: 3.0743 Iteration: 3123; Percent complete: 78.1%; Average loss: 2.6136 Iteration: 3124; Percent complete: 78.1%; Average loss: 2.7134 Iteration: 3125; Percent complete: 78.1%; Average loss: 2.7049 Iteration: 3126; Percent complete: 78.1%; Average loss: 2.8565 Iteration: 3127; Percent complete: 78.2%; Average loss: 2.9278 Iteration: 3128; Percent complete: 78.2%; Average loss: 2.7772 Iteration: 3129; Percent complete: 78.2%; Average loss: 2.8107 Iteration: 3130; Percent complete: 78.2%; Average loss: 2.8486 Iteration: 3131; Percent complete: 78.3%; Average loss: 2.7065 Iteration: 3132; Percent complete: 78.3%; Average loss: 2.6814 Iteration: 3133; Percent complete: 78.3%; Average loss: 2.8686 Iteration: 3134; Percent complete: 78.3%; Average loss: 3.1442 Iteration: 3135; Percent complete: 78.4%; Average loss: 2.8334 Iteration: 3136; Percent complete: 78.4%; Average loss: 2.7880 Iteration: 3137; Percent complete: 78.4%; Average loss: 2.9166 Iteration: 3138; Percent complete: 78.5%; Average loss: 2.7405 Iteration: 3139; Percent complete: 78.5%; Average loss: 3.0819 Iteration: 3140; Percent complete: 78.5%; Average loss: 2.9483 Iteration: 3141; Percent complete: 78.5%; Average loss: 2.7527 Iteration: 3142; Percent complete: 78.5%; Average loss: 2.8566 Iteration: 3143; Percent complete: 78.6%; Average loss: 2.8262 Iteration: 3144; Percent complete: 78.6%; Average loss: 2.8635 Iteration: 3145; Percent complete: 78.6%; Average loss: 2.7892 Iteration: 3146; Percent complete: 78.6%; Average loss: 2.7291 Iteration: 3147; Percent complete: 78.7%; Average loss: 2.9425 Iteration: 3148; Percent complete: 78.7%; Average loss: 2.6925 Iteration: 3149; Percent complete: 78.7%; Average loss: 2.6886 Iteration: 3150; Percent complete: 78.8%; Average loss: 2.8279 Iteration: 3151; Percent complete: 78.8%; Average loss: 2.7638 Iteration: 3152; Percent complete: 78.8%; Average loss: 2.7155 Iteration: 3153; Percent complete: 78.8%; Average loss: 2.6350 Iteration: 3154; Percent complete: 78.8%; Average loss: 3.0614 Iteration: 3155; Percent complete: 78.9%; Average loss: 2.8030 Iteration: 3156; Percent complete: 78.9%; Average loss: 2.8196 Iteration: 3157; Percent complete: 78.9%; Average loss: 2.8922 Iteration: 3158; Percent complete: 79.0%; Average loss: 2.8866 Iteration: 3159; Percent complete: 79.0%; Average loss: 2.8563 Iteration: 3160; Percent complete: 79.0%; Average loss: 2.7527 Iteration: 3161; Percent complete: 79.0%; Average loss: 2.8517 Iteration: 3162; Percent complete: 79.0%; Average loss: 2.5093 Iteration: 3163; Percent complete: 79.1%; Average loss: 3.0855 Iteration: 3164; Percent complete: 79.1%; Average loss: 2.8036 Iteration: 3165; Percent complete: 79.1%; Average loss: 3.0055 Iteration: 3166; Percent complete: 79.1%; Average loss: 2.7585 Iteration: 3167; Percent complete: 79.2%; Average loss: 2.9728 Iteration: 3168; Percent complete: 79.2%; Average loss: 2.8109 Iteration: 3169; Percent complete: 79.2%; Average loss: 2.6927 Iteration: 3170; Percent complete: 79.2%; Average loss: 2.9480 Iteration: 3171; Percent complete: 79.3%; Average loss: 2.8216 Iteration: 3172; Percent complete: 79.3%; Average loss: 2.7929 Iteration: 3173; Percent complete: 79.3%; Average loss: 2.9728 Iteration: 3174; Percent complete: 79.3%; Average loss: 2.7576 Iteration: 3175; Percent complete: 79.4%; Average loss: 2.8780 Iteration: 3176; Percent complete: 79.4%; Average loss: 2.8704 Iteration: 3177; Percent complete: 79.4%; Average loss: 2.7198 Iteration: 3178; Percent complete: 79.5%; Average loss: 3.0021 Iteration: 3179; Percent complete: 79.5%; Average loss: 2.7362 Iteration: 3180; Percent complete: 79.5%; Average loss: 2.8147 Iteration: 3181; Percent complete: 79.5%; Average loss: 2.6615 Iteration: 3182; Percent complete: 79.5%; Average loss: 2.7305 Iteration: 3183; Percent complete: 79.6%; Average loss: 2.7453 Iteration: 3184; Percent complete: 79.6%; Average loss: 2.8089 Iteration: 3185; Percent complete: 79.6%; Average loss: 2.9210 Iteration: 3186; Percent complete: 79.7%; Average loss: 2.5632 Iteration: 3187; Percent complete: 79.7%; Average loss: 2.9106 Iteration: 3188; Percent complete: 79.7%; Average loss: 3.0693 Iteration: 3189; Percent complete: 79.7%; Average loss: 2.7385 Iteration: 3190; Percent complete: 79.8%; Average loss: 3.0175 Iteration: 3191; Percent complete: 79.8%; Average loss: 2.9093 Iteration: 3192; Percent complete: 79.8%; Average loss: 2.8568 Iteration: 3193; Percent complete: 79.8%; Average loss: 2.7054 Iteration: 3194; Percent complete: 79.8%; Average loss: 2.9797 Iteration: 3195; Percent complete: 79.9%; Average loss: 2.9680 Iteration: 3196; Percent complete: 79.9%; Average loss: 2.8626 Iteration: 3197; Percent complete: 79.9%; Average loss: 2.7779 Iteration: 3198; Percent complete: 80.0%; Average loss: 3.0058 Iteration: 3199; Percent complete: 80.0%; Average loss: 2.7425 Iteration: 3200; Percent complete: 80.0%; Average loss: 2.8558 Iteration: 3201; Percent complete: 80.0%; Average loss: 3.0021 Iteration: 3202; Percent complete: 80.0%; Average loss: 2.8705 Iteration: 3203; Percent complete: 80.1%; Average loss: 2.8362 Iteration: 3204; Percent complete: 80.1%; Average loss: 2.8670 Iteration: 3205; Percent complete: 80.1%; Average loss: 2.6027 Iteration: 3206; Percent complete: 80.2%; Average loss: 2.7443 Iteration: 3207; Percent complete: 80.2%; Average loss: 2.8722 Iteration: 3208; Percent complete: 80.2%; Average loss: 3.0095 Iteration: 3209; Percent complete: 80.2%; Average loss: 3.0660 Iteration: 3210; Percent complete: 80.2%; Average loss: 2.9273 Iteration: 3211; Percent complete: 80.3%; Average loss: 2.8697 Iteration: 3212; Percent complete: 80.3%; Average loss: 2.9546 Iteration: 3213; Percent complete: 80.3%; Average loss: 2.8880 Iteration: 3214; Percent complete: 80.3%; Average loss: 2.8072 Iteration: 3215; Percent complete: 80.4%; Average loss: 2.9747 Iteration: 3216; Percent complete: 80.4%; Average loss: 2.7713 Iteration: 3217; Percent complete: 80.4%; Average loss: 2.8272 Iteration: 3218; Percent complete: 80.5%; Average loss: 2.8666 Iteration: 3219; Percent complete: 80.5%; Average loss: 2.6254 Iteration: 3220; Percent complete: 80.5%; Average loss: 2.8812 Iteration: 3221; Percent complete: 80.5%; Average loss: 2.9393 Iteration: 3222; Percent complete: 80.5%; Average loss: 2.8712 Iteration: 3223; Percent complete: 80.6%; Average loss: 2.7416 Iteration: 3224; Percent complete: 80.6%; Average loss: 2.6847 Iteration: 3225; Percent complete: 80.6%; Average loss: 2.8571 Iteration: 3226; Percent complete: 80.7%; Average loss: 2.6676 Iteration: 3227; Percent complete: 80.7%; Average loss: 2.7207 Iteration: 3228; Percent complete: 80.7%; Average loss: 2.8412 Iteration: 3229; Percent complete: 80.7%; Average loss: 2.7326 Iteration: 3230; Percent complete: 80.8%; Average loss: 2.7722 Iteration: 3231; Percent complete: 80.8%; Average loss: 2.6594 Iteration: 3232; Percent complete: 80.8%; Average loss: 2.7645 Iteration: 3233; Percent complete: 80.8%; Average loss: 2.6013 Iteration: 3234; Percent complete: 80.8%; Average loss: 2.7871 Iteration: 3235; Percent complete: 80.9%; Average loss: 2.9532 Iteration: 3236; Percent complete: 80.9%; Average loss: 2.7464 Iteration: 3237; Percent complete: 80.9%; Average loss: 2.7431 Iteration: 3238; Percent complete: 81.0%; Average loss: 2.6413 Iteration: 3239; Percent complete: 81.0%; Average loss: 2.8418 Iteration: 3240; Percent complete: 81.0%; Average loss: 2.6745 Iteration: 3241; Percent complete: 81.0%; Average loss: 2.7505 Iteration: 3242; Percent complete: 81.0%; Average loss: 2.7462 Iteration: 3243; Percent complete: 81.1%; Average loss: 2.7887 Iteration: 3244; Percent complete: 81.1%; Average loss: 2.6753 Iteration: 3245; Percent complete: 81.1%; Average loss: 2.7981 Iteration: 3246; Percent complete: 81.2%; Average loss: 2.9032 Iteration: 3247; Percent complete: 81.2%; Average loss: 2.7923 Iteration: 3248; Percent complete: 81.2%; Average loss: 2.5653 Iteration: 3249; Percent complete: 81.2%; Average loss: 2.5290 Iteration: 3250; Percent complete: 81.2%; Average loss: 2.6837 Iteration: 3251; Percent complete: 81.3%; Average loss: 2.8940 Iteration: 3252; Percent complete: 81.3%; Average loss: 2.6113 Iteration: 3253; Percent complete: 81.3%; Average loss: 2.9735 Iteration: 3254; Percent complete: 81.3%; Average loss: 2.8283 Iteration: 3255; Percent complete: 81.4%; Average loss: 2.9087 Iteration: 3256; Percent complete: 81.4%; Average loss: 2.8201 Iteration: 3257; Percent complete: 81.4%; Average loss: 2.8443 Iteration: 3258; Percent complete: 81.5%; Average loss: 2.8271 Iteration: 3259; Percent complete: 81.5%; Average loss: 2.8334 Iteration: 3260; Percent complete: 81.5%; Average loss: 2.8674 Iteration: 3261; Percent complete: 81.5%; Average loss: 2.7372 Iteration: 3262; Percent complete: 81.5%; Average loss: 2.9765 Iteration: 3263; Percent complete: 81.6%; Average loss: 2.9380 Iteration: 3264; Percent complete: 81.6%; Average loss: 2.5100 Iteration: 3265; Percent complete: 81.6%; Average loss: 2.6849 Iteration: 3266; Percent complete: 81.7%; Average loss: 2.6896 Iteration: 3267; Percent complete: 81.7%; Average loss: 2.8358 Iteration: 3268; Percent complete: 81.7%; Average loss: 2.6404 Iteration: 3269; Percent complete: 81.7%; Average loss: 2.6790 Iteration: 3270; Percent complete: 81.8%; Average loss: 2.8264 Iteration: 3271; Percent complete: 81.8%; Average loss: 2.7410 Iteration: 3272; Percent complete: 81.8%; Average loss: 2.5339 Iteration: 3273; Percent complete: 81.8%; Average loss: 2.7553 Iteration: 3274; Percent complete: 81.8%; Average loss: 3.1105 Iteration: 3275; Percent complete: 81.9%; Average loss: 2.7710 Iteration: 3276; Percent complete: 81.9%; Average loss: 2.7339 Iteration: 3277; Percent complete: 81.9%; Average loss: 2.6729 Iteration: 3278; Percent complete: 82.0%; Average loss: 2.6078 Iteration: 3279; Percent complete: 82.0%; Average loss: 2.9693 Iteration: 3280; Percent complete: 82.0%; Average loss: 3.0510 Iteration: 3281; Percent complete: 82.0%; Average loss: 3.0771 Iteration: 3282; Percent complete: 82.0%; Average loss: 2.5780 Iteration: 3283; Percent complete: 82.1%; Average loss: 2.8725 Iteration: 3284; Percent complete: 82.1%; Average loss: 2.6944 Iteration: 3285; Percent complete: 82.1%; Average loss: 2.6317 Iteration: 3286; Percent complete: 82.2%; Average loss: 2.7456 Iteration: 3287; Percent complete: 82.2%; Average loss: 2.7917 Iteration: 3288; Percent complete: 82.2%; Average loss: 2.6939 Iteration: 3289; Percent complete: 82.2%; Average loss: 2.7856 Iteration: 3290; Percent complete: 82.2%; Average loss: 2.9281 Iteration: 3291; Percent complete: 82.3%; Average loss: 2.8005 Iteration: 3292; Percent complete: 82.3%; Average loss: 2.7644 Iteration: 3293; Percent complete: 82.3%; Average loss: 2.7684 Iteration: 3294; Percent complete: 82.3%; Average loss: 2.8228 Iteration: 3295; Percent complete: 82.4%; Average loss: 2.8956 Iteration: 3296; Percent complete: 82.4%; Average loss: 2.7739 Iteration: 3297; Percent complete: 82.4%; Average loss: 2.7526 Iteration: 3298; Percent complete: 82.5%; Average loss: 2.6945 Iteration: 3299; Percent complete: 82.5%; Average loss: 2.8824 Iteration: 3300; Percent complete: 82.5%; Average loss: 2.8571 Iteration: 3301; Percent complete: 82.5%; Average loss: 2.6536 Iteration: 3302; Percent complete: 82.5%; Average loss: 2.9658 Iteration: 3303; Percent complete: 82.6%; Average loss: 2.9487 Iteration: 3304; Percent complete: 82.6%; Average loss: 2.9744 Iteration: 3305; Percent complete: 82.6%; Average loss: 2.7595 Iteration: 3306; Percent complete: 82.7%; Average loss: 2.8810 Iteration: 3307; Percent complete: 82.7%; Average loss: 2.6865 Iteration: 3308; Percent complete: 82.7%; Average loss: 2.7194 Iteration: 3309; Percent complete: 82.7%; Average loss: 2.7930 Iteration: 3310; Percent complete: 82.8%; Average loss: 2.7958 Iteration: 3311; Percent complete: 82.8%; Average loss: 2.8537 Iteration: 3312; Percent complete: 82.8%; Average loss: 2.7709 Iteration: 3313; Percent complete: 82.8%; Average loss: 2.8710 Iteration: 3314; Percent complete: 82.8%; Average loss: 2.5551 Iteration: 3315; Percent complete: 82.9%; Average loss: 2.7790 Iteration: 3316; Percent complete: 82.9%; Average loss: 2.6726 Iteration: 3317; Percent complete: 82.9%; Average loss: 2.7081 Iteration: 3318; Percent complete: 83.0%; Average loss: 2.9245 Iteration: 3319; Percent complete: 83.0%; Average loss: 2.7310 Iteration: 3320; Percent complete: 83.0%; Average loss: 2.6148 Iteration: 3321; Percent complete: 83.0%; Average loss: 2.7530 Iteration: 3322; Percent complete: 83.0%; Average loss: 2.9128 Iteration: 3323; Percent complete: 83.1%; Average loss: 2.7109 Iteration: 3324; Percent complete: 83.1%; Average loss: 2.9249 Iteration: 3325; Percent complete: 83.1%; Average loss: 2.7082 Iteration: 3326; Percent complete: 83.2%; Average loss: 2.8828 Iteration: 3327; Percent complete: 83.2%; Average loss: 2.6030 Iteration: 3328; Percent complete: 83.2%; Average loss: 2.6886 Iteration: 3329; Percent complete: 83.2%; Average loss: 2.6496 Iteration: 3330; Percent complete: 83.2%; Average loss: 2.7618 Iteration: 3331; Percent complete: 83.3%; Average loss: 2.5974 Iteration: 3332; Percent complete: 83.3%; Average loss: 2.6716 Iteration: 3333; Percent complete: 83.3%; Average loss: 2.7390 Iteration: 3334; Percent complete: 83.4%; Average loss: 2.9533 Iteration: 3335; Percent complete: 83.4%; Average loss: 2.9526 Iteration: 3336; Percent complete: 83.4%; Average loss: 2.9923 Iteration: 3337; Percent complete: 83.4%; Average loss: 2.5338 Iteration: 3338; Percent complete: 83.5%; Average loss: 2.8745 Iteration: 3339; Percent complete: 83.5%; Average loss: 2.9527 Iteration: 3340; Percent complete: 83.5%; Average loss: 2.5320 Iteration: 3341; Percent complete: 83.5%; Average loss: 2.9352 Iteration: 3342; Percent complete: 83.5%; Average loss: 2.9315 Iteration: 3343; Percent complete: 83.6%; Average loss: 2.6279 Iteration: 3344; Percent complete: 83.6%; Average loss: 2.7975 Iteration: 3345; Percent complete: 83.6%; Average loss: 2.8075 Iteration: 3346; Percent complete: 83.7%; Average loss: 2.8766 Iteration: 3347; Percent complete: 83.7%; Average loss: 2.7700 Iteration: 3348; Percent complete: 83.7%; Average loss: 2.6275 Iteration: 3349; Percent complete: 83.7%; Average loss: 2.8007 Iteration: 3350; Percent complete: 83.8%; Average loss: 2.7294 Iteration: 3351; Percent complete: 83.8%; Average loss: 2.8822 Iteration: 3352; Percent complete: 83.8%; Average loss: 2.6167 Iteration: 3353; Percent complete: 83.8%; Average loss: 2.6327 Iteration: 3354; Percent complete: 83.9%; Average loss: 2.8003 Iteration: 3355; Percent complete: 83.9%; Average loss: 2.7561 Iteration: 3356; Percent complete: 83.9%; Average loss: 2.7685 Iteration: 3357; Percent complete: 83.9%; Average loss: 2.5949 Iteration: 3358; Percent complete: 84.0%; Average loss: 2.7598 Iteration: 3359; Percent complete: 84.0%; Average loss: 2.8619 Iteration: 3360; Percent complete: 84.0%; Average loss: 2.6904 Iteration: 3361; Percent complete: 84.0%; Average loss: 2.4538 Iteration: 3362; Percent complete: 84.0%; Average loss: 2.9059 Iteration: 3363; Percent complete: 84.1%; Average loss: 2.6084 Iteration: 3364; Percent complete: 84.1%; Average loss: 2.8299 Iteration: 3365; Percent complete: 84.1%; Average loss: 2.7287 Iteration: 3366; Percent complete: 84.2%; Average loss: 2.9264 Iteration: 3367; Percent complete: 84.2%; Average loss: 2.7792 Iteration: 3368; Percent complete: 84.2%; Average loss: 2.9716 Iteration: 3369; Percent complete: 84.2%; Average loss: 2.7664 Iteration: 3370; Percent complete: 84.2%; Average loss: 2.6142 Iteration: 3371; Percent complete: 84.3%; Average loss: 2.9256 Iteration: 3372; Percent complete: 84.3%; Average loss: 2.7091 Iteration: 3373; Percent complete: 84.3%; Average loss: 2.8418 Iteration: 3374; Percent complete: 84.4%; Average loss: 2.5683 Iteration: 3375; Percent complete: 84.4%; Average loss: 2.8068 Iteration: 3376; Percent complete: 84.4%; Average loss: 2.4184 Iteration: 3377; Percent complete: 84.4%; Average loss: 2.6956 Iteration: 3378; Percent complete: 84.5%; Average loss: 2.7040 Iteration: 3379; Percent complete: 84.5%; Average loss: 2.7223 Iteration: 3380; Percent complete: 84.5%; Average loss: 2.8612 Iteration: 3381; Percent complete: 84.5%; Average loss: 2.6737 Iteration: 3382; Percent complete: 84.5%; Average loss: 2.9890 Iteration: 3383; Percent complete: 84.6%; Average loss: 2.9207 Iteration: 3384; Percent complete: 84.6%; Average loss: 2.9666 Iteration: 3385; Percent complete: 84.6%; Average loss: 2.5636 Iteration: 3386; Percent complete: 84.7%; Average loss: 2.8154 Iteration: 3387; Percent complete: 84.7%; Average loss: 2.7641 Iteration: 3388; Percent complete: 84.7%; Average loss: 2.7416 Iteration: 3389; Percent complete: 84.7%; Average loss: 2.6718 Iteration: 3390; Percent complete: 84.8%; Average loss: 2.9008 Iteration: 3391; Percent complete: 84.8%; Average loss: 2.7515 Iteration: 3392; Percent complete: 84.8%; Average loss: 2.6745 Iteration: 3393; Percent complete: 84.8%; Average loss: 3.0190 Iteration: 3394; Percent complete: 84.9%; Average loss: 2.7931 Iteration: 3395; Percent complete: 84.9%; Average loss: 2.8146 Iteration: 3396; Percent complete: 84.9%; Average loss: 2.6754 Iteration: 3397; Percent complete: 84.9%; Average loss: 2.6769 Iteration: 3398; Percent complete: 85.0%; Average loss: 2.6979 Iteration: 3399; Percent complete: 85.0%; Average loss: 2.8452 Iteration: 3400; Percent complete: 85.0%; Average loss: 2.7310 Iteration: 3401; Percent complete: 85.0%; Average loss: 2.8965 Iteration: 3402; Percent complete: 85.0%; Average loss: 3.0748 Iteration: 3403; Percent complete: 85.1%; Average loss: 2.8856 Iteration: 3404; Percent complete: 85.1%; Average loss: 2.5991 Iteration: 3405; Percent complete: 85.1%; Average loss: 2.6787 Iteration: 3406; Percent complete: 85.2%; Average loss: 2.6551 Iteration: 3407; Percent complete: 85.2%; Average loss: 2.7821 Iteration: 3408; Percent complete: 85.2%; Average loss: 2.8974 Iteration: 3409; Percent complete: 85.2%; Average loss: 2.9564 Iteration: 3410; Percent complete: 85.2%; Average loss: 2.6635 Iteration: 3411; Percent complete: 85.3%; Average loss: 2.7594 Iteration: 3412; Percent complete: 85.3%; Average loss: 2.6535 Iteration: 3413; Percent complete: 85.3%; Average loss: 2.8085 Iteration: 3414; Percent complete: 85.4%; Average loss: 2.9373 Iteration: 3415; Percent complete: 85.4%; Average loss: 2.6269 Iteration: 3416; Percent complete: 85.4%; Average loss: 2.8624 Iteration: 3417; Percent complete: 85.4%; Average loss: 2.9595 Iteration: 3418; Percent complete: 85.5%; Average loss: 2.5837 Iteration: 3419; Percent complete: 85.5%; Average loss: 2.7707 Iteration: 3420; Percent complete: 85.5%; Average loss: 2.8404 Iteration: 3421; Percent complete: 85.5%; Average loss: 2.7980 Iteration: 3422; Percent complete: 85.5%; Average loss: 3.0416 Iteration: 3423; Percent complete: 85.6%; Average loss: 2.7625 Iteration: 3424; Percent complete: 85.6%; Average loss: 2.8074 Iteration: 3425; Percent complete: 85.6%; Average loss: 2.7947 Iteration: 3426; Percent complete: 85.7%; Average loss: 2.7064 Iteration: 3427; Percent complete: 85.7%; Average loss: 3.0650 Iteration: 3428; Percent complete: 85.7%; Average loss: 2.8685 Iteration: 3429; Percent complete: 85.7%; Average loss: 2.8462 Iteration: 3430; Percent complete: 85.8%; Average loss: 2.7492 Iteration: 3431; Percent complete: 85.8%; Average loss: 2.7039 Iteration: 3432; Percent complete: 85.8%; Average loss: 2.8781 Iteration: 3433; Percent complete: 85.8%; Average loss: 2.6406 Iteration: 3434; Percent complete: 85.9%; Average loss: 2.8070 Iteration: 3435; Percent complete: 85.9%; Average loss: 2.5837 Iteration: 3436; Percent complete: 85.9%; Average loss: 2.9474 Iteration: 3437; Percent complete: 85.9%; Average loss: 2.7634 Iteration: 3438; Percent complete: 86.0%; Average loss: 2.8880 Iteration: 3439; Percent complete: 86.0%; Average loss: 2.8609 Iteration: 3440; Percent complete: 86.0%; Average loss: 2.7405 Iteration: 3441; Percent complete: 86.0%; Average loss: 2.6488 Iteration: 3442; Percent complete: 86.1%; Average loss: 2.8511 Iteration: 3443; Percent complete: 86.1%; Average loss: 2.6294 Iteration: 3444; Percent complete: 86.1%; Average loss: 2.6564 Iteration: 3445; Percent complete: 86.1%; Average loss: 2.6365 Iteration: 3446; Percent complete: 86.2%; Average loss: 2.5888 Iteration: 3447; Percent complete: 86.2%; Average loss: 2.7230 Iteration: 3448; Percent complete: 86.2%; Average loss: 2.4821 Iteration: 3449; Percent complete: 86.2%; Average loss: 3.0519 Iteration: 3450; Percent complete: 86.2%; Average loss: 2.8094 Iteration: 3451; Percent complete: 86.3%; Average loss: 2.8395 Iteration: 3452; Percent complete: 86.3%; Average loss: 2.8834 Iteration: 3453; Percent complete: 86.3%; Average loss: 2.8109 Iteration: 3454; Percent complete: 86.4%; Average loss: 2.8134 Iteration: 3455; Percent complete: 86.4%; Average loss: 2.7765 Iteration: 3456; Percent complete: 86.4%; Average loss: 2.5413 Iteration: 3457; Percent complete: 86.4%; Average loss: 2.8004 Iteration: 3458; Percent complete: 86.5%; Average loss: 2.6987 Iteration: 3459; Percent complete: 86.5%; Average loss: 3.0946 Iteration: 3460; Percent complete: 86.5%; Average loss: 2.8857 Iteration: 3461; Percent complete: 86.5%; Average loss: 2.5598 Iteration: 3462; Percent complete: 86.6%; Average loss: 2.8451 Iteration: 3463; Percent complete: 86.6%; Average loss: 2.8108 Iteration: 3464; Percent complete: 86.6%; Average loss: 2.6672 Iteration: 3465; Percent complete: 86.6%; Average loss: 2.6177 Iteration: 3466; Percent complete: 86.7%; Average loss: 2.6013 Iteration: 3467; Percent complete: 86.7%; Average loss: 2.7564 Iteration: 3468; Percent complete: 86.7%; Average loss: 2.6710 Iteration: 3469; Percent complete: 86.7%; Average loss: 2.7328 Iteration: 3470; Percent complete: 86.8%; Average loss: 2.7745 Iteration: 3471; Percent complete: 86.8%; Average loss: 2.5268 Iteration: 3472; Percent complete: 86.8%; Average loss: 2.6877 Iteration: 3473; Percent complete: 86.8%; Average loss: 2.7586 Iteration: 3474; Percent complete: 86.9%; Average loss: 2.9320 Iteration: 3475; Percent complete: 86.9%; Average loss: 2.7742 Iteration: 3476; Percent complete: 86.9%; Average loss: 2.7361 Iteration: 3477; Percent complete: 86.9%; Average loss: 2.6517 Iteration: 3478; Percent complete: 87.0%; Average loss: 2.6035 Iteration: 3479; Percent complete: 87.0%; Average loss: 2.8535 Iteration: 3480; Percent complete: 87.0%; Average loss: 2.7606 Iteration: 3481; Percent complete: 87.0%; Average loss: 2.7198 Iteration: 3482; Percent complete: 87.1%; Average loss: 2.6336 Iteration: 3483; Percent complete: 87.1%; Average loss: 2.5265 Iteration: 3484; Percent complete: 87.1%; Average loss: 2.8110 Iteration: 3485; Percent complete: 87.1%; Average loss: 3.0392 Iteration: 3486; Percent complete: 87.2%; Average loss: 2.7315 Iteration: 3487; Percent complete: 87.2%; Average loss: 2.8930 Iteration: 3488; Percent complete: 87.2%; Average loss: 2.7499 Iteration: 3489; Percent complete: 87.2%; Average loss: 2.9192 Iteration: 3490; Percent complete: 87.2%; Average loss: 2.4131 Iteration: 3491; Percent complete: 87.3%; Average loss: 2.8195 Iteration: 3492; Percent complete: 87.3%; Average loss: 2.6261 Iteration: 3493; Percent complete: 87.3%; Average loss: 2.6877 Iteration: 3494; Percent complete: 87.4%; Average loss: 2.9134 Iteration: 3495; Percent complete: 87.4%; Average loss: 3.0331 Iteration: 3496; Percent complete: 87.4%; Average loss: 2.8177 Iteration: 3497; Percent complete: 87.4%; Average loss: 2.7524 Iteration: 3498; Percent complete: 87.5%; Average loss: 2.8604 Iteration: 3499; Percent complete: 87.5%; Average loss: 2.7868 Iteration: 3500; Percent complete: 87.5%; Average loss: 2.8101 Iteration: 3501; Percent complete: 87.5%; Average loss: 2.6919 Iteration: 3502; Percent complete: 87.5%; Average loss: 2.6421 Iteration: 3503; Percent complete: 87.6%; Average loss: 2.7526 Iteration: 3504; Percent complete: 87.6%; Average loss: 2.8567 Iteration: 3505; Percent complete: 87.6%; Average loss: 2.8137 Iteration: 3506; Percent complete: 87.6%; Average loss: 2.7189 Iteration: 3507; Percent complete: 87.7%; Average loss: 2.8170 Iteration: 3508; Percent complete: 87.7%; Average loss: 2.6677 Iteration: 3509; Percent complete: 87.7%; Average loss: 2.7455 Iteration: 3510; Percent complete: 87.8%; Average loss: 2.6448 Iteration: 3511; Percent complete: 87.8%; Average loss: 2.7990 Iteration: 3512; Percent complete: 87.8%; Average loss: 2.7411 Iteration: 3513; Percent complete: 87.8%; Average loss: 2.8946 Iteration: 3514; Percent complete: 87.8%; Average loss: 2.5619 Iteration: 3515; Percent complete: 87.9%; Average loss: 2.7954 Iteration: 3516; Percent complete: 87.9%; Average loss: 2.7614 Iteration: 3517; Percent complete: 87.9%; Average loss: 2.8210 Iteration: 3518; Percent complete: 87.9%; Average loss: 2.5165 Iteration: 3519; Percent complete: 88.0%; Average loss: 2.9331 Iteration: 3520; Percent complete: 88.0%; Average loss: 2.8895 Iteration: 3521; Percent complete: 88.0%; Average loss: 2.6333 Iteration: 3522; Percent complete: 88.0%; Average loss: 2.7865 Iteration: 3523; Percent complete: 88.1%; Average loss: 2.8571 Iteration: 3524; Percent complete: 88.1%; Average loss: 2.7569 Iteration: 3525; Percent complete: 88.1%; Average loss: 2.7661 Iteration: 3526; Percent complete: 88.1%; Average loss: 2.8462 Iteration: 3527; Percent complete: 88.2%; Average loss: 2.9266 Iteration: 3528; Percent complete: 88.2%; Average loss: 2.5955 Iteration: 3529; Percent complete: 88.2%; Average loss: 2.7975 Iteration: 3530; Percent complete: 88.2%; Average loss: 2.8836 Iteration: 3531; Percent complete: 88.3%; Average loss: 2.7054 Iteration: 3532; Percent complete: 88.3%; Average loss: 2.7210 Iteration: 3533; Percent complete: 88.3%; Average loss: 2.6846 Iteration: 3534; Percent complete: 88.3%; Average loss: 2.6646 Iteration: 3535; Percent complete: 88.4%; Average loss: 2.8414 Iteration: 3536; Percent complete: 88.4%; Average loss: 2.6424 Iteration: 3537; Percent complete: 88.4%; Average loss: 2.8802 Iteration: 3538; Percent complete: 88.4%; Average loss: 2.6915 Iteration: 3539; Percent complete: 88.5%; Average loss: 2.8430 Iteration: 3540; Percent complete: 88.5%; Average loss: 2.9332 Iteration: 3541; Percent complete: 88.5%; Average loss: 2.8082 Iteration: 3542; Percent complete: 88.5%; Average loss: 2.7314 Iteration: 3543; Percent complete: 88.6%; Average loss: 2.5753 Iteration: 3544; Percent complete: 88.6%; Average loss: 2.9192 Iteration: 3545; Percent complete: 88.6%; Average loss: 2.6688 Iteration: 3546; Percent complete: 88.6%; Average loss: 2.6843 Iteration: 3547; Percent complete: 88.7%; Average loss: 2.8122 Iteration: 3548; Percent complete: 88.7%; Average loss: 2.6978 Iteration: 3549; Percent complete: 88.7%; Average loss: 2.7366 Iteration: 3550; Percent complete: 88.8%; Average loss: 2.8988 Iteration: 3551; Percent complete: 88.8%; Average loss: 2.6629 Iteration: 3552; Percent complete: 88.8%; Average loss: 2.8440 Iteration: 3553; Percent complete: 88.8%; Average loss: 2.7957 Iteration: 3554; Percent complete: 88.8%; Average loss: 3.0217 Iteration: 3555; Percent complete: 88.9%; Average loss: 2.7490 Iteration: 3556; Percent complete: 88.9%; Average loss: 2.8884 Iteration: 3557; Percent complete: 88.9%; Average loss: 2.9393 Iteration: 3558; Percent complete: 88.9%; Average loss: 2.8683 Iteration: 3559; Percent complete: 89.0%; Average loss: 2.9548 Iteration: 3560; Percent complete: 89.0%; Average loss: 2.6712 Iteration: 3561; Percent complete: 89.0%; Average loss: 2.6861 Iteration: 3562; Percent complete: 89.0%; Average loss: 2.4478 Iteration: 3563; Percent complete: 89.1%; Average loss: 2.7216 Iteration: 3564; Percent complete: 89.1%; Average loss: 2.7814 Iteration: 3565; Percent complete: 89.1%; Average loss: 2.8691 Iteration: 3566; Percent complete: 89.1%; Average loss: 2.7201 Iteration: 3567; Percent complete: 89.2%; Average loss: 2.8213 Iteration: 3568; Percent complete: 89.2%; Average loss: 2.8969 Iteration: 3569; Percent complete: 89.2%; Average loss: 2.7880 Iteration: 3570; Percent complete: 89.2%; Average loss: 2.8587 Iteration: 3571; Percent complete: 89.3%; Average loss: 2.7468 Iteration: 3572; Percent complete: 89.3%; Average loss: 2.7664 Iteration: 3573; Percent complete: 89.3%; Average loss: 2.6145 Iteration: 3574; Percent complete: 89.3%; Average loss: 2.7998 Iteration: 3575; Percent complete: 89.4%; Average loss: 2.9450 Iteration: 3576; Percent complete: 89.4%; Average loss: 2.7980 Iteration: 3577; Percent complete: 89.4%; Average loss: 2.8749 Iteration: 3578; Percent complete: 89.5%; Average loss: 2.6039 Iteration: 3579; Percent complete: 89.5%; Average loss: 2.7327 Iteration: 3580; Percent complete: 89.5%; Average loss: 2.7637 Iteration: 3581; Percent complete: 89.5%; Average loss: 2.5677 Iteration: 3582; Percent complete: 89.5%; Average loss: 2.6359 Iteration: 3583; Percent complete: 89.6%; Average loss: 2.8550 Iteration: 3584; Percent complete: 89.6%; Average loss: 2.7408 Iteration: 3585; Percent complete: 89.6%; Average loss: 2.6209 Iteration: 3586; Percent complete: 89.6%; Average loss: 2.7148 Iteration: 3587; Percent complete: 89.7%; Average loss: 2.8705 Iteration: 3588; Percent complete: 89.7%; Average loss: 2.9079 Iteration: 3589; Percent complete: 89.7%; Average loss: 2.8261 Iteration: 3590; Percent complete: 89.8%; Average loss: 2.7317 Iteration: 3591; Percent complete: 89.8%; Average loss: 2.5852 Iteration: 3592; Percent complete: 89.8%; Average loss: 2.5261 Iteration: 3593; Percent complete: 89.8%; Average loss: 2.8429 Iteration: 3594; Percent complete: 89.8%; Average loss: 2.8071 Iteration: 3595; Percent complete: 89.9%; Average loss: 2.5704 Iteration: 3596; Percent complete: 89.9%; Average loss: 2.6715 Iteration: 3597; Percent complete: 89.9%; Average loss: 2.7706 Iteration: 3598; Percent complete: 90.0%; Average loss: 2.7544 Iteration: 3599; Percent complete: 90.0%; Average loss: 2.6661 Iteration: 3600; Percent complete: 90.0%; Average loss: 2.7660 Iteration: 3601; Percent complete: 90.0%; Average loss: 2.6500 Iteration: 3602; Percent complete: 90.0%; Average loss: 2.6437 Iteration: 3603; Percent complete: 90.1%; Average loss: 2.6360 Iteration: 3604; Percent complete: 90.1%; Average loss: 2.7389 Iteration: 3605; Percent complete: 90.1%; Average loss: 2.8445 Iteration: 3606; Percent complete: 90.1%; Average loss: 2.6361 Iteration: 3607; Percent complete: 90.2%; Average loss: 2.7655 Iteration: 3608; Percent complete: 90.2%; Average loss: 2.5592 Iteration: 3609; Percent complete: 90.2%; Average loss: 2.7872 Iteration: 3610; Percent complete: 90.2%; Average loss: 2.5624 Iteration: 3611; Percent complete: 90.3%; Average loss: 2.7962 Iteration: 3612; Percent complete: 90.3%; Average loss: 2.7204 Iteration: 3613; Percent complete: 90.3%; Average loss: 2.7911 Iteration: 3614; Percent complete: 90.3%; Average loss: 2.6070 Iteration: 3615; Percent complete: 90.4%; Average loss: 2.7184 Iteration: 3616; Percent complete: 90.4%; Average loss: 2.7040 Iteration: 3617; Percent complete: 90.4%; Average loss: 2.6034 Iteration: 3618; Percent complete: 90.5%; Average loss: 2.5909 Iteration: 3619; Percent complete: 90.5%; Average loss: 2.7891 Iteration: 3620; Percent complete: 90.5%; Average loss: 2.7941 Iteration: 3621; Percent complete: 90.5%; Average loss: 2.7387 Iteration: 3622; Percent complete: 90.5%; Average loss: 2.8106 Iteration: 3623; Percent complete: 90.6%; Average loss: 2.8161 Iteration: 3624; Percent complete: 90.6%; Average loss: 2.6772 Iteration: 3625; Percent complete: 90.6%; Average loss: 2.8693 Iteration: 3626; Percent complete: 90.6%; Average loss: 2.6609 Iteration: 3627; Percent complete: 90.7%; Average loss: 2.2583 Iteration: 3628; Percent complete: 90.7%; Average loss: 2.5550 Iteration: 3629; Percent complete: 90.7%; Average loss: 2.6270 Iteration: 3630; Percent complete: 90.8%; Average loss: 2.6131 Iteration: 3631; Percent complete: 90.8%; Average loss: 2.7411 Iteration: 3632; Percent complete: 90.8%; Average loss: 2.8756 Iteration: 3633; Percent complete: 90.8%; Average loss: 2.7440 Iteration: 3634; Percent complete: 90.8%; Average loss: 2.6204 Iteration: 3635; Percent complete: 90.9%; Average loss: 2.7481 Iteration: 3636; Percent complete: 90.9%; Average loss: 2.4324 Iteration: 3637; Percent complete: 90.9%; Average loss: 2.6290 Iteration: 3638; Percent complete: 91.0%; Average loss: 2.5657 Iteration: 3639; Percent complete: 91.0%; Average loss: 2.7277 Iteration: 3640; Percent complete: 91.0%; Average loss: 2.8937 Iteration: 3641; Percent complete: 91.0%; Average loss: 2.3627 Iteration: 3642; Percent complete: 91.0%; Average loss: 2.7155 Iteration: 3643; Percent complete: 91.1%; Average loss: 2.7697 Iteration: 3644; Percent complete: 91.1%; Average loss: 2.8136 Iteration: 3645; Percent complete: 91.1%; Average loss: 2.8715 Iteration: 3646; Percent complete: 91.1%; Average loss: 2.6719 Iteration: 3647; Percent complete: 91.2%; Average loss: 2.6676 Iteration: 3648; Percent complete: 91.2%; Average loss: 2.5993 Iteration: 3649; Percent complete: 91.2%; Average loss: 2.7738 Iteration: 3650; Percent complete: 91.2%; Average loss: 2.8756 Iteration: 3651; Percent complete: 91.3%; Average loss: 2.8535 Iteration: 3652; Percent complete: 91.3%; Average loss: 2.7332 Iteration: 3653; Percent complete: 91.3%; Average loss: 2.9763 Iteration: 3654; Percent complete: 91.3%; Average loss: 2.6252 Iteration: 3655; Percent complete: 91.4%; Average loss: 2.6605 Iteration: 3656; Percent complete: 91.4%; Average loss: 2.6673 Iteration: 3657; Percent complete: 91.4%; Average loss: 2.7444 Iteration: 3658; Percent complete: 91.5%; Average loss: 2.5654 Iteration: 3659; Percent complete: 91.5%; Average loss: 2.3670 Iteration: 3660; Percent complete: 91.5%; Average loss: 2.9667 Iteration: 3661; Percent complete: 91.5%; Average loss: 2.7091 Iteration: 3662; Percent complete: 91.5%; Average loss: 2.6378 Iteration: 3663; Percent complete: 91.6%; Average loss: 2.5715 Iteration: 3664; Percent complete: 91.6%; Average loss: 2.4035 Iteration: 3665; Percent complete: 91.6%; Average loss: 2.6691 Iteration: 3666; Percent complete: 91.6%; Average loss: 2.8368 Iteration: 3667; Percent complete: 91.7%; Average loss: 2.5952 Iteration: 3668; Percent complete: 91.7%; Average loss: 3.0498 Iteration: 3669; Percent complete: 91.7%; Average loss: 2.6671 Iteration: 3670; Percent complete: 91.8%; Average loss: 2.7720 Iteration: 3671; Percent complete: 91.8%; Average loss: 2.7269 Iteration: 3672; Percent complete: 91.8%; Average loss: 2.8617 Iteration: 3673; Percent complete: 91.8%; Average loss: 2.6387 Iteration: 3674; Percent complete: 91.8%; Average loss: 2.7025 Iteration: 3675; Percent complete: 91.9%; Average loss: 2.7508 Iteration: 3676; Percent complete: 91.9%; Average loss: 2.7247 Iteration: 3677; Percent complete: 91.9%; Average loss: 3.0049 Iteration: 3678; Percent complete: 92.0%; Average loss: 2.6115 Iteration: 3679; Percent complete: 92.0%; Average loss: 2.7385 Iteration: 3680; Percent complete: 92.0%; Average loss: 2.5090 Iteration: 3681; Percent complete: 92.0%; Average loss: 2.7184 Iteration: 3682; Percent complete: 92.0%; Average loss: 2.5811 Iteration: 3683; Percent complete: 92.1%; Average loss: 2.5861 Iteration: 3684; Percent complete: 92.1%; Average loss: 2.6464 Iteration: 3685; Percent complete: 92.1%; Average loss: 2.4681 Iteration: 3686; Percent complete: 92.2%; Average loss: 2.7413 Iteration: 3687; Percent complete: 92.2%; Average loss: 2.6202 Iteration: 3688; Percent complete: 92.2%; Average loss: 2.6965 Iteration: 3689; Percent complete: 92.2%; Average loss: 2.5116 Iteration: 3690; Percent complete: 92.2%; Average loss: 2.7124 Iteration: 3691; Percent complete: 92.3%; Average loss: 2.8565 Iteration: 3692; Percent complete: 92.3%; Average loss: 2.8379 Iteration: 3693; Percent complete: 92.3%; Average loss: 2.7236 Iteration: 3694; Percent complete: 92.3%; Average loss: 2.9059 Iteration: 3695; Percent complete: 92.4%; Average loss: 2.6912 Iteration: 3696; Percent complete: 92.4%; Average loss: 2.7722 Iteration: 3697; Percent complete: 92.4%; Average loss: 2.7468 Iteration: 3698; Percent complete: 92.5%; Average loss: 2.7573 Iteration: 3699; Percent complete: 92.5%; Average loss: 2.5251 Iteration: 3700; Percent complete: 92.5%; Average loss: 2.5587 Iteration: 3701; Percent complete: 92.5%; Average loss: 2.8658 Iteration: 3702; Percent complete: 92.5%; Average loss: 2.7020 Iteration: 3703; Percent complete: 92.6%; Average loss: 2.6218 Iteration: 3704; Percent complete: 92.6%; Average loss: 2.7208 Iteration: 3705; Percent complete: 92.6%; Average loss: 2.5934 Iteration: 3706; Percent complete: 92.7%; Average loss: 2.8950 Iteration: 3707; Percent complete: 92.7%; Average loss: 2.4966 Iteration: 3708; Percent complete: 92.7%; Average loss: 2.5085 Iteration: 3709; Percent complete: 92.7%; Average loss: 2.7147 Iteration: 3710; Percent complete: 92.8%; Average loss: 2.5579 Iteration: 3711; Percent complete: 92.8%; Average loss: 2.8422 Iteration: 3712; Percent complete: 92.8%; Average loss: 2.6113 Iteration: 3713; Percent complete: 92.8%; Average loss: 2.6717 Iteration: 3714; Percent complete: 92.8%; Average loss: 2.5341 Iteration: 3715; Percent complete: 92.9%; Average loss: 2.6218 Iteration: 3716; Percent complete: 92.9%; Average loss: 2.6519 Iteration: 3717; Percent complete: 92.9%; Average loss: 2.4849 Iteration: 3718; Percent complete: 93.0%; Average loss: 2.6524 Iteration: 3719; Percent complete: 93.0%; Average loss: 2.4739 Iteration: 3720; Percent complete: 93.0%; Average loss: 2.7633 Iteration: 3721; Percent complete: 93.0%; Average loss: 2.4064 Iteration: 3722; Percent complete: 93.0%; Average loss: 2.7249 Iteration: 3723; Percent complete: 93.1%; Average loss: 2.5313 Iteration: 3724; Percent complete: 93.1%; Average loss: 2.6195 Iteration: 3725; Percent complete: 93.1%; Average loss: 2.7112 Iteration: 3726; Percent complete: 93.2%; Average loss: 2.9204 Iteration: 3727; Percent complete: 93.2%; Average loss: 2.6928 Iteration: 3728; Percent complete: 93.2%; Average loss: 2.6325 Iteration: 3729; Percent complete: 93.2%; Average loss: 2.6230 Iteration: 3730; Percent complete: 93.2%; Average loss: 2.7028 Iteration: 3731; Percent complete: 93.3%; Average loss: 2.7023 Iteration: 3732; Percent complete: 93.3%; Average loss: 2.7067 Iteration: 3733; Percent complete: 93.3%; Average loss: 2.7280 Iteration: 3734; Percent complete: 93.3%; Average loss: 2.6250 Iteration: 3735; Percent complete: 93.4%; Average loss: 2.3916 Iteration: 3736; Percent complete: 93.4%; Average loss: 2.4029 Iteration: 3737; Percent complete: 93.4%; Average loss: 2.4762 Iteration: 3738; Percent complete: 93.5%; Average loss: 2.6466 Iteration: 3739; Percent complete: 93.5%; Average loss: 2.6428 Iteration: 3740; Percent complete: 93.5%; Average loss: 2.6117 Iteration: 3741; Percent complete: 93.5%; Average loss: 2.5662 Iteration: 3742; Percent complete: 93.5%; Average loss: 2.6060 Iteration: 3743; Percent complete: 93.6%; Average loss: 2.6598 Iteration: 3744; Percent complete: 93.6%; Average loss: 2.6716 Iteration: 3745; Percent complete: 93.6%; Average loss: 2.6250 Iteration: 3746; Percent complete: 93.7%; Average loss: 2.5523 Iteration: 3747; Percent complete: 93.7%; Average loss: 2.6362 Iteration: 3748; Percent complete: 93.7%; Average loss: 2.7859 Iteration: 3749; Percent complete: 93.7%; Average loss: 2.7047 Iteration: 3750; Percent complete: 93.8%; Average loss: 2.6173 Iteration: 3751; Percent complete: 93.8%; Average loss: 2.5027 Iteration: 3752; Percent complete: 93.8%; Average loss: 2.6918 Iteration: 3753; Percent complete: 93.8%; Average loss: 3.0341 Iteration: 3754; Percent complete: 93.8%; Average loss: 2.6777 Iteration: 3755; Percent complete: 93.9%; Average loss: 2.5369 Iteration: 3756; Percent complete: 93.9%; Average loss: 2.6036 Iteration: 3757; Percent complete: 93.9%; Average loss: 2.8520 Iteration: 3758; Percent complete: 94.0%; Average loss: 2.6455 Iteration: 3759; Percent complete: 94.0%; Average loss: 2.7668 Iteration: 3760; Percent complete: 94.0%; Average loss: 2.4912 Iteration: 3761; Percent complete: 94.0%; Average loss: 2.7218 Iteration: 3762; Percent complete: 94.0%; Average loss: 2.9150 Iteration: 3763; Percent complete: 94.1%; Average loss: 2.8503 Iteration: 3764; Percent complete: 94.1%; Average loss: 2.8348 Iteration: 3765; Percent complete: 94.1%; Average loss: 2.6052 Iteration: 3766; Percent complete: 94.2%; Average loss: 2.5784 Iteration: 3767; Percent complete: 94.2%; Average loss: 2.6383 Iteration: 3768; Percent complete: 94.2%; Average loss: 2.5500 Iteration: 3769; Percent complete: 94.2%; Average loss: 2.4657 Iteration: 3770; Percent complete: 94.2%; Average loss: 2.6778 Iteration: 3771; Percent complete: 94.3%; Average loss: 2.5614 Iteration: 3772; Percent complete: 94.3%; Average loss: 2.7583 Iteration: 3773; Percent complete: 94.3%; Average loss: 2.5660 Iteration: 3774; Percent complete: 94.3%; Average loss: 2.6402 Iteration: 3775; Percent complete: 94.4%; Average loss: 2.6572 Iteration: 3776; Percent complete: 94.4%; Average loss: 2.7160 Iteration: 3777; Percent complete: 94.4%; Average loss: 2.7647 Iteration: 3778; Percent complete: 94.5%; Average loss: 2.7438 Iteration: 3779; Percent complete: 94.5%; Average loss: 2.6466 Iteration: 3780; Percent complete: 94.5%; Average loss: 2.5878 Iteration: 3781; Percent complete: 94.5%; Average loss: 2.4100 Iteration: 3782; Percent complete: 94.5%; Average loss: 2.6912 Iteration: 3783; Percent complete: 94.6%; Average loss: 2.6480 Iteration: 3784; Percent complete: 94.6%; Average loss: 2.7136 Iteration: 3785; Percent complete: 94.6%; Average loss: 2.5206 Iteration: 3786; Percent complete: 94.7%; Average loss: 2.6126 Iteration: 3787; Percent complete: 94.7%; Average loss: 2.7213 Iteration: 3788; Percent complete: 94.7%; Average loss: 2.4696 Iteration: 3789; Percent complete: 94.7%; Average loss: 2.7261 Iteration: 3790; Percent complete: 94.8%; Average loss: 2.6836 Iteration: 3791; Percent complete: 94.8%; Average loss: 2.7555 Iteration: 3792; Percent complete: 94.8%; Average loss: 2.8439 Iteration: 3793; Percent complete: 94.8%; Average loss: 2.4288 Iteration: 3794; Percent complete: 94.8%; Average loss: 2.8137 Iteration: 3795; Percent complete: 94.9%; Average loss: 2.8264 Iteration: 3796; Percent complete: 94.9%; Average loss: 2.4946 Iteration: 3797; Percent complete: 94.9%; Average loss: 2.8219 Iteration: 3798; Percent complete: 95.0%; Average loss: 2.5639 Iteration: 3799; Percent complete: 95.0%; Average loss: 2.8002 Iteration: 3800; Percent complete: 95.0%; Average loss: 2.6819 Iteration: 3801; Percent complete: 95.0%; Average loss: 2.7871 Iteration: 3802; Percent complete: 95.0%; Average loss: 2.5175 Iteration: 3803; Percent complete: 95.1%; Average loss: 2.5202 Iteration: 3804; Percent complete: 95.1%; Average loss: 2.6545 Iteration: 3805; Percent complete: 95.1%; Average loss: 2.6961 Iteration: 3806; Percent complete: 95.2%; Average loss: 2.5205 Iteration: 3807; Percent complete: 95.2%; Average loss: 2.6703 Iteration: 3808; Percent complete: 95.2%; Average loss: 2.6644 Iteration: 3809; Percent complete: 95.2%; Average loss: 2.5069 Iteration: 3810; Percent complete: 95.2%; Average loss: 2.6489 Iteration: 3811; Percent complete: 95.3%; Average loss: 2.9117 Iteration: 3812; Percent complete: 95.3%; Average loss: 2.7419 Iteration: 3813; Percent complete: 95.3%; Average loss: 2.6565 Iteration: 3814; Percent complete: 95.3%; Average loss: 2.6431 Iteration: 3815; Percent complete: 95.4%; Average loss: 2.4354 Iteration: 3816; Percent complete: 95.4%; Average loss: 2.3492 Iteration: 3817; Percent complete: 95.4%; Average loss: 2.5975 Iteration: 3818; Percent complete: 95.5%; Average loss: 2.8249 Iteration: 3819; Percent complete: 95.5%; Average loss: 2.8128 Iteration: 3820; Percent complete: 95.5%; Average loss: 2.6498 Iteration: 3821; Percent complete: 95.5%; Average loss: 2.7396 Iteration: 3822; Percent complete: 95.5%; Average loss: 2.6298 Iteration: 3823; Percent complete: 95.6%; Average loss: 2.7593 Iteration: 3824; Percent complete: 95.6%; Average loss: 2.4111 Iteration: 3825; Percent complete: 95.6%; Average loss: 2.5952 Iteration: 3826; Percent complete: 95.7%; Average loss: 2.5228 Iteration: 3827; Percent complete: 95.7%; Average loss: 2.5803 Iteration: 3828; Percent complete: 95.7%; Average loss: 2.5323 Iteration: 3829; Percent complete: 95.7%; Average loss: 2.5916 Iteration: 3830; Percent complete: 95.8%; Average loss: 2.5924 Iteration: 3831; Percent complete: 95.8%; Average loss: 2.4993 Iteration: 3832; Percent complete: 95.8%; Average loss: 2.8020 Iteration: 3833; Percent complete: 95.8%; Average loss: 2.9128 Iteration: 3834; Percent complete: 95.9%; Average loss: 2.6753 Iteration: 3835; Percent complete: 95.9%; Average loss: 2.5177 Iteration: 3836; Percent complete: 95.9%; Average loss: 2.6892 Iteration: 3837; Percent complete: 95.9%; Average loss: 2.5647 Iteration: 3838; Percent complete: 96.0%; Average loss: 2.7260 Iteration: 3839; Percent complete: 96.0%; Average loss: 2.7435 Iteration: 3840; Percent complete: 96.0%; Average loss: 2.6940 Iteration: 3841; Percent complete: 96.0%; Average loss: 2.7308 Iteration: 3842; Percent complete: 96.0%; Average loss: 2.4757 Iteration: 3843; Percent complete: 96.1%; Average loss: 2.9029 Iteration: 3844; Percent complete: 96.1%; Average loss: 2.4683 Iteration: 3845; Percent complete: 96.1%; Average loss: 2.4489 Iteration: 3846; Percent complete: 96.2%; Average loss: 2.6549 Iteration: 3847; Percent complete: 96.2%; Average loss: 2.6728 Iteration: 3848; Percent complete: 96.2%; Average loss: 2.6637 Iteration: 3849; Percent complete: 96.2%; Average loss: 2.5499 Iteration: 3850; Percent complete: 96.2%; Average loss: 2.4308 Iteration: 3851; Percent complete: 96.3%; Average loss: 2.6762 Iteration: 3852; Percent complete: 96.3%; Average loss: 2.5643 Iteration: 3853; Percent complete: 96.3%; Average loss: 2.7090 Iteration: 3854; Percent complete: 96.4%; Average loss: 2.5415 Iteration: 3855; Percent complete: 96.4%; Average loss: 2.7316 Iteration: 3856; Percent complete: 96.4%; Average loss: 2.8635 Iteration: 3857; Percent complete: 96.4%; Average loss: 2.5885 Iteration: 3858; Percent complete: 96.5%; Average loss: 2.5194 Iteration: 3859; Percent complete: 96.5%; Average loss: 2.5740 Iteration: 3860; Percent complete: 96.5%; Average loss: 2.4419 Iteration: 3861; Percent complete: 96.5%; Average loss: 2.7156 Iteration: 3862; Percent complete: 96.5%; Average loss: 2.7024 Iteration: 3863; Percent complete: 96.6%; Average loss: 2.5988 Iteration: 3864; Percent complete: 96.6%; Average loss: 2.5606 Iteration: 3865; Percent complete: 96.6%; Average loss: 2.6909 Iteration: 3866; Percent complete: 96.7%; Average loss: 2.7817 Iteration: 3867; Percent complete: 96.7%; Average loss: 2.4599 Iteration: 3868; Percent complete: 96.7%; Average loss: 2.5629 Iteration: 3869; Percent complete: 96.7%; Average loss: 2.6829 Iteration: 3870; Percent complete: 96.8%; Average loss: 2.5845 Iteration: 3871; Percent complete: 96.8%; Average loss: 2.7375 Iteration: 3872; Percent complete: 96.8%; Average loss: 2.6716 Iteration: 3873; Percent complete: 96.8%; Average loss: 2.6114 Iteration: 3874; Percent complete: 96.9%; Average loss: 2.6976 Iteration: 3875; Percent complete: 96.9%; Average loss: 2.5456 Iteration: 3876; Percent complete: 96.9%; Average loss: 2.7874 Iteration: 3877; Percent complete: 96.9%; Average loss: 2.8360 Iteration: 3878; Percent complete: 97.0%; Average loss: 2.7745 Iteration: 3879; Percent complete: 97.0%; Average loss: 2.9649 Iteration: 3880; Percent complete: 97.0%; Average loss: 2.6135 Iteration: 3881; Percent complete: 97.0%; Average loss: 2.5413 Iteration: 3882; Percent complete: 97.0%; Average loss: 2.9062 Iteration: 3883; Percent complete: 97.1%; Average loss: 2.7175 Iteration: 3884; Percent complete: 97.1%; Average loss: 2.9140 Iteration: 3885; Percent complete: 97.1%; Average loss: 2.7382 Iteration: 3886; Percent complete: 97.2%; Average loss: 2.4912 Iteration: 3887; Percent complete: 97.2%; Average loss: 2.7386 Iteration: 3888; Percent complete: 97.2%; Average loss: 2.6930 Iteration: 3889; Percent complete: 97.2%; Average loss: 2.5423 Iteration: 3890; Percent complete: 97.2%; Average loss: 2.6614 Iteration: 3891; Percent complete: 97.3%; Average loss: 2.8658 Iteration: 3892; Percent complete: 97.3%; Average loss: 2.7217 Iteration: 3893; Percent complete: 97.3%; Average loss: 2.5815 Iteration: 3894; Percent complete: 97.4%; Average loss: 2.4968 Iteration: 3895; Percent complete: 97.4%; Average loss: 2.6752 Iteration: 3896; Percent complete: 97.4%; Average loss: 2.7592 Iteration: 3897; Percent complete: 97.4%; Average loss: 2.7382 Iteration: 3898; Percent complete: 97.5%; Average loss: 2.7946 Iteration: 3899; Percent complete: 97.5%; Average loss: 2.6107 Iteration: 3900; Percent complete: 97.5%; Average loss: 2.6435 Iteration: 3901; Percent complete: 97.5%; Average loss: 2.6608 Iteration: 3902; Percent complete: 97.5%; Average loss: 2.5030 Iteration: 3903; Percent complete: 97.6%; Average loss: 2.6083 Iteration: 3904; Percent complete: 97.6%; Average loss: 2.5874 Iteration: 3905; Percent complete: 97.6%; Average loss: 2.6836 Iteration: 3906; Percent complete: 97.7%; Average loss: 2.6553 Iteration: 3907; Percent complete: 97.7%; Average loss: 2.7086 Iteration: 3908; Percent complete: 97.7%; Average loss: 2.5893 Iteration: 3909; Percent complete: 97.7%; Average loss: 2.6382 Iteration: 3910; Percent complete: 97.8%; Average loss: 2.5498 Iteration: 3911; Percent complete: 97.8%; Average loss: 2.4810 Iteration: 3912; Percent complete: 97.8%; Average loss: 2.6768 Iteration: 3913; Percent complete: 97.8%; Average loss: 2.4554 Iteration: 3914; Percent complete: 97.9%; Average loss: 2.6367 Iteration: 3915; Percent complete: 97.9%; Average loss: 2.5724 Iteration: 3916; Percent complete: 97.9%; Average loss: 2.6495 Iteration: 3917; Percent complete: 97.9%; Average loss: 2.4811 Iteration: 3918; Percent complete: 98.0%; Average loss: 2.4607 Iteration: 3919; Percent complete: 98.0%; Average loss: 2.8312 Iteration: 3920; Percent complete: 98.0%; Average loss: 2.7209 Iteration: 3921; Percent complete: 98.0%; Average loss: 2.7160 Iteration: 3922; Percent complete: 98.0%; Average loss: 2.8307 Iteration: 3923; Percent complete: 98.1%; Average loss: 2.7633 Iteration: 3924; Percent complete: 98.1%; Average loss: 2.9466 Iteration: 3925; Percent complete: 98.1%; Average loss: 2.5440 Iteration: 3926; Percent complete: 98.2%; Average loss: 2.4603 Iteration: 3927; Percent complete: 98.2%; Average loss: 2.5636 Iteration: 3928; Percent complete: 98.2%; Average loss: 2.9691 Iteration: 3929; Percent complete: 98.2%; Average loss: 2.6196 Iteration: 3930; Percent complete: 98.2%; Average loss: 2.5323 Iteration: 3931; Percent complete: 98.3%; Average loss: 2.3430 Iteration: 3932; Percent complete: 98.3%; Average loss: 2.9050 Iteration: 3933; Percent complete: 98.3%; Average loss: 2.5606 Iteration: 3934; Percent complete: 98.4%; Average loss: 2.7507 Iteration: 3935; Percent complete: 98.4%; Average loss: 2.7561 Iteration: 3936; Percent complete: 98.4%; Average loss: 2.6409 Iteration: 3937; Percent complete: 98.4%; Average loss: 2.8119 Iteration: 3938; Percent complete: 98.5%; Average loss: 2.5014 Iteration: 3939; Percent complete: 98.5%; Average loss: 2.7470 Iteration: 3940; Percent complete: 98.5%; Average loss: 2.6831 Iteration: 3941; Percent complete: 98.5%; Average loss: 2.6739 Iteration: 3942; Percent complete: 98.6%; Average loss: 2.5648 Iteration: 3943; Percent complete: 98.6%; Average loss: 2.7060 Iteration: 3944; Percent complete: 98.6%; Average loss: 2.7278 Iteration: 3945; Percent complete: 98.6%; Average loss: 2.7134 Iteration: 3946; Percent complete: 98.7%; Average loss: 2.5586 Iteration: 3947; Percent complete: 98.7%; Average loss: 2.6631 Iteration: 3948; Percent complete: 98.7%; Average loss: 2.4562 Iteration: 3949; Percent complete: 98.7%; Average loss: 2.6221 Iteration: 3950; Percent complete: 98.8%; Average loss: 2.5409 Iteration: 3951; Percent complete: 98.8%; Average loss: 2.7434 Iteration: 3952; Percent complete: 98.8%; Average loss: 2.6169 Iteration: 3953; Percent complete: 98.8%; Average loss: 2.3909 Iteration: 3954; Percent complete: 98.9%; Average loss: 2.6249 Iteration: 3955; Percent complete: 98.9%; Average loss: 2.7647 Iteration: 3956; Percent complete: 98.9%; Average loss: 2.6571 Iteration: 3957; Percent complete: 98.9%; Average loss: 2.4930 Iteration: 3958; Percent complete: 99.0%; Average loss: 2.4042 Iteration: 3959; Percent complete: 99.0%; Average loss: 2.9591 Iteration: 3960; Percent complete: 99.0%; Average loss: 2.6932 Iteration: 3961; Percent complete: 99.0%; Average loss: 2.5330 Iteration: 3962; Percent complete: 99.1%; Average loss: 2.7501 Iteration: 3963; Percent complete: 99.1%; Average loss: 2.7160 Iteration: 3964; Percent complete: 99.1%; Average loss: 2.5589 Iteration: 3965; Percent complete: 99.1%; Average loss: 2.5702 Iteration: 3966; Percent complete: 99.2%; Average loss: 2.7847 Iteration: 3967; Percent complete: 99.2%; Average loss: 2.6711 Iteration: 3968; Percent complete: 99.2%; Average loss: 2.6549 Iteration: 3969; Percent complete: 99.2%; Average loss: 2.7142 Iteration: 3970; Percent complete: 99.2%; Average loss: 2.5803 Iteration: 3971; Percent complete: 99.3%; Average loss: 2.5902 Iteration: 3972; Percent complete: 99.3%; Average loss: 2.5613 Iteration: 3973; Percent complete: 99.3%; Average loss: 2.6055 Iteration: 3974; Percent complete: 99.4%; Average loss: 2.6768 Iteration: 3975; Percent complete: 99.4%; Average loss: 2.5305 Iteration: 3976; Percent complete: 99.4%; Average loss: 2.8735 Iteration: 3977; Percent complete: 99.4%; Average loss: 2.6139 Iteration: 3978; Percent complete: 99.5%; Average loss: 2.5500 Iteration: 3979; Percent complete: 99.5%; Average loss: 2.3924 Iteration: 3980; Percent complete: 99.5%; Average loss: 2.4942 Iteration: 3981; Percent complete: 99.5%; Average loss: 2.5486 Iteration: 3982; Percent complete: 99.6%; Average loss: 2.6154 Iteration: 3983; Percent complete: 99.6%; Average loss: 2.5299 Iteration: 3984; Percent complete: 99.6%; Average loss: 2.5456 Iteration: 3985; Percent complete: 99.6%; Average loss: 2.6009 Iteration: 3986; Percent complete: 99.7%; Average loss: 2.8824 Iteration: 3987; Percent complete: 99.7%; Average loss: 2.6323 Iteration: 3988; Percent complete: 99.7%; Average loss: 2.5172 Iteration: 3989; Percent complete: 99.7%; Average loss: 2.6723 Iteration: 3990; Percent complete: 99.8%; Average loss: 2.7110 Iteration: 3991; Percent complete: 99.8%; Average loss: 2.7118 Iteration: 3992; Percent complete: 99.8%; Average loss: 2.4058 Iteration: 3993; Percent complete: 99.8%; Average loss: 2.3599 Iteration: 3994; Percent complete: 99.9%; Average loss: 2.4839 Iteration: 3995; Percent complete: 99.9%; Average loss: 2.4875 Iteration: 3996; Percent complete: 99.9%; Average loss: 2.5872 Iteration: 3997; Percent complete: 99.9%; Average loss: 2.6011 Iteration: 3998; Percent complete: 100.0%; Average loss: 2.6643 Iteration: 3999; Percent complete: 100.0%; Average loss: 2.6179 Iteration: 4000; Percent complete: 100.0%; Average loss: 2.4628","title":"\u8fd0\u884c\u8bad\u7ec3"},{"location":"beginner/chatbot_tutorial/#\u8fd0\u884c\u8bc4\u4f30","text":"\u8981\u548c\u8fd9\u4e2a\u6a21\u578b\u804a\u5929\uff0c\u8fd0\u884c\u4e0b\u9762\u7684\u4ee3\u7801\u5757\u3002 1 2 3 4 5 6 7 8 9 # Set dropout layers to eval mode encoder . eval () decoder . eval () # Initialize search module searcher = GreedySearchDecoder ( encoder , decoder ) # Begin chatting (uncomment and run the following line to begin) # evaluateInput(encoder, decoder, searcher, voc)","title":"\u8fd0\u884c\u8bc4\u4f30"},{"location":"beginner/chatbot_tutorial/#\u7ed3\u8bba","text":"\u4f19\u8ba1\u4eec\uff0c\u8fd9\u5c31\u662f\u8fd9\u4e00\u5207\u3002\u606d\u559c\u4f60\uff0c\u73b0\u5728\u77e5\u9053\u4e86\u6784\u5efa\u751f\u6210\u804a\u5929\u673a\u5668\u4eba\u6a21\u578b\u7684\u57fa\u7840\u77e5\u8bc6\uff01\u5982\u679c\u6709\u5174\u8da3\uff0c\u53ef\u4ee5\u5c1d\u8bd5\u901a\u8fc7\u8c03\u6574\u6a21\u578b\u548c\u8bad\u7ec3\u53c2\u6570\u4ee5\u53ca\u81ea\u5b9a\u4e49\u8bad\u7ec3\u6a21\u578b\u7684\u6570\u636e\u6765\u5b9a\u5236\u804a\u5929\u673a\u5668\u4eba\u7684\u884c\u4e3a\u3002 \u67e5\u770b\u5176\u4ed6\u6559\u7a0b\uff0c\u4e86\u89e3PyTorch\u4e2d\u66f4\u9177\u7684\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528\u7a0b\u5e8f\uff01","title":"\u7ed3\u8bba"},{"location":"beginner/data_loading_tutorial/","text":"\u6570\u636e\u52a0\u8f7d\u548c\u5904\u7406\u6559\u7a0b \u00b6 \u4f5c\u8005 \uff1a Sasank Chilamkurthy \u5bf9\u4e8e\u4efb\u4f55\u673a\u5668\u5b66\u4e60\u7684\u95ee\u9898\uff0c\u90fd\u4f1a\u6709\u5927\u91cf\u7684\u52aa\u529b\u88ab\u7528\u5728\u51c6\u5907\u6570\u636e\u4e0a\u9762\u3002 PyTorch\u63d0\u4f9b\u4e86\u8bb8\u591a\u5de5\u5177\u6765\u7b80\u5316\u6570\u636e\u52a0\u8f7d\u3001\u589e\u5f3a\u4ee3\u7801\u53ef\u8bfb\u6027\u3002 \u7740\u8fd9\u4e2a\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u770b\u5230\u5982\u4f55\u5728\u4e00\u4e2a\u975e\u51e1\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u52a0\u8f7d\u3001\u9884\u5904\u7406\u548c\u6269\u5145\u3002 \u8981\u8fd0\u884c\u8fd9\u4e2a\u6559\u7a0b\uff0c\u786e\u5b9a\u5b89\u88c5\u4e86\u4ee5\u4e0b\u5305\uff1a scikit-image \uff1a\u7528\u4e8e\u56fe\u50cfIO\u548c\u53d8\u6362 pandas \uff1a\u7528\u4e8e\u7b80\u5316CVS\u89e3\u6790 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from __future__ import print_function , division import os import torch import pandas as pd from skimage import io , transform import numpy as np import matplotlib.pyplot as plt from torch.utils.data import Dataset , DataLoader from torchvision import transforms , utils # Ignore warnings import warnings warnings . filterwarnings ( \"ignore\" ) plt . ion () # interactive mode \u6211\u4eec\u8981\u5904\u7406\u7684\u6570\u636e\u96c6\u662f\u4eba\u8138\u59ff\u6001\u3002 \u9762\u90e8\u6807\u6ce8\u5c31\u50cf\u8fd9\u6837\uff1a \u603b\u4e4b\uff0c\u6bcf\u4e2a\u9762\u90e8\u90fd\u6ce8\u91ca\u4e8668\u4e2a\u4e0d\u540c\u7684\u5173\u952e\u7279\u5f81\u70b9(landmark)\u3002 \u6ce8\u610f \uff1a \u4ece https://download.pytorch.org/tutorial/faces.zip \u4e0b\u8f7d\u8fd9\u4e2a\u6570\u636e\u96c6\uff0c\u56fe\u7247\u5728\u76ee\u5f55 data/faces/ \u3002 \u8fd9\u4e2a\u6570\u636e\u96c6\u5b9e\u9645\u4e0a\u662f\u901a\u8fc7\u5bf9\u6765\u81eaimagenet\u6807\u6ce8\u4e3a\u201dface\u201d\u7684\u4e00\u4e9b\u56fe\u50cf\u5e94\u7528 dlib \u8fd9\u4e00\u4f18\u79c0\u7684\u59ff\u6001\u4f30\u8ba1\u6cd5\u5f97\u6765\u7684\u3002 \u8fd9\u4e2a\u6570\u636e\u96c6\u91cc\u6709\u4e00\u4e2a\u5e26\u6709\u6807\u6ce8\u7684CSV\u6587\u4ef6\uff0c\u5b83\u770b\u8d77\u6765\u662f\u8fd9\u6837\u7684\uff1a image_name part_0_x part_0_y part_1_x part_1_y part_2_x \u2026 part_67_x part_67_y 0805personali01.jpg 27 83 27 98 \u2026 \u2026 84 134 1084239450_e76e00b7e7.jpg 70 236 71 257 \u2026 \u2026 128 312 \u8bfb\u53d6CSV\u5e76\u628a\u6807\u6ce8\u6570\u636e\u653e\u5230(N,2)\u6570\u7ec4\u4e2d\uff0c\u5176\u4e2dN\u662f\u5173\u952e\u7279\u5f81\u70b9\u7684\u6570\u91cf\u3002 1 2 3 4 5 6 7 8 9 10 landmarks_frame = pd . read_csv ( 'data/faces/face_landmarks.csv' ) n = 65 img_name = landmarks_frame . iloc [ n , 0 ] landmarks = landmarks_frame . iloc [ n , 1 :] . as_matrix () landmarks = landmarks . astype ( 'float' ) . reshape ( - 1 , 2 ) print ( 'Image name: {}' . format ( img_name )) print ( 'Landmarks shape: {}' . format ( landmarks . shape )) print ( 'First 4 Landmarks: {}' . format ( landmarks [: 4 ])) 1 2 3 4 5 6 Image name: person-7.jpg Landmarks shape: (68, 2) First 4 Landmarks: [[32. 65.] [33. 76.] [34. 86.] [34. 97.]] \u7f16\u5199\u4e00\u4e2a\u53ef\u4ee5\u663e\u793a\u56fe\u50cf\u53ca\u5176\u5173\u952e\u7279\u5f81\u70b9\u7684\u7b80\u5355\u8f85\u52a9\u51fd\u6570\uff0c\u7528\u5b83\u663e\u793a\u6837\u672c\u3002 1 2 3 4 5 6 7 8 9 def show_landmarks ( image , landmarks ): \"\"\"Show image with landmarks\"\"\" plt . imshow ( image ) plt . scatter ( landmarks [:, 0 ], landmarks [:, 1 ], s = 10 , marker = '.' , c = 'r' ) plt . pause ( 0.001 ) # pause a bit so that plots are updated plt . figure () show_landmarks ( io . imread ( os . path . join ( 'data/faces/' , img_name )), landmarks ) plt . show () Dataset \u7c7b \u00b6 torch.utils.data.Dataset \u662f\u9488\u5bf9\u6570\u636e\u96c6\u7684\u62bd\u8c61\u7c7b. \u6211\u4eec\u81ea\u5df1\u7684\u6570\u636e\u96c6\u7c7b\u5e94\u8be5\u7ee7\u627f\u81ea Dataset \uff0c\u8986\u76d6\u4ee5\u4e0b\u65b9\u6cd5\uff1a __len__ \uff1a\u7528 len(dataset) \u53ef\u4ee5\u8fd4\u56de\u6570\u636e\u96c6\u7684\u5927\u5c0f\u3002 __getitem__ \uff1a\u4f7f\u5f97\u50cf dataset[i] \u8fd9\u6837\u7684\u7d22\u5f15\u64cd\u4f5c\u53ef\u4ee5\u7528\u4e8e\u8bbf\u95ee\u7b2c i i \u4e2a\u6837\u672c \u73b0\u5728\uff0c\u4e3a\u6211\u4eec\u81ea\u5df1\u7684\u9762\u90e8\u5173\u952e\u7279\u5f81\u70b9\u6570\u636e\u96c6\u521b\u5efa\u4e00\u4e2a Dataset \u7c7b\u3002 \u6211\u4eec\u5728 __init__ \u8bfb\u53d6CSV\uff0c\u4f46\u662f\u628a\u8bfb\u53d6\u56fe\u7247\u7684\u5de5\u4f5c\u7559\u7ed9 __getitem__ \u3002 \u8fd9\u6837\u53ef\u4ee5\u8282\u7701\u5185\u5b58\uff0c\u56e0\u4e3a\u56fe\u7247\u4e0d\u662f\u4e00\u6b21\u6027\u7684\u5168\u90e8\u8bfb\u8fdb\u5185\u5b58\uff0c\u800c\u662f\u9700\u8981\u65f6\u624d\u8bfb\u53d6\u3002 \u6211\u4eec\u7684\u6570\u636e\u96c6\u4e2d\u7684\u6837\u672c\u5c06\u662f\u4e00\u4e2a dict \uff1a 1 { 'image' : image , 'landmarks' : landmarks } \u6211\u4eec\u7684\u6570\u636e\u96c6\u5c06\u63d0\u4f9b\u4e00\u4e2a\u53ef\u9009\u7684\u53c2\u6570 transform \uff0c\u8fd9\u6837\u4efb\u4f55\u9700\u8981\u7684\u5904\u7406\u8fc7\u7a0b\u53ef\u4ee5\u5728\u6837\u672c\u4e0a\u6267\u884c\u3002 \u5728\u4e0b\u4e00\u8282\u53ef\u4ee5\u770b\u5230 transform \u7684\u4f5c\u7528\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class FaceLandmarksDataset ( Dataset ): \"\"\"Face Landmarks dataset.\"\"\" def __init__ ( self , csv_file , root_dir , transform = None ): \"\"\" Args: csv_file (string): Path to the csv file with annotations. root_dir (string): Directory with all the images. transform (callable, optional): Optional transform to be applied on a sample. \"\"\" self . landmarks_frame = pd . read_csv ( csv_file ) self . root_dir = root_dir self . transform = transform def __len__ ( self ): return len ( self . landmarks_frame ) def __getitem__ ( self , idx ): img_name = os . path . join ( self . root_dir , self . landmarks_frame . iloc [ idx , 0 ]) image = io . imread ( img_name ) landmarks = self . landmarks_frame . iloc [ idx , 1 :] . as_matrix () landmarks = landmarks . astype ( 'float' ) . reshape ( - 1 , 2 ) sample = { 'image' : image , 'landmarks' : landmarks } if self . transform : sample = self . transform ( sample ) return sample \u5b9e\u4f8b\u5316\u8fd9\u4e2a\u7c7b\u5e76\u4e14\u904d\u5386\u6570\u636e\u6837\u672c\uff0c\u6253\u5370\u59344\u4e2a\u6837\u672c\u7684\u5927\u5c0f\u548c\u5173\u952e\u7279\u5f81\u70b9\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 face_dataset = FaceLandmarksDataset ( csv_file = 'data/faces/face_landmarks.csv' , root_dir = 'data/faces/' ) fig = plt . figure () for i , sample in enumerate ( face_dataset ): print ( i , sample [ 'image' ] . shape , sample [ 'landmarks' ] . shape ) ax = plt . subplot ( 1 , 4 , i + 1 ) plt . tight_layout () ax . set_title ( 'Sample #{}' . format ( i )) ax . axis ( 'off' ) show_landmarks ( ** sample ) if i == 3 : plt . show () break 1 0 (324, 215, 3) (68, 2) 1 1 (500, 333, 3) (68, 2) 1 2 (250, 258, 3) (68, 2) 1 3 (434, 290, 3) (68, 2) \u53d8\u6362 \u00b6 \u89c2\u5bdf\u4e0a\u9762\u7684\u6837\u672c\uff0c\u6211\u4eec\u53d1\u73b0\u4e00\u4e2a\u95ee\u9898\u2014\u2014\u56fe\u50cf\u4e0d\u662f\u540c\u6837\u7684\u5927\u5c0f\u3002 \u5927\u591a\u6570\u795e\u7ecf\u7f51\u7edc\u90fd\u671f\u5f85\u56fa\u5b9a\u5927\u5c0f\u7684\u56fe\u50cf\u3002 \u56e0\u6b64\uff0c\u9700\u8981\u7f16\u5199\u4e00\u4e9b\u9884\u5904\u7406\u4ee3\u7801\u3002 \u6211\u4eec\u521b\u5efa\u4e09\u79cd\u53d8\u6362\uff1a Rescale : \u7f29\u653e\u56fe\u50cf RandomCrop : \u968f\u673a\u88c1\u526a\u56fe\u50cf\u3002\u8fd9\u7b97\u662f\u6570\u636e\u589e\u52a0\u6269\u5145\u3002 ToTensor : \u5c06numpy\u56fe\u50cf\u8f6c\u4e3atorch\u56fe\u50cf\uff08\u9700\u8981\u4ea4\u6362\u5750\u6807\u8f74\uff09\u3002 \u628a\u4ed6\u4eec\u7f16\u5199\u4e3a\u53ef\u8c03\u7528\u7c7b\uff0c\u800c\u4e0d\u662f\u51fd\u6570\uff0c\u8fd9\u6837\uff0c\u6bcf\u6b21\u8c03\u7528\u5c31\u4e0d\u7528\u4f20\u9012\u53d8\u6362\u7684\u53c2\u6570\u3002 \u4e3a\u6b64\uff0c\u6211\u4eec\u9700\u8981\u5b9e\u73b0 __call__ \u65b9\u6cd5\uff0c\u5982\u679c\u6709\u5fc5\u8981\uff0c\u8fd8\u6709 __init__ \u65b9\u6cd5\u3002 \u7136\u540e\u50cf\u8fd9\u6837\u8fdb\u884c\u53d8\u6362\uff1a 1 2 tsfm = Transform ( params ) transformed_sample = tsfm ( sample ) \u8bf7\u6ce8\u610f\u4ee5\u4e0b\u8fd9\u4e9b\u53d8\u6362\u5982\u4f55\u88ab\u5e94\u7528\u4e8e\u56fe\u50cf\u548c\u5173\u952e\u7279\u5f81\u70b9\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 class Rescale ( object ): \"\"\"Rescale the image in a sample to a given size. Args: output_size (tuple or int): Desired output size. If tuple, output is matched to output_size. If int, smaller of image edges is matched to output_size keeping aspect ratio the same. \"\"\" def __init__ ( self , output_size ): assert isinstance ( output_size , ( int , tuple )) self . output_size = output_size def __call__ ( self , sample ): image , landmarks = sample [ 'image' ], sample [ 'landmarks' ] h , w = image . shape [: 2 ] if isinstance ( self . output_size , int ): if h > w : new_h , new_w = self . output_size * h / w , self . output_size else : new_h , new_w = self . output_size , self . output_size * w / h else : new_h , new_w = self . output_size new_h , new_w = int ( new_h ), int ( new_w ) img = transform . resize ( image , ( new_h , new_w )) # h and w are swapped for landmarks because for images, # x and y axes are axis 1 and 0 respectively landmarks = landmarks * [ new_w / w , new_h / h ] return { 'image' : img , 'landmarks' : landmarks } class RandomCrop ( object ): \"\"\"Crop randomly the image in a sample. Args: output_size (tuple or int): Desired output size. If int, square crop is made. \"\"\" def __init__ ( self , output_size ): assert isinstance ( output_size , ( int , tuple )) if isinstance ( output_size , int ): self . output_size = ( output_size , output_size ) else : assert len ( output_size ) == 2 self . output_size = output_size def __call__ ( self , sample ): image , landmarks = sample [ 'image' ], sample [ 'landmarks' ] h , w = image . shape [: 2 ] new_h , new_w = self . output_size top = np . random . randint ( 0 , h - new_h ) left = np . random . randint ( 0 , w - new_w ) image = image [ top : top + new_h , left : left + new_w ] landmarks = landmarks - [ left , top ] return { 'image' : image , 'landmarks' : landmarks } class ToTensor ( object ): \"\"\"Convert ndarrays in sample to Tensors.\"\"\" def __call__ ( self , sample ): image , landmarks = sample [ 'image' ], sample [ 'landmarks' ] # swap color axis because # numpy image: H x W x C # torch image: C X H X W image = image . transpose (( 2 , 0 , 1 )) return { 'image' : torch . from_numpy ( image ), 'landmarks' : torch . from_numpy ( landmarks )} \u7ec4\u5408\u8f6c\u6362 \u00b6 \u4e0b\u9762\uff0c\u5728\u6837\u672c\u4e0a\u5e94\u7528\u8f6c\u6362\u3002 \u5047\u8bbe\u6211\u4eec\u60f3\u8981\u5c06\u56fe\u50cf\u7684\u77ed\u8fb9\u91cd\u65b0\u7f29\u653e\u5230256\uff0c\u7136\u540e\u4ece\u4e2d\u968f\u673a\u88c1\u526a\u4e00\u4e2a224\u7684\u6b63\u65b9\u5f62\u3002 \u4e5f\u5c31\u662f\u8bf4\uff0c\u6211\u4eec\u9700\u8981\u7ec4\u5408 Rescale \u548c RandomCrop \u8f6c\u6362\u3002 torchvision.transforms.Compose \u662f\u4e00\u4e2a\u7b80\u5355\u7684\u53ef\u8c03\u7528\u7c7b\uff0c\u5b83\u7528\u4e8e\u5b8c\u6210\u8fd9\u6837\u7684\u5de5\u4f5c\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 scale = Rescale ( 256 ) crop = RandomCrop ( 128 ) composed = transforms . Compose ([ Rescale ( 256 ), RandomCrop ( 224 )]) # Apply each of the above transforms on sample. fig = plt . figure () sample = face_dataset [ 65 ] for i , tsfrm in enumerate ([ scale , crop , composed ]): transformed_sample = tsfrm ( sample ) ax = plt . subplot ( 1 , 3 , i + 1 ) plt . tight_layout () ax . set_title ( type ( tsfrm ) . __name__ ) show_landmarks ( ** transformed_sample ) plt . show () \u904d\u5386\u6570\u636e\u96c6 \u00b6 \u628a\u4e0a\u9762\u7684\u4f60\u540c\u653e\u5728\u4e00\u8d77\uff0c\u521b\u5efa\u4e00\u4e2a\u5e26\u6709\u7ec4\u5408\u53d8\u6362\u7684\u6570\u636e\u96c6\u3002 \u603b\u800c\u8a00\u4e4b\uff0c\u6bcf\u6b21\u4ece\u8fd9\u4e2a\u6570\u636e\u96c6\u91c7\u7528\u7684\u65f6\u5019\uff1a \u4ece\u6587\u4ef6\u4e2d\u5373\u65f6\u7684\u8bfb\u53d6\u56fe\u50cf \u5bf9\u8bfb\u53d6\u7684\u56fe\u50cf\u8fdb\u884c\u53d8\u6362 \u7531\u4e8e\u5176\u4e2d\u4e00\u4e2a\u53d8\u5316\u662f\u968f\u673a\u7684\uff0c\u6570\u636e\u5728\u91c7\u6837\u65f6\u662f\u6269\u5145\u7684 \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u4e4b\u524d\u7528\u8fc7\u7684 for ... in \u5faa\u73af\u904d\u5386\u6570\u636e\u96c6\u3002 1 2 3 4 5 6 7 8 9 10 11 12 transformed_dataset = FaceLandmarksDataset ( csv_file = 'data/faces/face_landmarks.csv' , root_dir = 'data/faces/' , transform = transforms . Compose ([ Rescale ( 256 ), RandomCrop ( 224 ), ToTensor () ])) for i , sample in enumerate ( transformed_dataset ): print ( i , sample [ 'image' ] . size (), sample [ 'landmarks' ] . size ()) if i == 3 : break 1 2 3 4 0 torch.Size([3, 224, 224]) torch.Size([68, 2]) 1 torch.Size([3, 224, 224]) torch.Size([68, 2]) 2 torch.Size([3, 224, 224]) torch.Size([68, 2]) 3 torch.Size([3, 224, 224]) torch.Size([68, 2]) \u4e0d\u8fc7\uff0c\u7b80\u5355\u7684 for \u5faa\u73af\u904d\u5386\u6570\u636e\u8ba9\u6211\u4eec\u4e22\u5931\u4e86\u4e0d\u5c11\u7279\u6027\uff0c\u7279\u522b\u662f\uff1a \u6570\u636e\u7684\u6279\u5904\u7406 \u4e71\u5e8f\u6392\u5217\u6570\u636e \u4f7f\u7528 multiprocessing \u5e76\u884c\u52a0\u8f7d\u6570\u636e\u3002 torch.utils.data.DataLoader \u662f\u4e00\u4e2a\u8fed\u4ee3\u5668\uff0c\u5b83\u63d0\u4f9b\u4e86\u6240\u6709\u8fd9\u4e9b\u7279\u6027\u3002 \u4e0b\u9762\u7684\u53c2\u6570\u5e94\u8be5\u6e05\u9664\u7684\u8bf4\u660e\u4e86\u95ee\u9898\u3002 \u5176\u4e2d\u4e00\u4e2a\u6709\u8da3\u7684\u53c2\u6570\u662f collate_fn \uff0c\u53ef\u4ee5\u6307\u5b9a\u54ea\u4e9b\u5177\u4f53\u7684\u6837\u672c\u9700\u8981\u5728\u6279\u6b21\u4e2d\u4f7f\u7528 collate_fn \u8fdb\u884c\u6574\u7406\u3002 \u5f53\u7136\uff0c\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u9ed8\u8ba4\u7684\u6574\u7406(collat)\u5c31\u53ef\u4ee5\u4e86\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 dataloader = DataLoader ( transformed_dataset , batch_size = 4 , shuffle = True , num_workers = 4 ) # Helper function to show a batch def show_landmarks_batch ( sample_batched ): \"\"\"Show image with landmarks for a batch of samples.\"\"\" images_batch , landmarks_batch = \\ sample_batched [ 'image' ], sample_batched [ 'landmarks' ] batch_size = len ( images_batch ) im_size = images_batch . size ( 2 ) grid = utils . make_grid ( images_batch ) plt . imshow ( grid . numpy () . transpose (( 1 , 2 , 0 ))) for i in range ( batch_size ): plt . scatter ( landmarks_batch [ i , :, 0 ] . numpy () + i * im_size , landmarks_batch [ i , :, 1 ] . numpy (), s = 10 , marker = '.' , c = 'r' ) plt . title ( 'Batch from dataloader' ) for i_batch , sample_batched in enumerate ( dataloader ): print ( i_batch , sample_batched [ 'image' ] . size (), sample_batched [ 'landmarks' ] . size ()) # observe 4th batch and stop. if i_batch == 3 : plt . figure () show_landmarks_batch ( sample_batched ) plt . axis ( 'off' ) plt . ioff () plt . show () break 1 2 3 4 0 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2]) 1 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2]) 2 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2]) 3 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2]) \u540e\u8bb0\uff1atorchvision \u00b6 \u5728\u8fd9\u4e2a\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5df2\u7ecf\u770b\u5230\u4e86\u5982\u4f55\u7f16\u5199\u548c\u4f7f\u7528 Dataset , transform \u548c Dataloader \u3002 torchvision \u5305\u63d0\u4f9b\u4e86\u4e00\u4e9b\u5e38\u7528\u7684\u6570\u636e\u96c6\u548c\u53d8\u6362\u65b9\u6cd5\uff0c\u7528\u5b83\uff0c\u751a\u81f3\u53ef\u80fd\u4e0d\u7528\u7f16\u5199\u81ea\u5df1\u7684\u7c7b\u3002 torchvision \u4e2d\u8fd8\u6709\u66f4\u901a\u7528\u7684\u6570\u636e\u96c6\u7c7b\u578b\uff0c\u5176\u4e2d\u4e00\u4e2a\u662f ImageFolder \u3002 \u5b83\u5047\u5b9a\u56fe\u50cf\u6309\u4ee5\u4e0b\u65b9\u5f0f\u7ec4\u7ec7\uff1a 1 2 3 4 5 6 7 8 9 root/ants/xxx.png root/ants/xxy.jpeg root/ants/xxz.png . . . root/bees/123.jpg root/bees/nsdf3.png root/bees/asd932_.png \u5176\u4e2d\uff0c\u2019ants\u2019\u3001\u2019bees\u2019\u7b49\u662f\u5206\u7c7b\u6807\u7b7e\u3002 \u5b83\u5177\u5907\u7c7b\u4f3c PIL.Image \u7684\u901a\u7528\u53d8\u6362\u65b9\u6cd5\uff0c\u50cf RandomHorizontalFlip \u3001 Scale \u3002 \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u5b83\u4eec\u6765\u7f16\u5199\u50cf\u8fd9\u6837\u7684 Dataloader \uff1a 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import torch from torchvision import transforms , datasets data_transform = transforms . Compose ([ transforms . RandomSizedCrop ( 224 ), transforms . RandomHorizontalFlip (), transforms . ToTensor (), transforms . Normalize ( mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ]) ]) hymenoptera_dataset = datasets . ImageFolder ( root = 'hymenoptera_data/train' , transform = data_transform ) dataset_loader = torch . utils . data . DataLoader ( hymenoptera_dataset , batch_size = 4 , shuffle = True , num_workers = 4 ) \u6709\u5173\u6559\u7a0b\u7684\u4ee3\u7801\u793a\u4f8b\uff0c\u8bf7\u53c2\u9605 \u8fc1\u79fb\u5b66\u4e60\u6559\u7a0b \u3002","title":"\u6570\u636e\u52a0\u8f7d\u548c\u5904\u7406\u6559\u7a0b"},{"location":"beginner/data_loading_tutorial/#\u6570\u636e\u52a0\u8f7d\u548c\u5904\u7406\u6559\u7a0b","text":"\u4f5c\u8005 \uff1a Sasank Chilamkurthy \u5bf9\u4e8e\u4efb\u4f55\u673a\u5668\u5b66\u4e60\u7684\u95ee\u9898\uff0c\u90fd\u4f1a\u6709\u5927\u91cf\u7684\u52aa\u529b\u88ab\u7528\u5728\u51c6\u5907\u6570\u636e\u4e0a\u9762\u3002 PyTorch\u63d0\u4f9b\u4e86\u8bb8\u591a\u5de5\u5177\u6765\u7b80\u5316\u6570\u636e\u52a0\u8f7d\u3001\u589e\u5f3a\u4ee3\u7801\u53ef\u8bfb\u6027\u3002 \u7740\u8fd9\u4e2a\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u770b\u5230\u5982\u4f55\u5728\u4e00\u4e2a\u975e\u51e1\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u52a0\u8f7d\u3001\u9884\u5904\u7406\u548c\u6269\u5145\u3002 \u8981\u8fd0\u884c\u8fd9\u4e2a\u6559\u7a0b\uff0c\u786e\u5b9a\u5b89\u88c5\u4e86\u4ee5\u4e0b\u5305\uff1a scikit-image \uff1a\u7528\u4e8e\u56fe\u50cfIO\u548c\u53d8\u6362 pandas \uff1a\u7528\u4e8e\u7b80\u5316CVS\u89e3\u6790 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from __future__ import print_function , division import os import torch import pandas as pd from skimage import io , transform import numpy as np import matplotlib.pyplot as plt from torch.utils.data import Dataset , DataLoader from torchvision import transforms , utils # Ignore warnings import warnings warnings . filterwarnings ( \"ignore\" ) plt . ion () # interactive mode \u6211\u4eec\u8981\u5904\u7406\u7684\u6570\u636e\u96c6\u662f\u4eba\u8138\u59ff\u6001\u3002 \u9762\u90e8\u6807\u6ce8\u5c31\u50cf\u8fd9\u6837\uff1a \u603b\u4e4b\uff0c\u6bcf\u4e2a\u9762\u90e8\u90fd\u6ce8\u91ca\u4e8668\u4e2a\u4e0d\u540c\u7684\u5173\u952e\u7279\u5f81\u70b9(landmark)\u3002 \u6ce8\u610f \uff1a \u4ece https://download.pytorch.org/tutorial/faces.zip \u4e0b\u8f7d\u8fd9\u4e2a\u6570\u636e\u96c6\uff0c\u56fe\u7247\u5728\u76ee\u5f55 data/faces/ \u3002 \u8fd9\u4e2a\u6570\u636e\u96c6\u5b9e\u9645\u4e0a\u662f\u901a\u8fc7\u5bf9\u6765\u81eaimagenet\u6807\u6ce8\u4e3a\u201dface\u201d\u7684\u4e00\u4e9b\u56fe\u50cf\u5e94\u7528 dlib \u8fd9\u4e00\u4f18\u79c0\u7684\u59ff\u6001\u4f30\u8ba1\u6cd5\u5f97\u6765\u7684\u3002 \u8fd9\u4e2a\u6570\u636e\u96c6\u91cc\u6709\u4e00\u4e2a\u5e26\u6709\u6807\u6ce8\u7684CSV\u6587\u4ef6\uff0c\u5b83\u770b\u8d77\u6765\u662f\u8fd9\u6837\u7684\uff1a image_name part_0_x part_0_y part_1_x part_1_y part_2_x \u2026 part_67_x part_67_y 0805personali01.jpg 27 83 27 98 \u2026 \u2026 84 134 1084239450_e76e00b7e7.jpg 70 236 71 257 \u2026 \u2026 128 312 \u8bfb\u53d6CSV\u5e76\u628a\u6807\u6ce8\u6570\u636e\u653e\u5230(N,2)\u6570\u7ec4\u4e2d\uff0c\u5176\u4e2dN\u662f\u5173\u952e\u7279\u5f81\u70b9\u7684\u6570\u91cf\u3002 1 2 3 4 5 6 7 8 9 10 landmarks_frame = pd . read_csv ( 'data/faces/face_landmarks.csv' ) n = 65 img_name = landmarks_frame . iloc [ n , 0 ] landmarks = landmarks_frame . iloc [ n , 1 :] . as_matrix () landmarks = landmarks . astype ( 'float' ) . reshape ( - 1 , 2 ) print ( 'Image name: {}' . format ( img_name )) print ( 'Landmarks shape: {}' . format ( landmarks . shape )) print ( 'First 4 Landmarks: {}' . format ( landmarks [: 4 ])) 1 2 3 4 5 6 Image name: person-7.jpg Landmarks shape: (68, 2) First 4 Landmarks: [[32. 65.] [33. 76.] [34. 86.] [34. 97.]] \u7f16\u5199\u4e00\u4e2a\u53ef\u4ee5\u663e\u793a\u56fe\u50cf\u53ca\u5176\u5173\u952e\u7279\u5f81\u70b9\u7684\u7b80\u5355\u8f85\u52a9\u51fd\u6570\uff0c\u7528\u5b83\u663e\u793a\u6837\u672c\u3002 1 2 3 4 5 6 7 8 9 def show_landmarks ( image , landmarks ): \"\"\"Show image with landmarks\"\"\" plt . imshow ( image ) plt . scatter ( landmarks [:, 0 ], landmarks [:, 1 ], s = 10 , marker = '.' , c = 'r' ) plt . pause ( 0.001 ) # pause a bit so that plots are updated plt . figure () show_landmarks ( io . imread ( os . path . join ( 'data/faces/' , img_name )), landmarks ) plt . show ()","title":"\u6570\u636e\u52a0\u8f7d\u548c\u5904\u7406\u6559\u7a0b"},{"location":"beginner/data_loading_tutorial/#dataset-\u7c7b","text":"torch.utils.data.Dataset \u662f\u9488\u5bf9\u6570\u636e\u96c6\u7684\u62bd\u8c61\u7c7b. \u6211\u4eec\u81ea\u5df1\u7684\u6570\u636e\u96c6\u7c7b\u5e94\u8be5\u7ee7\u627f\u81ea Dataset \uff0c\u8986\u76d6\u4ee5\u4e0b\u65b9\u6cd5\uff1a __len__ \uff1a\u7528 len(dataset) \u53ef\u4ee5\u8fd4\u56de\u6570\u636e\u96c6\u7684\u5927\u5c0f\u3002 __getitem__ \uff1a\u4f7f\u5f97\u50cf dataset[i] \u8fd9\u6837\u7684\u7d22\u5f15\u64cd\u4f5c\u53ef\u4ee5\u7528\u4e8e\u8bbf\u95ee\u7b2c i i \u4e2a\u6837\u672c \u73b0\u5728\uff0c\u4e3a\u6211\u4eec\u81ea\u5df1\u7684\u9762\u90e8\u5173\u952e\u7279\u5f81\u70b9\u6570\u636e\u96c6\u521b\u5efa\u4e00\u4e2a Dataset \u7c7b\u3002 \u6211\u4eec\u5728 __init__ \u8bfb\u53d6CSV\uff0c\u4f46\u662f\u628a\u8bfb\u53d6\u56fe\u7247\u7684\u5de5\u4f5c\u7559\u7ed9 __getitem__ \u3002 \u8fd9\u6837\u53ef\u4ee5\u8282\u7701\u5185\u5b58\uff0c\u56e0\u4e3a\u56fe\u7247\u4e0d\u662f\u4e00\u6b21\u6027\u7684\u5168\u90e8\u8bfb\u8fdb\u5185\u5b58\uff0c\u800c\u662f\u9700\u8981\u65f6\u624d\u8bfb\u53d6\u3002 \u6211\u4eec\u7684\u6570\u636e\u96c6\u4e2d\u7684\u6837\u672c\u5c06\u662f\u4e00\u4e2a dict \uff1a 1 { 'image' : image , 'landmarks' : landmarks } \u6211\u4eec\u7684\u6570\u636e\u96c6\u5c06\u63d0\u4f9b\u4e00\u4e2a\u53ef\u9009\u7684\u53c2\u6570 transform \uff0c\u8fd9\u6837\u4efb\u4f55\u9700\u8981\u7684\u5904\u7406\u8fc7\u7a0b\u53ef\u4ee5\u5728\u6837\u672c\u4e0a\u6267\u884c\u3002 \u5728\u4e0b\u4e00\u8282\u53ef\u4ee5\u770b\u5230 transform \u7684\u4f5c\u7528\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class FaceLandmarksDataset ( Dataset ): \"\"\"Face Landmarks dataset.\"\"\" def __init__ ( self , csv_file , root_dir , transform = None ): \"\"\" Args: csv_file (string): Path to the csv file with annotations. root_dir (string): Directory with all the images. transform (callable, optional): Optional transform to be applied on a sample. \"\"\" self . landmarks_frame = pd . read_csv ( csv_file ) self . root_dir = root_dir self . transform = transform def __len__ ( self ): return len ( self . landmarks_frame ) def __getitem__ ( self , idx ): img_name = os . path . join ( self . root_dir , self . landmarks_frame . iloc [ idx , 0 ]) image = io . imread ( img_name ) landmarks = self . landmarks_frame . iloc [ idx , 1 :] . as_matrix () landmarks = landmarks . astype ( 'float' ) . reshape ( - 1 , 2 ) sample = { 'image' : image , 'landmarks' : landmarks } if self . transform : sample = self . transform ( sample ) return sample \u5b9e\u4f8b\u5316\u8fd9\u4e2a\u7c7b\u5e76\u4e14\u904d\u5386\u6570\u636e\u6837\u672c\uff0c\u6253\u5370\u59344\u4e2a\u6837\u672c\u7684\u5927\u5c0f\u548c\u5173\u952e\u7279\u5f81\u70b9\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 face_dataset = FaceLandmarksDataset ( csv_file = 'data/faces/face_landmarks.csv' , root_dir = 'data/faces/' ) fig = plt . figure () for i , sample in enumerate ( face_dataset ): print ( i , sample [ 'image' ] . shape , sample [ 'landmarks' ] . shape ) ax = plt . subplot ( 1 , 4 , i + 1 ) plt . tight_layout () ax . set_title ( 'Sample #{}' . format ( i )) ax . axis ( 'off' ) show_landmarks ( ** sample ) if i == 3 : plt . show () break 1 0 (324, 215, 3) (68, 2) 1 1 (500, 333, 3) (68, 2) 1 2 (250, 258, 3) (68, 2) 1 3 (434, 290, 3) (68, 2)","title":"Dataset \u7c7b"},{"location":"beginner/data_loading_tutorial/#\u53d8\u6362","text":"\u89c2\u5bdf\u4e0a\u9762\u7684\u6837\u672c\uff0c\u6211\u4eec\u53d1\u73b0\u4e00\u4e2a\u95ee\u9898\u2014\u2014\u56fe\u50cf\u4e0d\u662f\u540c\u6837\u7684\u5927\u5c0f\u3002 \u5927\u591a\u6570\u795e\u7ecf\u7f51\u7edc\u90fd\u671f\u5f85\u56fa\u5b9a\u5927\u5c0f\u7684\u56fe\u50cf\u3002 \u56e0\u6b64\uff0c\u9700\u8981\u7f16\u5199\u4e00\u4e9b\u9884\u5904\u7406\u4ee3\u7801\u3002 \u6211\u4eec\u521b\u5efa\u4e09\u79cd\u53d8\u6362\uff1a Rescale : \u7f29\u653e\u56fe\u50cf RandomCrop : \u968f\u673a\u88c1\u526a\u56fe\u50cf\u3002\u8fd9\u7b97\u662f\u6570\u636e\u589e\u52a0\u6269\u5145\u3002 ToTensor : \u5c06numpy\u56fe\u50cf\u8f6c\u4e3atorch\u56fe\u50cf\uff08\u9700\u8981\u4ea4\u6362\u5750\u6807\u8f74\uff09\u3002 \u628a\u4ed6\u4eec\u7f16\u5199\u4e3a\u53ef\u8c03\u7528\u7c7b\uff0c\u800c\u4e0d\u662f\u51fd\u6570\uff0c\u8fd9\u6837\uff0c\u6bcf\u6b21\u8c03\u7528\u5c31\u4e0d\u7528\u4f20\u9012\u53d8\u6362\u7684\u53c2\u6570\u3002 \u4e3a\u6b64\uff0c\u6211\u4eec\u9700\u8981\u5b9e\u73b0 __call__ \u65b9\u6cd5\uff0c\u5982\u679c\u6709\u5fc5\u8981\uff0c\u8fd8\u6709 __init__ \u65b9\u6cd5\u3002 \u7136\u540e\u50cf\u8fd9\u6837\u8fdb\u884c\u53d8\u6362\uff1a 1 2 tsfm = Transform ( params ) transformed_sample = tsfm ( sample ) \u8bf7\u6ce8\u610f\u4ee5\u4e0b\u8fd9\u4e9b\u53d8\u6362\u5982\u4f55\u88ab\u5e94\u7528\u4e8e\u56fe\u50cf\u548c\u5173\u952e\u7279\u5f81\u70b9\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 class Rescale ( object ): \"\"\"Rescale the image in a sample to a given size. Args: output_size (tuple or int): Desired output size. If tuple, output is matched to output_size. If int, smaller of image edges is matched to output_size keeping aspect ratio the same. \"\"\" def __init__ ( self , output_size ): assert isinstance ( output_size , ( int , tuple )) self . output_size = output_size def __call__ ( self , sample ): image , landmarks = sample [ 'image' ], sample [ 'landmarks' ] h , w = image . shape [: 2 ] if isinstance ( self . output_size , int ): if h > w : new_h , new_w = self . output_size * h / w , self . output_size else : new_h , new_w = self . output_size , self . output_size * w / h else : new_h , new_w = self . output_size new_h , new_w = int ( new_h ), int ( new_w ) img = transform . resize ( image , ( new_h , new_w )) # h and w are swapped for landmarks because for images, # x and y axes are axis 1 and 0 respectively landmarks = landmarks * [ new_w / w , new_h / h ] return { 'image' : img , 'landmarks' : landmarks } class RandomCrop ( object ): \"\"\"Crop randomly the image in a sample. Args: output_size (tuple or int): Desired output size. If int, square crop is made. \"\"\" def __init__ ( self , output_size ): assert isinstance ( output_size , ( int , tuple )) if isinstance ( output_size , int ): self . output_size = ( output_size , output_size ) else : assert len ( output_size ) == 2 self . output_size = output_size def __call__ ( self , sample ): image , landmarks = sample [ 'image' ], sample [ 'landmarks' ] h , w = image . shape [: 2 ] new_h , new_w = self . output_size top = np . random . randint ( 0 , h - new_h ) left = np . random . randint ( 0 , w - new_w ) image = image [ top : top + new_h , left : left + new_w ] landmarks = landmarks - [ left , top ] return { 'image' : image , 'landmarks' : landmarks } class ToTensor ( object ): \"\"\"Convert ndarrays in sample to Tensors.\"\"\" def __call__ ( self , sample ): image , landmarks = sample [ 'image' ], sample [ 'landmarks' ] # swap color axis because # numpy image: H x W x C # torch image: C X H X W image = image . transpose (( 2 , 0 , 1 )) return { 'image' : torch . from_numpy ( image ), 'landmarks' : torch . from_numpy ( landmarks )}","title":"\u53d8\u6362"},{"location":"beginner/data_loading_tutorial/#\u7ec4\u5408\u8f6c\u6362","text":"\u4e0b\u9762\uff0c\u5728\u6837\u672c\u4e0a\u5e94\u7528\u8f6c\u6362\u3002 \u5047\u8bbe\u6211\u4eec\u60f3\u8981\u5c06\u56fe\u50cf\u7684\u77ed\u8fb9\u91cd\u65b0\u7f29\u653e\u5230256\uff0c\u7136\u540e\u4ece\u4e2d\u968f\u673a\u88c1\u526a\u4e00\u4e2a224\u7684\u6b63\u65b9\u5f62\u3002 \u4e5f\u5c31\u662f\u8bf4\uff0c\u6211\u4eec\u9700\u8981\u7ec4\u5408 Rescale \u548c RandomCrop \u8f6c\u6362\u3002 torchvision.transforms.Compose \u662f\u4e00\u4e2a\u7b80\u5355\u7684\u53ef\u8c03\u7528\u7c7b\uff0c\u5b83\u7528\u4e8e\u5b8c\u6210\u8fd9\u6837\u7684\u5de5\u4f5c\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 scale = Rescale ( 256 ) crop = RandomCrop ( 128 ) composed = transforms . Compose ([ Rescale ( 256 ), RandomCrop ( 224 )]) # Apply each of the above transforms on sample. fig = plt . figure () sample = face_dataset [ 65 ] for i , tsfrm in enumerate ([ scale , crop , composed ]): transformed_sample = tsfrm ( sample ) ax = plt . subplot ( 1 , 3 , i + 1 ) plt . tight_layout () ax . set_title ( type ( tsfrm ) . __name__ ) show_landmarks ( ** transformed_sample ) plt . show ()","title":"\u7ec4\u5408\u8f6c\u6362"},{"location":"beginner/data_loading_tutorial/#\u904d\u5386\u6570\u636e\u96c6","text":"\u628a\u4e0a\u9762\u7684\u4f60\u540c\u653e\u5728\u4e00\u8d77\uff0c\u521b\u5efa\u4e00\u4e2a\u5e26\u6709\u7ec4\u5408\u53d8\u6362\u7684\u6570\u636e\u96c6\u3002 \u603b\u800c\u8a00\u4e4b\uff0c\u6bcf\u6b21\u4ece\u8fd9\u4e2a\u6570\u636e\u96c6\u91c7\u7528\u7684\u65f6\u5019\uff1a \u4ece\u6587\u4ef6\u4e2d\u5373\u65f6\u7684\u8bfb\u53d6\u56fe\u50cf \u5bf9\u8bfb\u53d6\u7684\u56fe\u50cf\u8fdb\u884c\u53d8\u6362 \u7531\u4e8e\u5176\u4e2d\u4e00\u4e2a\u53d8\u5316\u662f\u968f\u673a\u7684\uff0c\u6570\u636e\u5728\u91c7\u6837\u65f6\u662f\u6269\u5145\u7684 \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u4e4b\u524d\u7528\u8fc7\u7684 for ... in \u5faa\u73af\u904d\u5386\u6570\u636e\u96c6\u3002 1 2 3 4 5 6 7 8 9 10 11 12 transformed_dataset = FaceLandmarksDataset ( csv_file = 'data/faces/face_landmarks.csv' , root_dir = 'data/faces/' , transform = transforms . Compose ([ Rescale ( 256 ), RandomCrop ( 224 ), ToTensor () ])) for i , sample in enumerate ( transformed_dataset ): print ( i , sample [ 'image' ] . size (), sample [ 'landmarks' ] . size ()) if i == 3 : break 1 2 3 4 0 torch.Size([3, 224, 224]) torch.Size([68, 2]) 1 torch.Size([3, 224, 224]) torch.Size([68, 2]) 2 torch.Size([3, 224, 224]) torch.Size([68, 2]) 3 torch.Size([3, 224, 224]) torch.Size([68, 2]) \u4e0d\u8fc7\uff0c\u7b80\u5355\u7684 for \u5faa\u73af\u904d\u5386\u6570\u636e\u8ba9\u6211\u4eec\u4e22\u5931\u4e86\u4e0d\u5c11\u7279\u6027\uff0c\u7279\u522b\u662f\uff1a \u6570\u636e\u7684\u6279\u5904\u7406 \u4e71\u5e8f\u6392\u5217\u6570\u636e \u4f7f\u7528 multiprocessing \u5e76\u884c\u52a0\u8f7d\u6570\u636e\u3002 torch.utils.data.DataLoader \u662f\u4e00\u4e2a\u8fed\u4ee3\u5668\uff0c\u5b83\u63d0\u4f9b\u4e86\u6240\u6709\u8fd9\u4e9b\u7279\u6027\u3002 \u4e0b\u9762\u7684\u53c2\u6570\u5e94\u8be5\u6e05\u9664\u7684\u8bf4\u660e\u4e86\u95ee\u9898\u3002 \u5176\u4e2d\u4e00\u4e2a\u6709\u8da3\u7684\u53c2\u6570\u662f collate_fn \uff0c\u53ef\u4ee5\u6307\u5b9a\u54ea\u4e9b\u5177\u4f53\u7684\u6837\u672c\u9700\u8981\u5728\u6279\u6b21\u4e2d\u4f7f\u7528 collate_fn \u8fdb\u884c\u6574\u7406\u3002 \u5f53\u7136\uff0c\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u9ed8\u8ba4\u7684\u6574\u7406(collat)\u5c31\u53ef\u4ee5\u4e86\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 dataloader = DataLoader ( transformed_dataset , batch_size = 4 , shuffle = True , num_workers = 4 ) # Helper function to show a batch def show_landmarks_batch ( sample_batched ): \"\"\"Show image with landmarks for a batch of samples.\"\"\" images_batch , landmarks_batch = \\ sample_batched [ 'image' ], sample_batched [ 'landmarks' ] batch_size = len ( images_batch ) im_size = images_batch . size ( 2 ) grid = utils . make_grid ( images_batch ) plt . imshow ( grid . numpy () . transpose (( 1 , 2 , 0 ))) for i in range ( batch_size ): plt . scatter ( landmarks_batch [ i , :, 0 ] . numpy () + i * im_size , landmarks_batch [ i , :, 1 ] . numpy (), s = 10 , marker = '.' , c = 'r' ) plt . title ( 'Batch from dataloader' ) for i_batch , sample_batched in enumerate ( dataloader ): print ( i_batch , sample_batched [ 'image' ] . size (), sample_batched [ 'landmarks' ] . size ()) # observe 4th batch and stop. if i_batch == 3 : plt . figure () show_landmarks_batch ( sample_batched ) plt . axis ( 'off' ) plt . ioff () plt . show () break 1 2 3 4 0 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2]) 1 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2]) 2 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2]) 3 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])","title":"\u904d\u5386\u6570\u636e\u96c6"},{"location":"beginner/data_loading_tutorial/#\u540e\u8bb0torchvision","text":"\u5728\u8fd9\u4e2a\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5df2\u7ecf\u770b\u5230\u4e86\u5982\u4f55\u7f16\u5199\u548c\u4f7f\u7528 Dataset , transform \u548c Dataloader \u3002 torchvision \u5305\u63d0\u4f9b\u4e86\u4e00\u4e9b\u5e38\u7528\u7684\u6570\u636e\u96c6\u548c\u53d8\u6362\u65b9\u6cd5\uff0c\u7528\u5b83\uff0c\u751a\u81f3\u53ef\u80fd\u4e0d\u7528\u7f16\u5199\u81ea\u5df1\u7684\u7c7b\u3002 torchvision \u4e2d\u8fd8\u6709\u66f4\u901a\u7528\u7684\u6570\u636e\u96c6\u7c7b\u578b\uff0c\u5176\u4e2d\u4e00\u4e2a\u662f ImageFolder \u3002 \u5b83\u5047\u5b9a\u56fe\u50cf\u6309\u4ee5\u4e0b\u65b9\u5f0f\u7ec4\u7ec7\uff1a 1 2 3 4 5 6 7 8 9 root/ants/xxx.png root/ants/xxy.jpeg root/ants/xxz.png . . . root/bees/123.jpg root/bees/nsdf3.png root/bees/asd932_.png \u5176\u4e2d\uff0c\u2019ants\u2019\u3001\u2019bees\u2019\u7b49\u662f\u5206\u7c7b\u6807\u7b7e\u3002 \u5b83\u5177\u5907\u7c7b\u4f3c PIL.Image \u7684\u901a\u7528\u53d8\u6362\u65b9\u6cd5\uff0c\u50cf RandomHorizontalFlip \u3001 Scale \u3002 \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u5b83\u4eec\u6765\u7f16\u5199\u50cf\u8fd9\u6837\u7684 Dataloader \uff1a 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import torch from torchvision import transforms , datasets data_transform = transforms . Compose ([ transforms . RandomSizedCrop ( 224 ), transforms . RandomHorizontalFlip (), transforms . ToTensor (), transforms . Normalize ( mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ]) ]) hymenoptera_dataset = datasets . ImageFolder ( root = 'hymenoptera_data/train' , transform = data_transform ) dataset_loader = torch . utils . data . DataLoader ( hymenoptera_dataset , batch_size = 4 , shuffle = True , num_workers = 4 ) \u6709\u5173\u6559\u7a0b\u7684\u4ee3\u7801\u793a\u4f8b\uff0c\u8bf7\u53c2\u9605 \u8fc1\u79fb\u5b66\u4e60\u6559\u7a0b \u3002","title":"\u540e\u8bb0\uff1atorchvision"},{"location":"beginner/transfer_learning_tutorial/","text":"\u8fc1\u79fb\u5b66\u4e60\u6559\u7a0b \u00b6 \u4f5c\u8005 : Sasank Chilamkurthy \u5728\u8fd9\u4e2a\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u5b66\u4e60\u5982\u4f55\u4f7f\u7528\u8fc1\u79fb\u5b66\u4e60\u8bad\u7ec3\u7f51\u7edc\u3002 \u66f4\u591a\u6709\u5173\u8fc1\u79fb\u5b66\u4e60\u7684\u5185\u5bb9\u53ef\u53c2\u9605 cs231n notes \u4e00\u4e9b\u6765\u81ea\u4e0a\u8ff0\u7b14\u8bb0\u7684\u6458\u8981\uff0c \u5728\u5b9e\u8df5\u4e2d\uff0c\u5f88\u5c11\u6709\u4eba\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u6574\u4e2a\u5377\u79ef\u7f51\u7edc\uff08\u968f\u673a\u521d\u59cb\u5316\uff09\uff0c\u56e0\u4e3a\u62e5\u6709\u8db3\u591f\u5927\u5c0f\u7684\u6570\u636e\u96c6\u7684\u60c5\u51b5\u662f\u76f8\u5bf9\u7f55\u89c1\u7684\u3002 \u76f8\u53cd\uff0c\u901a\u5e38\u5728\u975e\u5e38\u5927\u7684\u6570\u636e\u96c6\uff08\u4f8b\u5982ImageNet\uff0c\u5176\u5305\u542b\u5177\u67091000\u4e2a\u7c7b\u522b\u7684120\u4e07\u4e2a\u56fe\u50cf\uff09\u4e0a\u9884\u5148\u8bad\u7ec3\u53f7ConvNet\uff0c\u540e\u7eed\u4efb\u52a1\u4f7f\u7528\u5b83\u8fdb\u884c\u521d\u59cb\u5316\u6216\u8005\u7528\u4f5c\u56fa\u5b9a\u7279\u5f81\u63d0\u53d6\u5668\u3002 \u4e0b\u9762\u662f\u4e24\u79cd\u4e3b\u8981\u7684\u8fc1\u79fb\u5b66\u4e60\u573a\u666f\uff1a \u5377\u79ef\u7684\u5fae\u8c03 : \u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u4e0d\u7528\u968f\u673a\u521d\u59cb\u5316\uff0c\u800c\u662f\u4ece\u4e00\u4e2a\u9884\u8bad\u7ec3\u7f51\u7edc\uff08\u4f8b\u5982\u5728 imagenet 1000 \u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u8fc7\u7684\uff09\u5f00\u59cb\u521d\u59cb\u5316\u7f51\u7edc\u3002 \u540e\u9762\u7684\u8bad\u7ec3\u7167\u5e38\u8fdb\u884c\u3002 \u5377\u79ef\u7f51\u7edc\u4f5c\u4e3a\u56fa\u5b9a\u7279\u5f81\u63d0\u53d6\u5668 : \u8fd9\u79cd\u65b9\u5f0f\u4e0b\uff0c\u51bb\u7ed3\u9664\u4e86\u6700\u540e\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\u4e4b\u5916\u7684\u6240\u6709\u7f51\u7edc\u7684\u6743\u91cd\u3002 \u6700\u540e\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\u88ab\u66ff\u6362\u4e3a\u968f\u673a\u6743\u91cd\uff0c\u4e14\u53ea\u6709\u8fd9\u4e00\u4e2a\u5c42\u88ab\u8bad\u7ec3\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # License: BSD # Author: Sasank Chilamkurthy from __future__ import print_function , division import torch import torch.nn as nn import torch.optim as optim from torch.optim import lr_scheduler import numpy as np import torchvision from torchvision import datasets , models , transforms import matplotlib.pyplot as plt import time import os import copy plt . ion () # interactive mode \u52a0\u8f7d\u6570\u636e \u00b6 \u6211\u4eec\u7528 torchvision \u548c torch.utils.data \u5305\u52a0\u8f7d\u6570\u636e \u8fd9\u91cc\u8981\u89e3\u51b3\u7684\u95ee\u9898\u662f\u8bad\u7ec3\u4e00\u4e2a\u6a21\u578b\u7528\u4e8e\u5bf9 \u8682\u8681 \u548c \u871c\u8702 \u8fdb\u884c\u5206\u7c7b\u3002 \u8682\u8681\u548c\u871c\u8702\u5206\u522b\u6709\u5230\u5927\u7ea6120\u4e2a\u8bad\u7ec3\u56fe\u7ebf\uff0c75\u4e2a\u9a8c\u8bc1\u56fe\u50cf\u3002 \u6309\u7167\u4e00\u822c\u6807\u51c6\uff0c\u5982\u679c\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u7684\u8bdd\uff0c\u8fd9\u4e2a\u6570\u636e\u96c6\u662f\u5f88\u5c0f\u7684\u3002 \u4e0d\u8fc7\u6211\u4eec\u4f7f\u7528\u7684\u662f\u8fc1\u79fb\u5b66\u4e60\uff0c\u6a21\u578b\u5e94\u8be5\u80fd\u591f\u5f97\u5230\u8db3\u591f\u597d\u7684\u6cdb\u5316\u7ed3\u679c\u3002 \u8fd9\u4e2a\u6570\u636e\u96c6\u662fimagenet\u7684\u4e00\u4e2a\u975e\u5e38\u5c0f\u7684\u5b50\u96c6\u3002 \u6ce8\u610f \u4ece https://download.pytorch.org/tutorial/hymenoptera_data.zip \u4e0b\u8f7d\u6570\u636e\uff0c\u5e76\u89e3\u538b\u5230\u5f53\u524d\u76ee\u5f55\u4e0b\u7684 data \u5b50\u76ee\u5f55\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # Data augmentation and normalization for training # Just normalization for validation data_transforms = { 'train' : transforms . Compose ([ transforms . RandomResizedCrop ( 224 ), transforms . RandomHorizontalFlip (), transforms . ToTensor (), transforms . Normalize ([ 0.485 , 0.456 , 0.406 ], [ 0.229 , 0.224 , 0.225 ]) ]), 'val' : transforms . Compose ([ transforms . Resize ( 256 ), transforms . CenterCrop ( 224 ), transforms . ToTensor (), transforms . Normalize ([ 0.485 , 0.456 , 0.406 ], [ 0.229 , 0.224 , 0.225 ]) ]), } data_dir = 'data/hymenoptera_data' image_datasets = { x : datasets . ImageFolder ( os . path . join ( data_dir , x ), data_transforms [ x ]) for x in [ 'train' , 'val' ]} dataloaders = { x : torch . utils . data . DataLoader ( image_datasets [ x ], batch_size = 4 , shuffle = True , num_workers = 4 ) for x in [ 'train' , 'val' ]} dataset_sizes = { x : len ( image_datasets [ x ]) for x in [ 'train' , 'val' ]} class_names = image_datasets [ 'train' ] . classes device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) \u663e\u793a\u5c11\u91cf\u56fe\u50cf \u00b6 \u6211\u4eec\u663e\u793a\u5c11\u91cf\u56fe\u50cf\uff0c\u4ee5\u4fbf\u7406\u89e3\u6570\u636e\u589e\u5f3a\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def imshow ( inp , title = None ): \"\"\"Imshow for Tensor.\"\"\" inp = inp . numpy () . transpose (( 1 , 2 , 0 )) mean = np . array ([ 0.485 , 0.456 , 0.406 ]) std = np . array ([ 0.229 , 0.224 , 0.225 ]) inp = std * inp + mean inp = np . clip ( inp , 0 , 1 ) plt . imshow ( inp ) if title is not None : plt . title ( title ) plt . pause ( 0.001 ) # pause a bit so that plots are updated # Get a batch of training data inputs , classes = next ( iter ( dataloaders [ 'train' ])) # Make a grid from batch out = torchvision . utils . make_grid ( inputs ) imshow ( out , title = [ class_names [ x ] for x in classes ]) \u8bad\u7ec3\u6a21\u578b \u00b6 \u73b0\u5728\uff0c\u6211\u4eec\u7f16\u5199\u7528\u4e8e\u8bad\u7ec3\u6a21\u578b\u7684\u901a\u7528\u51fd\u6570\u3002\u5728\u51fd\u6570\u4e2d\uff0c\u6211\u4eec\u9700\u8981\uff1a \u5b89\u6392\u5b66\u4e60\u7387 \u4fdd\u5b58\u6700\u597d\u7684\u6a21\u578b \u4e0b\u9762\u4ee3\u7801\u4e2d\u7684 scheduler \u53c2\u6570\u662f\u6765\u81ea torch.optim.lr_scheduler \u7684 LR scheduler \u5bf9\u8c61\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def train_model ( model , criterion , optimizer , scheduler , num_epochs = 25 ): since = time . time () best_model_wts = copy . deepcopy ( model . state_dict ()) best_acc = 0.0 for epoch in range ( num_epochs ): print ( 'Epoch {}/{}' . format ( epoch , num_epochs - 1 )) print ( '-' * 10 ) # Each epoch has a training and validation phase for phase in [ 'train' , 'val' ]: if phase == 'train' : scheduler . step () model . train () # Set model to training mode else : model . eval () # Set model to evaluate mode running_loss = 0.0 running_corrects = 0 # Iterate over data. for inputs , labels in dataloaders [ phase ]: inputs = inputs . to ( device ) labels = labels . to ( device ) # zero the parameter gradients optimizer . zero_grad () # forward # track history if only in train with torch . set_grad_enabled ( phase == 'train' ): outputs = model ( inputs ) _ , preds = torch . max ( outputs , 1 ) loss = criterion ( outputs , labels ) # backward + optimize only if in training phase if phase == 'train' : loss . backward () optimizer . step () # statistics running_loss += loss . item () * inputs . size ( 0 ) running_corrects += torch . sum ( preds == labels . data ) epoch_loss = running_loss / dataset_sizes [ phase ] epoch_acc = running_corrects . double () / dataset_sizes [ phase ] print ( '{} Loss: {:.4f} Acc: {:.4f}' . format ( phase , epoch_loss , epoch_acc )) # deep copy the model if phase == 'val' and epoch_acc > best_acc : best_acc = epoch_acc best_model_wts = copy . deepcopy ( model . state_dict ()) print () time_elapsed = time . time () - since print ( 'Training complete in {:.0f}m {:.0f}s' . format ( time_elapsed // 60 , time_elapsed % 60 )) print ( 'Best val Acc: {:4f}' . format ( best_acc )) # load best model weights model . load_state_dict ( best_model_wts ) return model \u663e\u793a\u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c \u00b6 \u7528\u4e8e\u663e\u793a\u5c11\u91cf\u56fe\u50cf\u9884\u6d4b\u7ed3\u679c\u7684\u901a\u7528\u51fd\u6570 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def visualize_model ( model , num_images = 6 ): was_training = model . training model . eval () images_so_far = 0 fig = plt . figure () with torch . no_grad (): for i , ( inputs , labels ) in enumerate ( dataloaders [ 'val' ]): inputs = inputs . to ( device ) labels = labels . to ( device ) outputs = model ( inputs ) _ , preds = torch . max ( outputs , 1 ) for j in range ( inputs . size ()[ 0 ]): images_so_far += 1 ax = plt . subplot ( num_images // 2 , 2 , images_so_far ) ax . axis ( 'off' ) ax . set_title ( 'predicted: {}' . format ( class_names [ preds [ j ]])) imshow ( inputs . cpu () . data [ j ]) if images_so_far == num_images : model . train ( mode = was_training ) return model . train ( mode = was_training ) \u5fae\u8c03\u5377\u79ef\u7f51\u7edc \u00b6 \u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u91cd\u7f6e\u6700\u540e\u7684\u5168\u8fde\u63a5\u5c42\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 model_ft = models . resnet18 ( pretrained = True ) num_ftrs = model_ft . fc . in_features model_ft . fc = nn . Linear ( num_ftrs , 2 ) model_ft = model_ft . to ( device ) criterion = nn . CrossEntropyLoss () # Observe that all parameters are being optimized optimizer_ft = optim . SGD ( model_ft . parameters (), lr = 0.001 , momentum = 0.9 ) # Decay LR by a factor of 0.1 every 7 epochs exp_lr_scheduler = lr_scheduler . StepLR ( optimizer_ft , step_size = 7 , gamma = 0.1 ) \u8bad\u7ec3\u4e0e\u8bc4\u4f30 \u00b6 \u5728CPU\u4e0a\u5927\u7ea6\u9700\u898115-25\u5206\u949f\uff0c\u5728GPU\u4e0a\u9700\u8981\u5927\u7ea6\u4e0d\u5230\u4e00\u5206\u949f\u3002 1 2 model_ft = train_model ( model_ft , criterion , optimizer_ft , exp_lr_scheduler , num_epochs = 25 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 Epoch 0/24 ---------- train Loss: 0.6645 Acc: 0.6803 val Loss: 0.9809 Acc: 0.6340 Epoch 1/24 ---------- train Loss: 0.5326 Acc: 0.8115 val Loss: 0.1608 Acc: 0.9542 Epoch 2/24 ---------- train Loss: 0.4786 Acc: 0.8115 val Loss: 0.2549 Acc: 0.9281 Epoch 3/24 ---------- train Loss: 0.4241 Acc: 0.8361 val Loss: 0.2157 Acc: 0.9216 Epoch 4/24 ---------- train Loss: 0.4840 Acc: 0.8115 val Loss: 0.2490 Acc: 0.8954 Epoch 5/24 ---------- train Loss: 0.3529 Acc: 0.8607 val Loss: 0.3553 Acc: 0.8889 Epoch 6/24 ---------- train Loss: 0.5773 Acc: 0.7623 val Loss: 0.2592 Acc: 0.9150 Epoch 7/24 ---------- train Loss: 0.4033 Acc: 0.8279 val Loss: 0.2284 Acc: 0.9216 Epoch 8/24 ---------- train Loss: 0.3246 Acc: 0.8607 val Loss: 0.2262 Acc: 0.9150 Epoch 9/24 ---------- train Loss: 0.2883 Acc: 0.8975 val Loss: 0.2000 Acc: 0.9281 Epoch 10/24 ---------- train Loss: 0.2666 Acc: 0.9057 val Loss: 0.1866 Acc: 0.9412 Epoch 11/24 ---------- train Loss: 0.3363 Acc: 0.8402 val Loss: 0.1922 Acc: 0.9412 Epoch 12/24 ---------- train Loss: 0.2246 Acc: 0.8934 val Loss: 0.1951 Acc: 0.9346 Epoch 13/24 ---------- train Loss: 0.2927 Acc: 0.8566 val Loss: 0.2221 Acc: 0.9085 Epoch 14/24 ---------- train Loss: 0.3680 Acc: 0.8484 val Loss: 0.1812 Acc: 0.9281 Epoch 15/24 ---------- train Loss: 0.3115 Acc: 0.8730 val Loss: 0.1775 Acc: 0.9346 Epoch 16/24 ---------- train Loss: 0.3047 Acc: 0.8770 val Loss: 0.1900 Acc: 0.9281 Epoch 17/24 ---------- train Loss: 0.3971 Acc: 0.8402 val Loss: 0.1842 Acc: 0.9216 Epoch 18/24 ---------- train Loss: 0.3200 Acc: 0.8770 val Loss: 0.1882 Acc: 0.9346 Epoch 19/24 ---------- train Loss: 0.3006 Acc: 0.8525 val Loss: 0.1814 Acc: 0.9477 Epoch 20/24 ---------- train Loss: 0.3168 Acc: 0.8525 val Loss: 0.1763 Acc: 0.9346 Epoch 21/24 ---------- train Loss: 0.2938 Acc: 0.8689 val Loss: 0.1797 Acc: 0.9346 Epoch 22/24 ---------- train Loss: 0.2430 Acc: 0.8975 val Loss: 0.1756 Acc: 0.9346 Epoch 23/24 ---------- train Loss: 0.3436 Acc: 0.8320 val Loss: 0.1730 Acc: 0.9412 Epoch 24/24 ---------- train Loss: 0.2832 Acc: 0.8689 val Loss: 0.1683 Acc: 0.9412 Training complete in 1m 12s Best val Acc: 0.954248 1 visualize_model ( model_ft ) \u5377\u79ef\u7f51\u7edc\u4f5c\u4e3a\u56fa\u5b9a\u7279\u5f81\u63d0\u53d6\u5668 \u00b6 \u8981\u51bb\u7ed3\u51fa\u6700\u540e\u4e00\u5c42\u4e4b\u5916\u7684\u6574\u4e2a\u7f51\u7edc\uff0c\u6211\u4eec\u5f97\u8bbe\u7f6e requires_grad == False \u6765\u51bb\u7ed3\u53c2\u6570\uff0c\u8fd9\u6837\u4e00\u6765\uff0c backward() \u5c31\u4e0d\u518d\u8ba1\u7b97\u68af\u5ea6\u3002 \u9605\u8bfb\u8fd9\u4e2a\u6587\u6863\u53ef\u4ee5\u83b7\u5f97\u66f4\u591a\u4fe1\u606f\uff1a https://pytorch.org/docs/notes/autograd.html#excluding-subgraphs-from-backward \u3002. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 model_conv = torchvision . models . resnet18 ( pretrained = True ) for param in model_conv . parameters (): param . requires_grad = False # Parameters of newly constructed modules have requires_grad=True by default num_ftrs = model_conv . fc . in_features model_conv . fc = nn . Linear ( num_ftrs , 2 ) model_conv = model_conv . to ( device ) criterion = nn . CrossEntropyLoss () # Observe that only parameters of final layer are being optimized as # opposed to before. optimizer_conv = optim . SGD ( model_conv . fc . parameters (), lr = 0.001 , momentum = 0.9 ) # Decay LR by a factor of 0.1 every 7 epochs exp_lr_scheduler = lr_scheduler . StepLR ( optimizer_conv , step_size = 7 , gamma = 0.1 ) \u8bad\u7ec3\u4e0e\u8bc4\u4f30 \u00b6 \u5728CPU\u4e0a\u9700\u8981\u7684\u65f6\u95f4\u5927\u7ea6\u662f\u4e0a\u4e2a\u573a\u666f\u7684\u4e00\u534a\uff0c\u8fd9\u662f\u56e0\u4e3a\u4e0d\u9700\u8981\u4e3a\u5927\u591a\u6570\u7f51\u7edc\u8ba1\u7b97\u68af\u5ea6\u3002\u4e0d\u8fc7\uff0c\u524d\u5411\u8fd8\u662f\u8981\u8ba1\u7b97\u7684\u3002 1 2 model_conv = train_model ( model_conv , criterion , optimizer_conv , exp_lr_scheduler , num_epochs = 25 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 Epoch 0/24 ---------- train Loss: 0.6012 Acc: 0.6926 val Loss: 0.2920 Acc: 0.8954 Epoch 1/24 ---------- train Loss: 0.5843 Acc: 0.7500 val Loss: 0.1724 Acc: 0.9477 Epoch 2/24 ---------- train Loss: 0.5340 Acc: 0.7500 val Loss: 0.1489 Acc: 0.9673 Epoch 3/24 ---------- train Loss: 0.4718 Acc: 0.8074 val Loss: 0.1579 Acc: 0.9673 Epoch 4/24 ---------- train Loss: 0.5471 Acc: 0.7746 val Loss: 0.1791 Acc: 0.9542 Epoch 5/24 ---------- train Loss: 0.6842 Acc: 0.7213 val Loss: 0.4588 Acc: 0.8431 Epoch 6/24 ---------- train Loss: 0.5558 Acc: 0.7664 val Loss: 0.1744 Acc: 0.9673 Epoch 7/24 ---------- train Loss: 0.2958 Acc: 0.8770 val Loss: 0.1850 Acc: 0.9542 Epoch 8/24 ---------- train Loss: 0.3071 Acc: 0.8607 val Loss: 0.1790 Acc: 0.9542 Epoch 9/24 ---------- train Loss: 0.4278 Acc: 0.8320 val Loss: 0.1905 Acc: 0.9412 Epoch 10/24 ---------- train Loss: 0.2871 Acc: 0.8934 val Loss: 0.1990 Acc: 0.9412 Epoch 11/24 ---------- train Loss: 0.3389 Acc: 0.8566 val Loss: 0.1733 Acc: 0.9608 Epoch 12/24 ---------- train Loss: 0.2730 Acc: 0.8730 val Loss: 0.1759 Acc: 0.9542 Epoch 13/24 ---------- train Loss: 0.3903 Acc: 0.8279 val Loss: 0.1925 Acc: 0.9477 Epoch 14/24 ---------- train Loss: 0.3229 Acc: 0.8443 val Loss: 0.1836 Acc: 0.9542 Epoch 15/24 ---------- train Loss: 0.2893 Acc: 0.8730 val Loss: 0.1721 Acc: 0.9542 Epoch 16/24 ---------- train Loss: 0.3238 Acc: 0.8689 val Loss: 0.2043 Acc: 0.9412 Epoch 17/24 ---------- train Loss: 0.3544 Acc: 0.8361 val Loss: 0.1739 Acc: 0.9673 Epoch 18/24 ---------- train Loss: 0.2911 Acc: 0.8770 val Loss: 0.1906 Acc: 0.9346 Epoch 19/24 ---------- train Loss: 0.3854 Acc: 0.8279 val Loss: 0.1992 Acc: 0.9477 Epoch 20/24 ---------- train Loss: 0.3465 Acc: 0.8238 val Loss: 0.2030 Acc: 0.9346 Epoch 21/24 ---------- train Loss: 0.3416 Acc: 0.8402 val Loss: 0.1801 Acc: 0.9477 Epoch 22/24 ---------- train Loss: 0.3997 Acc: 0.8033 val Loss: 0.1990 Acc: 0.9477 Epoch 23/24 ---------- train Loss: 0.2379 Acc: 0.9139 val Loss: 0.1711 Acc: 0.9608 Epoch 24/24 ---------- train Loss: 0.3448 Acc: 0.8484 val Loss: 0.1732 Acc: 0.9608 Training complete in 0m 45s Best val Acc: 0.967320 1 2 3 4 visualize_model ( model_conv ) plt . ioff () plt . show ()","title":"\u8fc1\u79fb\u5b66\u4e60\u6559\u7a0b"},{"location":"beginner/transfer_learning_tutorial/#\u8fc1\u79fb\u5b66\u4e60\u6559\u7a0b","text":"\u4f5c\u8005 : Sasank Chilamkurthy \u5728\u8fd9\u4e2a\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u5b66\u4e60\u5982\u4f55\u4f7f\u7528\u8fc1\u79fb\u5b66\u4e60\u8bad\u7ec3\u7f51\u7edc\u3002 \u66f4\u591a\u6709\u5173\u8fc1\u79fb\u5b66\u4e60\u7684\u5185\u5bb9\u53ef\u53c2\u9605 cs231n notes \u4e00\u4e9b\u6765\u81ea\u4e0a\u8ff0\u7b14\u8bb0\u7684\u6458\u8981\uff0c \u5728\u5b9e\u8df5\u4e2d\uff0c\u5f88\u5c11\u6709\u4eba\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u6574\u4e2a\u5377\u79ef\u7f51\u7edc\uff08\u968f\u673a\u521d\u59cb\u5316\uff09\uff0c\u56e0\u4e3a\u62e5\u6709\u8db3\u591f\u5927\u5c0f\u7684\u6570\u636e\u96c6\u7684\u60c5\u51b5\u662f\u76f8\u5bf9\u7f55\u89c1\u7684\u3002 \u76f8\u53cd\uff0c\u901a\u5e38\u5728\u975e\u5e38\u5927\u7684\u6570\u636e\u96c6\uff08\u4f8b\u5982ImageNet\uff0c\u5176\u5305\u542b\u5177\u67091000\u4e2a\u7c7b\u522b\u7684120\u4e07\u4e2a\u56fe\u50cf\uff09\u4e0a\u9884\u5148\u8bad\u7ec3\u53f7ConvNet\uff0c\u540e\u7eed\u4efb\u52a1\u4f7f\u7528\u5b83\u8fdb\u884c\u521d\u59cb\u5316\u6216\u8005\u7528\u4f5c\u56fa\u5b9a\u7279\u5f81\u63d0\u53d6\u5668\u3002 \u4e0b\u9762\u662f\u4e24\u79cd\u4e3b\u8981\u7684\u8fc1\u79fb\u5b66\u4e60\u573a\u666f\uff1a \u5377\u79ef\u7684\u5fae\u8c03 : \u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u4e0d\u7528\u968f\u673a\u521d\u59cb\u5316\uff0c\u800c\u662f\u4ece\u4e00\u4e2a\u9884\u8bad\u7ec3\u7f51\u7edc\uff08\u4f8b\u5982\u5728 imagenet 1000 \u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u8fc7\u7684\uff09\u5f00\u59cb\u521d\u59cb\u5316\u7f51\u7edc\u3002 \u540e\u9762\u7684\u8bad\u7ec3\u7167\u5e38\u8fdb\u884c\u3002 \u5377\u79ef\u7f51\u7edc\u4f5c\u4e3a\u56fa\u5b9a\u7279\u5f81\u63d0\u53d6\u5668 : \u8fd9\u79cd\u65b9\u5f0f\u4e0b\uff0c\u51bb\u7ed3\u9664\u4e86\u6700\u540e\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\u4e4b\u5916\u7684\u6240\u6709\u7f51\u7edc\u7684\u6743\u91cd\u3002 \u6700\u540e\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\u88ab\u66ff\u6362\u4e3a\u968f\u673a\u6743\u91cd\uff0c\u4e14\u53ea\u6709\u8fd9\u4e00\u4e2a\u5c42\u88ab\u8bad\u7ec3\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # License: BSD # Author: Sasank Chilamkurthy from __future__ import print_function , division import torch import torch.nn as nn import torch.optim as optim from torch.optim import lr_scheduler import numpy as np import torchvision from torchvision import datasets , models , transforms import matplotlib.pyplot as plt import time import os import copy plt . ion () # interactive mode","title":"\u8fc1\u79fb\u5b66\u4e60\u6559\u7a0b"},{"location":"beginner/transfer_learning_tutorial/#\u52a0\u8f7d\u6570\u636e","text":"\u6211\u4eec\u7528 torchvision \u548c torch.utils.data \u5305\u52a0\u8f7d\u6570\u636e \u8fd9\u91cc\u8981\u89e3\u51b3\u7684\u95ee\u9898\u662f\u8bad\u7ec3\u4e00\u4e2a\u6a21\u578b\u7528\u4e8e\u5bf9 \u8682\u8681 \u548c \u871c\u8702 \u8fdb\u884c\u5206\u7c7b\u3002 \u8682\u8681\u548c\u871c\u8702\u5206\u522b\u6709\u5230\u5927\u7ea6120\u4e2a\u8bad\u7ec3\u56fe\u7ebf\uff0c75\u4e2a\u9a8c\u8bc1\u56fe\u50cf\u3002 \u6309\u7167\u4e00\u822c\u6807\u51c6\uff0c\u5982\u679c\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u7684\u8bdd\uff0c\u8fd9\u4e2a\u6570\u636e\u96c6\u662f\u5f88\u5c0f\u7684\u3002 \u4e0d\u8fc7\u6211\u4eec\u4f7f\u7528\u7684\u662f\u8fc1\u79fb\u5b66\u4e60\uff0c\u6a21\u578b\u5e94\u8be5\u80fd\u591f\u5f97\u5230\u8db3\u591f\u597d\u7684\u6cdb\u5316\u7ed3\u679c\u3002 \u8fd9\u4e2a\u6570\u636e\u96c6\u662fimagenet\u7684\u4e00\u4e2a\u975e\u5e38\u5c0f\u7684\u5b50\u96c6\u3002 \u6ce8\u610f \u4ece https://download.pytorch.org/tutorial/hymenoptera_data.zip \u4e0b\u8f7d\u6570\u636e\uff0c\u5e76\u89e3\u538b\u5230\u5f53\u524d\u76ee\u5f55\u4e0b\u7684 data \u5b50\u76ee\u5f55\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # Data augmentation and normalization for training # Just normalization for validation data_transforms = { 'train' : transforms . Compose ([ transforms . RandomResizedCrop ( 224 ), transforms . RandomHorizontalFlip (), transforms . ToTensor (), transforms . Normalize ([ 0.485 , 0.456 , 0.406 ], [ 0.229 , 0.224 , 0.225 ]) ]), 'val' : transforms . Compose ([ transforms . Resize ( 256 ), transforms . CenterCrop ( 224 ), transforms . ToTensor (), transforms . Normalize ([ 0.485 , 0.456 , 0.406 ], [ 0.229 , 0.224 , 0.225 ]) ]), } data_dir = 'data/hymenoptera_data' image_datasets = { x : datasets . ImageFolder ( os . path . join ( data_dir , x ), data_transforms [ x ]) for x in [ 'train' , 'val' ]} dataloaders = { x : torch . utils . data . DataLoader ( image_datasets [ x ], batch_size = 4 , shuffle = True , num_workers = 4 ) for x in [ 'train' , 'val' ]} dataset_sizes = { x : len ( image_datasets [ x ]) for x in [ 'train' , 'val' ]} class_names = image_datasets [ 'train' ] . classes device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" )","title":"\u52a0\u8f7d\u6570\u636e"},{"location":"beginner/transfer_learning_tutorial/#\u663e\u793a\u5c11\u91cf\u56fe\u50cf","text":"\u6211\u4eec\u663e\u793a\u5c11\u91cf\u56fe\u50cf\uff0c\u4ee5\u4fbf\u7406\u89e3\u6570\u636e\u589e\u5f3a\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def imshow ( inp , title = None ): \"\"\"Imshow for Tensor.\"\"\" inp = inp . numpy () . transpose (( 1 , 2 , 0 )) mean = np . array ([ 0.485 , 0.456 , 0.406 ]) std = np . array ([ 0.229 , 0.224 , 0.225 ]) inp = std * inp + mean inp = np . clip ( inp , 0 , 1 ) plt . imshow ( inp ) if title is not None : plt . title ( title ) plt . pause ( 0.001 ) # pause a bit so that plots are updated # Get a batch of training data inputs , classes = next ( iter ( dataloaders [ 'train' ])) # Make a grid from batch out = torchvision . utils . make_grid ( inputs ) imshow ( out , title = [ class_names [ x ] for x in classes ])","title":"\u663e\u793a\u5c11\u91cf\u56fe\u50cf"},{"location":"beginner/transfer_learning_tutorial/#\u8bad\u7ec3\u6a21\u578b","text":"\u73b0\u5728\uff0c\u6211\u4eec\u7f16\u5199\u7528\u4e8e\u8bad\u7ec3\u6a21\u578b\u7684\u901a\u7528\u51fd\u6570\u3002\u5728\u51fd\u6570\u4e2d\uff0c\u6211\u4eec\u9700\u8981\uff1a \u5b89\u6392\u5b66\u4e60\u7387 \u4fdd\u5b58\u6700\u597d\u7684\u6a21\u578b \u4e0b\u9762\u4ee3\u7801\u4e2d\u7684 scheduler \u53c2\u6570\u662f\u6765\u81ea torch.optim.lr_scheduler \u7684 LR scheduler \u5bf9\u8c61\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def train_model ( model , criterion , optimizer , scheduler , num_epochs = 25 ): since = time . time () best_model_wts = copy . deepcopy ( model . state_dict ()) best_acc = 0.0 for epoch in range ( num_epochs ): print ( 'Epoch {}/{}' . format ( epoch , num_epochs - 1 )) print ( '-' * 10 ) # Each epoch has a training and validation phase for phase in [ 'train' , 'val' ]: if phase == 'train' : scheduler . step () model . train () # Set model to training mode else : model . eval () # Set model to evaluate mode running_loss = 0.0 running_corrects = 0 # Iterate over data. for inputs , labels in dataloaders [ phase ]: inputs = inputs . to ( device ) labels = labels . to ( device ) # zero the parameter gradients optimizer . zero_grad () # forward # track history if only in train with torch . set_grad_enabled ( phase == 'train' ): outputs = model ( inputs ) _ , preds = torch . max ( outputs , 1 ) loss = criterion ( outputs , labels ) # backward + optimize only if in training phase if phase == 'train' : loss . backward () optimizer . step () # statistics running_loss += loss . item () * inputs . size ( 0 ) running_corrects += torch . sum ( preds == labels . data ) epoch_loss = running_loss / dataset_sizes [ phase ] epoch_acc = running_corrects . double () / dataset_sizes [ phase ] print ( '{} Loss: {:.4f} Acc: {:.4f}' . format ( phase , epoch_loss , epoch_acc )) # deep copy the model if phase == 'val' and epoch_acc > best_acc : best_acc = epoch_acc best_model_wts = copy . deepcopy ( model . state_dict ()) print () time_elapsed = time . time () - since print ( 'Training complete in {:.0f}m {:.0f}s' . format ( time_elapsed // 60 , time_elapsed % 60 )) print ( 'Best val Acc: {:4f}' . format ( best_acc )) # load best model weights model . load_state_dict ( best_model_wts ) return model","title":"\u8bad\u7ec3\u6a21\u578b"},{"location":"beginner/transfer_learning_tutorial/#\u663e\u793a\u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c","text":"\u7528\u4e8e\u663e\u793a\u5c11\u91cf\u56fe\u50cf\u9884\u6d4b\u7ed3\u679c\u7684\u901a\u7528\u51fd\u6570 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def visualize_model ( model , num_images = 6 ): was_training = model . training model . eval () images_so_far = 0 fig = plt . figure () with torch . no_grad (): for i , ( inputs , labels ) in enumerate ( dataloaders [ 'val' ]): inputs = inputs . to ( device ) labels = labels . to ( device ) outputs = model ( inputs ) _ , preds = torch . max ( outputs , 1 ) for j in range ( inputs . size ()[ 0 ]): images_so_far += 1 ax = plt . subplot ( num_images // 2 , 2 , images_so_far ) ax . axis ( 'off' ) ax . set_title ( 'predicted: {}' . format ( class_names [ preds [ j ]])) imshow ( inputs . cpu () . data [ j ]) if images_so_far == num_images : model . train ( mode = was_training ) return model . train ( mode = was_training )","title":"\u663e\u793a\u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c"},{"location":"beginner/transfer_learning_tutorial/#\u5fae\u8c03\u5377\u79ef\u7f51\u7edc","text":"\u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u91cd\u7f6e\u6700\u540e\u7684\u5168\u8fde\u63a5\u5c42\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 model_ft = models . resnet18 ( pretrained = True ) num_ftrs = model_ft . fc . in_features model_ft . fc = nn . Linear ( num_ftrs , 2 ) model_ft = model_ft . to ( device ) criterion = nn . CrossEntropyLoss () # Observe that all parameters are being optimized optimizer_ft = optim . SGD ( model_ft . parameters (), lr = 0.001 , momentum = 0.9 ) # Decay LR by a factor of 0.1 every 7 epochs exp_lr_scheduler = lr_scheduler . StepLR ( optimizer_ft , step_size = 7 , gamma = 0.1 )","title":"\u5fae\u8c03\u5377\u79ef\u7f51\u7edc"},{"location":"beginner/transfer_learning_tutorial/#\u8bad\u7ec3\u4e0e\u8bc4\u4f30","text":"\u5728CPU\u4e0a\u5927\u7ea6\u9700\u898115-25\u5206\u949f\uff0c\u5728GPU\u4e0a\u9700\u8981\u5927\u7ea6\u4e0d\u5230\u4e00\u5206\u949f\u3002 1 2 model_ft = train_model ( model_ft , criterion , optimizer_ft , exp_lr_scheduler , num_epochs = 25 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 Epoch 0/24 ---------- train Loss: 0.6645 Acc: 0.6803 val Loss: 0.9809 Acc: 0.6340 Epoch 1/24 ---------- train Loss: 0.5326 Acc: 0.8115 val Loss: 0.1608 Acc: 0.9542 Epoch 2/24 ---------- train Loss: 0.4786 Acc: 0.8115 val Loss: 0.2549 Acc: 0.9281 Epoch 3/24 ---------- train Loss: 0.4241 Acc: 0.8361 val Loss: 0.2157 Acc: 0.9216 Epoch 4/24 ---------- train Loss: 0.4840 Acc: 0.8115 val Loss: 0.2490 Acc: 0.8954 Epoch 5/24 ---------- train Loss: 0.3529 Acc: 0.8607 val Loss: 0.3553 Acc: 0.8889 Epoch 6/24 ---------- train Loss: 0.5773 Acc: 0.7623 val Loss: 0.2592 Acc: 0.9150 Epoch 7/24 ---------- train Loss: 0.4033 Acc: 0.8279 val Loss: 0.2284 Acc: 0.9216 Epoch 8/24 ---------- train Loss: 0.3246 Acc: 0.8607 val Loss: 0.2262 Acc: 0.9150 Epoch 9/24 ---------- train Loss: 0.2883 Acc: 0.8975 val Loss: 0.2000 Acc: 0.9281 Epoch 10/24 ---------- train Loss: 0.2666 Acc: 0.9057 val Loss: 0.1866 Acc: 0.9412 Epoch 11/24 ---------- train Loss: 0.3363 Acc: 0.8402 val Loss: 0.1922 Acc: 0.9412 Epoch 12/24 ---------- train Loss: 0.2246 Acc: 0.8934 val Loss: 0.1951 Acc: 0.9346 Epoch 13/24 ---------- train Loss: 0.2927 Acc: 0.8566 val Loss: 0.2221 Acc: 0.9085 Epoch 14/24 ---------- train Loss: 0.3680 Acc: 0.8484 val Loss: 0.1812 Acc: 0.9281 Epoch 15/24 ---------- train Loss: 0.3115 Acc: 0.8730 val Loss: 0.1775 Acc: 0.9346 Epoch 16/24 ---------- train Loss: 0.3047 Acc: 0.8770 val Loss: 0.1900 Acc: 0.9281 Epoch 17/24 ---------- train Loss: 0.3971 Acc: 0.8402 val Loss: 0.1842 Acc: 0.9216 Epoch 18/24 ---------- train Loss: 0.3200 Acc: 0.8770 val Loss: 0.1882 Acc: 0.9346 Epoch 19/24 ---------- train Loss: 0.3006 Acc: 0.8525 val Loss: 0.1814 Acc: 0.9477 Epoch 20/24 ---------- train Loss: 0.3168 Acc: 0.8525 val Loss: 0.1763 Acc: 0.9346 Epoch 21/24 ---------- train Loss: 0.2938 Acc: 0.8689 val Loss: 0.1797 Acc: 0.9346 Epoch 22/24 ---------- train Loss: 0.2430 Acc: 0.8975 val Loss: 0.1756 Acc: 0.9346 Epoch 23/24 ---------- train Loss: 0.3436 Acc: 0.8320 val Loss: 0.1730 Acc: 0.9412 Epoch 24/24 ---------- train Loss: 0.2832 Acc: 0.8689 val Loss: 0.1683 Acc: 0.9412 Training complete in 1m 12s Best val Acc: 0.954248 1 visualize_model ( model_ft )","title":"\u8bad\u7ec3\u4e0e\u8bc4\u4f30"},{"location":"beginner/transfer_learning_tutorial/#\u5377\u79ef\u7f51\u7edc\u4f5c\u4e3a\u56fa\u5b9a\u7279\u5f81\u63d0\u53d6\u5668","text":"\u8981\u51bb\u7ed3\u51fa\u6700\u540e\u4e00\u5c42\u4e4b\u5916\u7684\u6574\u4e2a\u7f51\u7edc\uff0c\u6211\u4eec\u5f97\u8bbe\u7f6e requires_grad == False \u6765\u51bb\u7ed3\u53c2\u6570\uff0c\u8fd9\u6837\u4e00\u6765\uff0c backward() \u5c31\u4e0d\u518d\u8ba1\u7b97\u68af\u5ea6\u3002 \u9605\u8bfb\u8fd9\u4e2a\u6587\u6863\u53ef\u4ee5\u83b7\u5f97\u66f4\u591a\u4fe1\u606f\uff1a https://pytorch.org/docs/notes/autograd.html#excluding-subgraphs-from-backward \u3002. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 model_conv = torchvision . models . resnet18 ( pretrained = True ) for param in model_conv . parameters (): param . requires_grad = False # Parameters of newly constructed modules have requires_grad=True by default num_ftrs = model_conv . fc . in_features model_conv . fc = nn . Linear ( num_ftrs , 2 ) model_conv = model_conv . to ( device ) criterion = nn . CrossEntropyLoss () # Observe that only parameters of final layer are being optimized as # opposed to before. optimizer_conv = optim . SGD ( model_conv . fc . parameters (), lr = 0.001 , momentum = 0.9 ) # Decay LR by a factor of 0.1 every 7 epochs exp_lr_scheduler = lr_scheduler . StepLR ( optimizer_conv , step_size = 7 , gamma = 0.1 )","title":"\u5377\u79ef\u7f51\u7edc\u4f5c\u4e3a\u56fa\u5b9a\u7279\u5f81\u63d0\u53d6\u5668"},{"location":"beginner/transfer_learning_tutorial/#\u8bad\u7ec3\u4e0e\u8bc4\u4f30_1","text":"\u5728CPU\u4e0a\u9700\u8981\u7684\u65f6\u95f4\u5927\u7ea6\u662f\u4e0a\u4e2a\u573a\u666f\u7684\u4e00\u534a\uff0c\u8fd9\u662f\u56e0\u4e3a\u4e0d\u9700\u8981\u4e3a\u5927\u591a\u6570\u7f51\u7edc\u8ba1\u7b97\u68af\u5ea6\u3002\u4e0d\u8fc7\uff0c\u524d\u5411\u8fd8\u662f\u8981\u8ba1\u7b97\u7684\u3002 1 2 model_conv = train_model ( model_conv , criterion , optimizer_conv , exp_lr_scheduler , num_epochs = 25 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 Epoch 0/24 ---------- train Loss: 0.6012 Acc: 0.6926 val Loss: 0.2920 Acc: 0.8954 Epoch 1/24 ---------- train Loss: 0.5843 Acc: 0.7500 val Loss: 0.1724 Acc: 0.9477 Epoch 2/24 ---------- train Loss: 0.5340 Acc: 0.7500 val Loss: 0.1489 Acc: 0.9673 Epoch 3/24 ---------- train Loss: 0.4718 Acc: 0.8074 val Loss: 0.1579 Acc: 0.9673 Epoch 4/24 ---------- train Loss: 0.5471 Acc: 0.7746 val Loss: 0.1791 Acc: 0.9542 Epoch 5/24 ---------- train Loss: 0.6842 Acc: 0.7213 val Loss: 0.4588 Acc: 0.8431 Epoch 6/24 ---------- train Loss: 0.5558 Acc: 0.7664 val Loss: 0.1744 Acc: 0.9673 Epoch 7/24 ---------- train Loss: 0.2958 Acc: 0.8770 val Loss: 0.1850 Acc: 0.9542 Epoch 8/24 ---------- train Loss: 0.3071 Acc: 0.8607 val Loss: 0.1790 Acc: 0.9542 Epoch 9/24 ---------- train Loss: 0.4278 Acc: 0.8320 val Loss: 0.1905 Acc: 0.9412 Epoch 10/24 ---------- train Loss: 0.2871 Acc: 0.8934 val Loss: 0.1990 Acc: 0.9412 Epoch 11/24 ---------- train Loss: 0.3389 Acc: 0.8566 val Loss: 0.1733 Acc: 0.9608 Epoch 12/24 ---------- train Loss: 0.2730 Acc: 0.8730 val Loss: 0.1759 Acc: 0.9542 Epoch 13/24 ---------- train Loss: 0.3903 Acc: 0.8279 val Loss: 0.1925 Acc: 0.9477 Epoch 14/24 ---------- train Loss: 0.3229 Acc: 0.8443 val Loss: 0.1836 Acc: 0.9542 Epoch 15/24 ---------- train Loss: 0.2893 Acc: 0.8730 val Loss: 0.1721 Acc: 0.9542 Epoch 16/24 ---------- train Loss: 0.3238 Acc: 0.8689 val Loss: 0.2043 Acc: 0.9412 Epoch 17/24 ---------- train Loss: 0.3544 Acc: 0.8361 val Loss: 0.1739 Acc: 0.9673 Epoch 18/24 ---------- train Loss: 0.2911 Acc: 0.8770 val Loss: 0.1906 Acc: 0.9346 Epoch 19/24 ---------- train Loss: 0.3854 Acc: 0.8279 val Loss: 0.1992 Acc: 0.9477 Epoch 20/24 ---------- train Loss: 0.3465 Acc: 0.8238 val Loss: 0.2030 Acc: 0.9346 Epoch 21/24 ---------- train Loss: 0.3416 Acc: 0.8402 val Loss: 0.1801 Acc: 0.9477 Epoch 22/24 ---------- train Loss: 0.3997 Acc: 0.8033 val Loss: 0.1990 Acc: 0.9477 Epoch 23/24 ---------- train Loss: 0.2379 Acc: 0.9139 val Loss: 0.1711 Acc: 0.9608 Epoch 24/24 ---------- train Loss: 0.3448 Acc: 0.8484 val Loss: 0.1732 Acc: 0.9608 Training complete in 0m 45s Best val Acc: 0.967320 1 2 3 4 visualize_model ( model_conv ) plt . ioff () plt . show ()","title":"\u8bad\u7ec3\u4e0e\u8bc4\u4f30"},{"location":"beginner/blitz/autograd_tutorial/","text":"Autograd: \u81ea\u52a8\u5fae\u5206(Automatic differentiation) \u00b6 autograd \u5305 \u662f PyTorch \u4e2d\u6240\u6709\u795e\u7ecf\u7f51\u7edc\u7684\u4e2d\u5fc3\u3002 \u6211\u4eec\u9996\u5148\u7b80\u8981\u67e5\u770b\uff0c\u7136\u540e\u8bad\u7ec3\u7b2c\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u3002 autograd \u5305\u4e3a Tensor \u4e0a\u7684\u6240\u6709\u8fd0\u7b97\u63d0\u4f9b\u81ea\u52a8\u5fae\u5206(automatic differentiation)\u3002 \u5b83\u662f\u4e00\u4e2a\u201c\u8fd0\u884c\u4e2d\u5b9a\u4e49\u201d(define-by-run)\u7684\u6846\u67b6\uff0c\u8fd9\u610f\u5473\u7740\u53cd\u5411\u4f20\u64ad(backpropagation)\u7b97\u6cd5\u7531\u4ee3\u7801\u5728\u8fd0\u884c\u65f6\u5b9a\u4e49\uff0c\u5e76\u4e14\u6bcf\u4e2a\u8fed\u4ee3\u90fd\u53ef\u4ee5\u4e0d\u540c\u3002 \u6211\u4eec\u901a\u8fc7\u66f4\u7b80\u5355\u7684\u672f\u8bed\u7684\u4f8b\u5b50\u6765\u770b\u770b\u8fd9\u4e9b\u3002 \u5f20\u91cf(Tensor) \u00b6 torch.Tensor \u662f\u8fd9\u4e2a\u5305\u7684\u4e2d\u5fc3\u7c7b\u3002 \u5c06\u5b83\u7684 .requires_grad \u5c5e\u6027\u8bbe\u7f6e\u4e3a True \uff0c\u5373\u5f00\u59cb\u8ddf\u8e2a\u5176\u4e0a\u7684\u6240\u6709\u8fd0\u7b97\u3002 \u5b8c\u6210\u8ba1\u7b97\u540e\u8c03\u7528 .backward() \u5373\u53ef\u81ea\u52a8\u5b8c\u6210\u68af\u5ea6\u8ba1\u7b97\u3002 \u8be5 tensor \u68af\u5ea6\u7684\u7d2f\u8ba1\u548c\u8bb0\u5f55\u5728 .grad \u5c5e\u6027\u3002 \u8981\u505c\u6b62 tensor \u7684\u5386\u53f2\u8ddf\u8e2a\uff0c\u53ef\u8c03\u7528 .detach() \uff0c\u5c06\u5b83\u4ece\u8ba1\u7b97\u5386\u53f2\u4e2d\u5206\u79bb\u51fa\u6765\uff0c\u9632\u6b62\u5b83\u5728\u5c06\u6765\u7684\u8ba1\u7b97\u4e2d\u88ab\u8ddf\u8e2a\u3002 \u8981\u9632\u6b62\u5386\u53f2\u8ddf\u8e2a\uff08\u548c\u5185\u5b58\u7684\u4f7f\u7528\uff09\uff0c\u8fd8\u53ef\u4ee5\u5c06\u4ee3\u7801\u5757\u653e\u7f6e\u5728 with torch.no_grad(): \u4e2d\u3002 \u8fd9\u5728\u8bc4\u4f30\u6a21\u578b\u65f6\u5c24\u5176\u6709\u7528\uff0c\u56e0\u4e3a\u6a21\u578b\u53ef\u80fd\u5177\u6709\u53ef\u8bad\u7ec3\u7684\u53c2\u6570 requires_grad=True \uff0c\u4f46\u8fdb\u884c\u8bc4\u4f30\u65f6\u4e0d\u9700\u8981\u68af\u5ea6\u3002 \u8fd8\u6709\u4e00\u4e2a\u7c7b\u5bf9\u4e8e autograd \u5b9e\u73b0\u975e\u5e38\u91cd\u8981\u2014\u2014 Function \u3002 Tensor \u548c Function \u4e92\u76f8\u8fde\u63a5\u5e76\u6784\u5efa\u4e00\u4e2a\u975e\u5faa\u73af\u56fe\uff0c\u5b83\u5c06\u5b8c\u6574\u7684\u8ba1\u7b97\u5386\u53f2\u8fdb\u884c\u4e86\u7f16\u7801\u8bb0\u5f55\u3002 \u6bcf\u4e2a tensor \u90fd\u6709\u4e00\u4e2a .grad_fn \u5c5e\u6027\uff0c\u8be5\u5c5e\u6027\u6307\u5411\u521b\u5efa\u8fd9\u4e2a Tensor \u7684 Function \uff08\u9664\u4e86\u7528\u6237\u521b\u5efa\u7684 Tensor\u2014\u2014\u4ed6\u4eec\u7684 grand_fn \u662f None \uff09 \u5982\u9700\u8ba1\u7b97\u5bfc\u6570(derivatives)\uff0c\u53ef\u4ee5\u8c03\u7528 Tensor \u7684 .backward() \u65b9\u6cd5\u3002 \u5982\u679c Tensor \u662f\u6807\u91cf\uff08\u5373\u5b83\u5305\u53ea\u542b\u5355\u5143\u7d20\u6570\u636e\uff09\uff0c\u5219 backward() \u4e0d\u9700\u8981\u6307\u5b9a\u53c2\u6570\uff0c\u4f46\u662f\u5982\u679c\u5b83\u6709\u591a\u4e2a\u5143\u7d20\uff0c\u5219\u9700\u8981\u6307\u5b9a\u4e00\u4e2a\u5f62\u72b6\u5339\u914d\u7684 tensor \u4f5c\u4e3a gradient \u53c2\u6570\u3002 1 import torch \u521b\u5efa\u4e00\u4e2a tensor\uff0c\u8bbe\u7f6e requires_grad=True \u8ddf\u8e2a\u5b83\u7684\u8ba1\u7b97 1 2 x = torch . ones ( 2 , 2 , requires_grad = True ) print ( x ) 1 2 tensor([[1., 1.], [1., 1.]], requires_grad=True) \u8fdb\u884c\u4e00\u6b21 tensor \u8fd0\u7b97\uff1a 1 2 y = x + 2 print ( y ) 1 2 tensor([[3., 3.], [3., 3.]], grad_fn=<AddBackward0>) y \u662f\u8fd9\u4e2a\u8fd0\u7b97\u7684\u7ed3\u679c\u5f20\u91cf\uff0c\u6240\u4ee5\u5b83\u4e5f\u6709 grad_fn \u3002 1 print ( y . grad_fn ) 1 <AddBackward0 object at 0x7f10a069dd68> \u5bf9 y \u505a\u66f4\u591a\u8fd0\u7b97 1 2 3 4 z = y * y * 3 out = z . mean () print ( z , out ) 1 2 tensor([[27., 27.], [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward1>) .requires_grad_( ... ) \u539f\u4f4d\u6539\u53d8\u4e86\u5df2\u6709 Tensor \u7684 requires_grad \u6807\u5fd7\u3002\u8be5\u8f93\u5165\u6807\u5fd7\u7684\u9ed8\u8ba4\u503c\u662f False \u3002 1 2 3 4 5 6 7 a = torch . randn ( 2 , 2 ) a = (( a * 3 ) / ( a - 1 )) print ( a . requires_grad ) a . requires_grad_ ( True ) print ( a . requires_grad ) b = ( a * a ) . sum () print ( b . grad_fn ) 1 2 3 False True <SumBackward0 object at 0x7f0ff5fc2240> \u68af\u5ea6(Gradients) \u00b6 \u73b0\u5728\u8fdb\u884c\u53cd\u5411\u4f20\u64ad(backprop)\u3002 \u7531\u4e8e out \u5305\u542b\u5355\u4e2a\u6807\u91cf\uff0c out.backward() \u76f8\u5f53\u4e8e out.backward(torch.tensor(1.)) \u3002 1 out . backward () \u8f93\u51fa\u68af\u5ea6 d(out)/dx 1 print ( x . grad ) 1 2 tensor([[4.5000, 4.5000], [4.5000, 4.5000]]) \u5f97\u5230\u4e00\u4e2a 4.5 \u77e9\u9635\u3002 \u5c06 out Tensor \u8bbe\u4e3a \u201c o o \u201d\u3002 \u6211\u4eec\u6709 o = \\frac{1}{4}\\sum_i z_i o = \\frac{1}{4}\\sum_i z_i z_i = 3(x_i+2)^2 z_i = 3(x_i+2)^2 \u4ee5\u53ca z_i\\bigr\\rvert_{x_i=1} = 27 z_i\\bigr\\rvert_{x_i=1} = 27 \u6240\u4ee5\uff0c \\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2) \\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2) \u90a3\u4e48 \\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5 \\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5 \u5728\u6570\u5b66\u4e0a\uff0c\u82e5\u6709\u5411\u91cf\u503c\u51fd\u6570 \\vec{y}=f(\\vec{x}) \\vec{y}=f(\\vec{x}) \uff0c \u5219 \\vec{y} \\vec{y} \u7684\u68af\u5ea6\u662f\u5173\u4e8e \\vec{x} \\vec{x} \u7684\u96c5\u53ef\u6bd4\u77e9\u9635\uff1a \\begin{align}J=\\left(\\begin{array}{ccc} \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}} \\end{array}\\right)\\end{align} \\begin{align}J=\\left(\\begin{array}{ccc} \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}} \\end{array}\\right)\\end{align} \u4e00\u822c\u6765\u8bf4\uff0c torch.autograd \u5c31\u662f\u4e00\u4e2a\u8ba1\u7b97\u5411\u91cf\u96c5\u53ef\u6bd4\u79ef\u7684\u5f15\u64ce\u3002 \u4e5f\u5c31\u662f\u8bf4\uff0c\u5bf9\u7ed9\u5b9a\u7684\u4efb\u610f\u5411\u91cf v=\\left(\\begin{array}{cccc} v_{1} & v_{2} & \\cdots & v_{m}\\end{array}\\right)^{T} v=\\left(\\begin{array}{cccc} v_{1} & v_{2} & \\cdots & v_{m}\\end{array}\\right)^{T} \uff0c \u8ba1\u7b97 v^{T}\\cdot J v^{T}\\cdot J \u7684\u79ef\u3002 \u5982\u679c v v \u6070\u597d\u662f\u6807\u91cf\u51fd\u6570 l=g\\left(\\vec{y}\\right) l=g\\left(\\vec{y}\\right) \u7684\u68af\u5ea6\uff0c\u6362\u800c\u8a00\u4e4b\uff0c v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T} v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T} \uff0c \u90a3\u4e48\uff0c\u7531\u94fe\u89c4\u5219(chain rule)\u53ef\u77e5\uff0c\u5411\u91cf\u96c5\u53ef\u6bd4\u79ef\u662f l l \u5173\u4e8e \\vec{x} \\vec{x} \u7684\u68af\u5ea6\uff1a \\begin{align}J^{T}\\cdot v=\\left(\\begin{array}{ccc} \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}} \\end{array}\\right)\\left(\\begin{array}{c} \\frac{\\partial l}{\\partial y_{1}}\\\\ \\vdots\\\\ \\frac{\\partial l}{\\partial y_{m}} \\end{array}\\right)=\\left(\\begin{array}{c} \\frac{\\partial l}{\\partial x_{1}}\\\\ \\vdots\\\\ \\frac{\\partial l}{\\partial x_{n}} \\end{array}\\right)\\end{align} \\begin{align}J^{T}\\cdot v=\\left(\\begin{array}{ccc} \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}} \\end{array}\\right)\\left(\\begin{array}{c} \\frac{\\partial l}{\\partial y_{1}}\\\\ \\vdots\\\\ \\frac{\\partial l}{\\partial y_{m}} \\end{array}\\right)=\\left(\\begin{array}{c} \\frac{\\partial l}{\\partial x_{1}}\\\\ \\vdots\\\\ \\frac{\\partial l}{\\partial x_{n}} \\end{array}\\right)\\end{align} (\u6ce8\u610f v^{T}\\cdot J v^{T}\\cdot J \u7ed9\u51fa\u4e86\u4e00\u4e2a\u884c\u5411\u91cf\uff0c\u5b83\u53ef\u88ab\u89c6\u4f5c J^{T}\\cdot v J^{T}\\cdot v \u5217\u5411\u91cf\u3002) \u5411\u91cf\u96c5\u53ef\u6bd4\u79ef\u7684\u8fd9\u4e9b\u6027\u8d28\u4f7f\u5f97\u5c06\u5916\u90e8\u68af\u5ea6\u9001\u5230\u975e\u6807\u91cf\u8f93\u51fa\u6a21\u578b\u4e2d\u53d8\u5f97\u975e\u5e38\u65b9\u4fbf\u3002 \u73b0\u5728\uff0c\u6211\u4eec\u770b\u4e00\u4e2a\u5411\u91cf\u96c5\u53ef\u6bd4\u79ef\u7684\u4f8b\u5b50\uff1a 1 2 3 4 5 6 7 x = torch . randn ( 3 , requires_grad = True ) y = x * 2 while y . data . norm () < 1000 : y = y * 2 print ( y ) 1 tensor([-977.1336, 264.0834, -116.1632], grad_fn=<MulBackward0>) \u73b0\u5728\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c y \u4e0d\u518d\u662f\u6807\u91cf\u4e86\u3002 torch.autograd \u65e0\u6cd5\u76f4\u63a5\u8ba1\u7b97\u5b8c\u6574\u7684\u5411\u91cf\u96c5\u53ef\u6bd4\u884c\u5217\u5f0f\uff0c \u4f46\u662f\u5982\u679c\u6211\u4eec\u53ea\u662f\u9700\u8981\u5411\u91cf-\u5411\u91cf-\u96c5\u53ef\u6bd4\u79ef, \u53ea\u8981\u628a\u5411\u91cf\u4f20\u7ed9\u53c2\u6570 backward : 1 2 3 4 v = torch . tensor ([ 0.1 , 1.0 , 0.0001 ], dtype = torch . float ) y . backward ( v ) print ( x . grad ) 1 tensor([2.0480e+02, 2.0480e+03, 2.0480e-01]) \u53e6\u5916\uff0c\u7528 .requires_grad=True: \u5305\u542b\u4ee3\u7801\u5757\u53ef\u4ee5\u8ba9 autograd \u505c\u6b62\u8ddf\u8e2a Tensor \u7684\u5386\u53f2 1 2 3 4 5 print ( x . requires_grad ) print (( x ** 2 ) . requires_grad ) with torch . no_grad (): print (( x ** 2 ) . requires_grad ) 1 2 3 True True False \u5ef6\u540e\u9605\u8bfb: autograd \u548c Function \u7684\u6587\u6863\u5728 https://pytorch.org/docs/autograd","title":"Autograd\uff1a\u81ea\u52a8\u5fae\u5206"},{"location":"beginner/blitz/autograd_tutorial/#autograd-\u81ea\u52a8\u5fae\u5206automatic-differentiation","text":"autograd \u5305 \u662f PyTorch \u4e2d\u6240\u6709\u795e\u7ecf\u7f51\u7edc\u7684\u4e2d\u5fc3\u3002 \u6211\u4eec\u9996\u5148\u7b80\u8981\u67e5\u770b\uff0c\u7136\u540e\u8bad\u7ec3\u7b2c\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u3002 autograd \u5305\u4e3a Tensor \u4e0a\u7684\u6240\u6709\u8fd0\u7b97\u63d0\u4f9b\u81ea\u52a8\u5fae\u5206(automatic differentiation)\u3002 \u5b83\u662f\u4e00\u4e2a\u201c\u8fd0\u884c\u4e2d\u5b9a\u4e49\u201d(define-by-run)\u7684\u6846\u67b6\uff0c\u8fd9\u610f\u5473\u7740\u53cd\u5411\u4f20\u64ad(backpropagation)\u7b97\u6cd5\u7531\u4ee3\u7801\u5728\u8fd0\u884c\u65f6\u5b9a\u4e49\uff0c\u5e76\u4e14\u6bcf\u4e2a\u8fed\u4ee3\u90fd\u53ef\u4ee5\u4e0d\u540c\u3002 \u6211\u4eec\u901a\u8fc7\u66f4\u7b80\u5355\u7684\u672f\u8bed\u7684\u4f8b\u5b50\u6765\u770b\u770b\u8fd9\u4e9b\u3002","title":"Autograd: \u81ea\u52a8\u5fae\u5206(Automatic differentiation)"},{"location":"beginner/blitz/autograd_tutorial/#\u5f20\u91cftensor","text":"torch.Tensor \u662f\u8fd9\u4e2a\u5305\u7684\u4e2d\u5fc3\u7c7b\u3002 \u5c06\u5b83\u7684 .requires_grad \u5c5e\u6027\u8bbe\u7f6e\u4e3a True \uff0c\u5373\u5f00\u59cb\u8ddf\u8e2a\u5176\u4e0a\u7684\u6240\u6709\u8fd0\u7b97\u3002 \u5b8c\u6210\u8ba1\u7b97\u540e\u8c03\u7528 .backward() \u5373\u53ef\u81ea\u52a8\u5b8c\u6210\u68af\u5ea6\u8ba1\u7b97\u3002 \u8be5 tensor \u68af\u5ea6\u7684\u7d2f\u8ba1\u548c\u8bb0\u5f55\u5728 .grad \u5c5e\u6027\u3002 \u8981\u505c\u6b62 tensor \u7684\u5386\u53f2\u8ddf\u8e2a\uff0c\u53ef\u8c03\u7528 .detach() \uff0c\u5c06\u5b83\u4ece\u8ba1\u7b97\u5386\u53f2\u4e2d\u5206\u79bb\u51fa\u6765\uff0c\u9632\u6b62\u5b83\u5728\u5c06\u6765\u7684\u8ba1\u7b97\u4e2d\u88ab\u8ddf\u8e2a\u3002 \u8981\u9632\u6b62\u5386\u53f2\u8ddf\u8e2a\uff08\u548c\u5185\u5b58\u7684\u4f7f\u7528\uff09\uff0c\u8fd8\u53ef\u4ee5\u5c06\u4ee3\u7801\u5757\u653e\u7f6e\u5728 with torch.no_grad(): \u4e2d\u3002 \u8fd9\u5728\u8bc4\u4f30\u6a21\u578b\u65f6\u5c24\u5176\u6709\u7528\uff0c\u56e0\u4e3a\u6a21\u578b\u53ef\u80fd\u5177\u6709\u53ef\u8bad\u7ec3\u7684\u53c2\u6570 requires_grad=True \uff0c\u4f46\u8fdb\u884c\u8bc4\u4f30\u65f6\u4e0d\u9700\u8981\u68af\u5ea6\u3002 \u8fd8\u6709\u4e00\u4e2a\u7c7b\u5bf9\u4e8e autograd \u5b9e\u73b0\u975e\u5e38\u91cd\u8981\u2014\u2014 Function \u3002 Tensor \u548c Function \u4e92\u76f8\u8fde\u63a5\u5e76\u6784\u5efa\u4e00\u4e2a\u975e\u5faa\u73af\u56fe\uff0c\u5b83\u5c06\u5b8c\u6574\u7684\u8ba1\u7b97\u5386\u53f2\u8fdb\u884c\u4e86\u7f16\u7801\u8bb0\u5f55\u3002 \u6bcf\u4e2a tensor \u90fd\u6709\u4e00\u4e2a .grad_fn \u5c5e\u6027\uff0c\u8be5\u5c5e\u6027\u6307\u5411\u521b\u5efa\u8fd9\u4e2a Tensor \u7684 Function \uff08\u9664\u4e86\u7528\u6237\u521b\u5efa\u7684 Tensor\u2014\u2014\u4ed6\u4eec\u7684 grand_fn \u662f None \uff09 \u5982\u9700\u8ba1\u7b97\u5bfc\u6570(derivatives)\uff0c\u53ef\u4ee5\u8c03\u7528 Tensor \u7684 .backward() \u65b9\u6cd5\u3002 \u5982\u679c Tensor \u662f\u6807\u91cf\uff08\u5373\u5b83\u5305\u53ea\u542b\u5355\u5143\u7d20\u6570\u636e\uff09\uff0c\u5219 backward() \u4e0d\u9700\u8981\u6307\u5b9a\u53c2\u6570\uff0c\u4f46\u662f\u5982\u679c\u5b83\u6709\u591a\u4e2a\u5143\u7d20\uff0c\u5219\u9700\u8981\u6307\u5b9a\u4e00\u4e2a\u5f62\u72b6\u5339\u914d\u7684 tensor \u4f5c\u4e3a gradient \u53c2\u6570\u3002 1 import torch \u521b\u5efa\u4e00\u4e2a tensor\uff0c\u8bbe\u7f6e requires_grad=True \u8ddf\u8e2a\u5b83\u7684\u8ba1\u7b97 1 2 x = torch . ones ( 2 , 2 , requires_grad = True ) print ( x ) 1 2 tensor([[1., 1.], [1., 1.]], requires_grad=True) \u8fdb\u884c\u4e00\u6b21 tensor \u8fd0\u7b97\uff1a 1 2 y = x + 2 print ( y ) 1 2 tensor([[3., 3.], [3., 3.]], grad_fn=<AddBackward0>) y \u662f\u8fd9\u4e2a\u8fd0\u7b97\u7684\u7ed3\u679c\u5f20\u91cf\uff0c\u6240\u4ee5\u5b83\u4e5f\u6709 grad_fn \u3002 1 print ( y . grad_fn ) 1 <AddBackward0 object at 0x7f10a069dd68> \u5bf9 y \u505a\u66f4\u591a\u8fd0\u7b97 1 2 3 4 z = y * y * 3 out = z . mean () print ( z , out ) 1 2 tensor([[27., 27.], [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward1>) .requires_grad_( ... ) \u539f\u4f4d\u6539\u53d8\u4e86\u5df2\u6709 Tensor \u7684 requires_grad \u6807\u5fd7\u3002\u8be5\u8f93\u5165\u6807\u5fd7\u7684\u9ed8\u8ba4\u503c\u662f False \u3002 1 2 3 4 5 6 7 a = torch . randn ( 2 , 2 ) a = (( a * 3 ) / ( a - 1 )) print ( a . requires_grad ) a . requires_grad_ ( True ) print ( a . requires_grad ) b = ( a * a ) . sum () print ( b . grad_fn ) 1 2 3 False True <SumBackward0 object at 0x7f0ff5fc2240>","title":"\u5f20\u91cf(Tensor)"},{"location":"beginner/blitz/autograd_tutorial/#\u68af\u5ea6gradients","text":"\u73b0\u5728\u8fdb\u884c\u53cd\u5411\u4f20\u64ad(backprop)\u3002 \u7531\u4e8e out \u5305\u542b\u5355\u4e2a\u6807\u91cf\uff0c out.backward() \u76f8\u5f53\u4e8e out.backward(torch.tensor(1.)) \u3002 1 out . backward () \u8f93\u51fa\u68af\u5ea6 d(out)/dx 1 print ( x . grad ) 1 2 tensor([[4.5000, 4.5000], [4.5000, 4.5000]]) \u5f97\u5230\u4e00\u4e2a 4.5 \u77e9\u9635\u3002 \u5c06 out Tensor \u8bbe\u4e3a \u201c o o \u201d\u3002 \u6211\u4eec\u6709 o = \\frac{1}{4}\\sum_i z_i o = \\frac{1}{4}\\sum_i z_i z_i = 3(x_i+2)^2 z_i = 3(x_i+2)^2 \u4ee5\u53ca z_i\\bigr\\rvert_{x_i=1} = 27 z_i\\bigr\\rvert_{x_i=1} = 27 \u6240\u4ee5\uff0c \\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2) \\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2) \u90a3\u4e48 \\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5 \\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5 \u5728\u6570\u5b66\u4e0a\uff0c\u82e5\u6709\u5411\u91cf\u503c\u51fd\u6570 \\vec{y}=f(\\vec{x}) \\vec{y}=f(\\vec{x}) \uff0c \u5219 \\vec{y} \\vec{y} \u7684\u68af\u5ea6\u662f\u5173\u4e8e \\vec{x} \\vec{x} \u7684\u96c5\u53ef\u6bd4\u77e9\u9635\uff1a \\begin{align}J=\\left(\\begin{array}{ccc} \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}} \\end{array}\\right)\\end{align} \\begin{align}J=\\left(\\begin{array}{ccc} \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}} \\end{array}\\right)\\end{align} \u4e00\u822c\u6765\u8bf4\uff0c torch.autograd \u5c31\u662f\u4e00\u4e2a\u8ba1\u7b97\u5411\u91cf\u96c5\u53ef\u6bd4\u79ef\u7684\u5f15\u64ce\u3002 \u4e5f\u5c31\u662f\u8bf4\uff0c\u5bf9\u7ed9\u5b9a\u7684\u4efb\u610f\u5411\u91cf v=\\left(\\begin{array}{cccc} v_{1} & v_{2} & \\cdots & v_{m}\\end{array}\\right)^{T} v=\\left(\\begin{array}{cccc} v_{1} & v_{2} & \\cdots & v_{m}\\end{array}\\right)^{T} \uff0c \u8ba1\u7b97 v^{T}\\cdot J v^{T}\\cdot J \u7684\u79ef\u3002 \u5982\u679c v v \u6070\u597d\u662f\u6807\u91cf\u51fd\u6570 l=g\\left(\\vec{y}\\right) l=g\\left(\\vec{y}\\right) \u7684\u68af\u5ea6\uff0c\u6362\u800c\u8a00\u4e4b\uff0c v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T} v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T} \uff0c \u90a3\u4e48\uff0c\u7531\u94fe\u89c4\u5219(chain rule)\u53ef\u77e5\uff0c\u5411\u91cf\u96c5\u53ef\u6bd4\u79ef\u662f l l \u5173\u4e8e \\vec{x} \\vec{x} \u7684\u68af\u5ea6\uff1a \\begin{align}J^{T}\\cdot v=\\left(\\begin{array}{ccc} \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}} \\end{array}\\right)\\left(\\begin{array}{c} \\frac{\\partial l}{\\partial y_{1}}\\\\ \\vdots\\\\ \\frac{\\partial l}{\\partial y_{m}} \\end{array}\\right)=\\left(\\begin{array}{c} \\frac{\\partial l}{\\partial x_{1}}\\\\ \\vdots\\\\ \\frac{\\partial l}{\\partial x_{n}} \\end{array}\\right)\\end{align} \\begin{align}J^{T}\\cdot v=\\left(\\begin{array}{ccc} \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}} \\end{array}\\right)\\left(\\begin{array}{c} \\frac{\\partial l}{\\partial y_{1}}\\\\ \\vdots\\\\ \\frac{\\partial l}{\\partial y_{m}} \\end{array}\\right)=\\left(\\begin{array}{c} \\frac{\\partial l}{\\partial x_{1}}\\\\ \\vdots\\\\ \\frac{\\partial l}{\\partial x_{n}} \\end{array}\\right)\\end{align} (\u6ce8\u610f v^{T}\\cdot J v^{T}\\cdot J \u7ed9\u51fa\u4e86\u4e00\u4e2a\u884c\u5411\u91cf\uff0c\u5b83\u53ef\u88ab\u89c6\u4f5c J^{T}\\cdot v J^{T}\\cdot v \u5217\u5411\u91cf\u3002) \u5411\u91cf\u96c5\u53ef\u6bd4\u79ef\u7684\u8fd9\u4e9b\u6027\u8d28\u4f7f\u5f97\u5c06\u5916\u90e8\u68af\u5ea6\u9001\u5230\u975e\u6807\u91cf\u8f93\u51fa\u6a21\u578b\u4e2d\u53d8\u5f97\u975e\u5e38\u65b9\u4fbf\u3002 \u73b0\u5728\uff0c\u6211\u4eec\u770b\u4e00\u4e2a\u5411\u91cf\u96c5\u53ef\u6bd4\u79ef\u7684\u4f8b\u5b50\uff1a 1 2 3 4 5 6 7 x = torch . randn ( 3 , requires_grad = True ) y = x * 2 while y . data . norm () < 1000 : y = y * 2 print ( y ) 1 tensor([-977.1336, 264.0834, -116.1632], grad_fn=<MulBackward0>) \u73b0\u5728\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c y \u4e0d\u518d\u662f\u6807\u91cf\u4e86\u3002 torch.autograd \u65e0\u6cd5\u76f4\u63a5\u8ba1\u7b97\u5b8c\u6574\u7684\u5411\u91cf\u96c5\u53ef\u6bd4\u884c\u5217\u5f0f\uff0c \u4f46\u662f\u5982\u679c\u6211\u4eec\u53ea\u662f\u9700\u8981\u5411\u91cf-\u5411\u91cf-\u96c5\u53ef\u6bd4\u79ef, \u53ea\u8981\u628a\u5411\u91cf\u4f20\u7ed9\u53c2\u6570 backward : 1 2 3 4 v = torch . tensor ([ 0.1 , 1.0 , 0.0001 ], dtype = torch . float ) y . backward ( v ) print ( x . grad ) 1 tensor([2.0480e+02, 2.0480e+03, 2.0480e-01]) \u53e6\u5916\uff0c\u7528 .requires_grad=True: \u5305\u542b\u4ee3\u7801\u5757\u53ef\u4ee5\u8ba9 autograd \u505c\u6b62\u8ddf\u8e2a Tensor \u7684\u5386\u53f2 1 2 3 4 5 print ( x . requires_grad ) print (( x ** 2 ) . requires_grad ) with torch . no_grad (): print (( x ** 2 ) . requires_grad ) 1 2 3 True True False \u5ef6\u540e\u9605\u8bfb: autograd \u548c Function \u7684\u6587\u6863\u5728 https://pytorch.org/docs/autograd","title":"\u68af\u5ea6(Gradients)"},{"location":"beginner/blitz/cifar10_tutorial/","text":"\u8bad\u7ec3\u5206\u7c7b\u5668 \u00b6 \u6211\u4eec\u5df2\u7ecf\u4e86\u89e3\u4e86\u5982\u4f55\u5b9a\u4e49\u795e\u7ecf\u7f51\u7edc\uff0c\u8ba1\u7b97\u635f\u8017\u5e76\u66f4\u65b0\u7f51\u7edc\u6743\u91cd\u3002 \u73b0\u5728\u4f60\u53ef\u80fd\u5728\u60f3\uff1a \u6709\u5173\u6570\u636e\u7684\u95ee\u9898 \u00b6 \u901a\u5e38\uff0c\u5f53\u5904\u7406\u56fe\u50cf\u3001\u6587\u5b57\u3001\u97f3\u9891\u6216\u8005\u89c6\u9891\u6570\u636e\u7684\u65f6\u5019\uff0c\u53ef\u4ee5\u4f7f\u7528\u6807\u51c6\u7684python\u5305\u5c06\u6570\u636e\u52a0\u8f7d\u5230numpy\u6570\u7ec4\u4e2d\u3002\u7136\u540e\u53ef\u4ee5\u5c06\u8fd9\u4e2a\u6570\u7ec4\u8f6c\u6362\u6210\u4e00\u4e2a torch.*Tensor \u3002 \u5bf9\u4e8e\u56fe\u50cf\uff0cPillow\uff0cOpenCV\u7b49\u8f6f\u4ef6\u5305\u5f88\u6709\u7528 \u5bf9\u4e8e\u97f3\u9891\uff0cscipy\uff0clibrosa\u7b49\u5305\u5f88\u6709\u7528 \u5bf9\u4e8e\u6587\u5b57\uff0c\u76f4\u63a5\u4f7f\u7528Python\u6216\u8005Cython\u52a0\u8f7d\uff0c\u6216\u8005NLTK\u548cSpaCY\u90fd\u53ef\u884c \u89c6\u89c9\u5f88\u7279\u6b8a\uff0c\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a torchvision \u7684\u5305\uff0c\u5b83\u63d0\u4f9b\u4e86\u6570\u636e\u52a0\u8f7d\u5668 torchvision.datasets \uff0c\u53ef\u4ee5\u4ece\u5e38\u89c1\u7684\u6570\u636e\u96c6\u52a0\u8f7d\u6570\u636e\uff0c\u5982Imagenet, CIFAR10, MNIST \u7b49\uff1b\u8fd8\u63d0\u4f9b\u7528\u4e8e\u56fe\u50cf\u7684\u6570\u636e\u8f6c\u6362\u5668 torch.utils.data.DataLoader \u3002 \u8fd9\u63d0\u4f9b\u4e86\u6781\u5927\u7684\u4fbf\u5229\u800c\u4e14\u80fd\u591f\u907f\u514d\u7f16\u5199\u6837\u677f\u4ee3\u7801\u3002 \u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u4f7f\u7528CIFAR10\u6570\u636e\u96c6\u3002\u5b83\u6709\u7c7b\uff1a\u2018airplane\u2019, \u2018automobile\u2019, \u2018bird\u2019, \u2018cat\u2019, \u2018deer\u2019, \u2018dog\u2019, \u2018frog\u2019, \u2018horse\u2019, \u2018ship\u2019,\u2018truck\u2019\u3002 CIFAR-10\u4e2d\u7684\u56fe\u50cf\u5c3a\u5bf8\u4e3a3x32x32\uff0c\u5373\u5c3a\u5bf8\u4e3a32x32\u50cf\u7d20\u76843\u901a\u9053\u5f69\u8272\u56fe\u50cf\u3002 cifar10 \u8bad\u7ec3\u56fe\u50cf\u5206\u7c7b\u5668 \u00b6 \u6211\u4eec\u5c06\u6309\u987a\u5e8f\u6267\u884c\u4ee5\u4e0b\u6b65\u9aa4\uff1a \u7528 torchvision \u52a0\u8f7d\u5e76\u89c4\u683c\u5316CIFAR10\u7684\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u96c6 \u5b9a\u4e49\u5377\u53ca\u795e\u7ecf\u7f51\u7edc(CNN) \u5b9a\u4e49\u635f\u5931\u51fd\u6570 \u4f7f\u7528\u8bad\u7ec3\u6570\u636e\u8bad\u7ec3\u7f51\u7edc \u4f7f\u7528\u6d4b\u8bd5\u6570\u636e\u6d4b\u8bd5\u7f51\u7edc 1. \u52a0\u8f7d\u5e76\u89c4\u683c\u5316CIFAR10 \u00b6 \u7528 torchvision \u52a0\u8f7dCIFAR10\u975e\u5e38\u5bb9\u6613\u3002 1 2 3 4 5 6 7 from time import time import torch import torchvision import torchvision.transforms as transforms from tqdm import tqdm torchvision\u6570\u636e\u96c6\u7684\u8f93\u51fa\u662f [0,1] \u533a\u95f4PILImage\u56fe\u50cf\u3002\u6211\u4eec\u628a\u5b83\u4eec\u8f6c\u6362\u4e3a\u89c4\u683c\u5316 [-1, 1] \u533a\u95f4\u7684\u5f20\u91cf\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.5 , 0.5 , 0.5 ), ( 0.5 , 0.5 , 0.5 ))]) trainset = torchvision . datasets . CIFAR10 ( root = './data' , train = True , download = True , transform = transform ) trainloader = torch . utils . data . DataLoader ( trainset , batch_size = 4 , shuffle = True , num_workers = 2 ) testset = torchvision . datasets . CIFAR10 ( root = './data' , train = False , download = True , transform = transform ) testloader = torch . utils . data . DataLoader ( testset , batch_size = 4 , shuffle = False , num_workers = 2 ) classes = ( 'plane' , 'car' , 'bird' , 'cat' , 'deer' , 'dog' , 'frog' , 'horse' , 'ship' , 'truck' ) 1 2 Files already downloaded and verified Files already downloaded and verified \u6211\u4eec\u6765\u5c55\u793a\u4e00\u4e9b\u8bad\u7ec3\u56fe\u50cf\uff0c\u633a\u6709\u8da3\u7684\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import matplotlib.pyplot as plt import numpy as np # functions to show an image def imshow ( img ): img = img / 2 + 0.5 # unnormalize npimg = img . numpy () plt . imshow ( np . transpose ( npimg , ( 1 , 2 , 0 ))) # get some random training images dataiter = iter ( trainloader ) images , labels = dataiter . next () # show images imshow ( torchvision . utils . make_grid ( images )) # print labels print ( ' ' . join ( ' %5s ' % classes [ labels [ j ]] for j in range ( 4 ))) 1 truck horse dog deer 2. \u5b9a\u4e49\u5377\u53ca\u795e\u7ecf\u7f51\u7edc(CNN) \u00b6 \u4ece\u524d\u6587\u7684\u795e\u7ecf\u7f51\u7edc\u7ae0\u8282\u590d\u5236\u4ee3\u7801\uff0c\u4fee\u6539\uff0c\u8ba9\u5b83\u91c7\u75283\u901a\u9053\u56fe\u50cf(\u800c\u4e0d\u662f\u4e4b\u524d\u5b9a\u4e49\u76841\u901a\u9053) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import torch.nn as nn import torch.nn.functional as F class Net ( nn . Module ): def __init__ ( self ): super ( Net , self ) . __init__ () self . conv1 = nn . Conv2d ( 3 , 6 , 5 ) self . pool = nn . MaxPool2d ( 2 , 2 ) self . conv2 = nn . Conv2d ( 6 , 16 , 5 ) self . fc1 = nn . Linear ( 16 * 5 * 5 , 120 ) self . fc2 = nn . Linear ( 120 , 84 ) self . fc3 = nn . Linear ( 84 , 10 ) def forward ( self , x ): x = self . pool ( F . relu ( self . conv1 ( x ))) x = self . pool ( F . relu ( self . conv2 ( x ))) x = x . view ( - 1 , 16 * 5 * 5 ) x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x net = Net () 3. \u5b9a\u4e49\u635f\u5931\u51fd\u6570 \u00b6 \u4f7f\u7528\u4ea4\u53c9\u71b5\u635f\u5931\u548c\u5e26\u6709\u52a8\u91cf\u7684SGD\u3002 1 2 3 4 import torch.optim as optim criterion = nn . CrossEntropyLoss () optimizer = optim . SGD ( net . parameters (), lr = 0.001 , momentum = 0.9 ) 4. \u8bad\u7ec3\u7f51\u7edc \u00b6 \u4e8b\u60c5\u5f00\u59cb\u53d8\u5f97\u6709\u8da3\u4e86\u3002 \u6211\u4eec\u53ea\u9700\u5faa\u73af\u904d\u5386\u6570\u636e\u8fed\u4ee3\u5668\uff0c\u5e76\u5c06\u8f93\u5165\u63d0\u4f9b\u7ed9\u7f51\u7edc\u5e76\u8fdb\u884c\u4f18\u5316\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 for epoch in range ( 2 ): # loop over the dataset multiple times ts = time () running_loss = 0.0 for i , data in enumerate ( trainloader , 0 ): # get the inputs inputs , labels = data # zero the parameter gradients optimizer . zero_grad () # forward + backward + optimize outputs = net ( inputs ) loss = criterion ( outputs , labels ) loss . backward () optimizer . step () # print statistics running_loss += loss . item () if i % 2000 == 1999 : # print every 2000 mini-batches print ( '[ %d , %5d ] loss: %.3f ' % ( epoch + 1 , i + 1 , running_loss / 2000 )) running_loss = 0.0 print ( 'Epoch {0} finished in {1:.3f} second(s)' . format ( epoch , time () - ts )) print ( 'Finished Training' ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 [1, 2000] loss: 2.237 [1, 4000] loss: 1.909 [1, 6000] loss: 1.723 [1, 8000] loss: 1.601 [1, 10000] loss: 1.531 [1, 12000] loss: 1.477 Epoch 0 finished in 75.642 second(s) [2, 2000] loss: 1.404 [2, 4000] loss: 1.372 [2, 6000] loss: 1.347 [2, 8000] loss: 1.353 [2, 10000] loss: 1.296 [2, 12000] loss: 1.263 Epoch 1 finished in 72.981 second(s) Finished Training 5. \u4f7f\u7528\u6d4b\u8bd5\u6570\u636e\u5bf9\u7f51\u7edc\u8fdb\u884c\u6d4b\u8bd5 \u00b6 \u5728\u8bad\u7ec3\u6570\u636e\u96c6\u4e0a\u5df2\u7ecf\u5bf9\u7f51\u7edc\u8fdb\u884c\u4e86\u4e24\u4e2a\u7eaa\u5143(epoch)\u7684\u8bad\u7ec3\u3002 \u4f46\u662f\u6211\u4eec\u8fd8\u8981\u8fdb\u884c\u68c0\u67e5\uff0c\u770b\u770b\u8fd9\u4e2a\u7f51\u7edc\u662f\u5426\u771f\u7684\u5b66\u4f1a\u4e86\u4ec0\u4e48\u3002 \u7528\u8fd9\u4e2a\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u9884\u6d4b\uff0c\u68c0\u67e5\u5b83\u8f93\u51fa\u7684\u5206\u7c7b\u6807\u7b7e\u662f\u5426\u548c\u5b9e\u9645\u503c\u4e00\u76f4\u3002\u5982\u679c\u9884\u6d4b\u6b63\u786e\uff0c\u628a\u6837\u672c\u52a0\u5230\u6b63\u786e\u9884\u6d4b\u5217\u8868\u3002 \u597d\u4e86\uff0c\u7b2c\u4e00\u6b65\u5148\u4ece\u6d4b\u8bd5\u96c6\u663e\u793a\u4e00\u4e2a\u56fe\u50cf\uff0c\u719f\u6089\u4e00\u4e0b\u3002 1 2 3 4 5 6 dataiter = iter ( testloader ) images , labels = dataiter . next () # print images imshow ( torchvision . utils . make_grid ( images )) print ( 'GroundTruth: ' , ' ' . join ( ' %5s ' % classes [ labels [ j ]] for j in range ( 4 ))) 1 GroundTruth: cat ship ship plane \u597d\u7684\uff0c\u73b0\u5728\u8ba9\u6211\u4eec\u770b\u770b\u795e\u7ecf\u7f51\u7edc\u8ba4\u4e3a\u4e0a\u9762\u8fd9\u4e9b\u4f8b\u5b50\u662f\u4ec0\u4e48\uff1a 1 outputs = net ( images ) \u8f93\u51fa\u503c\u662f10\u91cd\u7c7b\u578b\u7684\u80fd\u91cf\u503c\u3002 \u4e00\u4e2a\u7c7b\u578b\u7684\u80fd\u91cf\u503c\u8d8a\u9ad8\uff0c\u5c31\u8868\u660e\u7f51\u8def\u8d8a\u503e\u5411\u4e8e\u8ba4\u4e3a\u56fe\u7247\u662f\u8fd9\u79cd\u7c7b\u578b\u7684\u3002 \u90a3\u4e48\uff0c\u8ba9\u6211\u4eec\u5f97\u5230\u6700\u9ad8\u80fd\u91cf\u503c\u7684\u5e8f\u53f7\uff1a 1 2 3 4 _ , predicted = torch . max ( outputs , 1 ) print ( 'Predicted: ' , ' ' . join ( ' %5s ' % classes [ predicted [ j ]] for j in range ( 4 ))) 1 Predicted: cat ship ship ship \u7ed3\u679c\u4f3c\u4e4e\u8fd8\u884c\u3002 \u8ba9\u6211\u4eec\u770b\u770b\u8fd9\u4e2a\u7f51\u7edc\u5982\u4f55\u9884\u6d4b\u6574\u4e2a\u6d4b\u8bd5\u6570\u636e\u96c6\u3002 1 2 3 4 5 6 7 8 9 10 11 12 correct = 0 total = 0 with torch . no_grad (): for data in tqdm ( testloader ): images , labels = data outputs = net ( images ) _ , predicted = torch . max ( outputs . data , 1 ) total += labels . size ( 0 ) correct += ( predicted == labels ) . sum () . item () print ( 'Accuracy of the network on the 10000 test images: %d %% ' % ( 100 * correct / total )) 1 2 3 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [00:05<00:00, 420.29it/s] Accuracy of the network on the 10000 test images: 53 % \u770b\u8d77\u6765\u6bd4\u968f\u4fbf\u731c(\u4ece10\u4e2a\u7c7b\u578b\u4e2d\u968f\u673a\u9009\u4e00\u4e2a\u7684\u51c6\u786e\u5ea6\u662f10%)\u5f3a\u3002 \u8fd9\u4e2a\u7f51\u7edc\u4f3c\u4e4e\u5b66\u4f1a\u4e86\u4e00\u4e9b\u4e1c\u897f\u3002 \u55ef\uff0c\u54ea\u4e9b\u7c7b\u578b\u8868\u73b0\u597d\uff0c\u54ea\u4e9b\u4e0d\u597d\uff1a 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class_correct = list ( 0. for i in range ( 10 )) class_total = list ( 0. for i in range ( 10 )) with torch . no_grad (): for data in tqdm ( testloader ): images , labels = data outputs = net ( images ) _ , predicted = torch . max ( outputs , 1 ) c = ( predicted == labels ) . squeeze () for i in range ( 4 ): label = labels [ i ] class_correct [ label ] += c [ i ] . item () class_total [ label ] += 1 for i in range ( 10 ): print ( 'Accuracy of %5s : %2d %% ' % ( classes [ i ], 100 * class_correct [ i ] / class_total [ i ])) 1 2 3 4 5 6 7 8 9 10 11 12 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [00:05<00:00, 418.22it/s] Accuracy of plane : 29 % Accuracy of car : 68 % Accuracy of bird : 28 % Accuracy of cat : 42 % Accuracy of deer : 43 % Accuracy of dog : 45 % Accuracy of frog : 70 % Accuracy of horse : 56 % Accuracy of ship : 81 % Accuracy of truck : 73 % \u597d\u7684\uff0c\u63a5\u4e0b\u6765\u5462\uff1f \u6211\u4eec\u5982\u4f55\u5728GPU\u4e0a\u8fd0\u884c\u8fd9\u4e9b\u795e\u7ecf\u7f51\u7edc\uff1f \u5728GPU\u4e0a\u8bad\u7ec3 \u00b6 \u5c31\u50cf\u5c06Tensor\u8f6c\u5230GPU\u4e00\u6837\uff0c\u53ef\u5c06\u795e\u7ecf\u7f51\u7edc\u8f6c\u5230GPU\u3002 \u5982\u679c\u6709\u53ef\u7528\u7684CUDA\uff0c\u6211\u4eec\u9996\u5148\u83b7\u53d6\u7b2c\u4e00\u4e2a\u53ef\u89c1\u7684cuda\u8bbe\u5907\uff1a 1 2 3 4 5 device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) # Assuming that we are on a CUDA machine, this should print a CUDA device: print ( device ) 1 cuda:0 \u672c\u8282\u7684\u5176\u4f59\u90e8\u5206\u5047\u5b9a device \u662f\u4e00\u4e2aCUDA\u8bbe\u5907\u3002 \u7136\u540e\u8fd9\u4e9b\u65b9\u6cd5\u5c06\u9012\u5f52\u904d\u5386\u6240\u6709\u6a21\u5757\u5e76\u5c06\u5176\u53c2\u6570\u548c\u7f13\u51b2\u533a\u8f6c\u6362\u4e3aCUDA\u5f20\u91cf\uff1a 1 net . to ( device ) \u8bb0\u4f4f\u8fd8\u5f97\u5c06\u6bcf\u4e00\u6b65\u7684\u8f93\u5165\u548c\u76ee\u6807\u53d1\u9001\u5230GPU\uff1a 1 inputs , labels = inputs . to ( device ), labels . to ( device ) \u53ef\u80fd\u89c2\u5bdf\u4e0d\u5230\u4e0eCPU\u76f8\u6bd4\u7684\u6709\u660e\u663e\u52a0\u901f\uff0c\u90a3\u662f\u56e0\u4e3a\u7684\u7f51\u7edc\u592a\u5c0f\u3002 \u7ec3\u4e60 \u00b6 \u5c1d\u8bd5\u589e\u52a0\u7f51\u7edc\u7684\u5bbd\u5ea6\uff08\u7b2c\u4e00\u4e2a nn.Conv2d \u7684\u7b2c\u4e8c\u4e2a\u53c2\u6570\uff0c\u7b2c\u4e8c\u4e2a nn.Conv2d \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u2014\u2014\u5b83\u4eec\u5e94\u662f\u76f8\u540c\u7684\u6570\u503c\uff09\uff0c\u770b\u770b\u80fd\u83b7\u5f97\u4ec0\u4e48\u6837\u7684\u52a0\u901f\u3002 \u8fbe\u5230\u7684\u76ee\u6807\uff1a \u5728\u9ad8\u5c42\u6b21\u4e0a\u7406\u89e3PyTorch\u7684Tensor\u5e93\u548c\u795e\u7ecf\u7f51\u7edc\u3002 \u8bad\u7ec3\u4e00\u4e2a\u5c0f\u578b\u7684\u56fe\u50cf\u5206\u7c7b\u795e\u7ecf\u7f51\u7edc 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 class Net2 ( nn . Module ): def __init__ ( self ): super () . __init__ () self . conv1 = nn . Conv2d ( 3 , 12 , 5 ) self . pool = nn . MaxPool2d ( 2 , 2 ) self . conv2 = nn . Conv2d ( 12 , 16 , 5 ) self . fc1 = nn . Linear ( 16 * 5 * 5 , 120 ) self . fc2 = nn . Linear ( 120 , 84 ) self . fc3 = nn . Linear ( 84 , 10 ) def forward ( self , x ): x = self . pool ( F . relu ( self . conv1 ( x ))) x = self . pool ( F . relu ( self . conv2 ( x ))) x = x . view ( - 1 , 16 * 5 * 5 ) x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x net2 = Net2 () net2 . to ( device ) for epoch in range ( 2 ): # loop over the dataset multiple times ts = time () running_loss = 0.0 for i , data in enumerate ( trainloader , 0 ): # get the inputs inputs , labels = data inputs , labels = inputs . to ( device ), labels . to ( device ) # zero the parameter gradients optimizer . zero_grad () # forward + backward + optimize outputs = net2 ( inputs ) loss = criterion ( outputs , labels ) loss . backward () optimizer . step () # print statistics running_loss += loss . item () if i % 2000 == 1999 : # print every 2000 mini-batches print ( '[ %d , %5d ] loss: %.3f ' % ( epoch + 1 , i + 1 , running_loss / 2000 )) running_loss = 0.0 print ( 'Epoch {0} finished in {1:.3f} second(s)' . format ( epoch , time () - ts )) print ( 'Finished Training' ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 [1, 2000] loss: 2.305 [1, 4000] loss: 2.304 [1, 6000] loss: 2.304 [1, 8000] loss: 2.304 [1, 10000] loss: 2.303 [1, 12000] loss: 2.305 Epoch 0 finished in 72.346 second(s) [2, 2000] loss: 2.304 [2, 4000] loss: 2.304 [2, 6000] loss: 2.304 [2, 8000] loss: 2.303 [2, 10000] loss: 2.304 [2, 12000] loss: 2.305 Epoch 1 finished in 63.276 second(s) Finished Training \u5728\u591a\u4e2aGPU\u4e0a\u8fdb\u884c\u8bad\u7ec3 \u00b6 \u5982\u679c\u60f3\u4f7f\u7528\u6240\u6709\u7684GPU\u6765\u5927\u5e45\u52a0\u901f\uff0c\u8bf7\u53c2\u8003 data_parallel_tutorial\u3002 \u4e0b\u4e00\u6b65\u600e\u4e48\u529e\uff1f \u00b6 :doc: Train neural nets to play video games </intermediate/reinforcement_q_learning> Train a state-of-the-art ResNet network on imagenet _ Train a face generator using Generative Adversarial Networks _ Train a word-level language model using Recurrent LSTM networks _ More examples _ More tutorials _ Discuss PyTorch on the Forums _ Chat with other users on Slack _ 1","title":"\u8bad\u7ec3\u5206\u7c7b\u5668"},{"location":"beginner/blitz/cifar10_tutorial/#\u8bad\u7ec3\u5206\u7c7b\u5668","text":"\u6211\u4eec\u5df2\u7ecf\u4e86\u89e3\u4e86\u5982\u4f55\u5b9a\u4e49\u795e\u7ecf\u7f51\u7edc\uff0c\u8ba1\u7b97\u635f\u8017\u5e76\u66f4\u65b0\u7f51\u7edc\u6743\u91cd\u3002 \u73b0\u5728\u4f60\u53ef\u80fd\u5728\u60f3\uff1a","title":"\u8bad\u7ec3\u5206\u7c7b\u5668"},{"location":"beginner/blitz/cifar10_tutorial/#\u6709\u5173\u6570\u636e\u7684\u95ee\u9898","text":"\u901a\u5e38\uff0c\u5f53\u5904\u7406\u56fe\u50cf\u3001\u6587\u5b57\u3001\u97f3\u9891\u6216\u8005\u89c6\u9891\u6570\u636e\u7684\u65f6\u5019\uff0c\u53ef\u4ee5\u4f7f\u7528\u6807\u51c6\u7684python\u5305\u5c06\u6570\u636e\u52a0\u8f7d\u5230numpy\u6570\u7ec4\u4e2d\u3002\u7136\u540e\u53ef\u4ee5\u5c06\u8fd9\u4e2a\u6570\u7ec4\u8f6c\u6362\u6210\u4e00\u4e2a torch.*Tensor \u3002 \u5bf9\u4e8e\u56fe\u50cf\uff0cPillow\uff0cOpenCV\u7b49\u8f6f\u4ef6\u5305\u5f88\u6709\u7528 \u5bf9\u4e8e\u97f3\u9891\uff0cscipy\uff0clibrosa\u7b49\u5305\u5f88\u6709\u7528 \u5bf9\u4e8e\u6587\u5b57\uff0c\u76f4\u63a5\u4f7f\u7528Python\u6216\u8005Cython\u52a0\u8f7d\uff0c\u6216\u8005NLTK\u548cSpaCY\u90fd\u53ef\u884c \u89c6\u89c9\u5f88\u7279\u6b8a\uff0c\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a torchvision \u7684\u5305\uff0c\u5b83\u63d0\u4f9b\u4e86\u6570\u636e\u52a0\u8f7d\u5668 torchvision.datasets \uff0c\u53ef\u4ee5\u4ece\u5e38\u89c1\u7684\u6570\u636e\u96c6\u52a0\u8f7d\u6570\u636e\uff0c\u5982Imagenet, CIFAR10, MNIST \u7b49\uff1b\u8fd8\u63d0\u4f9b\u7528\u4e8e\u56fe\u50cf\u7684\u6570\u636e\u8f6c\u6362\u5668 torch.utils.data.DataLoader \u3002 \u8fd9\u63d0\u4f9b\u4e86\u6781\u5927\u7684\u4fbf\u5229\u800c\u4e14\u80fd\u591f\u907f\u514d\u7f16\u5199\u6837\u677f\u4ee3\u7801\u3002 \u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u4f7f\u7528CIFAR10\u6570\u636e\u96c6\u3002\u5b83\u6709\u7c7b\uff1a\u2018airplane\u2019, \u2018automobile\u2019, \u2018bird\u2019, \u2018cat\u2019, \u2018deer\u2019, \u2018dog\u2019, \u2018frog\u2019, \u2018horse\u2019, \u2018ship\u2019,\u2018truck\u2019\u3002 CIFAR-10\u4e2d\u7684\u56fe\u50cf\u5c3a\u5bf8\u4e3a3x32x32\uff0c\u5373\u5c3a\u5bf8\u4e3a32x32\u50cf\u7d20\u76843\u901a\u9053\u5f69\u8272\u56fe\u50cf\u3002 cifar10","title":"\u6709\u5173\u6570\u636e\u7684\u95ee\u9898"},{"location":"beginner/blitz/cifar10_tutorial/#\u8bad\u7ec3\u56fe\u50cf\u5206\u7c7b\u5668","text":"\u6211\u4eec\u5c06\u6309\u987a\u5e8f\u6267\u884c\u4ee5\u4e0b\u6b65\u9aa4\uff1a \u7528 torchvision \u52a0\u8f7d\u5e76\u89c4\u683c\u5316CIFAR10\u7684\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u96c6 \u5b9a\u4e49\u5377\u53ca\u795e\u7ecf\u7f51\u7edc(CNN) \u5b9a\u4e49\u635f\u5931\u51fd\u6570 \u4f7f\u7528\u8bad\u7ec3\u6570\u636e\u8bad\u7ec3\u7f51\u7edc \u4f7f\u7528\u6d4b\u8bd5\u6570\u636e\u6d4b\u8bd5\u7f51\u7edc","title":"\u8bad\u7ec3\u56fe\u50cf\u5206\u7c7b\u5668"},{"location":"beginner/blitz/cifar10_tutorial/#1-\u52a0\u8f7d\u5e76\u89c4\u683c\u5316cifar10","text":"\u7528 torchvision \u52a0\u8f7dCIFAR10\u975e\u5e38\u5bb9\u6613\u3002 1 2 3 4 5 6 7 from time import time import torch import torchvision import torchvision.transforms as transforms from tqdm import tqdm torchvision\u6570\u636e\u96c6\u7684\u8f93\u51fa\u662f [0,1] \u533a\u95f4PILImage\u56fe\u50cf\u3002\u6211\u4eec\u628a\u5b83\u4eec\u8f6c\u6362\u4e3a\u89c4\u683c\u5316 [-1, 1] \u533a\u95f4\u7684\u5f20\u91cf\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.5 , 0.5 , 0.5 ), ( 0.5 , 0.5 , 0.5 ))]) trainset = torchvision . datasets . CIFAR10 ( root = './data' , train = True , download = True , transform = transform ) trainloader = torch . utils . data . DataLoader ( trainset , batch_size = 4 , shuffle = True , num_workers = 2 ) testset = torchvision . datasets . CIFAR10 ( root = './data' , train = False , download = True , transform = transform ) testloader = torch . utils . data . DataLoader ( testset , batch_size = 4 , shuffle = False , num_workers = 2 ) classes = ( 'plane' , 'car' , 'bird' , 'cat' , 'deer' , 'dog' , 'frog' , 'horse' , 'ship' , 'truck' ) 1 2 Files already downloaded and verified Files already downloaded and verified \u6211\u4eec\u6765\u5c55\u793a\u4e00\u4e9b\u8bad\u7ec3\u56fe\u50cf\uff0c\u633a\u6709\u8da3\u7684\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import matplotlib.pyplot as plt import numpy as np # functions to show an image def imshow ( img ): img = img / 2 + 0.5 # unnormalize npimg = img . numpy () plt . imshow ( np . transpose ( npimg , ( 1 , 2 , 0 ))) # get some random training images dataiter = iter ( trainloader ) images , labels = dataiter . next () # show images imshow ( torchvision . utils . make_grid ( images )) # print labels print ( ' ' . join ( ' %5s ' % classes [ labels [ j ]] for j in range ( 4 ))) 1 truck horse dog deer","title":"1. \u52a0\u8f7d\u5e76\u89c4\u683c\u5316CIFAR10"},{"location":"beginner/blitz/cifar10_tutorial/#2-\u5b9a\u4e49\u5377\u53ca\u795e\u7ecf\u7f51\u7edccnn","text":"\u4ece\u524d\u6587\u7684\u795e\u7ecf\u7f51\u7edc\u7ae0\u8282\u590d\u5236\u4ee3\u7801\uff0c\u4fee\u6539\uff0c\u8ba9\u5b83\u91c7\u75283\u901a\u9053\u56fe\u50cf(\u800c\u4e0d\u662f\u4e4b\u524d\u5b9a\u4e49\u76841\u901a\u9053) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import torch.nn as nn import torch.nn.functional as F class Net ( nn . Module ): def __init__ ( self ): super ( Net , self ) . __init__ () self . conv1 = nn . Conv2d ( 3 , 6 , 5 ) self . pool = nn . MaxPool2d ( 2 , 2 ) self . conv2 = nn . Conv2d ( 6 , 16 , 5 ) self . fc1 = nn . Linear ( 16 * 5 * 5 , 120 ) self . fc2 = nn . Linear ( 120 , 84 ) self . fc3 = nn . Linear ( 84 , 10 ) def forward ( self , x ): x = self . pool ( F . relu ( self . conv1 ( x ))) x = self . pool ( F . relu ( self . conv2 ( x ))) x = x . view ( - 1 , 16 * 5 * 5 ) x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x net = Net ()","title":"2. \u5b9a\u4e49\u5377\u53ca\u795e\u7ecf\u7f51\u7edc(CNN)"},{"location":"beginner/blitz/cifar10_tutorial/#3-\u5b9a\u4e49\u635f\u5931\u51fd\u6570","text":"\u4f7f\u7528\u4ea4\u53c9\u71b5\u635f\u5931\u548c\u5e26\u6709\u52a8\u91cf\u7684SGD\u3002 1 2 3 4 import torch.optim as optim criterion = nn . CrossEntropyLoss () optimizer = optim . SGD ( net . parameters (), lr = 0.001 , momentum = 0.9 )","title":"3. \u5b9a\u4e49\u635f\u5931\u51fd\u6570"},{"location":"beginner/blitz/cifar10_tutorial/#4-\u8bad\u7ec3\u7f51\u7edc","text":"\u4e8b\u60c5\u5f00\u59cb\u53d8\u5f97\u6709\u8da3\u4e86\u3002 \u6211\u4eec\u53ea\u9700\u5faa\u73af\u904d\u5386\u6570\u636e\u8fed\u4ee3\u5668\uff0c\u5e76\u5c06\u8f93\u5165\u63d0\u4f9b\u7ed9\u7f51\u7edc\u5e76\u8fdb\u884c\u4f18\u5316\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 for epoch in range ( 2 ): # loop over the dataset multiple times ts = time () running_loss = 0.0 for i , data in enumerate ( trainloader , 0 ): # get the inputs inputs , labels = data # zero the parameter gradients optimizer . zero_grad () # forward + backward + optimize outputs = net ( inputs ) loss = criterion ( outputs , labels ) loss . backward () optimizer . step () # print statistics running_loss += loss . item () if i % 2000 == 1999 : # print every 2000 mini-batches print ( '[ %d , %5d ] loss: %.3f ' % ( epoch + 1 , i + 1 , running_loss / 2000 )) running_loss = 0.0 print ( 'Epoch {0} finished in {1:.3f} second(s)' . format ( epoch , time () - ts )) print ( 'Finished Training' ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 [1, 2000] loss: 2.237 [1, 4000] loss: 1.909 [1, 6000] loss: 1.723 [1, 8000] loss: 1.601 [1, 10000] loss: 1.531 [1, 12000] loss: 1.477 Epoch 0 finished in 75.642 second(s) [2, 2000] loss: 1.404 [2, 4000] loss: 1.372 [2, 6000] loss: 1.347 [2, 8000] loss: 1.353 [2, 10000] loss: 1.296 [2, 12000] loss: 1.263 Epoch 1 finished in 72.981 second(s) Finished Training","title":"4. \u8bad\u7ec3\u7f51\u7edc"},{"location":"beginner/blitz/cifar10_tutorial/#5-\u4f7f\u7528\u6d4b\u8bd5\u6570\u636e\u5bf9\u7f51\u7edc\u8fdb\u884c\u6d4b\u8bd5","text":"\u5728\u8bad\u7ec3\u6570\u636e\u96c6\u4e0a\u5df2\u7ecf\u5bf9\u7f51\u7edc\u8fdb\u884c\u4e86\u4e24\u4e2a\u7eaa\u5143(epoch)\u7684\u8bad\u7ec3\u3002 \u4f46\u662f\u6211\u4eec\u8fd8\u8981\u8fdb\u884c\u68c0\u67e5\uff0c\u770b\u770b\u8fd9\u4e2a\u7f51\u7edc\u662f\u5426\u771f\u7684\u5b66\u4f1a\u4e86\u4ec0\u4e48\u3002 \u7528\u8fd9\u4e2a\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u9884\u6d4b\uff0c\u68c0\u67e5\u5b83\u8f93\u51fa\u7684\u5206\u7c7b\u6807\u7b7e\u662f\u5426\u548c\u5b9e\u9645\u503c\u4e00\u76f4\u3002\u5982\u679c\u9884\u6d4b\u6b63\u786e\uff0c\u628a\u6837\u672c\u52a0\u5230\u6b63\u786e\u9884\u6d4b\u5217\u8868\u3002 \u597d\u4e86\uff0c\u7b2c\u4e00\u6b65\u5148\u4ece\u6d4b\u8bd5\u96c6\u663e\u793a\u4e00\u4e2a\u56fe\u50cf\uff0c\u719f\u6089\u4e00\u4e0b\u3002 1 2 3 4 5 6 dataiter = iter ( testloader ) images , labels = dataiter . next () # print images imshow ( torchvision . utils . make_grid ( images )) print ( 'GroundTruth: ' , ' ' . join ( ' %5s ' % classes [ labels [ j ]] for j in range ( 4 ))) 1 GroundTruth: cat ship ship plane \u597d\u7684\uff0c\u73b0\u5728\u8ba9\u6211\u4eec\u770b\u770b\u795e\u7ecf\u7f51\u7edc\u8ba4\u4e3a\u4e0a\u9762\u8fd9\u4e9b\u4f8b\u5b50\u662f\u4ec0\u4e48\uff1a 1 outputs = net ( images ) \u8f93\u51fa\u503c\u662f10\u91cd\u7c7b\u578b\u7684\u80fd\u91cf\u503c\u3002 \u4e00\u4e2a\u7c7b\u578b\u7684\u80fd\u91cf\u503c\u8d8a\u9ad8\uff0c\u5c31\u8868\u660e\u7f51\u8def\u8d8a\u503e\u5411\u4e8e\u8ba4\u4e3a\u56fe\u7247\u662f\u8fd9\u79cd\u7c7b\u578b\u7684\u3002 \u90a3\u4e48\uff0c\u8ba9\u6211\u4eec\u5f97\u5230\u6700\u9ad8\u80fd\u91cf\u503c\u7684\u5e8f\u53f7\uff1a 1 2 3 4 _ , predicted = torch . max ( outputs , 1 ) print ( 'Predicted: ' , ' ' . join ( ' %5s ' % classes [ predicted [ j ]] for j in range ( 4 ))) 1 Predicted: cat ship ship ship \u7ed3\u679c\u4f3c\u4e4e\u8fd8\u884c\u3002 \u8ba9\u6211\u4eec\u770b\u770b\u8fd9\u4e2a\u7f51\u7edc\u5982\u4f55\u9884\u6d4b\u6574\u4e2a\u6d4b\u8bd5\u6570\u636e\u96c6\u3002 1 2 3 4 5 6 7 8 9 10 11 12 correct = 0 total = 0 with torch . no_grad (): for data in tqdm ( testloader ): images , labels = data outputs = net ( images ) _ , predicted = torch . max ( outputs . data , 1 ) total += labels . size ( 0 ) correct += ( predicted == labels ) . sum () . item () print ( 'Accuracy of the network on the 10000 test images: %d %% ' % ( 100 * correct / total )) 1 2 3 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [00:05<00:00, 420.29it/s] Accuracy of the network on the 10000 test images: 53 % \u770b\u8d77\u6765\u6bd4\u968f\u4fbf\u731c(\u4ece10\u4e2a\u7c7b\u578b\u4e2d\u968f\u673a\u9009\u4e00\u4e2a\u7684\u51c6\u786e\u5ea6\u662f10%)\u5f3a\u3002 \u8fd9\u4e2a\u7f51\u7edc\u4f3c\u4e4e\u5b66\u4f1a\u4e86\u4e00\u4e9b\u4e1c\u897f\u3002 \u55ef\uff0c\u54ea\u4e9b\u7c7b\u578b\u8868\u73b0\u597d\uff0c\u54ea\u4e9b\u4e0d\u597d\uff1a 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class_correct = list ( 0. for i in range ( 10 )) class_total = list ( 0. for i in range ( 10 )) with torch . no_grad (): for data in tqdm ( testloader ): images , labels = data outputs = net ( images ) _ , predicted = torch . max ( outputs , 1 ) c = ( predicted == labels ) . squeeze () for i in range ( 4 ): label = labels [ i ] class_correct [ label ] += c [ i ] . item () class_total [ label ] += 1 for i in range ( 10 ): print ( 'Accuracy of %5s : %2d %% ' % ( classes [ i ], 100 * class_correct [ i ] / class_total [ i ])) 1 2 3 4 5 6 7 8 9 10 11 12 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2500/2500 [00:05<00:00, 418.22it/s] Accuracy of plane : 29 % Accuracy of car : 68 % Accuracy of bird : 28 % Accuracy of cat : 42 % Accuracy of deer : 43 % Accuracy of dog : 45 % Accuracy of frog : 70 % Accuracy of horse : 56 % Accuracy of ship : 81 % Accuracy of truck : 73 % \u597d\u7684\uff0c\u63a5\u4e0b\u6765\u5462\uff1f \u6211\u4eec\u5982\u4f55\u5728GPU\u4e0a\u8fd0\u884c\u8fd9\u4e9b\u795e\u7ecf\u7f51\u7edc\uff1f","title":"5. \u4f7f\u7528\u6d4b\u8bd5\u6570\u636e\u5bf9\u7f51\u7edc\u8fdb\u884c\u6d4b\u8bd5"},{"location":"beginner/blitz/cifar10_tutorial/#\u5728gpu\u4e0a\u8bad\u7ec3","text":"\u5c31\u50cf\u5c06Tensor\u8f6c\u5230GPU\u4e00\u6837\uff0c\u53ef\u5c06\u795e\u7ecf\u7f51\u7edc\u8f6c\u5230GPU\u3002 \u5982\u679c\u6709\u53ef\u7528\u7684CUDA\uff0c\u6211\u4eec\u9996\u5148\u83b7\u53d6\u7b2c\u4e00\u4e2a\u53ef\u89c1\u7684cuda\u8bbe\u5907\uff1a 1 2 3 4 5 device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) # Assuming that we are on a CUDA machine, this should print a CUDA device: print ( device ) 1 cuda:0 \u672c\u8282\u7684\u5176\u4f59\u90e8\u5206\u5047\u5b9a device \u662f\u4e00\u4e2aCUDA\u8bbe\u5907\u3002 \u7136\u540e\u8fd9\u4e9b\u65b9\u6cd5\u5c06\u9012\u5f52\u904d\u5386\u6240\u6709\u6a21\u5757\u5e76\u5c06\u5176\u53c2\u6570\u548c\u7f13\u51b2\u533a\u8f6c\u6362\u4e3aCUDA\u5f20\u91cf\uff1a 1 net . to ( device ) \u8bb0\u4f4f\u8fd8\u5f97\u5c06\u6bcf\u4e00\u6b65\u7684\u8f93\u5165\u548c\u76ee\u6807\u53d1\u9001\u5230GPU\uff1a 1 inputs , labels = inputs . to ( device ), labels . to ( device ) \u53ef\u80fd\u89c2\u5bdf\u4e0d\u5230\u4e0eCPU\u76f8\u6bd4\u7684\u6709\u660e\u663e\u52a0\u901f\uff0c\u90a3\u662f\u56e0\u4e3a\u7684\u7f51\u7edc\u592a\u5c0f\u3002","title":"\u5728GPU\u4e0a\u8bad\u7ec3"},{"location":"beginner/blitz/cifar10_tutorial/#\u7ec3\u4e60","text":"\u5c1d\u8bd5\u589e\u52a0\u7f51\u7edc\u7684\u5bbd\u5ea6\uff08\u7b2c\u4e00\u4e2a nn.Conv2d \u7684\u7b2c\u4e8c\u4e2a\u53c2\u6570\uff0c\u7b2c\u4e8c\u4e2a nn.Conv2d \u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u2014\u2014\u5b83\u4eec\u5e94\u662f\u76f8\u540c\u7684\u6570\u503c\uff09\uff0c\u770b\u770b\u80fd\u83b7\u5f97\u4ec0\u4e48\u6837\u7684\u52a0\u901f\u3002 \u8fbe\u5230\u7684\u76ee\u6807\uff1a \u5728\u9ad8\u5c42\u6b21\u4e0a\u7406\u89e3PyTorch\u7684Tensor\u5e93\u548c\u795e\u7ecf\u7f51\u7edc\u3002 \u8bad\u7ec3\u4e00\u4e2a\u5c0f\u578b\u7684\u56fe\u50cf\u5206\u7c7b\u795e\u7ecf\u7f51\u7edc 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 class Net2 ( nn . Module ): def __init__ ( self ): super () . __init__ () self . conv1 = nn . Conv2d ( 3 , 12 , 5 ) self . pool = nn . MaxPool2d ( 2 , 2 ) self . conv2 = nn . Conv2d ( 12 , 16 , 5 ) self . fc1 = nn . Linear ( 16 * 5 * 5 , 120 ) self . fc2 = nn . Linear ( 120 , 84 ) self . fc3 = nn . Linear ( 84 , 10 ) def forward ( self , x ): x = self . pool ( F . relu ( self . conv1 ( x ))) x = self . pool ( F . relu ( self . conv2 ( x ))) x = x . view ( - 1 , 16 * 5 * 5 ) x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x net2 = Net2 () net2 . to ( device ) for epoch in range ( 2 ): # loop over the dataset multiple times ts = time () running_loss = 0.0 for i , data in enumerate ( trainloader , 0 ): # get the inputs inputs , labels = data inputs , labels = inputs . to ( device ), labels . to ( device ) # zero the parameter gradients optimizer . zero_grad () # forward + backward + optimize outputs = net2 ( inputs ) loss = criterion ( outputs , labels ) loss . backward () optimizer . step () # print statistics running_loss += loss . item () if i % 2000 == 1999 : # print every 2000 mini-batches print ( '[ %d , %5d ] loss: %.3f ' % ( epoch + 1 , i + 1 , running_loss / 2000 )) running_loss = 0.0 print ( 'Epoch {0} finished in {1:.3f} second(s)' . format ( epoch , time () - ts )) print ( 'Finished Training' ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 [1, 2000] loss: 2.305 [1, 4000] loss: 2.304 [1, 6000] loss: 2.304 [1, 8000] loss: 2.304 [1, 10000] loss: 2.303 [1, 12000] loss: 2.305 Epoch 0 finished in 72.346 second(s) [2, 2000] loss: 2.304 [2, 4000] loss: 2.304 [2, 6000] loss: 2.304 [2, 8000] loss: 2.303 [2, 10000] loss: 2.304 [2, 12000] loss: 2.305 Epoch 1 finished in 63.276 second(s) Finished Training","title":"\u7ec3\u4e60"},{"location":"beginner/blitz/cifar10_tutorial/#\u5728\u591a\u4e2agpu\u4e0a\u8fdb\u884c\u8bad\u7ec3","text":"\u5982\u679c\u60f3\u4f7f\u7528\u6240\u6709\u7684GPU\u6765\u5927\u5e45\u52a0\u901f\uff0c\u8bf7\u53c2\u8003 data_parallel_tutorial\u3002","title":"\u5728\u591a\u4e2aGPU\u4e0a\u8fdb\u884c\u8bad\u7ec3"},{"location":"beginner/blitz/cifar10_tutorial/#\u4e0b\u4e00\u6b65\u600e\u4e48\u529e","text":":doc: Train neural nets to play video games </intermediate/reinforcement_q_learning> Train a state-of-the-art ResNet network on imagenet _ Train a face generator using Generative Adversarial Networks _ Train a word-level language model using Recurrent LSTM networks _ More examples _ More tutorials _ Discuss PyTorch on the Forums _ Chat with other users on Slack _ 1","title":"\u4e0b\u4e00\u6b65\u600e\u4e48\u529e\uff1f"},{"location":"beginner/blitz/data_parallel_tutorial/","text":"\u53ef\u9009\uff1a\u6570\u636e\u5e76\u884c \u00b6 \u4f5c\u8005 \uff1a Sung Kim \u4e0e Jenny Kang \u5728\u8fd9\u4e2a\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u5b66\u4e60\u5982\u4f55\u901a\u8fc7 DataParallel \u4f7f\u7528\u591aGPU\u3002 \u5728PyTorch\u4e2d\u4f7f\u7528GPU\u975e\u5e38\u7b80\u5355\u3002\u53ef\u4ee5\u628a\u6a21\u578b\u653e\u5230GPU\uff1a 1 2 device = torch . device ( \"cuda:0\" ) model . to ( device ) \u7136\u540e\uff0c\u590d\u5236\u5f20\u91cf\u5230GPU\uff1a 1 mytensor = my_tensor . to ( device ) \u6ce8\u610f\uff1a\u8c03\u7528 my_tensor.to(device) \u8fd4\u56de\u4e86 my_tensor \u7684\u65b0\u62f7\u8d1d\uff0c\u800c\u4e0d\u662f\u6539\u5199 my_tensor \u3002 \u9700\u8981\u5c06\u5b83\u590d\u5236\u5230\u65b0\u7684\u5f20\u91cf\u5bf9\u8c61\uff0c\u800c\u540e\u5728GPU\u4e0a\u4f7f\u7528\u8fd9\u4e2a\u65b0\u5f20\u91cf\u3002 \u5728\u591aGPU\u4e2d\u6267\u884c\u6b63\u5411\u3001\u53cd\u5411\u4f20\u64ad\u5f88\u7b80\u5355\u3002\u4e0d\u8fc7\uff0cPyTorch\u9ed8\u8ba4\u53ea\u7528\u4e00\u4e2aGPU\u3002 \u4f7f\u7528 DataParallel \u53ef\u4ee5\u7b80\u5355\u5730\u8ba9\u6a21\u578b\u5728\u591a\u4e2aGPU\u4e0a\u8fdb\u884c\u8fd0\u7b97\uff1a 1 model = nn . DataParallel ( model ) \u8fd9\u662f\u672c\u6559\u7a0b\u80cc\u540e\u7684\u6838\u5fc3\u3002\u6211\u4eec\u5c06\u5728\u4e0b\u9762\u66f4\u8be6\u7ec6\u5730\u63a2\u8ba8\u5b83\u3002 \u5bfc\u5165\u548c\u53c2\u6570 \u00b6 \u5bfc\u5165PyTorch\u6a21\u5757\u5e76\u5b9a\u4e49\u53c2\u6570\u3002 1 2 3 4 5 6 7 8 9 10 11 12 from tqdm import tqdm import torch import torch.nn as nn from torch.utils.data import Dataset , DataLoader # Parameters and DataLoaders input_size = 5 output_size = 2 batch_size = 30 data_size = 100 \u8bbe\u5907 1 device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) \u5047\u6570\u636e\u96c6 \u00b6 \u5236\u4f5c\u4e00\u4e2a\u5047\uff08\u968f\u673a\uff09\u6570\u636e\u96c6\u3002 \u53ea\u9700\u8981\u5b9e\u73b0getitem 1 2 3 4 5 6 7 8 9 10 11 12 13 14 class RandomDataset ( Dataset ): def __init__ ( self , size , length ): self . len = length self . data = torch . randn ( length , size ) def __getitem__ ( self , index ): return self . data [ index ] def __len__ ( self ): return self . len rand_loader = DataLoader ( dataset = RandomDataset ( input_size , data_size ), batch_size = batch_size , shuffle = True ) \u7b80\u5355\u6a21\u578b \u00b6 \u5728\u8fd9\u4e2a\u6f14\u793a\u4e2d\uff0c\u6211\u4eec\u7684\u6a21\u578b\u53ea\u8981\u8f93\u5165\uff0c\u6267\u884c\u4e00\u6b21\u73b0\u884c\u8fd0\u7b97\uff0c\u5e76\u7ed9\u51fa\u8f93\u51fa\u3002 \u53e6\u5916\uff0c DataParallel \u5728\u4efb\u4f55\u6a21\u578b(CNN\uff0cRNN\uff0cCapsule Net \u7b49)\u4e0a\u90fd\u53ef\u7528 \u6211\u4eec\u5728\u6a21\u578b\u4e2d\u653e\u7f6e\u4e86\u4e00\u4e2aprint\u8bed\u53e5\u6765\u76d1\u89c6\u8f93\u5165\u548c\u8f93\u51fa\u5f20\u91cf\u7684\u5927\u5c0f\u3002 \u8bf7\u6ce8\u610f\u6279\u6b210\u4e2d\u6253\u5370\u7684\u5185\u5bb9\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 class Model ( nn . Module ): # Our model def __init__ ( self , input_size , output_size ): super ( Model , self ) . __init__ () self . fc = nn . Linear ( input_size , output_size ) def forward ( self , input ): output = self . fc ( input ) print ( \" \\t In Model: input size\" , input . size (), \"output size\" , output . size ()) return output \u521b\u5efa\u6a21\u578b\u548c\u6570\u636e\u5e76\u53d1 \u00b6 \u8fd9\u662f\u672c\u6559\u7a0b\u7684\u6838\u5fc3\u90e8\u5206\u3002 \u9996\u5148\uff0c\u6211\u4eec\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u6a21\u578b\u5b9e\u4f8b\u5e76\u68c0\u67e5\u6211\u4eec\u662f\u5426\u6709\u591a\u4e2aGPU\u3002\u5982\u679c\u6211\u4eec\u6709\u591a\u4e2aGPU\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528 nn.DataParallel \u5305\u88c5\u6211\u4eec\u7684\u6a21\u578b\u3002 \u7136\u540e\u6211\u4eec\u53ef\u4ee5\u7528 model.to(device) \u5c06\u6211\u4eec\u7684\u6a21\u578b\u653e\u5728GPU\u4e0a 1 2 3 4 5 6 7 model = Model ( input_size , output_size ) if torch . cuda . device_count () > 1 : print ( \"Let's use\" , torch . cuda . device_count (), \"GPUs!\" ) # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs model = nn . DataParallel ( model ) model . to ( device ) 1 2 3 4 5 6 7 8 9 10 11 Let's use 3 GPUs! DataParallel( (module): Model( (fc): Linear(in_features=5, out_features=2, bias=True) ) ) \u8fd0\u884c\u6a21\u578b \u00b6 \u73b0\u5728\u53ef\u4ee5\u770b\u770b\u8f93\u5165\u548c\u8f93\u51fa\u5f20\u91cf\u7684\u5927\u5c0f\u3002 1 2 3 4 5 for data in tqdm ( rand_loader ): input = data . to ( device ) output = model ( input ) print ( \"Outside: input size\" , input . size (), \"output_size\" , output . size ()) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:07<00:00, 1.93s/it] In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2]) \u7ed3\u679c \u00b6 \u5982\u679c\u6ca1\u6709GPU\u6216\u53ea\u6709\u4e00\u4e2aGPU\uff0c\u5f53\u6211\u4eec\u6279\u6b21\u6267\u884c30\u4e2a\u8f93\u5165\u548c30\u4e2a\u8f93\u51fa\u7684\u65f6\u5019\uff0c\u6a21\u578b\u5f97\u5230\u4e8630\u4e2a\u8f93\u5165\u5e76\u671f\u5f8530\u4e2a\u8f93\u51fa\u3002 \u5982\u679c\u6709\u591a\u4e2aGPU\uff0c\u5c31\u53ef\u4ee5\u5f97\u5230\u50cf\u8fd9\u6837\u7684\u7ed3\u679c\u3002 2 GPU \u00b6 \u5982\u679c\u67092\u4e2aGPU\uff0c\u5c31\u4f1a\u770b\u5230\uff1a 1 2 3 4 5 6 7 8 9 10 11 12 13 Let's use 2 GPUs! In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2]) In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2]) 3 GPU \u00b6 \u5982\u679c\u67093\u4e2aGPU\uff0c\u5c31\u4f1a\u770b\u5230\uff1a 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Let's use 3 GPUs! In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2]) 8 GPU \u00b6 \u5982\u679c\u67098\u4e2aGPU\uff0c\u5c31\u4f1a\u770b\u5230 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 Let's use 8 GPUs! In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2]) \u603b\u7ed3 \u00b6 DataParallel\u81ea\u52a8\u62c6\u5206\u6570\u636e\u5e76\u5c06\u4f5c\u4e1a\u53d1\u9001\u5230\u591a\u4e2aGPU\u4e0a\u7684\u591a\u4e2a\u6a21\u578b\u3002 \u5728\u6bcf\u4e2a\u6a21\u578b\u5b8c\u6210\u5176\u5de5\u4f5c\u540e\uff0cDataParallel\u5c31\u4f1a\u5728\u51fd\u6570\u8fd4\u56de\u4e4b\u524d\u6536\u96c6\u5e76\u4e14\u5408\u5e76\u7ed3\u679c\u3002 \u6709\u5173\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u67e5\u770b https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html \u3002","title":"\u6570\u636e\u5e76\u884c"},{"location":"beginner/blitz/data_parallel_tutorial/#\u53ef\u9009\u6570\u636e\u5e76\u884c","text":"\u4f5c\u8005 \uff1a Sung Kim \u4e0e Jenny Kang \u5728\u8fd9\u4e2a\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u5b66\u4e60\u5982\u4f55\u901a\u8fc7 DataParallel \u4f7f\u7528\u591aGPU\u3002 \u5728PyTorch\u4e2d\u4f7f\u7528GPU\u975e\u5e38\u7b80\u5355\u3002\u53ef\u4ee5\u628a\u6a21\u578b\u653e\u5230GPU\uff1a 1 2 device = torch . device ( \"cuda:0\" ) model . to ( device ) \u7136\u540e\uff0c\u590d\u5236\u5f20\u91cf\u5230GPU\uff1a 1 mytensor = my_tensor . to ( device ) \u6ce8\u610f\uff1a\u8c03\u7528 my_tensor.to(device) \u8fd4\u56de\u4e86 my_tensor \u7684\u65b0\u62f7\u8d1d\uff0c\u800c\u4e0d\u662f\u6539\u5199 my_tensor \u3002 \u9700\u8981\u5c06\u5b83\u590d\u5236\u5230\u65b0\u7684\u5f20\u91cf\u5bf9\u8c61\uff0c\u800c\u540e\u5728GPU\u4e0a\u4f7f\u7528\u8fd9\u4e2a\u65b0\u5f20\u91cf\u3002 \u5728\u591aGPU\u4e2d\u6267\u884c\u6b63\u5411\u3001\u53cd\u5411\u4f20\u64ad\u5f88\u7b80\u5355\u3002\u4e0d\u8fc7\uff0cPyTorch\u9ed8\u8ba4\u53ea\u7528\u4e00\u4e2aGPU\u3002 \u4f7f\u7528 DataParallel \u53ef\u4ee5\u7b80\u5355\u5730\u8ba9\u6a21\u578b\u5728\u591a\u4e2aGPU\u4e0a\u8fdb\u884c\u8fd0\u7b97\uff1a 1 model = nn . DataParallel ( model ) \u8fd9\u662f\u672c\u6559\u7a0b\u80cc\u540e\u7684\u6838\u5fc3\u3002\u6211\u4eec\u5c06\u5728\u4e0b\u9762\u66f4\u8be6\u7ec6\u5730\u63a2\u8ba8\u5b83\u3002","title":"\u53ef\u9009\uff1a\u6570\u636e\u5e76\u884c"},{"location":"beginner/blitz/data_parallel_tutorial/#\u5bfc\u5165\u548c\u53c2\u6570","text":"\u5bfc\u5165PyTorch\u6a21\u5757\u5e76\u5b9a\u4e49\u53c2\u6570\u3002 1 2 3 4 5 6 7 8 9 10 11 12 from tqdm import tqdm import torch import torch.nn as nn from torch.utils.data import Dataset , DataLoader # Parameters and DataLoaders input_size = 5 output_size = 2 batch_size = 30 data_size = 100 \u8bbe\u5907 1 device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" )","title":"\u5bfc\u5165\u548c\u53c2\u6570"},{"location":"beginner/blitz/data_parallel_tutorial/#\u5047\u6570\u636e\u96c6","text":"\u5236\u4f5c\u4e00\u4e2a\u5047\uff08\u968f\u673a\uff09\u6570\u636e\u96c6\u3002 \u53ea\u9700\u8981\u5b9e\u73b0getitem 1 2 3 4 5 6 7 8 9 10 11 12 13 14 class RandomDataset ( Dataset ): def __init__ ( self , size , length ): self . len = length self . data = torch . randn ( length , size ) def __getitem__ ( self , index ): return self . data [ index ] def __len__ ( self ): return self . len rand_loader = DataLoader ( dataset = RandomDataset ( input_size , data_size ), batch_size = batch_size , shuffle = True )","title":"\u5047\u6570\u636e\u96c6"},{"location":"beginner/blitz/data_parallel_tutorial/#\u7b80\u5355\u6a21\u578b","text":"\u5728\u8fd9\u4e2a\u6f14\u793a\u4e2d\uff0c\u6211\u4eec\u7684\u6a21\u578b\u53ea\u8981\u8f93\u5165\uff0c\u6267\u884c\u4e00\u6b21\u73b0\u884c\u8fd0\u7b97\uff0c\u5e76\u7ed9\u51fa\u8f93\u51fa\u3002 \u53e6\u5916\uff0c DataParallel \u5728\u4efb\u4f55\u6a21\u578b(CNN\uff0cRNN\uff0cCapsule Net \u7b49)\u4e0a\u90fd\u53ef\u7528 \u6211\u4eec\u5728\u6a21\u578b\u4e2d\u653e\u7f6e\u4e86\u4e00\u4e2aprint\u8bed\u53e5\u6765\u76d1\u89c6\u8f93\u5165\u548c\u8f93\u51fa\u5f20\u91cf\u7684\u5927\u5c0f\u3002 \u8bf7\u6ce8\u610f\u6279\u6b210\u4e2d\u6253\u5370\u7684\u5185\u5bb9\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 class Model ( nn . Module ): # Our model def __init__ ( self , input_size , output_size ): super ( Model , self ) . __init__ () self . fc = nn . Linear ( input_size , output_size ) def forward ( self , input ): output = self . fc ( input ) print ( \" \\t In Model: input size\" , input . size (), \"output size\" , output . size ()) return output","title":"\u7b80\u5355\u6a21\u578b"},{"location":"beginner/blitz/data_parallel_tutorial/#\u521b\u5efa\u6a21\u578b\u548c\u6570\u636e\u5e76\u53d1","text":"\u8fd9\u662f\u672c\u6559\u7a0b\u7684\u6838\u5fc3\u90e8\u5206\u3002 \u9996\u5148\uff0c\u6211\u4eec\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u6a21\u578b\u5b9e\u4f8b\u5e76\u68c0\u67e5\u6211\u4eec\u662f\u5426\u6709\u591a\u4e2aGPU\u3002\u5982\u679c\u6211\u4eec\u6709\u591a\u4e2aGPU\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528 nn.DataParallel \u5305\u88c5\u6211\u4eec\u7684\u6a21\u578b\u3002 \u7136\u540e\u6211\u4eec\u53ef\u4ee5\u7528 model.to(device) \u5c06\u6211\u4eec\u7684\u6a21\u578b\u653e\u5728GPU\u4e0a 1 2 3 4 5 6 7 model = Model ( input_size , output_size ) if torch . cuda . device_count () > 1 : print ( \"Let's use\" , torch . cuda . device_count (), \"GPUs!\" ) # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs model = nn . DataParallel ( model ) model . to ( device ) 1 2 3 4 5 6 7 8 9 10 11 Let's use 3 GPUs! DataParallel( (module): Model( (fc): Linear(in_features=5, out_features=2, bias=True) ) )","title":"\u521b\u5efa\u6a21\u578b\u548c\u6570\u636e\u5e76\u53d1"},{"location":"beginner/blitz/data_parallel_tutorial/#\u8fd0\u884c\u6a21\u578b","text":"\u73b0\u5728\u53ef\u4ee5\u770b\u770b\u8f93\u5165\u548c\u8f93\u51fa\u5f20\u91cf\u7684\u5927\u5c0f\u3002 1 2 3 4 5 for data in tqdm ( rand_loader ): input = data . to ( device ) output = model ( input ) print ( \"Outside: input size\" , input . size (), \"output_size\" , output . size ()) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:07<00:00, 1.93s/it] In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])","title":"\u8fd0\u884c\u6a21\u578b"},{"location":"beginner/blitz/data_parallel_tutorial/#\u7ed3\u679c","text":"\u5982\u679c\u6ca1\u6709GPU\u6216\u53ea\u6709\u4e00\u4e2aGPU\uff0c\u5f53\u6211\u4eec\u6279\u6b21\u6267\u884c30\u4e2a\u8f93\u5165\u548c30\u4e2a\u8f93\u51fa\u7684\u65f6\u5019\uff0c\u6a21\u578b\u5f97\u5230\u4e8630\u4e2a\u8f93\u5165\u5e76\u671f\u5f8530\u4e2a\u8f93\u51fa\u3002 \u5982\u679c\u6709\u591a\u4e2aGPU\uff0c\u5c31\u53ef\u4ee5\u5f97\u5230\u50cf\u8fd9\u6837\u7684\u7ed3\u679c\u3002","title":"\u7ed3\u679c"},{"location":"beginner/blitz/data_parallel_tutorial/#2-gpu","text":"\u5982\u679c\u67092\u4e2aGPU\uff0c\u5c31\u4f1a\u770b\u5230\uff1a 1 2 3 4 5 6 7 8 9 10 11 12 13 Let's use 2 GPUs! In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2]) In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])","title":"2 GPU"},{"location":"beginner/blitz/data_parallel_tutorial/#3-gpu","text":"\u5982\u679c\u67093\u4e2aGPU\uff0c\u5c31\u4f1a\u770b\u5230\uff1a 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Let's use 3 GPUs! In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])","title":"3 GPU"},{"location":"beginner/blitz/data_parallel_tutorial/#8-gpu","text":"\u5982\u679c\u67098\u4e2aGPU\uff0c\u5c31\u4f1a\u770b\u5230 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 Let's use 8 GPUs! In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])","title":"8 GPU"},{"location":"beginner/blitz/data_parallel_tutorial/#\u603b\u7ed3","text":"DataParallel\u81ea\u52a8\u62c6\u5206\u6570\u636e\u5e76\u5c06\u4f5c\u4e1a\u53d1\u9001\u5230\u591a\u4e2aGPU\u4e0a\u7684\u591a\u4e2a\u6a21\u578b\u3002 \u5728\u6bcf\u4e2a\u6a21\u578b\u5b8c\u6210\u5176\u5de5\u4f5c\u540e\uff0cDataParallel\u5c31\u4f1a\u5728\u51fd\u6570\u8fd4\u56de\u4e4b\u524d\u6536\u96c6\u5e76\u4e14\u5408\u5e76\u7ed3\u679c\u3002 \u6709\u5173\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u67e5\u770b https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html \u3002","title":"\u603b\u7ed3"},{"location":"beginner/blitz/neural_networks_tutorial/","text":"\u795e\u7ecf\u7f51\u7edc(Neural Networks) \u00b6 \u53ef\u4ee5\u4f7f\u7528 torch.nn \u5305\u6784\u9020\u795e\u7ecf\u7f51\u7edc\u3002 \u73b0\u5728\u6211\u4eec\u5df2\u7ecf\u521d\u6b65\u4e86\u89e3\u4e86 autograd \uff0c nn \u4f9d\u9760 autograd \u5b9a\u4e49\u6a21\u578b\u4ee5\u53ca\u6c42\u5fae\u5206\u3002 \u4e00\u4e2a nn.Module \u5305\u542b\u591a\u4e2a\u5c42\uff0c\u4e00\u4e2a\u8fd4\u56de output \u7684 forward(input) \u65b9\u6cd5\u3002 \u4f8b\u5982\uff0c\u8fd9\u4e2a\u6570\u5b57\u56fe\u50cf\u5206\u7c7b\u7684\u7f51\u7edc\u56fe\uff1a convnet \u5b83\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u524d\u9988\u7f51\u7edc\u3002\u5b83\u63a5\u53d7\u8f93\u5165\uff0c\u4e00\u4e2a\u63a5\u4e00\u4e2a\u5730\u901a\u8fc7\u51e0\u4e2a\u5c42\u8f93\u5165\uff0c\u7136\u540e\u6700\u7ec8\u7ed9\u51fa\u8f93\u51fa\u3002 \u795e\u7ecf\u7f51\u7edc\u7684\u5178\u578b\u8bad\u7ec3\u7a0b\u5e8f\u5982\u4e0b\uff1a \u5b9a\u4e49\u5177\u6709\u4e00\u4e9b\u53ef\u5b66\u4e60\u53c2\u6570\uff08\u6216\u6743\u91cd\uff09\u7684\u795e\u7ecf\u7f51\u7edc \u8fed\u4ee3\u8f93\u5165\u6570\u636e\u96c6 \u901a\u8fc7\u7f51\u7edc\u5904\u7406\u8f93\u5165 \u8ba1\u7b97\u635f\u5931\uff08\u8f93\u51fa\u8ddd\u79bb\u6b63\u786e\u591a\u8fdc\uff09 \u5c06\u6e10\u53d8\u4f20\u64ad\u56de\u7f51\u7edc\u53c2\u6570 \u66f4\u65b0\u7f51\u7edc\u6743\u91cd\uff0c\u901a\u5e38\u4f7f\u7528\u7b80\u5355\u7684\u66f4\u65b0\u89c4\u5219\uff1a weight = weight - learning_rate * gradient \u5b9a\u4e49\u7f51\u7edc \u00b6 \u6211\u4eec\u6765\u5b9a\u4e49\u7f51\u7edc: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 import torch import torch.nn as nn import torch.nn.functional as F class Net ( nn . Module ): def __init__ ( self ): super ( Net , self ) . __init__ () # 1 input image channel, 6 output channels, 5x5 square convolution # kernel self . conv1 = nn . Conv2d ( 1 , 6 , 5 ) self . conv2 = nn . Conv2d ( 6 , 16 , 5 ) # an affine operation: y = Wx + b self . fc1 = nn . Linear ( 16 * 5 * 5 , 120 ) self . fc2 = nn . Linear ( 120 , 84 ) self . fc3 = nn . Linear ( 84 , 10 ) def forward ( self , x ): # Max pooling over a (2, 2) window x = F . max_pool2d ( F . relu ( self . conv1 ( x )), ( 2 , 2 )) # If the size is a square you can only specify a single number x = F . max_pool2d ( F . relu ( self . conv2 ( x )), 2 ) x = x . view ( - 1 , self . num_flat_features ( x )) x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x def num_flat_features ( self , x ): size = x . size ()[ 1 :] # all dimensions except the batch dimension num_features = 1 for s in size : num_features *= s return num_features net = Net () print ( net ) 1 2 3 4 5 6 7 Net( (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)) (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)) (fc1): Linear(in_features=400, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=10, bias=True) ) \u6211\u4eec\u53ea\u9700\u5b9a\u4e49 forward \u51fd\u6570\uff0c backward \u51fd\u6570\uff08\u68af\u5ea6\u5728\u8fd9\u91cc\u88ab\u8ba1\u7b97\uff09\u7531 autograd \u81ea\u52a8\u751f\u6210\u3002 \u5728 forward \u51fd\u6570\u4e2d\u53ef\u4ee5\u4f7f\u7528\u4efb\u4f55\u4e00\u79cd\u5f20\u91cf\u8fd0\u7b97\u3002 \u6a21\u578b\u7684\u53ef\u5b66\u4e60\u53c2\u6570\u7531 net.parameters() \u8fd4\u56de 1 2 3 params = list ( net . parameters ()) print ( len ( params )) print ( params [ 0 ] . size ()) # conv1's .weight 1 2 10 torch.Size([6, 1, 5, 5]) \u8ba9\u6211\u4eec\u5c1d\u8bd5\u4e00\u4e2a\u968f\u673a\u768432x32\u8f93\u5165\u3002\u6ce8\u610f\uff1a\u6b64\u7f51\u7edc\uff08LeNet\uff09\u7684\u9884\u671f\u8f93\u5165\u5927\u5c0f\u4e3a32x32\u3002\u8981\u5728MNIST\u6570\u636e\u96c6\u4e0a\u4f7f\u7528\u6b64\u7f51\u7edc\uff0c\u8bf7\u5c06\u6570\u636e\u96c6\u4e2d\u7684\u56fe\u50cf\u8c03\u6574\u4e3a32x32\u3002 1 2 3 input = torch . randn ( 1 , 1 , 32 , 32 ) out = net ( input ) print ( out ) 1 2 tensor([[ 0.0108, -0.0120, -0.0415, -0.0964, 0.0957, 0.0217, -0.1095, 0.0581, 0.0173, -0.0540]], grad_fn=<AddmmBackward>) \u5c06\u6240\u6709\u53c2\u6570\u548c\u5e26\u6709\u968f\u673a\u68af\u5ea6\u7684\u53cd\u5411\u4f20\u64ad\u7684\u68af\u5ea6\u7f13\u51b2\u533a\u5f52\u96f6\uff1a 1 2 net . zero_grad () out . backward ( torch . randn ( 1 , 10 )) Note\uff1a torch.nn \u4ec5\u652f\u6301\u5c0f\u6279\u6b21\u3002\u6574\u4e2a torch.nn \u5305\u4ec5\u652f\u6301\u5c0f\u6279\u6b21\u7684\u6837\u672c\uff0c\u800c\u4e0d\u662f\u5355\u4e2a\u6837\u672c\u3002 \u4f8b\u5982\uff0c nn.Conv2d \u91c7\u7528 nSamples x nChannels x Height x Width 4\u7ef4\u5f20\u91cf\u3002 \u5982\u679c\u662f\u5355\u4e2a\u6837\u672c\uff0c\u8981\u7528 input.unsqueeze(0) \u628a\u5b83\u52a0\u5230\u4e00\u4e2a\u5047\u7684\u6279\u6b21\u7ef4\u5ea6\u3002 \u5728\u7ee7\u7eed\u4e4b\u524d\uff0c\u8ba9\u6211\u4eec\u56de\u987e\u4e00\u4e0b\u5230\u76ee\u524d\u4e3a\u6b62\u770b\u5230\u7684\u6240\u6709\u8bfe\u7a0b\u3002 \u6982\u62ec\uff1a - torch.Tensor - \u5b83\u662f\u652f\u6301\u50cf backward() \u8fd9\u79cdautograd\u8fd0\u7b97\u7684 \u591a\u7ef4\u6570\u7ec4 \uff0c\u8fd8\u80fd \u4fdd\u5b58 \u5f20\u91cf\u7684 \u68af\u5ea6 \u3002 - nn.Module - \u795e\u7ecf\u7f51\u7edc\u6a21\u5757\u3002\u63d0\u4f9b \u65b9\u4fbf\u7684\u53c2\u6570\u5c01\u88c5\u65b9\u5f0f \uff0c\u79fb\u81f3GPU\u3001\u5bfc\u51fa\u3001\u52a0\u8f7d\u7b49\u8f85\u52a9\u529f\u80fd\u3002 - nn.Parameter - \u4e00\u79cd\u5f20\u91cf\uff0c \u5f53\u8d4b\u503c\u7ed9 Module \u5bf9\u8c61\u7684\u5c5e\u6027\u65f6\uff0c\u5b83\u4f5c\u4e3a\u53c2\u6570\u88ab\u81ea\u52a8\u6ce8\u518c \u3002 - autograd.Function - \u5b9e\u73b0 autograde\u8fd0\u7b97\u7684 forward() \u548c backward() \u5b9a\u4e49 \u3002 \u6bcf\u6b21 Tensor \u8fd0\u7b97\u81f3\u5c11\u521b\u5efa\u4e00\u4e2a Function \u8282\u70b9\uff0c\u8be5\u8282\u70b9\u8fde\u63a5\u5230\u521b\u5efa Tensor \u7684 Function \u5bf9\u8c61\uff0c\u5e76 \u7f16\u7801\u5176\u5386\u53f2 \u3002 \u5728\u8fd9\u4e00\u70b9\u4e0a\uff0c\u6211\u4eec\u6db5\u76d6\u4e86\uff1a - \u5b9a\u4e49\u5ba1\u673a\u6784\u7f51\u7edcDefining a neural network - \u5904\u7406\u8f93\u5165\u8c03\u7528backward \u8fd8\u5269\u4e0b\uff1a - \u8ba1\u7b97\u635f\u5931 - \u66f4\u65b0\u7f51\u7edc\u6743\u91cd \u635f\u5931\u51fd\u6570 \u00b6 \u635f\u5931\u51fd\u6570\u91c7\u7528 output \uff0c target \u8f93\u5165\u5bf9\uff0c\u8ba1\u7b97\u8f93\u51fa\u4e0e\u76ee\u6807\u7684\u8ddd\u79bb\u4f30\u7b97\u503c\u3002 nn \u5305\u4e0b\u6709\u591a\u79cd\u4e0d\u540c\u7684 \u635f\u5931\u51fd\u6570 \u3002 \u6bd4\u5982 nn.MSELoss \u5c31\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u635f\u5931\u51fd\u6570\uff0c\u5b83\u8ba1\u7b97\u8f93\u5165\u548c\u76ee\u6807\u4e4b\u95f4\u7684\u5747\u65b9\u8bef\u5dee\u3002 \u4f8b\u5982\uff1a 1 2 3 4 5 6 7 output = net ( input ) target = torch . randn ( 10 ) # a dummy target, for example target = target . view ( 1 , - 1 ) # make it the same shape as output criterion = nn . MSELoss () loss = criterion ( output , target ) print ( loss ) 1 tensor(1.4750, grad_fn=<MseLossBackward>) \u73b0\u5728\uff0c\u5982\u679c\u6309 loss \u7684\u53cd\u65b9\u5411\uff0c\u4f7f\u7528 .grad_fn \u5c5e\u6027\uff0c\u5c31\u53ef\u770b\u5230\u8fd9\u6837\u7684\u8ba1\u7b97\u56fe\uff1a 1 2 3 4 input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d -> view -> linear -> relu -> linear -> relu -> linear -> MSELoss -> loss \u6240\u4ee5\uff0c\u5f53\u6211\u4eec\u8c03\u7528 loss.backward() \uff0c\u5c31\u4f1a\u6c42\u6574\u4e2a\u56fe\u5173\u4e8e\u635f\u5931\u7684\u5fae\u5206\uff0c\u56fe\u4e2d\u6240\u6709\u5177\u6709 requires_grad=True \u7684 Tensor \u5bf9\u8c61\u7684 .grad \u5f20\u91cf\u5c5e\u6027\u90fd\u4f7f\u7528\u68af\u5ea6\u7d2f\u52a0\u3002 \u4e3a\u4e86\u8bf4\u660e\u8fd9\u4e00\u70b9\uff0c\u6211\u4eec\u8fdb\u884c\u51e0\u6b65\u53cd\u5411\uff1a 1 2 3 print ( loss . grad_fn ) # MSELoss print ( loss . grad_fn . next_functions [ 0 ][ 0 ]) # Linear print ( loss . grad_fn . next_functions [ 0 ][ 0 ] . next_functions [ 0 ][ 0 ]) # ReLU 1 2 3 <MseLossBackward object at 0x7fa1140c1fd0> <AddmmBackward object at 0x7fa1140c6080> <AccumulateGrad object at 0x7fa1140c1fd0> \u53cd\u5411\u4f20\u64ad \u00b6 \u8981\u53cd\u5411\u4f20\u64ad\u8bef\u5dee\uff0c\u6211\u4eec\u9501\u8981\u505a\u7684\u5c31\u662f loss.backward() \u3002 \u4e0d\u8fc7\u6211\u4eec\u9700\u8981\u6e05\u9664\u5df2\u6709\u7684\u68af\u5ea6\uff0c\u5426\u5219\u68af\u5ea6\u5c06\u88ab\u7d2f\u79ef\u5230\u5df2\u6709\u7684\u68af\u5ea6\u4e0a\u3002 \u73b0\u5728\u53ef\u4ee5\u8c03\u7528 loss.backward() \uff0c\u770b\u770b conv1 \u5728\u8c03\u7528\u4e4b\u524d\u548c\u4e4b\u540e\u548c\u504f\u5dee\u68af\u5ea6\u3002 1 2 3 4 5 6 7 8 9 net . zero_grad () # zeroes the gradient buffers of all parameters print ( 'conv1.bias.grad before backward' ) print ( net . conv1 . bias . grad ) loss . backward () print ( 'conv1.bias.grad after backward' ) print ( net . conv1 . bias . grad ) 1 2 3 4 conv1.bias.grad before backward tensor([0., 0., 0., 0., 0., 0.]) conv1.bias.grad after backward tensor([-0.0245, -0.0181, 0.0134, 0.0136, 0.0242, 0.0009]) \u5230\u76ee\u524d\u4f4d\u7f6e, \u6211\u4eec\u5df2\u7ecf\u770b\u5230\u4e86\u5982\u4f55\u4f7f\u7528\u635f\u5931\u51fd\u6570\u3002 \u5ef6\u540e\u9605\u8bfb\uff1a \u795e\u7ecf\u7f51\u7edc\u5305\u5177\u6709\u7531\u4e8e\u6784\u5efa\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u5404\u79cd\u6a21\u5757\u548c\u635f\u5931\u51fd\u6570\u3002\u5e26\u6709\u6587\u6863\u7684\u5b8c\u6574\u5217\u8868\u5728 https://pytorch.org/docs/nn \u8fd8\u5269\u4e0b\u4e00\u4e2a\u8981\u5b66\u4e60\u7684\u662f\uff1a \u66f4\u65b0\u7f51\u7edc\u7684\u6743\u91cd \u66f4\u65b0\u6743\u91cd \u00b6 \u968f\u673a\u68af\u5ea6\u4e0b\u964d\u662f(SGD)\u662f\u6700\u7b80\u5355\u7684\u66f4\u65b0\u89c4\u5219\uff1a 1 weight = weight - learning_rate * gradient \u53ef\u4ee5\u7528\u7b80\u5355\u7684Python\u4ee3\u7801\u5b9e\u73b0\u5b83\uff1a 1 2 3 learning_rate = 0.01 for f in net . parameters (): f . data . sub_ ( f . grad . data * learning_rate ) \u4e0d\u8fc7\uff0c\u5f53\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u65f6\uff0c\u8fd8\u9700\u8981\u4f7f\u7528\u5404\u79cd\u4e0d\u540c\u7684\u66f4\u65b0\u89c4\u5219\uff0c\u4f8b\u5982SGD\uff0cNesterov-SGD\uff0cAdam\uff0cRMSProp\u7b49\u3002 \u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u5c0f\u5305\uff1a torch.optim \uff0c\u5b83\u5b9e\u73b0\u4e86\u6240\u6709\u8fd9\u4e9b\u65b9\u6cd5\u3002\u4f7f\u7528\u5b83\u975e\u5e38\u7b80\u5355\uff1a 1 2 3 4 5 6 7 8 9 10 11 import torch.optim as optim # create your optimizer optimizer = optim . SGD ( net . parameters (), lr = 0.01 ) # in your training loop: optimizer . zero_grad () # zero the gradient buffers output = net ( input ) loss = criterion ( output , target ) loss . backward () optimizer . step () # Does the update Note\uff1a \u89c2\u5bdf\u5982\u4f55\u4f7f\u7528 optimizer.zero_grad() \u624b\u52a8\u5c06\u68af\u5ea6\u7f13\u51b2\u533a\u8bbe\u7f6e\u4e3a\u96f6\u3002\u8fd9\u662f\u56e0\u4e3a\u68af\u5ea6\u7d2f\u79ef\u7684\uff0c\u53c2\u89c1 \u53cd\u5411\u4f20\u64ad \u90e8\u5206\u4e2d\u7684\u8bf4\u660e\u3002","title":"\u795e\u7ecf\u7f51\u7edc"},{"location":"beginner/blitz/neural_networks_tutorial/#\u795e\u7ecf\u7f51\u7edcneural-networks","text":"\u53ef\u4ee5\u4f7f\u7528 torch.nn \u5305\u6784\u9020\u795e\u7ecf\u7f51\u7edc\u3002 \u73b0\u5728\u6211\u4eec\u5df2\u7ecf\u521d\u6b65\u4e86\u89e3\u4e86 autograd \uff0c nn \u4f9d\u9760 autograd \u5b9a\u4e49\u6a21\u578b\u4ee5\u53ca\u6c42\u5fae\u5206\u3002 \u4e00\u4e2a nn.Module \u5305\u542b\u591a\u4e2a\u5c42\uff0c\u4e00\u4e2a\u8fd4\u56de output \u7684 forward(input) \u65b9\u6cd5\u3002 \u4f8b\u5982\uff0c\u8fd9\u4e2a\u6570\u5b57\u56fe\u50cf\u5206\u7c7b\u7684\u7f51\u7edc\u56fe\uff1a convnet \u5b83\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u524d\u9988\u7f51\u7edc\u3002\u5b83\u63a5\u53d7\u8f93\u5165\uff0c\u4e00\u4e2a\u63a5\u4e00\u4e2a\u5730\u901a\u8fc7\u51e0\u4e2a\u5c42\u8f93\u5165\uff0c\u7136\u540e\u6700\u7ec8\u7ed9\u51fa\u8f93\u51fa\u3002 \u795e\u7ecf\u7f51\u7edc\u7684\u5178\u578b\u8bad\u7ec3\u7a0b\u5e8f\u5982\u4e0b\uff1a \u5b9a\u4e49\u5177\u6709\u4e00\u4e9b\u53ef\u5b66\u4e60\u53c2\u6570\uff08\u6216\u6743\u91cd\uff09\u7684\u795e\u7ecf\u7f51\u7edc \u8fed\u4ee3\u8f93\u5165\u6570\u636e\u96c6 \u901a\u8fc7\u7f51\u7edc\u5904\u7406\u8f93\u5165 \u8ba1\u7b97\u635f\u5931\uff08\u8f93\u51fa\u8ddd\u79bb\u6b63\u786e\u591a\u8fdc\uff09 \u5c06\u6e10\u53d8\u4f20\u64ad\u56de\u7f51\u7edc\u53c2\u6570 \u66f4\u65b0\u7f51\u7edc\u6743\u91cd\uff0c\u901a\u5e38\u4f7f\u7528\u7b80\u5355\u7684\u66f4\u65b0\u89c4\u5219\uff1a weight = weight - learning_rate * gradient","title":"\u795e\u7ecf\u7f51\u7edc(Neural Networks)"},{"location":"beginner/blitz/neural_networks_tutorial/#\u5b9a\u4e49\u7f51\u7edc","text":"\u6211\u4eec\u6765\u5b9a\u4e49\u7f51\u7edc: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 import torch import torch.nn as nn import torch.nn.functional as F class Net ( nn . Module ): def __init__ ( self ): super ( Net , self ) . __init__ () # 1 input image channel, 6 output channels, 5x5 square convolution # kernel self . conv1 = nn . Conv2d ( 1 , 6 , 5 ) self . conv2 = nn . Conv2d ( 6 , 16 , 5 ) # an affine operation: y = Wx + b self . fc1 = nn . Linear ( 16 * 5 * 5 , 120 ) self . fc2 = nn . Linear ( 120 , 84 ) self . fc3 = nn . Linear ( 84 , 10 ) def forward ( self , x ): # Max pooling over a (2, 2) window x = F . max_pool2d ( F . relu ( self . conv1 ( x )), ( 2 , 2 )) # If the size is a square you can only specify a single number x = F . max_pool2d ( F . relu ( self . conv2 ( x )), 2 ) x = x . view ( - 1 , self . num_flat_features ( x )) x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x def num_flat_features ( self , x ): size = x . size ()[ 1 :] # all dimensions except the batch dimension num_features = 1 for s in size : num_features *= s return num_features net = Net () print ( net ) 1 2 3 4 5 6 7 Net( (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)) (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)) (fc1): Linear(in_features=400, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=10, bias=True) ) \u6211\u4eec\u53ea\u9700\u5b9a\u4e49 forward \u51fd\u6570\uff0c backward \u51fd\u6570\uff08\u68af\u5ea6\u5728\u8fd9\u91cc\u88ab\u8ba1\u7b97\uff09\u7531 autograd \u81ea\u52a8\u751f\u6210\u3002 \u5728 forward \u51fd\u6570\u4e2d\u53ef\u4ee5\u4f7f\u7528\u4efb\u4f55\u4e00\u79cd\u5f20\u91cf\u8fd0\u7b97\u3002 \u6a21\u578b\u7684\u53ef\u5b66\u4e60\u53c2\u6570\u7531 net.parameters() \u8fd4\u56de 1 2 3 params = list ( net . parameters ()) print ( len ( params )) print ( params [ 0 ] . size ()) # conv1's .weight 1 2 10 torch.Size([6, 1, 5, 5]) \u8ba9\u6211\u4eec\u5c1d\u8bd5\u4e00\u4e2a\u968f\u673a\u768432x32\u8f93\u5165\u3002\u6ce8\u610f\uff1a\u6b64\u7f51\u7edc\uff08LeNet\uff09\u7684\u9884\u671f\u8f93\u5165\u5927\u5c0f\u4e3a32x32\u3002\u8981\u5728MNIST\u6570\u636e\u96c6\u4e0a\u4f7f\u7528\u6b64\u7f51\u7edc\uff0c\u8bf7\u5c06\u6570\u636e\u96c6\u4e2d\u7684\u56fe\u50cf\u8c03\u6574\u4e3a32x32\u3002 1 2 3 input = torch . randn ( 1 , 1 , 32 , 32 ) out = net ( input ) print ( out ) 1 2 tensor([[ 0.0108, -0.0120, -0.0415, -0.0964, 0.0957, 0.0217, -0.1095, 0.0581, 0.0173, -0.0540]], grad_fn=<AddmmBackward>) \u5c06\u6240\u6709\u53c2\u6570\u548c\u5e26\u6709\u968f\u673a\u68af\u5ea6\u7684\u53cd\u5411\u4f20\u64ad\u7684\u68af\u5ea6\u7f13\u51b2\u533a\u5f52\u96f6\uff1a 1 2 net . zero_grad () out . backward ( torch . randn ( 1 , 10 )) Note\uff1a torch.nn \u4ec5\u652f\u6301\u5c0f\u6279\u6b21\u3002\u6574\u4e2a torch.nn \u5305\u4ec5\u652f\u6301\u5c0f\u6279\u6b21\u7684\u6837\u672c\uff0c\u800c\u4e0d\u662f\u5355\u4e2a\u6837\u672c\u3002 \u4f8b\u5982\uff0c nn.Conv2d \u91c7\u7528 nSamples x nChannels x Height x Width 4\u7ef4\u5f20\u91cf\u3002 \u5982\u679c\u662f\u5355\u4e2a\u6837\u672c\uff0c\u8981\u7528 input.unsqueeze(0) \u628a\u5b83\u52a0\u5230\u4e00\u4e2a\u5047\u7684\u6279\u6b21\u7ef4\u5ea6\u3002 \u5728\u7ee7\u7eed\u4e4b\u524d\uff0c\u8ba9\u6211\u4eec\u56de\u987e\u4e00\u4e0b\u5230\u76ee\u524d\u4e3a\u6b62\u770b\u5230\u7684\u6240\u6709\u8bfe\u7a0b\u3002 \u6982\u62ec\uff1a - torch.Tensor - \u5b83\u662f\u652f\u6301\u50cf backward() \u8fd9\u79cdautograd\u8fd0\u7b97\u7684 \u591a\u7ef4\u6570\u7ec4 \uff0c\u8fd8\u80fd \u4fdd\u5b58 \u5f20\u91cf\u7684 \u68af\u5ea6 \u3002 - nn.Module - \u795e\u7ecf\u7f51\u7edc\u6a21\u5757\u3002\u63d0\u4f9b \u65b9\u4fbf\u7684\u53c2\u6570\u5c01\u88c5\u65b9\u5f0f \uff0c\u79fb\u81f3GPU\u3001\u5bfc\u51fa\u3001\u52a0\u8f7d\u7b49\u8f85\u52a9\u529f\u80fd\u3002 - nn.Parameter - \u4e00\u79cd\u5f20\u91cf\uff0c \u5f53\u8d4b\u503c\u7ed9 Module \u5bf9\u8c61\u7684\u5c5e\u6027\u65f6\uff0c\u5b83\u4f5c\u4e3a\u53c2\u6570\u88ab\u81ea\u52a8\u6ce8\u518c \u3002 - autograd.Function - \u5b9e\u73b0 autograde\u8fd0\u7b97\u7684 forward() \u548c backward() \u5b9a\u4e49 \u3002 \u6bcf\u6b21 Tensor \u8fd0\u7b97\u81f3\u5c11\u521b\u5efa\u4e00\u4e2a Function \u8282\u70b9\uff0c\u8be5\u8282\u70b9\u8fde\u63a5\u5230\u521b\u5efa Tensor \u7684 Function \u5bf9\u8c61\uff0c\u5e76 \u7f16\u7801\u5176\u5386\u53f2 \u3002 \u5728\u8fd9\u4e00\u70b9\u4e0a\uff0c\u6211\u4eec\u6db5\u76d6\u4e86\uff1a - \u5b9a\u4e49\u5ba1\u673a\u6784\u7f51\u7edcDefining a neural network - \u5904\u7406\u8f93\u5165\u8c03\u7528backward \u8fd8\u5269\u4e0b\uff1a - \u8ba1\u7b97\u635f\u5931 - \u66f4\u65b0\u7f51\u7edc\u6743\u91cd","title":"\u5b9a\u4e49\u7f51\u7edc"},{"location":"beginner/blitz/neural_networks_tutorial/#\u635f\u5931\u51fd\u6570","text":"\u635f\u5931\u51fd\u6570\u91c7\u7528 output \uff0c target \u8f93\u5165\u5bf9\uff0c\u8ba1\u7b97\u8f93\u51fa\u4e0e\u76ee\u6807\u7684\u8ddd\u79bb\u4f30\u7b97\u503c\u3002 nn \u5305\u4e0b\u6709\u591a\u79cd\u4e0d\u540c\u7684 \u635f\u5931\u51fd\u6570 \u3002 \u6bd4\u5982 nn.MSELoss \u5c31\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u635f\u5931\u51fd\u6570\uff0c\u5b83\u8ba1\u7b97\u8f93\u5165\u548c\u76ee\u6807\u4e4b\u95f4\u7684\u5747\u65b9\u8bef\u5dee\u3002 \u4f8b\u5982\uff1a 1 2 3 4 5 6 7 output = net ( input ) target = torch . randn ( 10 ) # a dummy target, for example target = target . view ( 1 , - 1 ) # make it the same shape as output criterion = nn . MSELoss () loss = criterion ( output , target ) print ( loss ) 1 tensor(1.4750, grad_fn=<MseLossBackward>) \u73b0\u5728\uff0c\u5982\u679c\u6309 loss \u7684\u53cd\u65b9\u5411\uff0c\u4f7f\u7528 .grad_fn \u5c5e\u6027\uff0c\u5c31\u53ef\u770b\u5230\u8fd9\u6837\u7684\u8ba1\u7b97\u56fe\uff1a 1 2 3 4 input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d -> view -> linear -> relu -> linear -> relu -> linear -> MSELoss -> loss \u6240\u4ee5\uff0c\u5f53\u6211\u4eec\u8c03\u7528 loss.backward() \uff0c\u5c31\u4f1a\u6c42\u6574\u4e2a\u56fe\u5173\u4e8e\u635f\u5931\u7684\u5fae\u5206\uff0c\u56fe\u4e2d\u6240\u6709\u5177\u6709 requires_grad=True \u7684 Tensor \u5bf9\u8c61\u7684 .grad \u5f20\u91cf\u5c5e\u6027\u90fd\u4f7f\u7528\u68af\u5ea6\u7d2f\u52a0\u3002 \u4e3a\u4e86\u8bf4\u660e\u8fd9\u4e00\u70b9\uff0c\u6211\u4eec\u8fdb\u884c\u51e0\u6b65\u53cd\u5411\uff1a 1 2 3 print ( loss . grad_fn ) # MSELoss print ( loss . grad_fn . next_functions [ 0 ][ 0 ]) # Linear print ( loss . grad_fn . next_functions [ 0 ][ 0 ] . next_functions [ 0 ][ 0 ]) # ReLU 1 2 3 <MseLossBackward object at 0x7fa1140c1fd0> <AddmmBackward object at 0x7fa1140c6080> <AccumulateGrad object at 0x7fa1140c1fd0>","title":"\u635f\u5931\u51fd\u6570"},{"location":"beginner/blitz/neural_networks_tutorial/#\u53cd\u5411\u4f20\u64ad","text":"\u8981\u53cd\u5411\u4f20\u64ad\u8bef\u5dee\uff0c\u6211\u4eec\u9501\u8981\u505a\u7684\u5c31\u662f loss.backward() \u3002 \u4e0d\u8fc7\u6211\u4eec\u9700\u8981\u6e05\u9664\u5df2\u6709\u7684\u68af\u5ea6\uff0c\u5426\u5219\u68af\u5ea6\u5c06\u88ab\u7d2f\u79ef\u5230\u5df2\u6709\u7684\u68af\u5ea6\u4e0a\u3002 \u73b0\u5728\u53ef\u4ee5\u8c03\u7528 loss.backward() \uff0c\u770b\u770b conv1 \u5728\u8c03\u7528\u4e4b\u524d\u548c\u4e4b\u540e\u548c\u504f\u5dee\u68af\u5ea6\u3002 1 2 3 4 5 6 7 8 9 net . zero_grad () # zeroes the gradient buffers of all parameters print ( 'conv1.bias.grad before backward' ) print ( net . conv1 . bias . grad ) loss . backward () print ( 'conv1.bias.grad after backward' ) print ( net . conv1 . bias . grad ) 1 2 3 4 conv1.bias.grad before backward tensor([0., 0., 0., 0., 0., 0.]) conv1.bias.grad after backward tensor([-0.0245, -0.0181, 0.0134, 0.0136, 0.0242, 0.0009]) \u5230\u76ee\u524d\u4f4d\u7f6e, \u6211\u4eec\u5df2\u7ecf\u770b\u5230\u4e86\u5982\u4f55\u4f7f\u7528\u635f\u5931\u51fd\u6570\u3002 \u5ef6\u540e\u9605\u8bfb\uff1a \u795e\u7ecf\u7f51\u7edc\u5305\u5177\u6709\u7531\u4e8e\u6784\u5efa\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u5404\u79cd\u6a21\u5757\u548c\u635f\u5931\u51fd\u6570\u3002\u5e26\u6709\u6587\u6863\u7684\u5b8c\u6574\u5217\u8868\u5728 https://pytorch.org/docs/nn \u8fd8\u5269\u4e0b\u4e00\u4e2a\u8981\u5b66\u4e60\u7684\u662f\uff1a \u66f4\u65b0\u7f51\u7edc\u7684\u6743\u91cd","title":"\u53cd\u5411\u4f20\u64ad"},{"location":"beginner/blitz/neural_networks_tutorial/#\u66f4\u65b0\u6743\u91cd","text":"\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u662f(SGD)\u662f\u6700\u7b80\u5355\u7684\u66f4\u65b0\u89c4\u5219\uff1a 1 weight = weight - learning_rate * gradient \u53ef\u4ee5\u7528\u7b80\u5355\u7684Python\u4ee3\u7801\u5b9e\u73b0\u5b83\uff1a 1 2 3 learning_rate = 0.01 for f in net . parameters (): f . data . sub_ ( f . grad . data * learning_rate ) \u4e0d\u8fc7\uff0c\u5f53\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u65f6\uff0c\u8fd8\u9700\u8981\u4f7f\u7528\u5404\u79cd\u4e0d\u540c\u7684\u66f4\u65b0\u89c4\u5219\uff0c\u4f8b\u5982SGD\uff0cNesterov-SGD\uff0cAdam\uff0cRMSProp\u7b49\u3002 \u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u5c0f\u5305\uff1a torch.optim \uff0c\u5b83\u5b9e\u73b0\u4e86\u6240\u6709\u8fd9\u4e9b\u65b9\u6cd5\u3002\u4f7f\u7528\u5b83\u975e\u5e38\u7b80\u5355\uff1a 1 2 3 4 5 6 7 8 9 10 11 import torch.optim as optim # create your optimizer optimizer = optim . SGD ( net . parameters (), lr = 0.01 ) # in your training loop: optimizer . zero_grad () # zero the gradient buffers output = net ( input ) loss = criterion ( output , target ) loss . backward () optimizer . step () # Does the update Note\uff1a \u89c2\u5bdf\u5982\u4f55\u4f7f\u7528 optimizer.zero_grad() \u624b\u52a8\u5c06\u68af\u5ea6\u7f13\u51b2\u533a\u8bbe\u7f6e\u4e3a\u96f6\u3002\u8fd9\u662f\u56e0\u4e3a\u68af\u5ea6\u7d2f\u79ef\u7684\uff0c\u53c2\u89c1 \u53cd\u5411\u4f20\u64ad \u90e8\u5206\u4e2d\u7684\u8bf4\u660e\u3002","title":"\u66f4\u65b0\u6743\u91cd"},{"location":"beginner/blitz/tensor_tutorial/","text":"\u4ec0\u4e48\u662fPyTorch? \u00b6 \u5b83\u662f\u4e00\u4e2a\u57fa\u4e8ePython\u7684\u79d1\u5b66\u8ba1\u7b97\u8f6f\u4ef6\u5305\uff0c\u9488\u5bf9\u4e24\u7ec4\u53d7\u4f17\uff1a NumPy \u7684\u66ff\u4ee3\u54c1\uff0c\u53ef\u4ee5\u4f7f\u7528 GPU \u7684\u5f3a\u5927\u529f\u80fd \u6df1\u5ea6\u5b66\u4e60\u7814\u7a76\u5e73\u53f0\uff0c\u63d0\u4f9b\u6700\u5927\u7684\u7075\u6d3b\u6027\u548c\u901f\u5ea6 \u5f00\u59cb \u00b6 \u5f20\u91cf(Tensor) \u00b6 Tensor(\u5f20\u91cf)\u7c7b\u4f3c\u4e8eNumPy\u7684ndarrays(\u591a\u7ef4\u6570\u7ec4)\uff0c\u4e0d\u8fc7Tensor\u53ef\u4ee5\u4f7f\u7528GPU\u52a0\u901f\u8ba1\u7b97\u3002 1 2 from __future__ import print_function import torch \u6784\u9020\u4e00\u4e2a\u672a\u521d\u59cb\u5316\u76845x3\u77e9\u9635\uff1a 1 2 x = torch . empty ( 5 , 3 ) print ( x ) 1 2 3 4 5 tensor([[-2.6621e+09, 4.5587e-41, -2.6621e+09], [ 4.5587e-41, 3.3129e-18, 2.6302e+20], [ 6.1949e-04, 2.5640e-09, 1.6408e-07], [ 3.1472e+12, 2.7095e-09, 1.0374e-08], [ 6.4830e-10, 1.2398e+16, 1.8503e+20]]) \u6784\u9020\u4e00\u4e2a\u968f\u673a\u521d\u59cb\u5316\u7684\u77e9\u9635\uff1a 1 2 x = torch . rand ( 5 , 3 ) print ( x ) 1 2 3 4 5 tensor([[0.8017, 0.1650, 0.3771], [0.8411, 0.4406, 0.3873], [0.3508, 0.8021, 0.5265], [0.0255, 0.8830, 0.0428], [0.1457, 0.5150, 0.1314]]) \u6784\u9020\u4e00\u4e2a\u96f6\u586b\u5145\uff0c\u6570\u636e\u7c7b\u578b( dtype )\u4e3a\u957f\u6574\u5f62( long )\u7684\u77e9\u9635: 1 2 x = torch . zeros ( 5 , 3 , dtype = torch . long ) print ( x ) 1 2 3 4 5 tensor([[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]]) \u76f4\u63a5\u4ece\u6570\u636e\u6784\u9020 tensor: 1 2 x = torch . tensor ([ 5.5 , 3 ]) print ( x ) 1 tensor([5.5000, 3.0000]) \u6216\u8005\u5728\u5df2\u6709 tensor \u7684\u57fa\u7840\u4e0a\u521b\u5efa tensor\u3002 \u8fd9\u79cd\u65b9\u6cd5\u5c06\u91cd\u7528\u8f93\u5165 tensor \u7684\u5c5e\u6027\uff0c\u5982 dtype , \u9664\u975e\u63d0\u4f9b\u65b0\u7684\u503c 1 2 3 4 5 x = x . new_ones ( 5 , 3 , dtype = torch . double ) # new_* methods take in sizes print ( x ) x = torch . randn_like ( x , dtype = torch . float ) # override dtype! print ( x ) # result has the same size 1 2 3 4 5 6 7 8 9 10 tensor([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.]], dtype=torch.float64) tensor([[ 0.4672, -2.5856, -0.5848], [-0.0800, -0.0157, -0.1485], [-1.4250, 1.9369, -1.9591], [ 0.2027, -0.4884, -0.9408], [ 1.0423, 0.9927, -0.4951]]) \u83b7\u53d6\u5176\u5927\u5c0f: 1 print ( x . size ()) 1 torch.Size([5, 3]) \u6ce8\u610f : torch.Size \u662f tuple(\u5143\u7ec4)\uff0c\u5b83\u652f\u6301\u6240\u6709 tuple \u64cd\u4f5c\u3002 \u8fd0\u7b97 \u00b6 \u8fd0\u7b97\u6709\u591a\u79cd\u8bed\u6cd5\u3002\u5728\u4e0b\u9762\u7684\u793a\u4f8b\u4e2d\uff0c\u6211\u4eec\u5c06\u770b\u4e00\u770b\u52a0\u6cd5\u8fd0\u7b97 \u52a0\u6cd5: \u8bed\u6cd5 1 1 2 y = torch . rand ( 5 , 3 ) print ( x + y ) 1 2 3 4 5 tensor([[ 1.4545, -1.7998, -0.3246], [-0.0237, 0.9005, 0.3812], [-1.0698, 2.0101, -1.0791], [ 0.5078, 0.4077, -0.0533], [ 1.8212, 1.6021, -0.3893]]) \u52a0\u6cd5: \u8bed\u6cd5 2 1 print ( torch . add ( x , y )) 1 2 3 4 5 tensor([[ 1.4545, -1.7998, -0.3246], [-0.0237, 0.9005, 0.3812], [-1.0698, 2.0101, -1.0791], [ 0.5078, 0.4077, -0.0533], [ 1.8212, 1.6021, -0.3893]]) \u52a0\u6cd5: \u5c06\u8f93\u51fa tensor \u4f5c\u4e3a\u53c2\u6570 1 2 3 result = torch . empty ( 5 , 3 ) torch . add ( x , y , out = result ) print ( result ) 1 2 3 4 5 tensor([[ 1.4545, -1.7998, -0.3246], [-0.0237, 0.9005, 0.3812], [-1.0698, 2.0101, -1.0791], [ 0.5078, 0.4077, -0.0533], [ 1.8212, 1.6021, -0.3893]]) \u52a0\u6cd5: \u539f\u4f4d\u4fee\u6539 1 2 3 # adds x to y y . add_ ( x ) print ( y ) 1 2 3 4 5 tensor([[ 1.4545, -1.7998, -0.3246], [-0.0237, 0.9005, 0.3812], [-1.0698, 2.0101, -1.0791], [ 0.5078, 0.4077, -0.0533], [ 1.8212, 1.6021, -0.3893]]) \u6ce8\u610f : \u4efb\u4f55\u539f\u4f4d\u4fee\u6539 tensor \u7684\u8fd0\u7b97\u65b9\u6cd5\u90fd\u662f\u4ee5 _ \u7ed3\u5c3e\u7684\u3002 \u4f8b\u5982\uff1a x.copy_(y) \uff0c x.t_() \uff0c\u4f1a\u6539\u53d8 x \u53ef\u4ee5\u4f7f\u7528\u7c7b\u4f3c NumPy \u7684\u7d22\u5f15\u8fd0\u7b97\uff01 1 print ( x [:, 1 ]) 1 tensor([-2.5856, -0.0157, 1.9369, -0.4884, 0.9927]) \u8c03\u6574\u5927\u5c0f\uff1a\u5982\u679c\u8981\u5bf9 tensor \u8c03\u6574\u5927\u5c0f/\u6539\u53d8\u5f62\u72b6\uff0c\u53ef\u4ee5\u4f7f\u7528 torch.view : 1 2 3 4 x = torch . randn ( 4 , 4 ) y = x . view ( 16 ) z = x . view ( - 1 , 8 ) # the size -1 is inferred from other dimensions print ( x . size (), y . size (), z . size ()) 1 torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8]) \u5bf9\u4e8e\u53ea\u6709\u4e00\u4e2a\u5143\u7d20\u7684 tensor\uff0c\u53ef\u4ee5\u7528 .item() \u5f97\u5230\u8fd9\u4e2a\u503c\u5bf9\u5e94\u7684 Python \u6570\u503c 1 2 3 x = torch . randn ( 1 ) print ( x ) print ( x . item ()) 1 2 tensor([1.0215]) 1.0214648246765137 \u5ef6\u540e\u9605\u8bfb \uff1a 100+ Tensor \u8fd0\u7b97\uff0c\u5305\u62ec\uff1a\u79fb\u8c03\u3001\u7d22\u5f15\u3001\u5207\u7247\u3001\u6570\u5b66\u8fd0\u7b97\u3001\u7ebf\u6027\u4ee3\u6570\u3001\u968f\u673a\u6570\u7b49\uff0c\u53ef\u53c2\u8003 https://pytorch.org/docs/torch NumPy \u4e92\u8f6c \u00b6 \u5c06 Torch Tensor \u8f6c\u6362\u4e3a NumPy \u6570\u7ec4\uff08\u53cd\u4e4b\u4ea6\u7136\uff09\u662f\u4e00\u4ef6\u8f7b\u800c\u6613\u4e3e\u7684\u4e8b\u3002 Torch Tensor \u548c NumPy \u6570\u7ec4\u5171\u4eab\u5176\u5e95\u5c42\u5185\u5b58\u4f4d\u7f6e\uff0c\u66f4\u6539\u4e00\u4e2a\u4f1a\u5bfc\u81f4\u53e6\u4e00\u4e2a\u7684\u6539\u53d8\u3002 \u5c06 Torch Tensor \u8f6c\u6362\u4e3a NumPy \u6570\u7ec4 \u00b6 1 2 a = torch . ones ( 5 ) print ( a ) 1 tensor([1., 1., 1., 1., 1.]) 1 2 b = a . numpy () print ( b ) 1 [1. 1. 1. 1. 1.] \u4e86\u89e3numpy\u6570\u7ec4\u7684\u503c\u5982\u4f55\u53d8\u5316\u3002 1 2 3 a . add_ ( 1 ) print ( a ) print ( b ) 1 2 tensor([2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2.] \u5c06 NumPy \u6570\u7ec4\u8f6c\u6362\u4e3a Torch Tensor \u00b6 \u4e86\u89e3\u66f4\u6539 NumPy \u6570\u7ec4\u5982\u4f55\u81ea\u52a8\u66f4\u6539 Torch Tensor 1 2 3 4 5 6 import numpy as np a = np . ones ( 5 ) b = torch . from_numpy ( a ) np . add ( a , 1 , out = a ) print ( a ) print ( b ) 1 2 [2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64) \u9664\u4e86 CharTensor \uff0c\u6240\u6709 CPU \u4e0a\u7684 Tensor \u90fd\u53ef\u4ee5\u548c Numpy \u4e92\u76f8\u8f6c\u6362\u3002 CUDA Tensor \u00b6 \u53ef\u4ee5\u4f7f\u7528 .to \u65b9\u6cd5\u79fb\u52a8Tensor\u5bf9\u8c61\u5230\u4efb\u4f55\u8bbe\u5907\u3002 1 2 3 4 5 6 7 8 9 # \u53ea\u6709 CUDA \u53ef\u7528\u65f6\uff0c\u624d\u8fd0\u884c\u8fd9\u4e2a cell # \u4f7f\u7528 ``torch.device`` \u5bf9\u8c61\u5c06 tensor \u79fb\u5165\u4ee5\u53ca\u79fb\u51fa GPU if torch . cuda . is_available (): device = torch . device ( \"cuda\" ) # CUDA \u8bbe\u5907\u5bf9\u8c61 y = torch . ones_like ( x , device = device ) # \u76f4\u63a5\u5728 GPU \u4e0a\u521b\u5efa tensor x = x . to ( device ) # \u6216\u8005\u7528 ``.to(\"cuda\")`` z = x + y print ( z ) print ( z . to ( \"cpu\" , torch . double )) # ``.to`` \u4e5f\u53ef\u4ee5\u540c\u65f6\u66f4\u6539 ``dtype``! 1 2 tensor([2.0215], device='cuda:0') tensor([2.0215], dtype=torch.float64)","title":"\u4ec0\u4e48\u662fPyTorch\uff1f"},{"location":"beginner/blitz/tensor_tutorial/#\u4ec0\u4e48\u662fpytorch","text":"\u5b83\u662f\u4e00\u4e2a\u57fa\u4e8ePython\u7684\u79d1\u5b66\u8ba1\u7b97\u8f6f\u4ef6\u5305\uff0c\u9488\u5bf9\u4e24\u7ec4\u53d7\u4f17\uff1a NumPy \u7684\u66ff\u4ee3\u54c1\uff0c\u53ef\u4ee5\u4f7f\u7528 GPU \u7684\u5f3a\u5927\u529f\u80fd \u6df1\u5ea6\u5b66\u4e60\u7814\u7a76\u5e73\u53f0\uff0c\u63d0\u4f9b\u6700\u5927\u7684\u7075\u6d3b\u6027\u548c\u901f\u5ea6","title":"\u4ec0\u4e48\u662fPyTorch?"},{"location":"beginner/blitz/tensor_tutorial/#\u5f00\u59cb","text":"","title":"\u5f00\u59cb"},{"location":"beginner/blitz/tensor_tutorial/#\u5f20\u91cftensor","text":"Tensor(\u5f20\u91cf)\u7c7b\u4f3c\u4e8eNumPy\u7684ndarrays(\u591a\u7ef4\u6570\u7ec4)\uff0c\u4e0d\u8fc7Tensor\u53ef\u4ee5\u4f7f\u7528GPU\u52a0\u901f\u8ba1\u7b97\u3002 1 2 from __future__ import print_function import torch \u6784\u9020\u4e00\u4e2a\u672a\u521d\u59cb\u5316\u76845x3\u77e9\u9635\uff1a 1 2 x = torch . empty ( 5 , 3 ) print ( x ) 1 2 3 4 5 tensor([[-2.6621e+09, 4.5587e-41, -2.6621e+09], [ 4.5587e-41, 3.3129e-18, 2.6302e+20], [ 6.1949e-04, 2.5640e-09, 1.6408e-07], [ 3.1472e+12, 2.7095e-09, 1.0374e-08], [ 6.4830e-10, 1.2398e+16, 1.8503e+20]]) \u6784\u9020\u4e00\u4e2a\u968f\u673a\u521d\u59cb\u5316\u7684\u77e9\u9635\uff1a 1 2 x = torch . rand ( 5 , 3 ) print ( x ) 1 2 3 4 5 tensor([[0.8017, 0.1650, 0.3771], [0.8411, 0.4406, 0.3873], [0.3508, 0.8021, 0.5265], [0.0255, 0.8830, 0.0428], [0.1457, 0.5150, 0.1314]]) \u6784\u9020\u4e00\u4e2a\u96f6\u586b\u5145\uff0c\u6570\u636e\u7c7b\u578b( dtype )\u4e3a\u957f\u6574\u5f62( long )\u7684\u77e9\u9635: 1 2 x = torch . zeros ( 5 , 3 , dtype = torch . long ) print ( x ) 1 2 3 4 5 tensor([[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]]) \u76f4\u63a5\u4ece\u6570\u636e\u6784\u9020 tensor: 1 2 x = torch . tensor ([ 5.5 , 3 ]) print ( x ) 1 tensor([5.5000, 3.0000]) \u6216\u8005\u5728\u5df2\u6709 tensor \u7684\u57fa\u7840\u4e0a\u521b\u5efa tensor\u3002 \u8fd9\u79cd\u65b9\u6cd5\u5c06\u91cd\u7528\u8f93\u5165 tensor \u7684\u5c5e\u6027\uff0c\u5982 dtype , \u9664\u975e\u63d0\u4f9b\u65b0\u7684\u503c 1 2 3 4 5 x = x . new_ones ( 5 , 3 , dtype = torch . double ) # new_* methods take in sizes print ( x ) x = torch . randn_like ( x , dtype = torch . float ) # override dtype! print ( x ) # result has the same size 1 2 3 4 5 6 7 8 9 10 tensor([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.]], dtype=torch.float64) tensor([[ 0.4672, -2.5856, -0.5848], [-0.0800, -0.0157, -0.1485], [-1.4250, 1.9369, -1.9591], [ 0.2027, -0.4884, -0.9408], [ 1.0423, 0.9927, -0.4951]]) \u83b7\u53d6\u5176\u5927\u5c0f: 1 print ( x . size ()) 1 torch.Size([5, 3]) \u6ce8\u610f : torch.Size \u662f tuple(\u5143\u7ec4)\uff0c\u5b83\u652f\u6301\u6240\u6709 tuple \u64cd\u4f5c\u3002","title":"\u5f20\u91cf(Tensor)"},{"location":"beginner/blitz/tensor_tutorial/#\u8fd0\u7b97","text":"\u8fd0\u7b97\u6709\u591a\u79cd\u8bed\u6cd5\u3002\u5728\u4e0b\u9762\u7684\u793a\u4f8b\u4e2d\uff0c\u6211\u4eec\u5c06\u770b\u4e00\u770b\u52a0\u6cd5\u8fd0\u7b97 \u52a0\u6cd5: \u8bed\u6cd5 1 1 2 y = torch . rand ( 5 , 3 ) print ( x + y ) 1 2 3 4 5 tensor([[ 1.4545, -1.7998, -0.3246], [-0.0237, 0.9005, 0.3812], [-1.0698, 2.0101, -1.0791], [ 0.5078, 0.4077, -0.0533], [ 1.8212, 1.6021, -0.3893]]) \u52a0\u6cd5: \u8bed\u6cd5 2 1 print ( torch . add ( x , y )) 1 2 3 4 5 tensor([[ 1.4545, -1.7998, -0.3246], [-0.0237, 0.9005, 0.3812], [-1.0698, 2.0101, -1.0791], [ 0.5078, 0.4077, -0.0533], [ 1.8212, 1.6021, -0.3893]]) \u52a0\u6cd5: \u5c06\u8f93\u51fa tensor \u4f5c\u4e3a\u53c2\u6570 1 2 3 result = torch . empty ( 5 , 3 ) torch . add ( x , y , out = result ) print ( result ) 1 2 3 4 5 tensor([[ 1.4545, -1.7998, -0.3246], [-0.0237, 0.9005, 0.3812], [-1.0698, 2.0101, -1.0791], [ 0.5078, 0.4077, -0.0533], [ 1.8212, 1.6021, -0.3893]]) \u52a0\u6cd5: \u539f\u4f4d\u4fee\u6539 1 2 3 # adds x to y y . add_ ( x ) print ( y ) 1 2 3 4 5 tensor([[ 1.4545, -1.7998, -0.3246], [-0.0237, 0.9005, 0.3812], [-1.0698, 2.0101, -1.0791], [ 0.5078, 0.4077, -0.0533], [ 1.8212, 1.6021, -0.3893]]) \u6ce8\u610f : \u4efb\u4f55\u539f\u4f4d\u4fee\u6539 tensor \u7684\u8fd0\u7b97\u65b9\u6cd5\u90fd\u662f\u4ee5 _ \u7ed3\u5c3e\u7684\u3002 \u4f8b\u5982\uff1a x.copy_(y) \uff0c x.t_() \uff0c\u4f1a\u6539\u53d8 x \u53ef\u4ee5\u4f7f\u7528\u7c7b\u4f3c NumPy \u7684\u7d22\u5f15\u8fd0\u7b97\uff01 1 print ( x [:, 1 ]) 1 tensor([-2.5856, -0.0157, 1.9369, -0.4884, 0.9927]) \u8c03\u6574\u5927\u5c0f\uff1a\u5982\u679c\u8981\u5bf9 tensor \u8c03\u6574\u5927\u5c0f/\u6539\u53d8\u5f62\u72b6\uff0c\u53ef\u4ee5\u4f7f\u7528 torch.view : 1 2 3 4 x = torch . randn ( 4 , 4 ) y = x . view ( 16 ) z = x . view ( - 1 , 8 ) # the size -1 is inferred from other dimensions print ( x . size (), y . size (), z . size ()) 1 torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8]) \u5bf9\u4e8e\u53ea\u6709\u4e00\u4e2a\u5143\u7d20\u7684 tensor\uff0c\u53ef\u4ee5\u7528 .item() \u5f97\u5230\u8fd9\u4e2a\u503c\u5bf9\u5e94\u7684 Python \u6570\u503c 1 2 3 x = torch . randn ( 1 ) print ( x ) print ( x . item ()) 1 2 tensor([1.0215]) 1.0214648246765137 \u5ef6\u540e\u9605\u8bfb \uff1a 100+ Tensor \u8fd0\u7b97\uff0c\u5305\u62ec\uff1a\u79fb\u8c03\u3001\u7d22\u5f15\u3001\u5207\u7247\u3001\u6570\u5b66\u8fd0\u7b97\u3001\u7ebf\u6027\u4ee3\u6570\u3001\u968f\u673a\u6570\u7b49\uff0c\u53ef\u53c2\u8003 https://pytorch.org/docs/torch","title":"\u8fd0\u7b97"},{"location":"beginner/blitz/tensor_tutorial/#numpy-\u4e92\u8f6c","text":"\u5c06 Torch Tensor \u8f6c\u6362\u4e3a NumPy \u6570\u7ec4\uff08\u53cd\u4e4b\u4ea6\u7136\uff09\u662f\u4e00\u4ef6\u8f7b\u800c\u6613\u4e3e\u7684\u4e8b\u3002 Torch Tensor \u548c NumPy \u6570\u7ec4\u5171\u4eab\u5176\u5e95\u5c42\u5185\u5b58\u4f4d\u7f6e\uff0c\u66f4\u6539\u4e00\u4e2a\u4f1a\u5bfc\u81f4\u53e6\u4e00\u4e2a\u7684\u6539\u53d8\u3002","title":"NumPy \u4e92\u8f6c"},{"location":"beginner/blitz/tensor_tutorial/#\u5c06-torch-tensor-\u8f6c\u6362\u4e3a-numpy-\u6570\u7ec4","text":"1 2 a = torch . ones ( 5 ) print ( a ) 1 tensor([1., 1., 1., 1., 1.]) 1 2 b = a . numpy () print ( b ) 1 [1. 1. 1. 1. 1.] \u4e86\u89e3numpy\u6570\u7ec4\u7684\u503c\u5982\u4f55\u53d8\u5316\u3002 1 2 3 a . add_ ( 1 ) print ( a ) print ( b ) 1 2 tensor([2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2.]","title":"\u5c06 Torch Tensor \u8f6c\u6362\u4e3a NumPy \u6570\u7ec4"},{"location":"beginner/blitz/tensor_tutorial/#\u5c06-numpy-\u6570\u7ec4\u8f6c\u6362\u4e3a-torch-tensor","text":"\u4e86\u89e3\u66f4\u6539 NumPy \u6570\u7ec4\u5982\u4f55\u81ea\u52a8\u66f4\u6539 Torch Tensor 1 2 3 4 5 6 import numpy as np a = np . ones ( 5 ) b = torch . from_numpy ( a ) np . add ( a , 1 , out = a ) print ( a ) print ( b ) 1 2 [2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64) \u9664\u4e86 CharTensor \uff0c\u6240\u6709 CPU \u4e0a\u7684 Tensor \u90fd\u53ef\u4ee5\u548c Numpy \u4e92\u76f8\u8f6c\u6362\u3002","title":"\u5c06 NumPy \u6570\u7ec4\u8f6c\u6362\u4e3a Torch Tensor"},{"location":"beginner/blitz/tensor_tutorial/#cuda-tensor","text":"\u53ef\u4ee5\u4f7f\u7528 .to \u65b9\u6cd5\u79fb\u52a8Tensor\u5bf9\u8c61\u5230\u4efb\u4f55\u8bbe\u5907\u3002 1 2 3 4 5 6 7 8 9 # \u53ea\u6709 CUDA \u53ef\u7528\u65f6\uff0c\u624d\u8fd0\u884c\u8fd9\u4e2a cell # \u4f7f\u7528 ``torch.device`` \u5bf9\u8c61\u5c06 tensor \u79fb\u5165\u4ee5\u53ca\u79fb\u51fa GPU if torch . cuda . is_available (): device = torch . device ( \"cuda\" ) # CUDA \u8bbe\u5907\u5bf9\u8c61 y = torch . ones_like ( x , device = device ) # \u76f4\u63a5\u5728 GPU \u4e0a\u521b\u5efa tensor x = x . to ( device ) # \u6216\u8005\u7528 ``.to(\"cuda\")`` z = x + y print ( z ) print ( z . to ( \"cpu\" , torch . double )) # ``.to`` \u4e5f\u53ef\u4ee5\u540c\u65f6\u66f4\u6539 ``dtype``! 1 2 tensor([2.0215], device='cuda:0') tensor([2.0215], dtype=torch.float64)","title":"CUDA Tensor"}]}